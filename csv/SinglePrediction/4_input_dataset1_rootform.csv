Bug text root form,issue key,Story point
ewererwf dsf dfd seff wefgf aeada,12,
occat miss index elasticsearch occasion applic still n't index elasticsearch . need investig might occur .,USERGRID-1027,8.0
re-index use amazonasynceventservic support updat date see implement exampl inmemoryasynceventservic org/apache/usergrid/corepersistence/asyncevents/eventbuilderimpl.java:166 thi filter base modified/upd date event index . need ensur amazonasynceventservic consum implement someth similar or look produc filter event even drop aw queue .,USERGRID-900,3.0
fix prune usergrid tool 2.1 the usergrid tool broken varieti way two-dot-o two-dot-o-dev brand . the tool compil . mani reli outdat 1.0 concept . mani obsolet . 1 . delet obsolet tool 2 . make tool compil,USERGRID-872,3.0
"notif post throw npe { code } curl -x post -d ' { `` payload '' : { `` apple-dev '' : `` hello world ! '' } } ' 'http : //api.usergrid.com/brandon.apigee/baas-integration-tests/devices/ * /notif ? client_id=redact & client_secret=redact ' { code } throw : { code } { `` error '' : `` runtim '' , `` timestamp '' : 1436981233645 , `` durat '' : 0 , `` except '' : `` java.lang.runtimeexcept '' , `` error_descript '' : `` java.lang.nullpointerexcept '' } { code } 1 ) accord doc , work : http : //apigee.com/docs/app-services/content/creating-and-managing-notif 2 ) if n't n't suppos , n't throw npe . note notifi sandbox cert creat name 'apple-dev ' . -- - updat it work { code } /devic ; ql=select * /notif { code } address also updat document reflect way work . -- - updat it also appear although npe return via api , notif * * creat /notif . in particular test , devic /devic collect : { code } { `` uuid '' : `` 11c5e62a-2a84-11e5-af28-65b8f0f35f5f '' , `` type '' : `` notif '' , `` creat '' : 1436918242690 , `` modifi '' : 1436918242690 , `` payload '' : { `` apple-dev '' : `` hello world ! '' } , `` debug '' : fals , `` state '' : `` creat '' , `` metadata '' : { `` path '' : `` /notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f '' , `` collect '' : { `` receipt '' : `` /notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f/receipt '' } } } { code }",USERGRID-858,2.0
sn < - > sq subscript not creat cross-region,USERGRID-798,0.0
"org ( presum app ) credenti ca n't creat connect default if tri creat new connect use org app credenti , fail : { code } { `` durat '' : 1 , `` error '' : `` unauthor '' , `` error_descript '' : `` subject permiss [ applic : post : uuid : /col1/uuid/relationship/uid ] '' , `` except '' : `` org.apache.shiro.authz.unauthorizedexcept '' , `` timestamp '' : `` .... '' } { code } the workaround seem give { code } post / * * { code } guest role . expect behavior : app org credenti ( box ) abl creat delet connect , well retriev depth : { code } get /collection/uuid/verb get /collection/uuid/connecting/verb get /collection/uuid/connections/verb { code } etc . in short , test urap integration_test pass net new instanc without set guest role permiss { code } / * * { code }",USERGRID-797,3.0
"'order creat desc ' caus result locat queri ( affect 1.0 , sure 2.0 ) make follow api call return result : { code } http : //api.usergrid.com/org/app/collect ? ql=locat % 20within % 201000 % 20of % 2051.73213 % 2c % 20-1.20631 % 20order % 20bi % 20creat % 20desc { code } one work : { code } http : //api.usergrid.com/org/app/collect ? ql=locat % 20within % 201000 % 20of % 2051.73213 % 2c % 20-1.20631 { code } definit affect 1.0 , test 2.x",USERGRID-787,3.0
"limit queri result highli variabl number result thi behavior observ build two-dot-o branch commit id 2d1c8b8ac7b20b63a11d83adca56839d8b409cca . for exampl , limit=2 give 1 sometim , limit=750 give anywher 625 749 . for exampl script : { code } # ! /bin/bash count ` seq 1 10 ` curl -s `` http : //example.com/appservices/testorg/sandbox/scmock ? limit=750 '' > file $ { count } grep uuid file $ { count } | wc rm file $ { count } done { code } produc result : 685 2055 36305 750 2250 39750 749 2247 39697 742 2226 39326 750 2250 39750 749 2247 39697 747 2241 39591 744 2232 39432 750 2250 39750 749 2247 39697 a differ count everi time .",USERGRID-778,3.0
refactor app info migrat make function,USERGRID-777,3.0
"queri without 'ql ' return null pointer instead meaning messag http : //api.usergrid.com/vta/sandbox/stop ? select * locat within 3 37.415449732 , -121.920367767 - return 500/npe 4xx , prefer better error messag .",USERGRID-776,3.0
except mask clientcredentialssecurityfilt an except mask caus issu deploy . here class line . we specif except catch least log . http : //github.com/apache/incubator-usergrid/blob/49ae4ac5b8d5d77e90e6e6c6e9d8b299a5423863/stack/rest/src/main/java/org/apache/usergrid/rest/security/shiro/filters/clientcredentialssecurityfilter.java # l65,USERGRID-774,3.0
"column queri issu from http : //community.apigee.com/questions/2432/error-analysis-of-response-when-trying-to-curl-que.html # answer-2454 . i seen similar issu . i use org devnexu app 2015 , publicli visibl . in ui , i set path /session queri select name , titl , room titl contain 'java ' , i get respons error . howev , i captur queri made browser ( http : //api.usergrid.com/devnexus/2015/sess ? ql=select % 20name % 2ctitl % 2croom % 20where % 20titl % 20contain % 20 % 27java % 27 % 20order % 20bi % 20creat % 20desc & access_token=ywmtuzeubnpkeesm49lerp5xkqaaauyls2hcr3mkizctkzhiylt1vuiiygn9uu ) use chrome dev tool , queri work curl . you see result someth simpl select titl .",USERGRID-773,3.0
token gener app-level credenti use success header they use access_token qparam header . they need work header well .,USERGRID-770,0.0
acl work expect ? when set follow acl rule /assets/ * user access /assets/xxx also /asset mean list asset . i expect allow /assets/ * would allow /assets/xxx,USERGRID-769,3.0
"bad charact queri caus 500 respons ; expect 400 ? if i make follow call : http : //api.usergrid.com/org/app/collect ? ql=where % 20thirdpropertytypeint % 20 % 3e % 2030000 % 20 & & % 20thirdpropertytypeint % 20 % 3c % 2040000 & limit=10 i get respons : { code } { `` error '' : `` null_point '' , `` timestamp '' : 1435176848310 , `` durat '' : 1 , `` except '' : `` java.lang.nullpointerexcept '' } { code } the caus ampersand queri string n't url encod ( % 26 instead & ) . i would expect queri string bad , return 400 bad request , rather omin nullpointerexception/500 .",USERGRID-768,3.0
portal show progress indic registr when registering/sign form indic chang state 'regist ' button press .,USERGRID-763,3.0
"[ fix ? ] npe cpentitymanager.valid need investig 2.1 releas . 2015-06-16 22:47:20,361 [ http-bio-8080-exec-7 ] error org.apache.usergrid.corepersistence.cpentitymanager- unabl load entiti saparticles-fix : e466f39a-cb97-11e4-b30c-f7cc05dca022 java.lang.nullpointerexcept org.apache.usergrid.corepersistence.cpentitymanager.valid ( cpentitymanager.java:956 ) org.apache.usergrid.corepersistence.cpentitymanager.valid ( cpentitymanager.java:942 ) org.apache.usergrid.corepersistence.cpentitymanager.updateproperti ( cpentitymanager.java:1030 ) org.apache.usergrid.services.abstractservice.updateent ( abstractservice.java:456 ) org.apache.usergrid.services.abstractservice.updateent ( abstractservice.java:444 ) org.apache.usergrid.services.abstractcollectionservice.putitembynam ( abstractcollectionservice.java:305 ) org.apache.usergrid.services.abstractservice.invokeitemwithnam ( abstractservice.java:677 ) org.apache.usergrid.services.abstractservice.invok ( abstractservice.java:628 ) org.apache.usergrid.services.abstractservice.invok ( abstractservice.java:544 ) org.apache.usergrid.services.servicerequest.execut ( servicerequest.java:226 ) org.apache.usergrid.services.servicerequest.execut ( servicerequest.java:193 ) org.apache.usergrid.rest.applications.serviceresource.executeservicerequest ( serviceresource.java:251 ) org.apache.usergrid.rest.applications.serviceresource.executeputwithmap ( serviceresource.java:371 ) org.apache.usergrid.rest.applications.serviceresource.executeput ( serviceresource.java:420 ) sun.reflect.generatedmethodaccessor167.invok ( unknown sourc ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) java.lang.reflect.method.invok ( method.java:497 ) com.sun.jersey.spi.container.javamethodinvokerfactori $ 1.invok ( javamethodinvokerfactory.java:60 ) com.sun.jersey.server.impl.model.method.dispatch.abstractresourcemethoddispatchprovid $ typeoutinvoker._dispatch ( abstractresourcemethoddispatchprovider.java:185 ) com.sun.jersey.server.impl.model.method.dispatch.resourcejavamethoddispatcher.dispatch ( resourcejavamethoddispatcher.java:75 ) com.sun.jersey.server.impl.uri.rules.httpmethodrule.accept ( httpmethodrule.java:302 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.resourceclassrule.accept ( resourceclassrule.java:108 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.rootresourceclassesrule.accept ( rootresourceclassesrule.java:84 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1542 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1473 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1419 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1409 ) com.sun.jersey.spi.container.servlet.webcomponent.servic ( webcomponent.java:409 ) com.sun.jersey.spi.container.servlet.servletcontainer.servic ( servletcontainer.java:540 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:909 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:857 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:811 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.shiro.web.servlet.abstractshirofilter.executechain ( abstractshirofilter.java:449 ) org.apache.shiro.web.servlet.abstractshirofilt $ 1.call ( abstractshirofilter.java:365 ) org.apache.shiro.subject.support.subjectcallable.docal ( subjectcallable.java:90 ) org.apache.shiro.subject.support.subjectcallable.cal ( subjectcallable.java:83 ) org.apache.shiro.subject.support.delegatingsubject.execut ( delegatingsubject.java:383 ) org.apache.shiro.web.servlet.abstractshirofilter.dofilterintern ( abstractshirofilter.java:362 ) org.apache.shiro.web.servlet.onceperrequestfilter.dofilt ( onceperrequestfilter.java:125 ) org.springframework.web.filter.delegatingfilterproxy.invokedeleg ( delegatingfilterproxy.java:346 ) org.springframework.web.filter.delegatingfilterproxy.dofilt ( delegatingfilterproxy.java:259 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.usergrid.rest.filters.contenttypefilter.dofilt ( contenttypefilter.java:92 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.catalina.core.standardwrappervalve.invok ( standardwrappervalve.java:220 ) org.apache.catalina.core.standardcontextvalve.invok ( standardcontextvalve.java:122 ) org.apache.catalina.authenticator.authenticatorbase.invok ( authenticatorbase.java:503 ) org.apache.catalina.core.standardhostvalve.invok ( standardhostvalve.java:170 ) org.apache.catalina.valves.errorreportvalve.invok ( errorreportvalve.java:103 ) org.apache.catalina.valves.accesslogvalve.invok ( accesslogvalve.java:950 ) org.apache.catalina.core.standardenginevalve.invok ( standardenginevalve.java:116 ) org.apache.catalina.connector.coyoteadapter.servic ( coyoteadapter.java:421 ) org.apache.coyote.http11.abstracthttp11processor.process ( abstracthttp11processor.java:1070 ) org.apache.coyote.abstractprotocol $ abstractconnectionhandler.process ( abstractprotocol.java:611 ) org.apache.tomcat.util.net.jioendpoint $ socketprocessor.run ( jioendpoint.java:316 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1142 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:617 ) org.apache.tomcat.util.threads.taskthread $ wrappingrunnable.run ( taskthread.java:61 ) java.lang.thread.run ( thread.java:745 )",USERGRID-756,3.0
empti respons use cursor ~50 page,USERGRID-753,3.0
test not pass : assetresourceit,USERGRID-745,1.0
"temp file asset upload remov , caus file system use inod after run usergrid 2.0 1 month tomcat , satur inod disk tomcat machin . our temporari file upload get remov . file structur appear { code } ls -lrt /var/cache/tomcat7/temp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime377627531473294092.tmp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime2634951521985073318.tmp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime1066361445384372180.tmp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime3554741621747313255.tmp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime1805372544199883785.tmp -rw-r -- r -- 1 tomcat tomcat 0 jun 9 23:23 mime414525331725926400.tmp { code } we need ensur temporari file remov . note file remain 0 byte . thi seem caus edg case 0 byte file upload , possibl upload fail .",USERGRID-740,5.0
2.0 correctli handl infer type date option time thi caus issu field blank . we need resolv product instal .,USERGRID-738,3.0
sn queue manag use arn read instead url there bug org.apache.usergrid.persistence.queue.impl.snsqueuemanagerimpl result arn queue use url use .,USERGRID-729,1.0
"launcher url point apige the launcher contain url : http : //apigee.github.io/usergrid-portal/ . thi chang reflect new url . do still host admin portal github like apige use ? ... point local instanc specifi api_url argument queri string ? rgd , malaka",USERGRID-728,3.0
"issu get start step github the get start document point apige link henc n't provid expect result . thi could drive away potenti user . document need updat . link : http : //github.com/apache/incubator-usergrid section : get start admin portal unabl access : - http : //github.com/apigee/usergrid-port - http : //apigee.github.com/usergrid-portal/ ? api_url=http : //localhost:8080 rgd , malaka",USERGRID-727,3.0
snsqueuemanagerimpl need better error handl messag it would better preced messag state queue found . com.amazonaws.services.sqs.model.queuedoesnotexistexcept : the specifi queue exist wsdl version . ( servic : amazonsq ; statu code : 400 ; error code : aws.simplequeueservice.nonexistentqueu ; request id : 70c94af9-7d46-5d2b-9a7e-07155ddd1ed5 ) com.amazonaws.http.amazonhttpclient.handleerrorrespons ( amazonhttpclient.java:1160 ) com.amazonaws.http.amazonhttpclient.executeonerequest ( amazonhttpclient.java:748 ) com.amazonaws.http.amazonhttpclient.executehelp ( amazonhttpclient.java:467 ) com.amazonaws.http.amazonhttpclient.execut ( amazonhttpclient.java:302 ) com.amazonaws.services.sqs.amazonsqsclient.invok ( amazonsqsclient.java:2422 ) com.amazonaws.services.sqs.amazonsqsclient.receivemessag ( amazonsqsclient.java:1130 ) org.apache.usergrid.persistence.queue.impl.snsqueuemanagerimpl.getmessag ( snsqueuemanagerimpl.java:234 ) org.apache.usergrid.corepersistence.asyncevents.amazonasynceventservice.tak ( amazonasynceventservice.java:153 ) org.apache.usergrid.corepersistence.asyncevents.amazonasynceventservice.access $ 300 ( amazonasynceventservice.java:66 ) org.apache.usergrid.corepersistence.asyncevents.amazonasynceventservic $ 2.call ( amazonasynceventservice.java:379 ) org.apache.usergrid.corepersistence.asyncevents.amazonasynceventservic $ 2.call ( amazonasynceventservice.java:366 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observable.unsafesubscrib ( observable.java:7495 ) rx.internal.operators.operatorsubscribeon $ 1 $ 1.call ( operatorsubscribeon.java:62 ) rx.internal.schedulers.scheduledaction.run ( scheduledaction.java:55 ) java.util.concurrent.executor $ runnableadapter.cal ( executors.java:511 ) java.util.concurrent.futuretask.run ( futuretask.java:266 ) java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.access $ 201 ( scheduledthreadpoolexecutor.java:180 ) java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.run ( scheduledthreadpoolexecutor.java:293 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1142 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:617 ) java.lang.thread.run ( thread.java:745 ),USERGRID-725,1.0
test not pass : devicesresourceit seem graph compact issu devicesresourceit.putwithuuidshouldcreateafterdelet,USERGRID-704,1.0
test not pass : managementresourceit testsuperuseronlywhenvalidateexternaltokensen extern test broken .,USERGRID-703,2.0
"test not pass : staleindexcleanuptest result : fail test : staleindexcleanuptest.testcleanuponupdate:462 expect candid without earlier stale entiti expect : < 10 > : < 60 > test run : 258 , failur : 1 , error : 0 , skip : 25",USERGRID-700,1.0
test not pass : notificationsit.testpaging:90,USERGRID-699,2.0
test not pass : connectionresourcetest.connectionsloopbacktest:103 nullpoint,USERGRID-698,2.0
test not pass : adminusersit adminusersit.mgmtuserfeed:190 » uniforminterfac get http : //localhost:10006/ma ...,USERGRID-697,2.0
test not pass : registrationit registrationit.addexistingadminusertoorganization:304 » uniforminterfac post ... registrationit.addnewadminuserwithnopwdtoorganization:245- > postaddadmintoorg:90 » clienthandl registrationit.postaddtoorganization:222- > abstractrestit.getadmintoken:173 » uniforminterfac registrationit.postcreateorgandadmin:134- > abstractrestit.getadmintoken:173 » uniforminterfac registrationit.putaddtoorganizationfail:195- > abstractrestit.getadmintoken:173 » uniforminterfac,USERGRID-695,2.0
test not pass : contenttyperesourceit contenttyperesourceit.formencodedcontenttype:152 expect : < 200 > : < 500 > contenttyperesourceit.noacceptget:262 expect : < 200 > : < 400 >,USERGRID-693,2.0
test not pass : permissionsresourceit permissionsresourceit.applicationpermissions:262 expect : < [ noca ] > : < [ 4peak ] > permissionsresourceit.deleteusergroup:157 null,USERGRID-692,2.0
test not pass : organizationsit,USERGRID-690,2.0
test not pass : managementresourceit 11 test fail,USERGRID-689,2.0
test not pass : andorquerytest fix andorquerytest : queryreturncheck queryreturncheckwithshorthand,USERGRID-687,1.0
test not pass : connectionsserviceit.testentityconnect,USERGRID-685,3.0
ug 1.0 bug db setup separ lock keyspac it seem keyspac get creat /database/setup . cassandra.system.keyspace=sso_ug10 # cassandra.application.keyspace=usergrid_appl cassandra.application.keyspace=sso_ug10_app # cassandra.lock.keyspace=lock cassandra.lock.keyspace=sso_ug10_lock,USERGRID-684,3.0
app credenti work access collect within app ( 2.0 ) in order access collect seem org credenti requir app credenti work .,USERGRID-670,5.0
test not pass : rolesserviceit.deleterol,USERGRID-664,1.0
test not pass : connectionsserviceit,USERGRID-663,1.0
test not pass : appinfomigrationplugintest,USERGRID-662,1.0
test not pass : serviceinvocationit,USERGRID-661,1.0
test not pass : groupserviceit - servic,USERGRID-660,1.0
test not pass : organizationit fail,USERGRID-659,3.0
"issu uniqu valu - npe pleas ensur appropri null check behavior happen : 2015-05-12 17:05:37,811 [ http-bio-8080-exec-14 ] error org.apache.usergrid.rest.exceptions.abstractexceptionmapper- java.lang.nullpointerexcept server error ( 500 ) java.lang.nullpointerexcept org.apache.usergrid.services.abstractconnectionsservice.getitembynam ( abstractconnectionsservice.java:238 ) org.apache.usergrid.services.abstractservice.invokeitemwithnam ( abstractservice.java:671 ) org.apache.usergrid.services.abstractservice.invok ( abstractservice.java:628 ) org.apache.usergrid.services.abstractservice.invok ( abstractservice.java:544 ) org.apache.usergrid.services.servicerequest.execut ( servicerequest.java:226 ) org.apache.usergrid.services.servicerequest.invokemultipl ( servicerequest.java:262 ) org.apache.usergrid.services.servicerequest.execut ( servicerequest.java:229 ) org.apache.usergrid.services.servicerequest.execut ( servicerequest.java:193 ) org.apache.usergrid.rest.applications.serviceresource.executeservicerequest ( serviceresource.java:251 ) org.apache.usergrid.rest.applications.serviceresource.executeget ( serviceresource.java:297 ) sun.reflect.generatedmethodaccessor142.invok ( unknown sourc ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) java.lang.reflect.method.invok ( method.java:497 ) com.sun.jersey.spi.container.javamethodinvokerfactori $ 1.invok ( javamethodinvokerfactory.java:60 ) com.sun.jersey.server.impl.model.method.dispatch.abstractresourcemethoddispatchprovid $ typeoutinvoker._dispatch ( abstractresourcemethoddispatchprovider.java:185 ) com.sun.jersey.server.impl.model.method.dispatch.resourcejavamethoddispatcher.dispatch ( resourcejavamethoddispatcher.java:75 ) com.sun.jersey.server.impl.uri.rules.httpmethodrule.accept ( httpmethodrule.java:302 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.resourceclassrule.accept ( resourceclassrule.java:108 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.rootresourceclassesrule.accept ( rootresourceclassesrule.java:84 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1542 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1473 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1419 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1409 ) com.sun.jersey.spi.container.servlet.webcomponent.servic ( webcomponent.java:409 ) com.sun.jersey.spi.container.servlet.servletcontainer.servic ( servletcontainer.java:540 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:909 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:857 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:811 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.shiro.web.servlet.abstractshirofilter.executechain ( abstractshirofilter.java:449 ) org.apache.shiro.web.servlet.abstractshirofilt $ 1.call ( abstractshirofilter.java:365 ) org.apache.shiro.subject.support.subjectcallable.docal ( subjectcallable.java:90 ) org.apache.shiro.subject.support.subjectcallable.cal ( subjectcallable.java:83 ) org.apache.shiro.subject.support.delegatingsubject.execut ( delegatingsubject.java:383 ) org.apache.shiro.web.servlet.abstractshirofilter.dofilterintern ( abstractshirofilter.java:362 ) org.apache.shiro.web.servlet.onceperrequestfilter.dofilt ( onceperrequestfilter.java:125 ) org.springframework.web.filter.delegatingfilterproxy.invokedeleg ( delegatingfilterproxy.java:346 ) org.springframework.web.filter.delegatingfilterproxy.dofilt ( delegatingfilterproxy.java:259 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.usergrid.rest.filters.contenttypefilter.dofilt ( contenttypefilter.java:92 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.catalina.core.standardwrappervalve.invok ( standardwrappervalve.java:220 ) org.apache.catalina.core.standardcontextvalve.invok ( standardcontextvalve.java:122 ) org.apache.catalina.authenticator.authenticatorbase.invok ( authenticatorbase.java:503 ) org.apache.catalina.core.standardhostvalve.invok ( standardhostvalve.java:170 ) org.apache.catalina.valves.errorreportvalve.invok ( errorreportvalve.java:103 ) org.apache.catalina.valves.accesslogvalve.invok ( accesslogvalve.java:950 ) org.apache.catalina.core.standardenginevalve.invok ( standardenginevalve.java:116 ) org.apache.catalina.connector.coyoteadapter.servic ( coyoteadapter.java:421 ) org.apache.coyote.http11.abstracthttp11processor.process ( abstracthttp11processor.java:1070 ) org.apache.coyote.abstractprotocol $ abstractconnectionhandler.process ( abstractprotocol.java:611 ) org.apache.tomcat.util.net.jioendpoint $ socketprocessor.run ( jioendpoint.java:316 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1142 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:617 ) org.apache.tomcat.util.threads.taskthread $ wrappingrunnable.run ( taskthread.java:61 ) java.lang.thread.run ( thread.java:745 )",USERGRID-647,3.0
"test not pass : iteratingqueryit fail due bad serial tri deseri markededg edg abstractcursorseri line 47 2015-05-08 14:33:25,926 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205920001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a69ba0a-f5c1-11e4-ab26-47475f9887d5 , type='test ' } , version=7a69ba63-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:25,942 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205934001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a6bdcea-f5c1-11e4-8198-83878cd74a93 , type='test ' } , version=7a6c0456-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:25,957 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205949001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a6e26da-f5c1-11e4-8f8d-9b115b1298d8 , type='test ' } , version=7a6e2739-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:25,970 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205964001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a7070ca-f5c1-11e4-9deb-7d7bf418ca86 , type='test ' } , version=7a70712c-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:25,984 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205978001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a7293aa-f5c1-11e4-8cbc-6b63f058386d , type='test ' } , version=7a72bb1f-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:25,998 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117205991001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a748f7a-f5c1-11e4-b755-cfcc9cf90313 , type='test ' } , version=7a748fe2-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:26,013 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117206006001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a76d96a-f5c1-11e4-b6ba-5103774b6f21 , type='test ' } , version=7a7700e5-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:26,027 debug ( main ) indexserviceimpl - ad edg indexedgeimpl { timestamp=1431117206021001 } searchedgeimpl { nodeid=simpleid { uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b , type='appl ' } , name='zzzcollzzz|test ' , nodetype=target } batch entiti entiti { id=simpleid { uuid=7a79235a-f5c1-11e4-a640-1f069a6b43c9 , type='test ' } , version=7a7923c8-f5c1-11e4-9ef9-324ce75ff58b } 2015-05-08 14:33:26,112 info ( main ) indexrefreshcommandimpl - found record refresh uuid : 7a7c0980-f5c1-11e4-b2bd-5994708e0639 took ms:75 2015-05-08 14:33:26,112 info ( main ) iteratingqueryit - write took 571 ms disconnect target vm , address : '127.0.0.1:49588 ' , transport : 'socket' 2015-05-08 14:34:59,660 info ( main ) coreappl - test allinconnectionnotyp ( org.apache.usergrid.persistence.query.iteratingqueryit ) : finish applic org.apache.usergrid.corepersistence.pipeline.cursor.cursorparseexcept : unabl deseri valu org.apache.usergrid.corepersistence.pipeline.cursor.abstractcursorserializer.fromjsonnod ( abstractcursorserializer.java:51 ) org.apache.usergrid.corepersistence.pipeline.cursor.requestcursor.getcursor ( requestcursor.java:75 ) org.apache.usergrid.corepersistence.pipeline.pipelinecontext.getcursor ( pipelinecontext.java:68 ) org.apache.usergrid.corepersistence.pipeline.read.abstractpathfilter.getseekvalu ( abstractpathfilter.java:50 ) org.apache.usergrid.corepersistence.pipeline.read.graph.abstractreadgraphfilter.lambda $ call $ 2 ( abstractreadgraphfilter.java:73 ) org.apache.usergrid.corepersistence.pipeline.read.graph.abstractreadgraphfilt $ $ lambda $ 100/1957269967.call ( unknown sourc ) rx.internal.operators.operatormap $ 1.onnext ( operatormap.java:55 ) rx.internal.operators.operatormap $ 1.onnext ( operatormap.java:55 ) rx.internal.util.scalarsynchronousobserv $ 1.call ( scalarsynchronousobservable.java:43 ) rx.internal.util.scalarsynchronousobserv $ 1.call ( scalarsynchronousobservable.java:32 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observ $ 1.call ( observable.java:144 ) rx.observ $ 1.call ( observable.java:136 ) rx.observable.subscrib ( observable.java:7585 ) rx.internal.operators.blockingoperatortoiterator.toiter ( blockingoperatortoiterator.java:53 ) rx.observables.blockingobservable.getiter ( blockingobservable.java:156 ) org.apache.usergrid.corepersistence.results.observablequeryexecutor.hasnext ( observablequeryexecutor.java:114 ) org.apache.usergrid.corepersistence.results.observablequeryexecutor.next ( observablequeryexecutor.java:124 ) org.apache.usergrid.corepersistence.cprelationmanager.searchconnectedent ( cprelationmanager.java:948 ) org.apache.usergrid.corepersistence.cpentitymanager.searchconnectedent ( cpentitymanager.java:1546 ) org.apache.usergrid.persistence.query.iteratingqueryit $ connectionnotypehelper.getresult ( iteratingqueryit.java:278 ) org.apache.usergrid.persistence.query.iteratingqueryit.allin ( iteratingqueryit.java:1130 ) org.apache.usergrid.persistence.query.iteratingqueryit.allinconnectionnotyp ( iteratingqueryit.java:71 ) sun.reflect.nativemethodaccessorimpl.invoke0 ( nativ method ) sun.reflect.nativemethodaccessorimpl.invok ( nativemethodaccessorimpl.java:62 ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) org.junit.runners.model.frameworkmethod $ 1.runreflectivecal ( frameworkmethod.java:50 ) org.junit.internal.runners.model.reflectivecallable.run ( reflectivecallable.java:12 ) org.junit.runners.model.frameworkmethod.invokeexplos ( frameworkmethod.java:47 ) org.junit.internal.runners.statements.invokemethod.evalu ( invokemethod.java:17 ) org.apache.usergrid.coreappl $ 1.evalu ( coreapplication.java:145 ) org.junit.rules.runrules.evalu ( runrules.java:20 ) org.junit.runners.parentrunner.runleaf ( parentrunner.java:325 ) org.junit.runners.blockjunit4classrunner.runchild ( blockjunit4classrunner.java:78 ) org.junit.runners.blockjunit4classrunner.runchild ( blockjunit4classrunner.java:57 ) org.junit.runners.parentrunn $ 3.run ( parentrunner.java:290 ) org.junit.runners.parentrunn $ 1.schedul ( parentrunner.java:71 ) org.junit.runners.parentrunner.runchildren ( parentrunner.java:288 ) org.junit.runners.parentrunner.access $ 000 ( parentrunner.java:58 ) org.junit.runners.parentrunn $ 2.evalu ( parentrunner.java:268 ) org.apache.usergrid.coreitsetupimpl $ 1.evalu ( coreitsetupimpl.java:76 ) org.junit.rules.runrules.evalu ( runrules.java:20 ) org.junit.runners.parentrunner.run ( parentrunner.java:363 ) org.junit.runner.junitcore.run ( junitcore.java:137 ) com.intellij.rt.execution.junit.junitstarter.main ( junitstarter.java:68 ) caus : com.fasterxml.jackson.databind.exc.unrecognizedpropertyexcept : unrecogn field `` delet '' ( class org.apache.usergrid.persistence.graph.impl.simpleedg ) , mark ignor ( 4 known properti : `` type '' , `` targetnod '' , `` sourcenod '' , `` timestamp '' ] ) [ sourc : n/a ; line : -1 , column : -1 ] ( refer chain : org.apache.usergrid.persistence.graph.impl.simpleedg [ `` delet '' ] ) com.fasterxml.jackson.databind.exc.unrecognizedpropertyexception.from ( unrecognizedpropertyexception.java:51 ) com.fasterxml.jackson.databind.deserializationcontext.reportunknownproperti ( deserializationcontext.java:671 ) com.fasterxml.jackson.databind.deser.std.stddeserializer.handleunknownproperti ( stddeserializer.java:773 ) com.fasterxml.jackson.databind.deser.beandeserializerbase.handleunknownproperti ( beandeserializerbase.java:1297 ) com.fasterxml.jackson.databind.deser.beandeserializerbase.handleunknownvanilla ( beandeserializerbase.java:1275 ) com.fasterxml.jackson.databind.deser.beandeserializer.vanilladeseri ( beandeserializer.java:247 ) com.fasterxml.jackson.databind.deser.beandeserializer.deseri ( beandeserializer.java:118 ) com.fasterxml.jackson.databind.objectmapper._readvalu ( objectmapper.java:2965 ) com.fasterxml.jackson.databind.objectmapper.readvalu ( objectmapper.java:1587 ) com.fasterxml.jackson.databind.objectmapper.treetovalu ( objectmapper.java:1931 ) org.apache.usergrid.corepersistence.pipeline.cursor.abstractcursorserializer.fromjsonnod ( abstractcursorserializer.java:48 ) ... 74 caus : rx.exceptions.onerrorthrow $ onnextvalu : onerror emit onnext valu : org.apache.usergrid.corepersistence.pipeline.read.filterresult.class rx.exceptions.onerrorthrowable.addvalueaslastcaus ( onerrorthrowable.java:101 ) rx.internal.operators.operatormap $ 1.onnext ( operatormap.java:58 ) ... 68",USERGRID-644,2.0
test not pass : staleindexcleanup fail due old version get return search java.lang.assertionerror : expect : e2283e4d-f504-11e4-b4ae-324ce75ff58b actual : e2150469-f504-11e4-b4ae-324ce75ff58b < click see differ > org.junit.assert.fail ( assert.java:88 ) org.junit.assert.failnotequ ( assert.java:834 ) org.junit.assert.assertequ ( assert.java:118 ) org.junit.assert.assertequ ( assert.java:144 ) org.apache.usergrid.corepersistence.staleindexcleanuptest.testupdateversionmaxfirst ( staleindexcleanuptest.java:187 ) sun.reflect.nativemethodaccessorimpl.invoke0 ( nativ method ) sun.reflect.nativemethodaccessorimpl.invok ( nativemethodaccessorimpl.java:62 ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) org.junit.runners.model.frameworkmethod $ 1.runreflectivecal ( frameworkmethod.java:50 ) org.junit.internal.runners.model.reflectivecallable.run ( reflectivecallable.java:12 ) org.junit.runners.model.frameworkmethod.invokeexplos ( frameworkmethod.java:47 ) org.junit.internal.runners.statements.invokemethod.evalu ( invokemethod.java:17 ) org.junit.internal.runners.statements.runbefores.evalu ( runbefores.java:26 ) org.junit.internal.runners.statements.runafters.evalu ( runafters.java:27 ) org.apache.usergrid.coreappl $ 1.evalu ( coreapplication.java:145 ) org.junit.rules.runrules.evalu ( runrules.java:20 ) org.junit.runners.parentrunner.runleaf ( parentrunner.java:325 ) org.junit.runners.blockjunit4classrunner.runchild ( blockjunit4classrunner.java:78 ) org.junit.runners.blockjunit4classrunner.runchild ( blockjunit4classrunner.java:57 ) org.junit.runners.parentrunn $ 3.run ( parentrunner.java:290 ) org.junit.runners.parentrunn $ 1.schedul ( parentrunner.java:71 ) org.junit.runners.parentrunner.runchildren ( parentrunner.java:288 ) org.junit.runners.parentrunner.access $ 000 ( parentrunner.java:58 ) org.junit.runners.parentrunn $ 2.evalu ( parentrunner.java:268 ) org.apache.usergrid.coreitsetupimpl $ 1.evalu ( coreitsetupimpl.java:76 ) org.junit.rules.runrules.evalu ( runrules.java:20 ) org.junit.runners.parentrunner.run ( parentrunner.java:363 ) org.junit.runner.junitcore.run ( junitcore.java:137 ) com.intellij.junit4.junit4ideatestrunner.startrunnerwitharg ( junit4ideatestrunner.java:78 ) com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart ( junitstarter.java:212 ) com.intellij.rt.execution.junit.junitstarter.main ( junitstarter.java:68 ) sun.reflect.nativemethodaccessorimpl.invoke0 ( nativ method ) sun.reflect.nativemethodaccessorimpl.invok ( nativemethodaccessorimpl.java:62 ) com.intellij.rt.execution.application.appmain.main ( appmain.java:140 ),USERGRID-642,3.0
entiti return collect ql=select * entiti exist / { org } / { app } / { collect } / { name } return / { org } / { app } / { collect } ? ql=select * for refer : http : //apigeesc.atlassian.net/browse/apibaas-1560,USERGRID-639,3.0
test not pass : notsubpropertyit,USERGRID-637,2.0
test not pass : intersectionunionpagingit,USERGRID-636,2.0
test not pass : intersectiontransitivepagingit,USERGRID-635,2.0
test not pass : entitymanagerfactoryimplit,USERGRID-634,1.0
test not pass : permissionsit,USERGRID-633,0.0
test not pass : performanceentityrebuildindextest,USERGRID-632,1.0
test not pass : pathqueryit,USERGRID-631,2.0
test not pass : indexit,USERGRID-630,2.0
test not pass : geoquerybooleantest,USERGRID-629,2.0
test not pass : geoit,USERGRID-628,1.0
test not pass : entitymanagerit,USERGRID-627,0.0
test not pass : entitydictionaryit,USERGRID-626,2.0
test not pass : countingmutatorit,USERGRID-625,2.0
test not pass : messagesit,USERGRID-624,2.0
test not pass : allentitiesinsystemobservableit,USERGRID-623,2.0
test not pass : indexservicetest,USERGRID-622,2.0
test not pass : inmemoryasyncindexservicetest,USERGRID-621,2.0
test not pass : iteratingqueryit + collectionit,USERGRID-587,1.0
test not pass : staleindexcleanup seem like stale entiti n't clean .,USERGRID-584,1.0
es cursor work 2.1,USERGRID-578,3.0
"new app dropdown click org administr page when creat app portal sometim take long time reflect portal . even /users/m return app list still reflect portal . when use '+ add new app ' top page newli creat app reflect app dropdown click org administr page . see jeff get url record issu reproduc . in addit , sometim immedi result 'no applic access author ' .",USERGRID-570,3.0
updat default accesstoken ttl app give 404 put call organ 's app updat accesstoken ttl give 404 . the rest call document - http : //apigee.com/docs/api-baas/content/changing-token-time-live-ttl thi fail two-dot-o two-dot-o-dev . the code handl api updat recent ( past month ) .,USERGRID-566,3.0
[ spike ] cross-collect / graph filter work rest call natur work : get /pet ; ql=select * type='cat'/belongsto/own ; ql=select * city='dalla ',USERGRID-551,5.0
"ensur sq consum robust stop consum messag in past : for reason sq queue consum stop process messag . pleas investig , ensur code robust , add log messag . current : test confirm , review jeff and/or todd loop process sq messag robust .",USERGRID-546,3.0
"configur notifi apn error non-p12 certif upload use i upload x509 certif instead p12 certif configur ap notifi . i experienc follow : 1 ) the ui error i upload non p12 certif . -- similar pars , consol error non-p12 certif attempt use . if accept x509 certif doc ui page updat . 2 ) when send push notif baa report success messag histori notif entiti push notif receiv -- it seem possibl work sinc p12 certif . it look could except get swallow . 3 ) i delet notifi . -- dont know relat certif",USERGRID-519,3.0
baa queri return entiti match input queri error,USERGRID-516,3.0
"delet call limit x return > x reslut when delet call ql=select * lmit=100 , > 100 result return . thi may may impli > 100 entiti delet .",USERGRID-508,3.0
broken link contribut page on usergrid homepag http : //usergrid.incubator.apache.org `` contribut guidelin '' link point http : //cwiki.apache.org/confluence/display/usergrid/github+based+contribution+workflow realli http : //cwiki.apache.org/confluence/display/usergrid/usergrid+external+contributors+guid,USERGRID-498,3.0
"long paus error es deletebyqueri when singl thread put usergrid , one , es time deletebyqueri block caller . the respons time usergrid goe 200m ~60 time . note error es log indic 1 minut timeout . [ 2015-03-10 17:37:42,947 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:42,947 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:42,948 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:42,948 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:42,949 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:42,949 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,007 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,008 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,012 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,013 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,072 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,073 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,073 ] [ debug ] [ action.deletebyqueri ] [ res003si ] observ : timeout notif cluster servic . timeout set [ 1m ] , time sinc start [ 1m ] [ 2015-03-10 17:37:43,073 ] [ debug ] [ action.deletebyqueri",USERGRID-471,3.0
"reject es index job drop . when index queue full es reject index request usergrid drop . the result could entiti return . we handl soon . at minimum , put sq queue someth similar futur handl .",USERGRID-466,5.0
content length header sent post ping ident,USERGRID-457,3.0
"invalid token use activ new user account onc token use activ account , invalid .",USERGRID-450,3.0
"admin portal - $ { user } permiss allow , in admin portal , tri enter permiss like : /users/ { $ user } /has/stuff/mystuff it give error : '' path must begin slash , path allow : / , a-z , 0-9 , dot , dash , path format : /path/ /path//path allow ''",USERGRID-449,3.0
"remov redund appinfo collect managementserviceimpl there flaw two-dot-o cpentitymanagerfactori . the factori store collect `` appinfo '' type entiti , managementserviceimpl store redund collect `` application_info '' entiti . the problem becom evid tri delet applic . the applic delet `` appinfo '' collect . when call manag org/app end-point still see applic end-point use `` application_info '' . to fix : - ensur one collect store - add code migrat exist app inform collect",USERGRID-448,3.0
investig elasticsearch timeout long-run gc paus es tomcat i seen mani 16 tomcat node paus time across cluster . i took stack trace attach . thi one jump : '' http-bio-8080-exec-2 '' daemon prio=10 tid=0x00007f0cbc003800 nid=0x20ff wait condit [ 0x00007f0d370dd000 ] java.lang.thread.st : wait ( park ) sun.misc.unsafe.park ( nativ method ) - park wait < 0x00000007ee1de4d8 > ( org.elasticsearch.common.util.concurrent.basefutur $ sync ) java.util.concurrent.locks.locksupport.park ( locksupport.java:186 ) java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt ( abstractqueuedsynchronizer.java:834 ) java.util.concurrent.locks.abstractqueuedsynchronizer.doacquiresharedinterrupt ( abstractqueuedsynchronizer.java:994 ) java.util.concurrent.locks.abstractqueuedsynchronizer.acquiresharedinterrupt ( abstractqueuedsynchronizer.java:1303 ) org.elasticsearch.common.util.concurrent.basefutur $ sync.get ( basefuture.java:274 ) org.elasticsearch.common.util.concurrent.basefuture.get ( basefuture.java:113 ) org.elasticsearch.action.support.adapteractionfuture.actionget ( adapteractionfuture.java:45 ) org.apache.usergrid.persistence.index.impl.esentityindeximpl.search ( esentityindeximpl.java:357 ) org.apache.usergrid.corepersistence.cprelationmanager.searchcollect ( cprelationmanager.java:963 ) org.apache.usergrid.corepersistence.cpentitymanager.searchcollect ( cpentitymanager.java:624 ) org.apache.usergrid.corepersistence.cpentitymanagerfactory.lookupappl ( cpentitymanagerfactory.java:433 ) org.apache.usergrid.rest.organizations.organizationresource.getapplicationbynam ( organizationresource.java:137 ) sun.reflect.generatedmethodaccessor113.invok ( unknown sourc ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) java.lang.reflect.method.invok ( method.java:601 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.dispatch ( sublocatorrule.java:197 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.invokesubloc ( sublocatorrule.java:183 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:110 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept ( sublocatorrule.java:137 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.resourceclassrule.accept ( resourceclassrule.java:108 ) com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept ( righthandpathrule.java:147 ) com.sun.jersey.server.impl.uri.rules.rootresourceclassesrule.accept ( rootresourceclassesrule.java:84 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1542 ) com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest ( webapplicationimpl.java:1473 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1419 ) com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest ( webapplicationimpl.java:1409 ) com.sun.jersey.spi.container.servlet.webcomponent.servic ( webcomponent.java:409 ) com.sun.jersey.spi.container.servlet.servletcontainer.servic ( servletcontainer.java:540 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:909 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:857 ) com.sun.jersey.spi.container.servlet.servletcontainer.dofilt ( servletcontainer.java:811 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.shiro.web.servlet.abstractshirofilter.executechain ( abstractshirofilter.java:449 ) org.apache.shiro.web.servlet.abstractshirofilt $ 1.call ( abstractshirofilter.java:365 ) org.apache.shiro.subject.support.subjectcallable.docal ( subjectcallable.java:90 ) org.apache.shiro.subject.support.subjectcallable.cal ( subjectcallable.java:83 ) org.apache.shiro.subject.support.delegatingsubject.execut ( delegatingsubject.java:383 ) org.apache.shiro.web.servlet.abstractshirofilter.dofilterintern ( abstractshirofilter.java:362 ) org.apache.shiro.web.servlet.onceperrequestfilter.dofilt ( onceperrequestfilter.java:125 ) org.springframework.web.filter.delegatingfilterproxy.invokedeleg ( delegatingfilterproxy.java:346 ) org.springframework.web.filter.delegatingfilterproxy.dofilt ( delegatingfilterproxy.java:259 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.usergrid.rest.filters.contenttypefilter.dofilt ( contenttypefilter.java:92 ) org.apache.catalina.core.applicationfilterchain.internaldofilt ( applicationfilterchain.java:241 ) org.apache.catalina.core.applicationfilterchain.dofilt ( applicationfilterchain.java:208 ) org.apache.catalina.core.standardwrappervalve.invok ( standardwrappervalve.java:220 ) org.apache.catalina.core.standardcontextvalve.invok ( standardcontextvalve.java:122 ) org.apache.catalina.authenticator.authenticatorbase.invok ( authenticatorbase.java:503 ) org.apache.catalina.core.standardhostvalve.invok ( standardhostvalve.java:170 ) org.apache.catalina.valves.errorreportvalve.invok ( errorreportvalve.java:103 ) org.apache.catalina.valves.accesslogvalve.invok ( accesslogvalve.java:950 ) org.apache.catalina.core.standardenginevalve.invok ( standardenginevalve.java:116 ) org.apache.catalina.connector.coyoteadapter.servic ( coyoteadapter.java:421 ) org.apache.coyote.http11.abstracthttp11processor.process ( abstracthttp11processor.java:1070 ) org.apache.coyote.abstractprotocol $ abstractconnectionhandler.process ( abstractprotocol.java:611 ) org.apache.tomcat.util.net.jioendpoint $ socketprocessor.run ( jioendpoint.java:314 ) - lock < 0x00000007eda23278 > ( org.apache.tomcat.util.net.socketwrapp ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1145 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:615 ) org.apache.tomcat.util.threads.taskthread $ wrappingrunnable.run ( taskthread.java:61 ) java.lang.thread.run ( thread.java:722 ) todd call one : '' collectiontasks-18 '' daemon prio=10 tid=0x00007f0cfc004000 nid=0x21b7 wait condit [ 0x00007f0cb87c6000 ] java.lang.thread.st : wait ( park ) sun.misc.unsafe.park ( nativ method ) - park wait < 0x00000007ed32ccf0 > ( org.elasticsearch.common.util.concurrent.basefutur $ sync ) java.util.concurrent.locks.locksupport.park ( locksupport.java:186 ) java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt ( abstractqueuedsynchronizer.java:834 ) java.util.concurrent.locks.abstractqueuedsynchronizer.doacquiresharedinterrupt ( abstractqueuedsynchronizer.java:994 ) java.util.concurrent.locks.abstractqueuedsynchronizer.acquiresharedinterrupt ( abstractqueuedsynchronizer.java:1303 ) org.elasticsearch.common.util.concurrent.basefutur $ sync.get ( basefuture.java:274 ) org.elasticsearch.common.util.concurrent.basefuture.get ( basefuture.java:113 ) org.elasticsearch.action.support.adapteractionfuture.actionget ( adapteractionfuture.java:45 ) org.apache.usergrid.persistence.index.impl.esentityindeximpl.deletepreviousvers ( esentityindeximpl.java:533 ) org.apache.usergrid.corepersistence.events.entityversioncreatedhandler.versioncr ( entityversioncreatedhandler.java:67 ) org.apache.usergrid.persistence.collection.impl.entityversioncreatedtask.fireev ( entityversioncreatedtask.java:97 ) org.apache.usergrid.persistence.collection.impl.entityversioncreatedtask.cal ( entityversioncreatedtask.java:83 ) org.apache.usergrid.persistence.collection.impl.entityversioncreatedtask.cal ( entityversioncreatedtask.java:38 ) java.util.concurrent.futuretask $ sync.innerrun ( futuretask.java:334 ) java.util.concurrent.futuretask.run ( futuretask.java:166 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1145 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:615 ) java.lang.thread.run ( thread.java:722 ),USERGRID-442,3.0
"schedul run fail system idl thi error happen consist cluster system larg idl : 2015-02-27 23:19:40,060 [ jobschedulerservic ] error org.apache.usergrid.batch.service.jobschedulerservice- schedul run fail , error org.apache.usergrid.persistence.exceptions.queueexcept : unabl obtain lock queue '/job ' ' 5'second org.apache.usergrid.mq.cassandra.io.consumertransaction.getresult ( consumertransaction.java:206 ) org.apache.usergrid.mq.cassandra.queuemanagerimpl.getfromqueu ( queuemanagerimpl.java:412 ) org.apache.usergrid.batch.service.schedulerserviceimpl.getjob ( schedulerserviceimpl.java:164 ) org.apache.usergrid.batch.service.jobschedulerservice.runoneiter ( jobschedulerservice.java:111 ) com.google.common.util.concurrent.abstractscheduledservic $ 1 $ 1.run ( abstractscheduledservice.java:170 ) java.util.concurrent.executor $ runnableadapter.cal ( executors.java:471 ) java.util.concurrent.futuretask $ sync.innerrunandreset ( futuretask.java:351 ) java.util.concurrent.futuretask.runandreset ( futuretask.java:178 ) java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.access $ 301 ( scheduledthreadpoolexecutor.java:178 ) java.util.concurrent.scheduledthreadpoolexecutor $ scheduledfuturetask.run ( scheduledthreadpoolexecutor.java:293 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1145 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:615 ) java.lang.thread.run ( thread.java:722 )",USERGRID-439,3.0
admin portal - connect list data page throw error step reproduc : 1 . creat entiti two differ collect 2 . connect 3 . read connect data explor post /review { `` name '' : '' myreview '' } post /user { `` usernam '' : '' fred '' } post /users/fred/wrote/reviews/myreview get /users/fred/wrote/reviews/ < == call give error thi happen data explor tri make /index call : get /users/fred/wrote/reviews/index which return 500 error,USERGRID-435,3.0
"[ spike ] verifi es re-index , exist document forc updat when run reindex last night , document appear es return via search candidateresultset . delet document es re-index resolv issu . we need ensur 're updat exist document exist es re-index forc index updat exist entiti .",USERGRID-425,3.0
"can build android sdk i build android sdk usergrid 1.0.1 . printout build_sdk_zip.sh modifi `` -u '' `` -e '' option ad ( `` mvn clean instal -u -e '' ) : ubuntu @ ubuntu-virtualbox : ~/downloads/incubator-usergrid-master/sdks/android $ ./build_release_zip.sh 0.0.8 [ info ] error stacktrac turn . [ info ] scan project ... download : http : //repo.maven.apache.org/maven2/org/apache/usergrid/usergrid/1.0.0/usergrid-1.0.0.pom [ error ] the build could read 1 project - > [ help 1 ] org.apache.maven.project.projectbuildingexcept : some problem encount process pom : [ fatal ] non-resolv parent pom : could find artifact org.apache.usergrid : usergrid : pom:1.0.0 central ( http : //repo.maven.apache.org/maven2 ) 'parent.relativepath ' point wrong local pom @ line 27 , column 10 org.apache.maven.project.defaultprojectbuilder.build ( defaultprojectbuilder.java:364 ) org.apache.maven.defaultmaven.collectproject ( defaultmaven.java:672 ) org.apache.maven.defaultmaven.getprojectsformavenreactor ( defaultmaven.java:663 ) org.apache.maven.defaultmaven.doexecut ( defaultmaven.java:250 ) org.apache.maven.defaultmaven.execut ( defaultmaven.java:155 ) org.apache.maven.cli.mavencli.execut ( mavencli.java:584 ) org.apache.maven.cli.mavencli.domain ( mavencli.java:213 ) org.apache.maven.cli.mavencli.main ( mavencli.java:157 ) sun.reflect.nativemethodaccessorimpl.invoke0 ( nativ method ) sun.reflect.nativemethodaccessorimpl.invok ( nativemethodaccessorimpl.java:57 ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) java.lang.reflect.method.invok ( method.java:606 ) org.codehaus.plexus.classworlds.launcher.launcher.launchenhanc ( launcher.java:289 ) org.codehaus.plexus.classworlds.launcher.launcher.launch ( launcher.java:229 ) org.codehaus.plexus.classworlds.launcher.launcher.mainwithexitcod ( launcher.java:415 ) org.codehaus.plexus.classworlds.launcher.launcher.main ( launcher.java:356 ) [ error ] [ error ] the project org.apache.usergrid : usergrid-android:0.0.8 ( /home/ubuntu/downloads/incubator-usergrid-master/sdks/android/pom.xml ) 1 error [ error ] non-resolv parent pom : could find artifact org.apache.usergrid : usergrid : pom:1.0.0 central ( http : //repo.maven.apache.org/maven2 ) 'parent.relativepath ' point wrong local pom @ line 27 , column 10 - > [ help 2 ] org.apache.maven.model.resolution.unresolvablemodelexcept : could find artifact org.apache.usergrid : usergrid : pom:1.0.0 central ( http : //repo.maven.apache.org/maven2 ) org.apache.maven.project.projectmodelresolver.resolvemodel ( projectmodelresolver.java:159 ) org.apache.maven.model.building.defaultmodelbuilder.readparentextern ( defaultmodelbuilder.java:817 ) org.apache.maven.model.building.defaultmodelbuilder.readpar ( defaultmodelbuilder.java:669 ) org.apache.maven.model.building.defaultmodelbuilder.build ( defaultmodelbuilder.java:307 ) org.apache.maven.project.defaultprojectbuilder.build ( defaultprojectbuilder.java:411 ) org.apache.maven.project.defaultprojectbuilder.build ( defaultprojectbuilder.java:380 ) org.apache.maven.project.defaultprojectbuilder.build ( defaultprojectbuilder.java:344 ) org.apache.maven.defaultmaven.collectproject ( defaultmaven.java:672 ) org.apache.maven.defaultmaven.getprojectsformavenreactor ( defaultmaven.java:663 ) org.apache.maven.defaultmaven.doexecut ( defaultmaven.java:250 ) org.apache.maven.defaultmaven.execut ( defaultmaven.java:155 ) org.apache.maven.cli.mavencli.execut ( mavencli.java:584 ) org.apache.maven.cli.mavencli.domain ( mavencli.java:213 ) org.apache.maven.cli.mavencli.main ( mavencli.java:157 ) sun.reflect.nativemethodaccessorimpl.invoke0 ( nativ method ) sun.reflect.nativemethodaccessorimpl.invok ( nativemethodaccessorimpl.java:57 ) sun.reflect.delegatingmethodaccessorimpl.invok ( delegatingmethodaccessorimpl.java:43 ) java.lang.reflect.method.invok ( method.java:606 ) org.codehaus.plexus.classworlds.launcher.launcher.launchenhanc ( launcher.java:289 ) org.codehaus.plexus.classworlds.launcher.launcher.launch ( launcher.java:229 ) org.codehaus.plexus.classworlds.launcher.launcher.mainwithexitcod ( launcher.java:415 ) org.codehaus.plexus.classworlds.launcher.launcher.main ( launcher.java:356 ) caus : org.eclipse.aether.resolution.artifactresolutionexcept : could find artifact org.apache.usergrid : usergrid : pom:1.0.0 central ( http : //repo.maven.apache.org/maven2 ) org.eclipse.aether.internal.impl.defaultartifactresolver.resolv ( defaultartifactresolver.java:459 ) org.eclipse.aether.internal.impl.defaultartifactresolver.resolveartifact ( defaultartifactresolver.java:262 ) org.eclipse.aether.internal.impl.defaultartifactresolver.resolveartifact ( defaultartifactresolver.java:239 ) org.eclipse.aether.internal.impl.defaultrepositorysystem.resolveartifact ( defaultrepositorysystem.java:295 ) org.apache.maven.project.projectmodelresolver.resolvemodel ( projectmodelresolver.java:155 ) ... 21 caus : org.eclipse.aether.transfer.artifactnotfoundexcept : could find artifact org.apache.usergrid : usergrid : pom:1.0.0 central ( http : //repo.maven.apache.org/maven2 ) org.eclipse.aether.connector.wagon.wagonrepositoryconnector $ 6.wrap ( wagonrepositoryconnector.java:1012 ) org.eclipse.aether.connector.wagon.wagonrepositoryconnector $ 6.wrap ( wagonrepositoryconnector.java:1004 ) org.eclipse.aether.connector.wagon.wagonrepositoryconnector $ gettask.run ( wagonrepositoryconnector.java:725 ) org.eclipse.aether.util.concurrency.runnableerrorforward $ 1.run ( runnableerrorforwarder.java:67 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1145 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:615 ) java.lang.thread.run ( thread.java:745 ) [ error ] [ error ] re-run maven use -x switch enabl full debug log . [ error ] [ error ] for inform error possibl solut , pleas read follow articl : [ error ] [ help 1 ] http : //cwiki.apache.org/confluence/display/maven/projectbuildingexcept [ error ] [ help 2 ] http : //cwiki.apache.org/confluence/display/maven/unresolvablemodelexcept ubuntu @ ubuntu-virtualbox : ~/downloads/incubator-usergrid-master/sdks/android $",USERGRID-350,3.0
fix propertiesresourceit,USERGRID-349,3.0
fix accesstokenit,USERGRID-347,3.0
fix matrixquerytest,USERGRID-346,3.0
fix retrieveuserstest,USERGRID-345,1.0
fix applicationrequestcounterit,USERGRID-343,3.0
fix adminemailencodingit,USERGRID-342,1.0
fix browsercompatibilitytest,USERGRID-341,3.0
fix registrationit,USERGRID-340,1.0
fix ownershipresourceit fail contextualconnectionownership test everytim . the other pass .,USERGRID-338,1.0
fix contenttyperesourceit,USERGRID-336,1.0
test not pass : collectionsresourceit,USERGRID-335,2.0
fix assetresourceit,USERGRID-334,3.0
fix applicationresourceit,USERGRID-333,3.0
fix applicationrequestcounterit,USERGRID-332,3.0
"usergrid launcher throw except `` initi databas '' uncheck if run launcher check `` initi databas on start '' button , first time start server launcher exit error messag like : 2015-01-06 16:03:51,476 error ( pool-2-thread-1 ) [ org.apache.cassandra.config.databasedescriptor ] - fatal configur error org.apache.cassandra.exceptions.configurationexcept : can locat file : tmp/cassandra.yaml org.apache.cassandra.config.databasedescriptor.getstorageconfigurl ( databasedescriptor.java:123 ) org.apache.cassandra.config.databasedescriptor.loadyaml ( databasedescriptor.java:140 ) org.apache.cassandra.config.databasedescriptor. < clinit > ( databasedescriptor.java:132 ) org.apache.cassandra.service.cassandradaemon.setup ( cassandradaemon.java:216 ) org.apache.cassandra.service.cassandradaemon.activ ( cassandradaemon.java:447 ) org.apache.usergrid.launcher.embeddedserverhelp $ cassandrarunner.run ( embeddedserverhelper.java:190 ) java.util.concurrent.threadpoolexecutor.runwork ( threadpoolexecutor.java:1145 ) java.util.concurrent.threadpoolexecutor $ worker.run ( threadpoolexecutor.java:615 ) java.lang.thread.run ( thread.java:745 )",USERGRID-316,3.0
build applic collect delet async distribut workflow akka usergrid actorsystem modul use usergrid 2.1.1 onward . we need develop asynch distribut workflow leverag actorsystem thing like data cleanup app/collect delet .,USERGRID-286,3.0
"limit honor subsequ request cursor the follow scenario work 1.0 . we 've broken 2.0 . step reproduc . 1 ) perform queri return cursor , limit may may suppli . curl -x get `` http : //localhost:8080/usergrid/test1/us ? limit=20 '' 2 ) perform next page queri chang limit curl -x get `` http : //localhost:8080/usergrid/test1/us ? limit=40 & cursor=cxvlcnlbbmrgzxrjadsxozk4nza6udfxa08zsvrtdgfxnncwrmz2vkzudzswow== '' * what happen * a respons 40 entiti , assum 40 return . * what actual happen * a respons 20 entiti , origin queri , request .",USERGRID-263,3.0
"es cursor error translat user when advanc beyond cursor 's end , pass invalid cursor , follow error return user . { code } body= { `` error '' : '' elasticsearch_illegal_argu '' , '' timestamp '' :1417654353286 , '' durat '' :0 , '' error_descript '' : '' fail decod scrollid '' , '' except '' : '' org.elasticsearch.elasticsearchillegalargumentexcept '' } { code } we catch error , return friendlier error messag user .",USERGRID-262,3.0
"ca n't post connect after set role prescrib , i receiv error termin tri creat connect : warn : except occur bodi skip java.lang.illegalstateexcept : can skip byte avail org.glassfish.grizzly.http.server.io.inputbuffer.skip ( inputbuffer.java:600 ) … i tri issu command shell portal : post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot { `` data '' : `` learn usergrid ” } post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot [ { `` data '' : `` learn usergrid '' } ] either command return : /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot { `` action '' : `` post '' , `` applic '' : `` f2b952fa-22ee-11e4-9b4b-e9ea3d610fab '' , `` param '' : { `` access_token '' : [ `` ywmtmzwu0iqueesclt1pguhfegaaauf7j0uw32rktiypwsnvohbvatmkmnjft3 '' ] } , `` path '' : `` /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot '' , `` uri '' : `` http : //localhost:8080/test.2/note-pad/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot '' , `` entiti '' : [ ] , `` timestamp '' : 1408063432982 , `` durat '' : 6 , `` organ '' : `` test.2 '' , `` applicationnam '' : “ note-pad '' } howev , entiti creat . i ’ also tri use curl result : curl -h `` author : bearer ywmt2j3jacqleesr1fvx65wfraaaauf67ffu8cqvwoyiavxxioea177uf05noa8 '' -x post -d ' [ { `` data '' : '' lear usergrid '' } ] ' http : //localhost:8080/test.2/note-pad/users/me/mynot i execut post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynot well , made differ . -charl",USERGRID-249,3.0
"index queue depend sole aw sq in two-dot-o branch faulti queue implement replac set queue interfac aw sq implement interfac . usergrid depend commerci servic usergrid launcher abl run standalon without extern servic . so , need queue implement emb usergrid run junit test depend queue , run usergrid launcher ( ideal ) one also run remot . apach qpid seem good candid default queue implement .",USERGRID-241,3.0
"two-dot-o : error portal entiti creat after entiti creat , portal attempt load entiti get 500 error server form bad url two slash applic specifi .",USERGRID-237,3.0
portal look helpjson.json wrong place if deploy portal path `` / '' get 404 possibl 500 error portal load page abl load helpjson.json .,USERGRID-236,3.0
"inconsist miss resourc statu code assum valid credenti , i get differ statu code depend author method . bearer token : curl -x get -i -h `` accept : application/json '' -h `` author : bearer xxx '' 'http : //localhost:8080/test-organization/test-app/dogs/fido' 404 error : servic resourc found client id & secret : curl -x get -i -h `` accept : application/json '' 'http : //localhost:8080/test-organization/test-app/dogs/fido ? client_id=xxx & client_secret=xxx' http/1.1 401 unauthor note : in case , client receiv bodi content : { `` error '' : '' service_resource_not_found '' , '' timestamp '' :1412358355742 , '' durat '' :0 , '' except '' : '' org.apache.usergrid.services.exceptions.serviceresourcenotfoundexcept '' , '' error_descript '' : '' servic resourc found '' }",USERGRID-235,3.0
default collect creat databas setup step reproduc : 1 ) setup new usergrid system 2 ) http get url /system/database/setup 3 ) http get url /system/superuser/setup 4 ) login portal 5 ) see `` role '' collect but collect : /activ /asset /devic /folder /group /role /token /user,USERGRID-231,3.0
unabl run usergrid line internet when tri run launcher connect internet spring bean error unabl load spring xsd e.g . : http : //www.springframework.org/schema/bean http : //www.springframework.org/schema/beans/spring-beans-3.1.xsd line 22 usergrid-standalone-context.xml launcher modul . ive tri run sourc build a-127 command it issu .,USERGRID-230,3.0
"possibl bug usergrid-lib.min.j typeerror : c null use ug ( master ) i turn firebug firefox log ug portal , i see follow consol error usergrid-lib.min.j ( line 27 , col 92 ) ... ==thi & & c. $ $ nextsibl ) ) ( ; c ! ==thi & & ! ( d=c. $ $ nextsibl ) ; ) c=c. $ parent } ( ... first thing first , anyon els reproduc ? i idea i look javascript yet .",USERGRID-215,3.0
fix `` start page '' two-dot-o branch,USERGRID-211,3.0
"java document mislead user ( develop ) . java class usergrid sdk still refer apige sdk class . ex : through method explan logoutappuserasync ( ) method client.java usergrid android sdk , refer class call `` datacli '' . there class call `` datacli '' usergrid android sdk , apige sdk . therefor difficult figur go code base .",USERGRID-179,3.0
lack document avail featur rest api there proper document avail featur rest api . it bit time consum thing apach usergrid without document .,USERGRID-178,3.0
"some requir field requir user page admin portal on user page admin portal , sever field mark requir .",USERGRID-161,3.0
select ( filter ) queri dot notat n't return result step reproduc : 1 . creat collect popul entiti follow data : { `` foo.bar '' : `` baz '' } 2 . run get queri collect filter : http : //api.usergrid.com/org/app/collection/ ? ql=select foo.bar 3 . no result return `` select '' filter n't work dot-not key name . attach real-world use-cas fail .,USERGRID-160,3.0
"can chang custom entiti name if custom entiti attempt updat entiti 's name ( via put ) , system accept request return success ( 200 ) , name remain unchang . eg . use ugc : ugc creat foo `` name : 'bar ' '' ugc updat foo/bar `` name : 'baz ' '' ( note : referenc entiti put via uuid effect . )",USERGRID-82,3.0
"applic end-point return incorrect count here 's exampl obvious bad count : `` dog '' : { `` titl '' : `` dog '' , `` count '' : -248 , `` name '' : `` dog '' , `` type '' : `` dog '' } , `` medic '' : { `` titl '' : `` medic '' , `` count '' : 3 , `` name '' : `` medic '' , `` type '' : `` medic '' } , `` someth '' : { `` titl '' : `` someth '' , `` count '' : -12 , `` name '' : `` someth '' , `` type '' : `` someth '' } , i suspect",USERGRID-58,3.0
"activitystream index `` top-level properti like categori , content , titl index default , /activ ? ql=categori = 'report ' object actor , object , provid , gener & target deep-index queri like : /activ ? ql=object.uuid = 'ca16c9a1-ab5b-11e1-8b99-1231381c404f ' /activ ? ql=author.nam = 'tim ' etc . ''",USERGRID-40,3.0
"can delet entiti connect `` to repro , attempt delet entiti connect : curl -x delet `` '' http : //api.com/fdsafdsa/testapp/dog/dachsund '' '' notic get error messag : { `` '' error '' '' : '' '' class_cast '' '' , '' '' timestamp '' '' :1386200483978 , '' '' durat '' '' :0 , '' '' except '' '' : '' '' java.lang.classcastexcept '' '' , '' '' error_descript '' '' : '' '' org.usergrid.persistence.cassandra.connectedentityrefimpl cast org.usergrid.persistence.cassandra.connectionrefimpl '' '' } it expect entiti delet connect delet well . ''",USERGRID-35,3.0
"/collection/id/connected/ * path either return entiti , empti set 404 `` not clear path suppos itõ accept , alway return empti set result error exampl : curl http : //api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/follow = > return empti set despit follow relationship exist curl http : //api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/us = > return empti set despit user connect curl http : //api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/d2740256-e312-11e1-ad17-12313d2b9232 = > return error despit entiti uuid connect 1 . either accept name uuid end , return entiti exist òconnectedó entiti , 404 exist list connect entiti 2 . or accept relationship identifi return connect entiti identifi , empti set none 3 . or accept collect name / type return connect entiti type , empti set none 4 . or tell suppos provid put doc sdk : p ''",USERGRID-30,3.0
"we n't seem pars '++ ' correctli `` i made simpl app test usergrid 's capabl i 'm troubl updat entiti contain html entiti . i.e . i add c++ entiti collect call languag i also c collect . whenev i tri updat c++ , i grab via collect queri '' '' select * owned='testus ' lang= ' c++ ' '' '' i get back c entiti . i tri encod string , c % 2b % 2b get doubl encod c % 252b % 252b thi typic call make without tri encod anyth : http : //api.usergrid.com/clydebyrdiii ... although seem encod correctli return entiti lang= ' c ' lang= ' c++ ' ; and confirm scott : nope . if i creat entiti attribut = ' c++ ' , i find queri : ' c * ' ' c ' ' c+++'. ''",USERGRID-29,3.0
limit connect queri work also `` limit '' n't seem work connect e.g . http : //api.usergrid.com/fdsafdsa/test/users/me/connecting/shar ? limit=2 & access_token=ywmtsq92bbcseeoyaq1trlcn_aaaaudupjgtlh06penbkfrmoxta8jg1xwruwv . return case 3,USERGRID-27,3.0
"offer option delet inbound and/or outbound connect entiti delet `` per ed , inbound connect clean entiti delet : for exampl , entiti rock entiti paper like entiti scissor , like connect scissor delet scissor delet . it would great option delet inbound connect entiti delet ( flag also possibl delet queri ) . ''",USERGRID-26,3.0
collect counter app endpoint complet `` the count item collect return call /org/app complet ( usual factor 10 ) . thi possibl relat usergrid-17 . can someth fix ? possibl solut two part : 1 . investig code path ensur counter invok need 2 . creat job invok automat manual recount entiti collect '',USERGRID-24,3.0
"fresh admin user token wo n't work /management/users/m `` after log : curl -x post `` '' http : //api.usergrid.com/management/token '' '' -d ' { `` '' grant_typ '' '' : '' '' password '' '' , '' '' usernam '' '' : '' '' fdsafdsa '' '' , '' '' password '' '' : '' '' fdsafdsafdsa '' '' } ' the user abl log token : curl -x get -i -h `` '' author : bearer ywmt997xkhayeeo7oblzqpxkkaaaaudriyfhe8gizlo8ihjztox2jq7kxpbfthc '' '' `` '' http : //api.usergrid.com/management/usersfdsafdsa '' '' but token work endpoint . * * * updat * * * : look like problem /management/users/ < usernam email > endpoint case-insensit usernam email address . we need updat api call case-insensitive. ''",USERGRID-23,3.0
"bad geo queri return entir collect `` when badli form geo queri sent get , api return entir collect , return noth , return sort queri pars error . for exampl , miss 'locat ' param queri statement : within 16000 37.774989 , -122.419413 return first 10 entiti collect even though queri ca n't parsed. ''",USERGRID-19,3.0
random segment fault train,MXNET-1108,1.0
bug multiboxtargetforward singl box label,MXNET-1033,3.0
investig roi_align oper issu,MXNET-974,3.0
"fix bug matric multipl dimens , one dimens much larger",MXNET-921,3.0
infershap return fals caught symbol mode,MXNET-865,3.0
convert paramet dtype float32 float64 gluon ?,MXNET-816,2.0
differ behaviour customop latest mxnet,MXNET-812,5.0
test_arang failur - jetson tx2 ( cpu ),MXNET-811,1.0
src/operator/./bilinear_sampler-inl.h:105 : have implement data req combin ! gdata_req=0 ggrid_req=1,MXNET-810,2.0
whi tanh activ layer gener valu greater 1 ?,MXNET-809,1.0
undefin behavior mx.sym.wher shape-mismatch cond,MXNET-806,2.0
mx.nd.topk work ndarray type float16,MXNET-805,5.0
nd.softmax ( ) n't support grad_req='add ',MXNET-804,5.0
inconsist / wrong output sum,MXNET-803,1.0
autograd fail use ` take ` oper repeatedli,MXNET-802,1.0
non-determinist backward scatter_nd,MXNET-801,2.0
gradient function return enough gradient,MXNET-799,5.0
dangl output dtype ! = float32 : gradient comput fail,MXNET-798,1.0
prelu activ comput fault expand_shap function,MXNET-797,5.0
dropout may mask valu even ratio=0.0,MXNET-792,1.0
gluon rais error user call nd.waital ( ),MXNET-579,5.0
3d dilat support work,MXNET-578,8.0
crash run gluon image-classification.pi exampl float16,MXNET-577,5.0
symbol .json file compat .param file gener sinc mxnet 1.2,MXNET-576,5.0
"reactiv drain agent leav agent drain state . when reactiv agent 's drain state , master eras drain map , eras estim drain time . howev , n't send messag agent , agent still drain wait task termin , stay state , ultim make task get launch get drop due agent still drain state . seem like either : * disallow user reactiv still drain , * send messag agent , agent move drain .",MESOS-10096,3.0
"master 's agent drain vlog print incorrect task count . thi logic print framework count map rather task count : http : //github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp # l6318-l6319 { code } // check agent task run oper pend . ( ! slave- > pendingtasks.empti ( ) || ! slave- > tasks.empti ( ) || ! slave- > operations.empti ( ) ) { vlog ( 1 ) < < `` drain agent `` < < slaveid < < `` `` < < slave- > pendingtasks.s ( ) < < `` pend task , `` < < slave- > tasks.siz ( ) < < `` task , `` < < slave- > operations.s ( ) < < `` oper '' ; return ; } { code } sinc { { hashmap < frameworkid , hashmap < taskid , task > > } } .",MESOS-10094,1.0
"libprocess properli escap subprocess argument string window when run test meso window , i discov follow command would execut success pass docker container { { taskinfo.command } } : { noformat } python -c `` print ( 'hello world ' ) '' { noformat } the follow error found task sandbox : { noformat } file `` < string > '' , line 1 `` print ( 'hello ^ syntaxerror : eol scan string liter { noformat }",MESOS-10093,2.0
"csi plugin report duplic volum crash agent . the csi spec requir volum uniqu identifi id , thu slrp current assum { { listvolum } } call return duplic volum . howev , slrp use non-conform csi plugin report duplic volum , volum would corrupt slrp checkpoint caus agent crash next reconcili : { noformat } f0829 07:13:55.171332 12721 provider.cpp:1089 ] check fail : ! checkpointedmap.contain ( resource.disk ( ) .sourc ( ) .id ( ) ) { noformat } mesos-9254 introduc period reconcili make problem much easier manifest .",MESOS-9956,2.0
"master handl return unreach agent draining/deactiv the master two code path handl agent reregistr messag , one culmin { { master : :___reregisterslav } } { { master : : } } { { __reregisterslav } } . the two path continu . look like miss double-underscor case initi implement . thi path unreach agent take , when/if come back cluster . the result unreach agent mark drain , get sent appropri messag unless forc reregist ( i.e . restart manual ) .",MESOS-9934,3.0
slavetest.drainingagentrejectlaunch flaki we saw { { slavetest.drainingagentrejectlaunch } } fail repeatedli asf jenkin ci . { noformat } .. / .. /src/tests/slave_tests.cpp:12408 : failur fail wait 15sec runningupdate2 { noformat },MESOS-9895,1.0
"mesos.updateframeworkv0test.suppressedrol flaki . observ ci , log attach . { noformat } mesos-ec2-ubuntu-14.04-ssl.mesos.updateframeworkv0test.suppressedrol ( updateframeworkv0test ) error messag .. / .. /src/tests/master/update_framework_tests.cpp:1117 mock function call time expect - return directli . function call : agentad ( @ 0x7fb254001c40 32-byte object < 90-7a 6c-85 b2-7f 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 f0-85 00-54 b2-7f 00-00 > ) expect : call actual : call twice - over-satur activ stacktrac .. / .. /src/tests/master/update_framework_tests.cpp:1117 mock function call time expect - return directli . function call : agentad ( @ 0x7fb254001c40 32-byte object < 90-7a 6c-85 b2-7f 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 f0-85 00-54 b2-7f 00-00 > ) expect : call actual : call twice - over-satur activ { noformat }",MESOS-9882,1.0
"meso respond correctli oper fail for test persist volum { { operation_failed/error } } feedback , sshed mesos-ag made unabl creat subdirectori { { /srv/mesos/work/volum } } , howev , meso respond oper fail respons . instead , receiv { { operation_finish } } feedback . step recreat issu : 1 . ssh magent . 2 . make imposs creat persist volum ( expect agent crash reregist , master releas oper { { operation_drop } } ) : * cd /srv/mesos/work ( n't exist mkdir /srv/mesos/work/volum ) * chattr -rv +i volum ( subdirectori creat ) 3 . launch servic persist volum constraint use magent modifi . log schedul receiv ` operation_finish ` : ( also see screenshot ) 2019-06-27 21:57:11.879 [ 12768651|rdar : //12768651 ] [ jarvis-mesos-dispatcher-105 ] info c.a.j.s.servicepodinst - store operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 feedback=operation_finish podinstanceid=4g3k02s1gjb0q serviceid=yifan-badagents-1 * 2019-06-27 21:55:23 : task reach state task_fail meso reason : reason_container_launch_fail meso messag : fail launch contain : fail chang ownership persist volum '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90 ' uid 264 gid 264 : no file directori",MESOS-9875,8.0
simultan adding/remov role framework 's role suppress role crash master . call update_framework new role ad 'frameworkinfo.rol ` ` suppressed_rol ` crash master . the first place n't expect increas ` suppress ` alloc metric : [ http : //github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/hierarchical.cpp # l507 ] [ http : //github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/metrics.cpp # l255 ] probabl similar place . ad new role suppress state via re-subscrib also trigger bug - n't check,MESOS-9870,5.0
"make pushgaug support float point stat . current , pushgaug model counter . thu support float point stat . thi prevent mani exist pullgaug use . we need add support float point stat .",MESOS-9861,1.0
"reviv call specifi role ( ) clear filter role framework . as point [ ~asekretenko ] , reviv implement alloc incorrectli clear declin filter framework 's role , rather specifi reviv call : http : //github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp # l1392 thi clear filter role specifi reviv call .",MESOS-9856,3.0
/role endpoint return guarante limit .,MESOS-9854,2.0
migrat alloc metric pushgaug . we migrat metric master actor use pushgaug instead pullgaug better perform .,MESOS-9851,5.0
"master report disconnect resourc provid . mesos-9384 attempt make master garbage-collect disconnect resourc provid . howev , disconnect resourc provid none connect one chang , follow code snippet would make master ignor agent updat skip garbag collect : http : //github.com/apache/mesos/blob/2ae1296c668686d234be92b00bd7abbc0a6194b0/src/master/master.cpp # l8186-l8234 the condit ignor agent updat trigger one follow condit : 1 . the resourc provid resourc , agent 's total resourc remain . 2 . when agent restart reregist , resourc provid resourc reset . as result , master still keep record disconnect resourc provid report .",MESOS-9831,2.0
"do n't use revers dn hostnam valid upon connect first resolv hostnam forget http : //github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp # l1462-l1504 later use revers dn remot address get back hostnam http : //github.com/apache/mesos/blob/4708c2a368e12a89669135f47777d0dd05d9b0b2/3rdparty/libprocess/src/posix/libevent/libevent_ssl_socket.cpp # l548-l556 verifi server certif * * . instead , verifi server certif hostnam use client initi connect .",MESOS-9811,5.0
"memori leak caus infinit chain futur ` uridiskprofileadaptor ` . befor mesos-8906 , { { uridiskprofileadaptor } } updat promis watcher poll profil matrix becom larger size , prevent follow code { { watch } } function creat infinit chain futur profil matrix keep : http : //github.com/apache/mesos/blob/fa410f2fb8efb988590f4da2d4cfffbb2ce70637/src/resource_provider/storage/uri_disk_profile_adaptor.cpp # l159-l160 howev , patch mesos-8906 remov size check { { notifi } } function allow profil selector updat . as result , watch function call , return futur chain new promis everi time poll made , henc creat memori leak . a jemalloc call graph 2hr trace attach .",MESOS-9803,2.0
"unpublish volum fail publish crash agent csi v1 . the csi v1 volum manag recov fail ` publishvolum ` call ` unpublishvolum ` , mistakenli assum target path , suppos creat csi plugin success publish , alway exist . if volum publish fail , subsequ unpublish would crash agent follow messag : { noformat } f0412 20:20:12.254420 7540 v1_volume_manager.cpp:1161 ] check fail : os : :exist ( targetpath ) { noformat }",MESOS-9729,1.0
"agent crash slrp recov drop oper . mesos-9537 fix persist drop oper slrp , recoveri codepath n't account : [ http : //github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp # l1278 ] which caus agent crash follow messag slrp recoveri : { noformat } reach unreach statement /pkg/src/mesos/src/resource_provider/storage/provider.cpp:1283 { noformat }",MESOS-9661,1.0
"resourc provid manag assum oper trigger framework when agent tri appli oper resourc provid resourc , invok { { resourceprovidermanag : :applyoper } } turn invok { { resourceprovidermanagerprocess : :applyoper } } . that function current assum receiv messag contain valid { { frameworkid } } , { noformat } void resourceprovidermanagerprocess : :applyoper ( const applyoperationmessag & messag ) { const offer : :oper & oper = message.operation_info ( ) ; const frameworkid & frameworkid = message.framework_id ( ) ; // ` framework_id ` ` option ` . { noformat } sinc { { frameworkid } } trivial proto type , instead one { { requir } } field { { valu } } , messag compos { { frameworkid } } serial lead failur turn trigger { { check } } failur agent 's function interfac manag . a typic scenario would want support oper api call destroy leftov persist volum reserv .",MESOS-9612,5.0
"remov resourc provid consum break resourc publish . current , agent publish resourc consid `` use '' via resourc provid manag whenev ask publish subport . if resourc provid activ user ( e.g. , task even executor ) remov , user stay around fail _ani resourc publishing_ node sinc `` use '' resourc provid subscrib . we either updat agent code delta , provid workaround effect resourc provid manag .",MESOS-9607,2.0
"statu updat stream oper affect agent default resourc store `` meta/slaves/ < slave_id > /operations/ '' the stream current creat { { meta/operations/ } } recov { { meta/slaves/latest } } n't exist . after discuss [ ~greggomann ] [ ~kaysoki ] , agre creat { { meta/slaves/ < slave_id > /operations/ } } instead . note : n't forget add correspond entri ascii draw { { slave/paths.hpp } .",MESOS-9597,2.0
"oper statu updat stream properli garbag collect . after success handl acknowledg termin oper statu updat oper affect agent 's default resourc , agent garbag collect correspond oper statu updat stream .",MESOS-9574,2.0
"agent tri recov oper statu updat stream n't creat yet . if agent fail checkpoint new oper oper statu updat stream creat , recoveri process fail . thi happen agent tri recov oper statu updat stream even n't creat yet . in order prevent recoveri failur , agent obtain id stream recov walk directori oper statu updat stream store . the agent also garbag collect stream checkpoint state n't contain correspond oper .",MESOS-9573,2.0
"slrp clean mount directori destroy mount disk . when stage publish csi volum , slrp creat follow mount point oper : { noformat } < work_dir > /csi/ < rp_type > / < rp_name > /mounts/ < volume_id > /stage < work_dir > /csi/ < rp_type > / < rp_name > /mounts/ < volume_id > /target { noformat } these directori clean volum unpublished/unstag . howev , parent directori , namli { { < work_dir > /csi/ < rp_type > / < rp_name > /mounts/ < volume_id > } } never clean .",MESOS-9568,2.0
operation_unreach operation_gone_by_oper updat n't includ agent/rp id,MESOS-9559,2.0
"oper leak framework struct agent remov current , agent remov master , oper remov { { framework } } struct . we ensur occur case .",MESOS-9557,2.0
"slrp clean destroy persist volum . when persist volum creat { { root } } disk destroy , agent clean data : http : //github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp # l4397 howev , case pv slrp disk . the agent reli slrp cleanup : http : //github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp # l4472 but slrp simpli updat metadata noth : http : //github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/resource_provider/storage/provider.cpp # l2805 thi would lead data leakag framework call ` create_disk ` unreserv .",MESOS-9544,5.0
"hierarch alloc check failur oper shutdown framework finish when non-specul oper like e.g. , { { create_disk } } becom termin origin framework torn , run assert failur alloc . { noformat } i0129 11:55:35.764394 57857 master.cpp:11373 ] updat state oper 'oper ' ( uuid : 10a782bd-9e60-42da-90d6-c00997a25645 ) framework a4d0499b-c0d3-4abf-8458-73e595d061ce-0000 ( latest state : operation_pend , statu updat state : operation_finish ) f0129 11:55:35.764744 57925 hierarchical.cpp:834 ] check fail : frameworks.contain ( frameworkid ) { noformat } with non-specul oper like e.g. , { { create_disk } } becam possibl oper outliv origin framework . thi possibl specul oper like { { reserv } } alway appli immedi master . the master take account , instead uncondit call { { alloc : :updatealloc } } assert framework still known alloc . reproduc : * regist framework master . * add master resourc provid . * let framework trigger non-specul oper like { { create_disk . } } * tear framework termin oper statu updat reach master ; caus master e.g. , remov framework alloc . * let termin , success oper statu updat reach master * 💥 to solv cleanup lifetim oper . sinc oper outliv framework ( unlik e.g. , task ) , probabl need differ approach .",MESOS-9542,5.0
"slrp send inconsist statu updat drop oper . the bug manifest follow scenario : 1 . upon receiv profil updat , slrp send { { update_st } } agent new resourc version . 2 . at time , agent send { { apply_oper } } slrp origin resourc version . 3 . the slrp ask statu updat manag ( sum ) repli { { operation_drop } } framework resourc version mismatch . the statu updat requir ack . then , simpli discard oper ( i.e. , bookkeep ) . 4 . the agent find miss oper { { update_st } } send { { reconcile_oper } } . 5 . the slrp ask sum repli { { operation_drop } } agent ( without framework id set ) longer know oper . 6 . the sum return error latter { { operation_drop } } inconsist earlier one sinc framework id .",MESOS-9537,3.0
"cniisolatortest.root_cleanupafterreboot flaki . { noformat } error messag .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:2685 mock function call time expect - return directli . function call : statusupd ( 0x7fffc7c05aa0 , @ 0x7fe637918430 136-byte object < 80-24 29-45 e6-7f 00-00 00-00 00-00 00-00 00-00 3e-e8 00-00 00-00 00-00 00-b8 0e-20 f0-55 00-00 c0-03 07-18 e6-7f 00-00 20-17 05-18 e6-7f 00-00 10-50 05-18 e6-7f 00-00 50-d1 04-18 e6-7f 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 f0-89 16-e9 58-2b d7-41 00-00 00-00 01-00 00-00 18-00 00-00 0b-00 00-00 > ) expect : call 3 time actual : call 4 time - over-satur activ stacktrac .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:2685 mock function call time expect - return directli . function call : statusupd ( 0x7fffc7c05aa0 , @ 0x7fe637918430 136-byte object < 80-24 29-45 e6-7f 00-00 00-00 00-00 00-00 00-00 3e-e8 00-00 00-00 00-00 00-b8 0e-20 f0-55 00-00 c0-03 07-18 e6-7f 00-00 20-17 05-18 e6-7f 00-00 10-50 05-18 e6-7f 00-00 50-d1 04-18 e6-7f 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 f0-89 16-e9 58-2b d7-41 00-00 00-00 01-00 00-00 18-00 00-00 0b-00 00-00 > ) expect : call 3 time actual : call 4 time - over-satur activ { noformat } it commit http : //github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29",MESOS-9533,2.0
"slrp treat grpc timeout non-termin error , instead report operation_fail . 1. framework execut create_disk oper . 2 . the slrp issu createvolum rpc plugin 3 . the rpc call time 4 . the agent/slrp translat non-termin grpc timeout error ( deadlineexceed ) `` createvolum '' call operation_fail , termin . 5. framework receiv * termin * operation_fail statu , execut anoth create_disk oper . 6 . the second create_disk oper timeout . 7 . the first create_disk oper actual complet plugin , unbeknownst slrp . 8 . there 's orphan volum storag system one track . propos solut : slrp make intellig decis non-termin grpc error . for exampl , timeout like expect potenti long-run storag oper consid termin . in case , slrp not report operation_fail instead re-issu * * * * ( idempot ) createvolum call plugin ascertain statu request volum creation . agent log 3 orphan vol : { code } [ jdefelic @ ec101 dcos-46889 ] $ grep -e 3bd1a1a9-43d3-485c-9275-59cebd64b07c agent.log jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:10:27.896306 13189 provider.cpp:1548 ] receiv create_disk oper 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ( uuid : 3bd1a1a9-43d3-485c-9275-59cebd64b07c ) jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : e0109 11:11:27.904057 13190 provider.cpp:1605 ] fail appli oper ( uuid : 3bd1a1a9-43d3-485c-9275-59cebd64b07c ) : deadlin exceed jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.904058 13192 status_update_manager_process.hpp:152 ] receiv oper statu updat operation_fail ( statu uuid : 8c1ddad1-4adb-4df5-91fe-235d265a71d8 ) oper uuid 3bd1a1a9-43d3-485c-9275-59cebd64b07c ( framework-suppli id 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.904331 13192 status_update_manager_process.hpp:929 ] checkpoint updat oper statu updat operation_fail ( statu uuid : 8c1ddad1-4adb-4df5-91fe-235d265a71d8 ) oper uuid 3bd1a1a9-43d3-485c-9275-59cebd64b07c ( framework-suppli id 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.947286 13189 slave.cpp:7696 ] handl resourc provid messag 'update_operation_statu : ( uuid : 3bd1a1a9-43d3-485c-9275-59cebd64b07c ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) ' jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.947376 13189 slave.cpp:8034 ] updat state oper 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ( uuid : 3bd1a1a9-43d3-485c-9275-59cebd64b07c ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.947407 13189 slave.cpp:7890 ] forward statu updat oper 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ( operation_uuid : 3bd1a1a9-43d3-485c-9275-59cebd64b07c ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.952689 13193 status_update_manager_process.hpp:252 ] receiv oper statu updat acknowledg ( uuid : 8c1ddad1-4adb-4df5-91fe-235d265a71d8 ) stream 3bd1a1a9-43d3-485c-9275-59cebd64b07c jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.952725 13193 status_update_manager_process.hpp:929 ] checkpoint ack oper statu updat operation_fail ( statu uuid : 8c1ddad1-4adb-4df5-91fe-235d265a71d8 ) oper uuid 3bd1a1a9-43d3-485c-9275-59cebd64b07c ( framework-suppli id 'a1bdfrehy4zlsnpzbdrzp1h-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 [ jdefelic @ ec101 dcos-46889 ] $ grep -e 4acf1495-1a36-4939-a71b-75ca5aa73657 agent.log jan 09 11:10:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:10:28.452811 13192 provider.cpp:1548 ] receiv create_disk oper 'a5mu6jqxypt9iwxm75cwuho-0 ' ( uuid : 4acf1495-1a36-4939-a71b-75ca5aa73657 ) jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : e0109 11:11:28.460510 13190 provider.cpp:1605 ] fail appli oper ( uuid : 4acf1495-1a36-4939-a71b-75ca5aa73657 ) : deadlin exceed jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.460511 13186 status_update_manager_process.hpp:152 ] receiv oper statu updat operation_fail ( statu uuid : e810608b-58ac-47eb-bf19-9abcca6907a2 ) oper uuid 4acf1495-1a36-4939-a71b-75ca5aa73657 ( framework-suppli id 'a5mu6jqxypt9iwxm75cwuho-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.460793 13186 status_update_manager_process.hpp:929 ] checkpoint updat oper statu updat operation_fail ( statu uuid : e810608b-58ac-47eb-bf19-9abcca6907a2 ) oper uuid 4acf1495-1a36-4939-a71b-75ca5aa73657 ( framework-suppli id 'a5mu6jqxypt9iwxm75cwuho-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.504062 13191 slave.cpp:7696 ] handl resourc provid messag 'update_operation_statu : ( uuid : 4acf1495-1a36-4939-a71b-75ca5aa73657 ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) ' jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.504133 13191 slave.cpp:8034 ] updat state oper 'a5mu6jqxypt9iwxm75cwuho-0 ' ( uuid : 4acf1495-1a36-4939-a71b-75ca5aa73657 ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.504159 13191 slave.cpp:7890 ] forward statu updat oper 'a5mu6jqxypt9iwxm75cwuho-0 ' ( operation_uuid : 4acf1495-1a36-4939-a71b-75ca5aa73657 ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.509495 13194 status_update_manager_process.hpp:252 ] receiv oper statu updat acknowledg ( uuid : e810608b-58ac-47eb-bf19-9abcca6907a2 ) stream 4acf1495-1a36-4939-a71b-75ca5aa73657 jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:28.509521 13194 status_update_manager_process.hpp:929 ] checkpoint ack oper statu updat operation_fail ( statu uuid : e810608b-58ac-47eb-bf19-9abcca6907a2 ) oper uuid 4acf1495-1a36-4939-a71b-75ca5aa73657 ( framework-suppli id 'a5mu6jqxypt9iwxm75cwuho-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 [ jdefelic @ ec101 dcos-46889 ] $ grep -e ca2bed2f-480e-4d35-af9e-1161a44c5b9b agent.log jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:10:27.458933 13186 provider.cpp:1548 ] receiv create_disk oper 'a3avaf97ushu6ziiphygdry-0 ' ( uuid : ca2bed2f-480e-4d35-af9e-1161a44c5b9b ) jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : e0109 11:11:27.469853 13189 provider.cpp:1605 ] fail appli oper ( uuid : ca2bed2f-480e-4d35-af9e-1161a44c5b9b ) : deadlin exceed jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.469859 13186 status_update_manager_process.hpp:152 ] receiv oper statu updat operation_fail ( statu uuid : bb7807e8-dc2f-4f64-b611-d24a1e559317 ) oper uuid ca2bed2f-480e-4d35-af9e-1161a44c5b9b ( framework-suppli id 'a3avaf97ushu6ziiphygdry-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.470120 13186 status_update_manager_process.hpp:929 ] checkpoint updat oper statu updat operation_fail ( statu uuid : bb7807e8-dc2f-4f64-b611-d24a1e559317 ) oper uuid ca2bed2f-480e-4d35-af9e-1161a44c5b9b ( framework-suppli id 'a3avaf97ushu6ziiphygdry-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.513059 13192 slave.cpp:7696 ] handl resourc provid messag 'update_operation_statu : ( uuid : ca2bed2f-480e-4d35-af9e-1161a44c5b9b ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) ' jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.513129 13192 slave.cpp:8034 ] updat state oper 'a3avaf97ushu6ziiphygdry-0 ' ( uuid : ca2bed2f-480e-4d35-af9e-1161a44c5b9b ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ( latest state : operation_fail , statu updat state : operation_fail ) jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.513147 13192 slave.cpp:7890 ] forward statu updat oper 'a3avaf97ushu6ziiphygdry-0 ' ( operation_uuid : ca2bed2f-480e-4d35-af9e-1161a44c5b9b ) framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.518623 13191 status_update_manager_process.hpp:252 ] receiv oper statu updat acknowledg ( uuid : bb7807e8-dc2f-4f64-b611-d24a1e559317 ) stream ca2bed2f-480e-4d35-af9e-1161a44c5b9b jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.intern mesos-ag [ 13170 ] : i0109 11:11:27.518656 13191 status_update_manager_process.hpp:929 ] checkpoint ack oper statu updat operation_fail ( statu uuid : bb7807e8-dc2f-4f64-b611-d24a1e559317 ) oper uuid ca2bed2f-480e-4d35-af9e-1161a44c5b9b ( framework-suppli id 'a3avaf97ushu6ziiphygdry-0 ' ) framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 ' agent c0b7cc7e-db35-450d-bf25-9e3183a07161-s1 { code }",MESOS-9517,8.0
"ioswitchboard cleanup could get stuck due fd leak race . our check contain got stuck destroy turn stuck parent contain . it block i/o switchboard cleanup : 1223 18:04:41.000000 16269 switchboard.cpp:814 ] send sigterm i/o switchboard server ( pid : 62854 ) sinc contain 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09 destroy .... 1227 04:45:38.000000 5189 switchboard.cpp:916 ] i/o switchboard server process contain 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09 termin ( status=n/a ) note timestamp . * root caus : * fundament , caus race * .discard ( ) * trigger check contain timeout iosb extract containerio object . thi race could expos overloaded/slow agent process . pleas see race trigger : # right iosb server process run , check contain time checker process return failur , would close http connect agent . # from agent side , connect break , handler trigger discard return futur result containerizer- > launch ( ) 's futur transit discard state . # in container , discard state propag back iosb prepar ( ) , stop continu * extract containerio * ( impli object clean fd ( one end pipe creat iosb ) close destructor ) . # agent start destroy contain due discard launch result , ask iosb cleanup contain . # iosb server still run , agent send sigterm . # sigterm handler unblock iosb redirect ( redirect stdout/stderr contain logger exit ) . # io : :redirect ( ) call io : :splice ( ) read end pipe forev . thi issu * easi reproduc unless * busi agent , timeout happen exactli * after * iosb server run * befor * iosb extract containerio .",MESOS-9502,8.0
slrp set rp id produc operationstatu .,MESOS-9479,1.0
"master may send ` framework_upd ` new framework id oper api . in oper stream api , master send { { framework_ad } } framework subscrib id : [ http : //github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp # l2653-l2679 ] [ http : //github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp # l2951-l2988 ] that mean master failov , framework recov agent subscrib id , { { framework_upd } } framework id previous unknown sent subscrib .",MESOS-9470,3.0
"complet framework updat stream may retri forev sinc agent/rp current gc oper statu updat stream framework torn , 's possibl activ updat stream associ complet framework may remain continu retri forev . we add mechan complet stream framework becom complet . a coupl option come discuss : * have master acknowledg updat associ complet framework . note sinc complet framework current track master memori , master failov could prevent work perfectli . * extend rp api allow gc particular updat stream , agent gc stream associ framework receiv { { shutdownframeworkmessag } } . thi would also requir addit new method statu updat manag .",MESOS-9434,2.0
"check failur ` storagelocalresourceproviderprocess : :applycreatedisk ` . observ follow agent failur one stage cluster : { noformat } nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.641331 26684 http.cpp:1799 ] process get_ag call nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.650429 26679 http.cpp:1117 ] http post /slave ( 1 ) /api/v1/resource_provid 172.31.8.65:57790 nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.650629 26679 manager.cpp:672 ] subscrib resourc provid { `` attribut '' : [ { `` name '' : '' lvm-vg-name '' , '' text '' : { `` valu '' : '' lvm-double-1540383639 '' } , '' type '' : '' scalar '' } , { `` name '' : '' dss-asset-id '' , '' text '' : { `` valu '' : '' 6abzv6w2drk4ygcir3icvo '' } , '' type '' : '' scalar '' } ] , '' default_reserv '' : [ { `` princip '' : '' storage-princip '' , '' role '' : '' dcos-storag '' , '' type '' : '' dynam '' } ] , '' id '' : { `` valu '' : '' 8326e931-41f2-4f45-9174-13fe35c19300 '' } , '' name '' : '' rp_6abzv6w2drk4ygcir3icvo '' , '' storag '' : { `` plugin '' : { `` contain '' : [ { `` command '' : { `` environ '' : { `` variabl '' : [ { `` name '' : '' path '' , '' type '' : '' valu '' , '' valu '' : '' /opt/mesosphere/bin : /usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin '' } , { `` name '' : '' ld_library_path '' , '' type '' : '' valu '' , '' valu '' : '' /opt/mesosphere/lib '' } , { `` name '' : '' container_logger_destination_typ '' , '' type '' : '' valu '' , '' valu '' : '' journald+logrot '' } , { `` name '' : '' container_logger_extra_label '' , '' type '' : '' valu '' , '' valu '' : '' { \ '' csi_plugin\ '' : \ '' csilvm\ '' } '' } ] } , '' shell '' : true , '' uri '' : [ { `` execut '' : true , '' extract '' : fals , '' valu '' : '' < possibly-sensit > '' } ] , '' valu '' : '' echo \ '' * : * rwm\ '' > /sys/fs/cgroup/devic ` cat /proc/self/cgroup | grep devic | cut -d : -f 3 ` /devices.allow ; exec ./csilvm -devices=/dev/xvdk , /dev/xvdj -volume-group=lvm-double-1540383639 -unix-addr-env=csi_endpoint -tag=6abzv6w2drk4ygcir3icvo '' } , '' resourc '' : [ { `` name '' : '' cpu '' , '' scalar '' : { `` valu '' :0.1 } , '' type '' : '' scalar '' } , { `` name '' : '' mem '' , '' scalar '' : { `` valu '' :128.0 } , '' type '' : '' scalar '' } , { `` name '' : '' disk '' , '' scalar '' : { `` valu '' :10.0 } , '' type '' : '' scalar '' } ] , '' servic '' : [ `` controller_servic '' , '' node_servic '' ] } ] , '' name '' : '' plugin_6abzv6w2drk4ygcir3icvo '' , '' type '' : '' io.mesosphere.dcos.storage.csilvm '' } } , '' type '' : '' org.apache.mesos.rp.local.storag '' } nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.690474 26685 provider.cpp:546 ] receiv subscrib event nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.690521 26685 provider.cpp:1492 ] subscrib id 8326e931-41f2-4f45-9174-13fe35c19300 nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : i1116 11:57:24.690657 26681 status_update_manager_process.hpp:314 ] recov oper statu updat manag nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : f1116 11:57:24.691496 26682 provider.cpp:3121 ] check fail : resource.disk ( ) .sourc ( ) .has_profil ( ) ! = resource.disk ( ) .sourc ( ) .has_id ( ) ( 1 vs. 1 ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : * * * check failur stack trace : * * * nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb099e9fd googl : :logmessag : :fail ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb09a082d googl : :logmessag : :sendtolog ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb099e5ec googl : :logmessag : :flush ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb09a1129 googl : :logmessagefat : :~logmessagefat ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb01654ca meso : :intern : :storagelocalresourceproviderprocess : :applycreatedisk ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb017c683 meso : :intern : :storagelocalresourceproviderprocess : :_applyoper ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb017d64a _zzn5mesos8internal35storagelocalresourceproviderprocess26reconcileoperationstatusesevenkulrkns0_26statusupdatemanagerprocessin2id4uuidens0_27updateoperationstatusrecordens0_28updateoperationstatusmessageee5stateee_clesa_ nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb017dd21 _zno6lambda12callableonceifn7process6futurei7nothingeevee10callablefnins_8internal7partializn5mesos8internal35storagelocalresourceproviderprocess26reconcileoperationstatuseseveulrknsb_26statusupdatemanagerprocessin2id4uuidensb_27updateoperationstatusrecordensb_28updateoperationstatusmessageee5stateee_isj_eeeeclev nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecafa0ce97 _zno6lambda12callableonceifvpn7process11processbaseeee10callablefnins_8internal7partializns1_8internal8dispatchins1_6futurei7nothingeeeclins0_ifsd_veeeeesd_rkns1_4upideot_eulst10unique_ptrins1_7promiseisc_eest14default_deleteisp_eeosh_s3_e_jss_sh_st12_placeholderili1eeeeeecleos3_ nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb08eec51 process : :processbas : :consum ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb09056cc process : :processmanag : :resum ( ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecb090b186 _znst6thread5_implist12_bind_simpleifzn7process14processmanager12init_threadseveulve_veee6_m_runev nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecad5d8070 ( unknown ) nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecacdf6e25 start_thread nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.r mesos-ag [ 26663 ] : @ 0x7fecacb20bad __clone { noformat }",MESOS-9395,5.0
"ucr contain launch stuck provis imag fetch . we observ meso container stuck provis launch meso contain use docker imag : ` kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9 ` : the imag pull never finish . insuffici imag content still imag store stage directori /var/lib/mesos/slave/store/docker/staging/eglyqo , forev . { noformat } ok-22:50:06-root @ int-agent89-mwst9 : /var/lib/mesos/slave/store/docker/staging/eglyqo # ls -alh total 1.1g drwx -- -- -- . 2 root root 4.0k oct 15 13:02 . drwxr-xr-x . 3 root root 20 oct 15 22:40 .. -rw-r -- r -- . 1 root root 59k oct 15 13:02 manifest -rw-r -- r -- . 1 root root 2.6k oct 15 13:02 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63 -rw-r -- r -- . 1 root root 440 oct 15 13:02 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66 -rw-r -- r -- . 1 root root 248 oct 15 13:02 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a -rw-r -- r -- . 1 root root 240 oct 15 13:02 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb -rw-r -- r -- . 1 root root 562 oct 15 13:02 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1 -rw-r -- r -- . 1 root root 11m oct 15 13:02 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d -rw-r -- r -- . 1 root root 130 oct 15 13:02 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50 -rw-r -- r -- . 1 root root 176 oct 15 13:02 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312 -rw-r -- r -- . 1 root root 380 oct 15 13:02 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a -rw-r -- r -- . 1 root root 71m oct 15 13:02 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604 -rw-r -- r -- . 1 root root 1.4k oct 15 13:02 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1 -rw-r -- r -- . 1 root root 653k oct 15 13:02 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01 -rw-r -- r -- . 1 root root 184 oct 15 13:02 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90 -rw-r -- r -- . 1 root root 366k oct 15 13:02 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94 -rw-r -- r -- . 1 root root 23k oct 15 13:02 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2 -rw-r -- r -- . 1 root root 384m oct 15 13:02 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0 -rw-r -- r -- . 1 root root 1.5k oct 15 13:02 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc -rw-r -- r -- . 1 root root 48m oct 15 13:02 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c -rw-r -- r -- . 1 root root 30m oct 15 13:02 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf -rw-r -- r -- . 1 root root 306m oct 15 13:02 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f -rw-r -- r -- . 1 root root 435 oct 15 13:02 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d -rw-r -- r -- . 1 root root 5.5k oct 15 13:02 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5 -rw-r -- r -- . 1 root root 39m oct 15 13:02 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10 -rw-r -- r -- . 1 root root 615 oct 15 13:02 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b -rw-r -- r -- . 1 root root 712 oct 15 13:02 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2 -rw-r -- r -- . 1 root root 12k oct 15 13:02 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728 -rw-r -- r -- . 1 root root 861 oct 15 13:02 sha256 : a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352 -rw-r -- r -- . 1 root root 32 oct 15 13:02 sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 -rw-r -- r -- . 1 root root 266k oct 15 13:02 sha256 : b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276 -rw-r -- r -- . 1 root root 1.6k oct 15 13:02 sha256 : b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74 -rw-r -- r -- . 1 root root 4.2m oct 15 13:02 sha256 : b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c -rw-r -- r -- . 1 root root 1.1k oct 15 13:02 sha256 : b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58 -rw-r -- r -- . 1 root root 2.8k oct 15 13:02 sha256 : b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747 -rw-r -- r -- . 1 root root 6.3m oct 15 13:02 sha256 : b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5 -rw-r -- r -- . 1 root root 1.8k oct 15 13:02 sha256 : c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215 -rw-r -- r -- . 1 root root 4.1k oct 15 13:02 sha256 : cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac -rw-r -- r -- . 1 root root 355 oct 15 13:02 sha256 : cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87 -rw-r -- r -- . 1 root root 165m oct 15 13:02 sha256 : d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3 -rw-r -- r -- . 1 root root 872k oct 15 13:02 sha256 : da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac -rw-r -- r -- . 1 root root 431 oct 15 13:02 sha256 : edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a -rw-r -- r -- . 1 root root 19m oct 15 13:02 sha256 : f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6 -rw-r -- r -- . 1 root root 198 oct 15 13:02 sha256 : f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec -rw-r -- r -- . 1 root root 550k oct 15 13:02 sha256 : f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320 -rw-r -- r -- . 1 root root 676 oct 15 13:02 sha256 : fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa { noformat } it clear yet sha pull finish , use imag anoth empti machin ucr . the machin contain run correctli , follow stage directori move layer dir : { noformat } -rw-r -- r -- . 1 root root 2.6k oct 15 18:03 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63 -rw-r -- r -- . 1 root root 440 oct 15 18:03 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66 -rw-r -- r -- . 1 root root 248 oct 15 18:03 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a -rw-r -- r -- . 1 root root 240 oct 15 18:03 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb -rw-r -- r -- . 1 root root 562 oct 15 18:03 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1 -rw-r -- r -- . 1 root root 11m oct 15 18:03 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d -rw-r -- r -- . 1 root root 130 oct 15 18:03 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50 -rw-r -- r -- . 1 root root 176 oct 15 18:03 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312 -rw-r -- r -- . 1 root root 380 oct 15 18:03 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a -rw-r -- r -- . 1 root root 71m oct 15 18:03 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604 -rw-r -- r -- . 1 root root 1.4k oct 15 18:03 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1 -rw-r -- r -- . 1 root root 653k oct 15 18:03 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01 -rw-r -- r -- . 1 root root 184 oct 15 18:03 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90 -rw-r -- r -- . 1 root root 366k oct 15 18:03 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94 -rw-r -- r -- . 1 root root 23k oct 15 18:03 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2 -rw-r -- r -- . 1 root root 122m oct 15 18:03 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0 -rw-r -- r -- . 1 root root 1.5k oct 15 18:03 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc -rw-r -- r -- . 1 root root 48m oct 15 18:03 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c -rw-r -- r -- . 1 root root 30m oct 15 18:03 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf -rw-r -- r -- . 1 root root 92m oct 15 18:03 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f -rw-r -- r -- . 1 root root 435 oct 15 18:03 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d -rw-r -- r -- . 1 root root 5.5k oct 15 18:03 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5 -rw-r -- r -- . 1 root root 39m oct 15 18:03 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10 -rw-r -- r -- . 1 root root 615 oct 15 18:03 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b -rw-r -- r -- . 1 root root 712 oct 15 18:03 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2 -rw-r -- r -- . 1 root root 12k oct 15 18:03 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728 -rw-r -- r -- . 1 root root 861 oct 15 18:03 sha256 : a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352 -rw-r -- r -- . 1 root root 32 oct 15 18:03 sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 -rw-r -- r -- . 1 root root 266k oct 15 18:03 sha256 : b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276 -rw-r -- r -- . 1 root root 1.6k oct 15 18:03 sha256 : b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74 -rw-r -- r -- . 1 root root 4.2m oct 15 18:03 sha256 : b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c -rw-r -- r -- . 1 root root 1.1k oct 15 18:03 sha256 : b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58 -rw-r -- r -- . 1 root root 2.8k oct 15 18:03 sha256 : b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747 -rw-r -- r -- . 1 root root 6.3m oct 15 18:03 sha256 : b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5 -rw-r -- r -- . 1 root root 1.8k oct 15 18:03 sha256 : c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215 -rw-r -- r -- . 1 root root 44m oct 15 18:03 sha256 : c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b -rw-r -- r -- . 1 root root 4.1k oct 15 18:03 sha256 : cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac -rw-r -- r -- . 1 root root 355 oct 15 18:03 sha256 : cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87 -rw-r -- r -- . 1 root root 82m oct 15 18:03 sha256 : d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3 -rw-r -- r -- . 1 root root 872k oct 15 18:03 sha256 : da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac -rw-r -- r -- . 1 root root 431 oct 15 18:03 sha256 : edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a -rw-r -- r -- . 1 root root 19m oct 15 18:03 sha256 : f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6 -rw-r -- r -- . 1 root root 198 oct 15 18:03 sha256 : f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec -rw-r -- r -- . 1 root root 550k oct 15 18:03 sha256 : f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320 -rw-r -- r -- . 1 root root 676 oct 15 18:03 sha256 : fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa { noformat } by compar two case , see one layer ` 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324 ` miss problemat agent node , last layer fetch . here manifest refer : { noformat } ok-17:42:20-root @ int-agent89-mwst9 : /var/lib/mesos/slave/store/docker/staging/eglyqo # cat manifest { `` schemavers '' : 1 , `` name '' : `` kvish/jenkins-dev '' , `` tag '' : `` 595c74f713f609fd1d3b05a40d35113fc03227c9 '' , `` architectur '' : `` amd64 '' , `` fslayer '' : [ { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0 '' } , { `` blobsum '' : `` sha256 : c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215 '' } , { `` blobsum '' : `` sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2 '' } , { `` blobsum '' : `` sha256 : f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320 '' } , { `` blobsum '' : `` sha256 : b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5 '' } , { `` blobsum '' : `` sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f '' } , { `` blobsum '' : `` sha256 : b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58 '' } , { `` blobsum '' : `` sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d '' } , { `` blobsum '' : `` sha256 : cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87 '' } , { `` blobsum '' : `` sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1 '' } , { `` blobsum '' : `` sha256 : fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa '' } , { `` blobsum '' : `` sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312 '' } , { `` blobsum '' : `` sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2 '' } , { `` blobsum '' : `` sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1 '' } , { `` blobsum '' : `` sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b '' } , { `` blobsum '' : `` sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63 '' } , { `` blobsum '' : `` sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728 '' } , { `` blobsum '' : `` sha256 : f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec '' } , { `` blobsum '' : `` sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf '' } , { `` blobsum '' : `` sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10 '' } , { `` blobsum '' : `` sha256 : f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747 '' } , { `` blobsum '' : `` sha256 : b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a '' } , { `` blobsum '' : `` sha256 : a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352 '' } , { `` blobsum '' : `` sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a '' } , { `` blobsum '' : `` sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94 '' } , { `` blobsum '' : `` sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01 '' } , { `` blobsum '' : `` sha256 : b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276 '' } , { `` blobsum '' : `` sha256 : d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50 '' } , { `` blobsum '' : `` sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac '' } , { `` blobsum '' : `` sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c '' } , { `` blobsum '' : `` sha256 : b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c '' } , { `` blobsum '' : `` sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d '' } , { `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' } , { `` blobsum '' : `` sha256 : c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b '' } ] , `` histori '' : [ { `` v1compat '' : `` { \ '' architecture\ '' : \ '' amd64\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' \ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' nobody\ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' exposedports\ '' : { \ '' 50000/tcp\ '' : { } , \ '' 8080/tcp\ '' : { } } , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' , \ '' lang=c.utf-8\ '' , \ '' java_home=/docker-java-home\ '' , \ '' java_version=8u162\ '' , \ '' java_debian_version=8u162-b12-1~deb9u1\ '' , \ '' ca_certificates_java_version=20170531+nmu1\ '' , \ '' jenkins_home=/var/jenkinsdcos_home\ '' , \ '' jenkins_slave_agent_port=50000\ '' , \ '' jenkins_version=2.107.2\ '' , \ '' jenkins_uc=http : //updates.jenkins.io\ '' , \ '' jenkins_uc_experimental=http : //updates.jenkins.io/experimental\ '' , \ '' copy_reference_file_log=/var/jenkinsdcos_home/copy_reference_file.log\ '' , \ '' jenkins_folder=/usr/share/jenkins\ '' , \ '' jenkins_csp_opts=sandbox ; default-src 'none ' ; img-src 'self ' ; style-src 'self ' ; \ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' /usr/local/jenkins/bin/run.sh\ '' ] , \ '' argsescaped\ '' : true , \ '' image\ '' : \ '' sha256 : c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\ '' , \ '' volumes\ '' : { \ '' /var/jenkins_home\ '' : { } } , \ '' workingdir\ '' : \ '' /tmp\ '' , \ '' entrypoint\ '' : [ \ '' /sbin/tini\ '' , \ '' -- \ '' , \ '' /usr/local/bin/jenkins.sh\ '' ] , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' container\ '' : \ '' e4111508e68c304ec5b36009773b41384b96fd887b61177cd42935b9757567fd\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' e4111508e68c\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' nobody\ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' exposedports\ '' : { \ '' 50000/tcp\ '' : { } , \ '' 8080/tcp\ '' : { } } , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' , \ '' lang=c.utf-8\ '' , \ '' java_home=/docker-java-home\ '' , \ '' java_version=8u162\ '' , \ '' java_debian_version=8u162-b12-1~deb9u1\ '' , \ '' ca_certificates_java_version=20170531+nmu1\ '' , \ '' jenkins_home=/var/jenkinsdcos_home\ '' , \ '' jenkins_slave_agent_port=50000\ '' , \ '' jenkins_version=2.107.2\ '' , \ '' jenkins_uc=http : //updates.jenkins.io\ '' , \ '' jenkins_uc_experimental=http : //updates.jenkins.io/experimental\ '' , \ '' copy_reference_file_log=/var/jenkinsdcos_home/copy_reference_file.log\ '' , \ '' jenkins_folder=/usr/share/jenkins\ '' , \ '' jenkins_csp_opts=sandbox ; default-src 'none ' ; img-src 'self ' ; style-src 'self ' ; \ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) \ '' , \ '' cmd [ \\\ '' /bin/sh\\\ '' \\\ '' -c\\\ '' \\\ '' /usr/local/jenkins/bin/run.sh\\\ '' ] \ '' ] , \ '' argsescaped\ '' : true , \ '' image\ '' : \ '' sha256 : c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\ '' , \ '' volumes\ '' : { \ '' /var/jenkins_home\ '' : { } } , \ '' workingdir\ '' : \ '' /tmp\ '' , \ '' entrypoint\ '' : [ \ '' /sbin/tini\ '' , \ '' -- \ '' , \ '' /usr/local/bin/jenkins.sh\ '' ] , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : { } } , \ '' created\ '' : \ '' 2018-09-26t17:33:57.6822239z\ '' , \ '' docker_version\ '' : \ '' 18.03.0-ce\ '' , \ '' id\ '' : \ '' fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' parent\ '' : \ '' bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\ '' , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\ '' , \ '' parent\ '' : \ '' 2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:57.3350528z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c echo 2.0 \\u003 /usr/share/jenkins/ref/jenkins.install.upgradewizard.state\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\ '' , \ '' parent\ '' : \ '' 36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:56.0461597z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) user nobody\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\ '' , \ '' parent\ '' : \ '' ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:55.6692099z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c chmod -r ugo+rw \\\ '' $ jenkins_home\\\ '' \\\ '' $ { jenkins_fold } \\\ '' \\u0026\\u0026 chmod -r ugo+r \\\ '' $ { jenkins_stag } \\\ '' \\u0026\\u0026 chmod -r ugo+rx /usr/local/jenkins/bin/ \\u0026\\u0026 chmod -r ugo+rw /var/jenkins_home/ \\u0026\\u0026 chmod -r ugo+rw /var/lib/nginx/ /var/nginx/ /var/log/nginx \\u0026\\u0026 chmod ugo+rx /usr/local/jenkins/bin/ * \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\ '' , \ '' parent\ '' : \ '' c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:49.7534514z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c groupadd -g $ { gid } nobodi \\u0026\\u0026 usermod -u $ { uid } -g $ { gid } $ { user } \\u0026\\u0026 usermod -a -g user nobodi \\u0026\\u0026 echo \\\ '' nobodi : x:65534:65534 : nobodi : /nonexist : /usr/sbin/nologin\\\ '' \\u003e\\u003 /etc/passwd\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\ '' , \ '' parent\ '' : \ '' 2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:48.3150654z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) add c84b80e3ceaef7f211a221093369729eeb89e5cfc5f3d0a5cd4917e7b6c7027f /usr/share/jenkins/ref//plugins/metrics-graphite.hpi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\ '' , \ '' parent\ '' : \ '' a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:47.8920446z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) add f4d41c9bf39651b20107d62d85c101014320946e6a33763e5519ec18aee77858 /usr/share/jenkins/ref//plugins/prometheus.hpi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\ '' , \ '' parent\ '' : \ '' f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:46.775839z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) add 652f0ad5e9ad70b4db10957b64265f808b45c63d8ef07b107d3082450084164c /usr/share/jenkins/ref//plugins/mesos.hpi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\ '' , \ '' parent\ '' : \ '' 0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\ '' , \ '' created\ '' : \ '' 2018-09-26t17:33:45.5611867z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c /usr/local/bin/install-plugins.sh blueocean-bitbucket-pipelin : $ { blueocean_vers } blueocean-common : $ { blueocean_vers } blueocean-config : $ { blueocean_vers } blueocean-dashboard : $ { blueocean_vers } blueocean-ev : $ { blueocean_vers } blueocean-git-pipelin : $ { blueocean_vers } blueocean-github-pipelin : $ { blueocean_vers } blueocean-i18n : $ { blueocean_vers } blueocean-jwt : $ { blueocean_vers } blueocean-jira : $ { blueocean_vers } blueocean-person : $ { blueocean_vers } blueocean-pipeline-api-impl : $ { blueocean_vers } blueocean-pipeline-editor : $ { blueocean_vers } blueocean-pipeline-scm-api : $ { blueocean_vers } blueocean-rest-impl : $ { blueocean_vers } blueocean-rest : $ { blueocean_vers } blueocean-web : $ { blueocean_vers } blueocean : $ { blueocean_vers } ant:1.8 ansicolor:0.5.2 antisamy-markup-formatter:1.5 artifactory:2.15.1 authentication-tokens:1.3 azure-credentials:1.6.0 azure-vm-agents:0.7.0 branch-api:2.0.19 build-name-setter:1.6.9 build-timeout:1.19 cloudbees-folder:6.4 conditional-buildstep:1.3.6 config-file-provider:2.18 copyartifact:1.39.1 cvs:2.14 docker-build-publish:1.3.2 docker-workflow:1.15.1 durable-task:1.22 ec2:1.39 embeddable-build-status:1.9 external-monitor-job:1.7 ghprb:1.40.0 git:3.8.0 git-client:2.7.1 git-server:1.7 github:1.29.0 github-api:1.90 github-branch-source:2.3.3 github-organization-folder:1.6 gitlab-plugin:1.5.5 gradle:1.28 greenballs:1.15 handlebars:1.1.1 ivy:1.28 jackson2-api:2.8.11.3 job-dsl:1.68 jobconfighistory:2.18 jquery:1.12.4-0 ldap:1.20 mapdb-api:1.0.9.0 marathon:1.6.0 matrix-auth:2.2 matrix-project:1.13 maven-plugin:3.1.2 metrics:3.1.2.11 monitoring:1.72.0 nant:1.4.3 node-iterator-api:1.5.0 pam-auth:1.3 parameterized-trigger:2.35.2 pipeline-build-step:2.7 pipeline-github-lib:1.0 pipeline-input-step:2.8 pipeline-milestone-step:1.3.1 pipeline-model-api:1.2.8 pipeline-model-definition:1.2.8 pipeline-model-extensions:1.2.8 pipeline-rest-api:2.10 pipeline-stage-step:2.3 pipeline-stage-view:2.10 plain-credentials:1.4 prometheus:1.2.0 rebuild:1.28 role-strategy:2.7.0 run-condition:1.0 s3:0.11.0 saferestart:0.3 saml:1.0.5 scm-api:2.2.6 ssh-agent:1.15 ssh-slaves:1.26 subversion:2.10.5 timestamper:1.8.9 translation:1.16 variant:1.1 windows-slaves:1.3.1 workflow-aggregator:2.5 workflow-api:2.27 workflow-basic-steps:2.6 workflow-cps:2.48 workflow-cps-global-lib:2.9 workflow-durable-task-step:2.19 workflow-job:2.18 workflow-multibranch:2.17 workflow-scm-step:2.6 workflow-step-api:2.14 workflow-support:2.18\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\ '' , \ '' parent\ '' : \ '' 1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:24.2544617z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:59ced817d4cd74453e0658c69f937959d2b4d86cfe15d699cd1fdcf2f6867067 /usr/share/jenkins/ref//init.groovy.d/mesos-auth.groovi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\ '' , \ '' parent\ '' : \ '' 35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:23.9384301z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:8ca0529d27d0fa91b7848e39a5d04e55df01746ab31ca6bae1816f062667f8cc /usr/share/jenkins/ref//nodemonitors.xml \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\ '' , \ '' parent\ '' : \ '' 12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:23.609004z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file : beed7a659bf7217db04b70fa4220df32e07015c6f20edf4d73b5cab69354542 /usr/share/jenkins/ref//jenkins.model.jenkinslocationconfiguration.xml \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\ '' , \ '' parent\ '' : \ '' 87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:23.3055734z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:46468ed2b6fa66eeea868396b18d952f8cbdd0df6529ec2a4d5782a1acc7ee7a /usr/share/jenkins/ref//config.xml \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\ '' , \ '' parent\ '' : \ '' a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:23.003904z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:6b54409cf8c3ce4dae538b70b64f8755636613e71806e479c5d8f081224c63e9 /var/nginx/nginx.conf \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\ '' , \ '' parent\ '' : \ '' 39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:22.6859214z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c mkdir -p /var/log/nginx/jenkin /var/nginx/\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\ '' , \ '' parent\ '' : \ '' 0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:21.2086534z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file : a4cf73ccc8a0e4b1a7acef249766ce76b31bf76d03f97ac157d6eccfab30d4f5 /usr/local/jenkins/bin/run.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\ '' , \ '' parent\ '' : \ '' f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:20.9064351z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:3377f08a63084052efa9902be76b1eb669229849b476b52f448697333457e769 /usr/local/jenkins/bin/dcos-account.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\ '' , \ '' parent\ '' : \ '' 2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:20.5594535z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:5814edade36c8c883f19e868796f1ae1d46d6990af813451101abec8196856d4 /usr/local/jenkins/bin/export-libssl.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\ '' , \ '' parent\ '' : \ '' ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\ '' , \ '' created\ '' : \ '' 2018-09-26t17:31:20.2349213z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:8206c6af7dc8888193958fd9428ba085ae19c8282c26eb05fb9f4c4f46973a4 /usr/local/jenkins/bin/bootstrap.pi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\ '' , \ '' parent\ '' : \ '' 9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\ '' , \ '' created\ '' : \ '' 2018-07-09t20:54:30.984299193z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c echo 'networkaddress.cache.ttl=60 ' \\u003e\\u003 $ { java_hom } /jre/lib/security/java.security\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\ '' , \ '' parent\ '' : \ '' e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\ '' , \ '' created\ '' : \ '' 2018-07-09t20:54:29.524404063z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c mkdir -p \\\ '' $ { jenkins_hom } \\\ '' \\\ '' $ { jenkins_fold } /war\\\ '' \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\ '' , \ '' parent\ '' : \ '' 84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\ '' , \ '' created\ '' : \ '' 2018-07-09t20:54:28.236876676z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c echo \\\ '' deb http : //ftp.debian.org/debian test main\\\ '' \\u003e\\u003 /etc/apt/sources.list \\u0026\\u0026 apt-get updat \\u0026\\u0026 apt-get -t test instal -y git\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\ '' , \ '' parent\ '' : \ '' a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\ '' , \ '' created\ '' : \ '' 2018-07-09t20:54:14.100019856z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c curl -fssl \\\ '' $ libmesos_download_url\\\ '' -o libmesos-bundle.tar.gz \\u0026\\u0026 echo \\\ '' $ libmesos_download_sha256 libmesos-bundle.tar.gz\\\ '' | sha256sum -c - \\u0026\\u0026 tar -c / -xzf libmesos-bundle.tar.gz \\u0026\\u0026 rm libmesos-bundle.tar.gz\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\ '' , \ '' parent\ '' : \ '' bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\ '' , \ '' created\ '' : \ '' 2018-07-09t20:54:00.580952612z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |11 blueocean_version=1.5.0 jenkins_dcos_home=/var/jenkinsdcos_hom jenkins_staging=/usr/share/jenkins/ref/ libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133 prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobodi /bin/sh -c apt-get updat \\u0026\\u0026 apt-get instal -y nginx python zip jq\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\ '' , \ '' parent\ '' : \ '' c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:46.425927046z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) user root\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\ '' , \ '' parent\ '' : \ '' 662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:46.096470837z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_csp_opts=sandbox ; default-src 'none ' ; img-src 'self ' ; style-src 'self ' ; \ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\ '' , \ '' parent\ '' : \ '' 5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:45.797188526z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env copy_reference_file_log=/var/jenkinsdcos_home/copy_reference_file.log\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\ '' , \ '' parent\ '' : \ '' 25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:45.462915577z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_home=/var/jenkinsdcos_home\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\ '' , \ '' parent\ '' : \ '' 1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:45.124088811z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg gid=99\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\ '' , \ '' parent\ '' : \ '' fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:44.827537014z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg uid=99\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\ '' , \ '' parent\ '' : \ '' 3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:44.458211965z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg user=nobody\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\ '' , \ '' parent\ '' : \ '' ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:44.10755361z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg jenkins_dcos_home=/var/jenkinsdcos_home\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\ '' , \ '' parent\ '' : \ '' b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:43.757033301z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg statsd_plug_hash=929d4a6cb3d3ce5f1e03af73075b13687d4879c8\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\ '' , \ '' parent\ '' : \ '' d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:43.442946812z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg prometheus_plug_hash=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\ '' , \ '' parent\ '' : \ '' fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\ '' , \ '' created\ '' : \ '' 2018-07-09t20:53:43.116440726z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg mesos_plug_hash=347c1ac133dc0cb6282a0dde820acd5b4eb21133\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\ '' , \ '' parent\ '' : \ '' f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:04.5174488z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg jenkins_staging=/usr/share/jenkins/ref/\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\ '' , \ '' parent\ '' : \ '' 9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:04.1863586z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg blueocean_version=1.5.0\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\ '' , \ '' parent\ '' : \ '' 72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:03.8152478z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg libmesos_download_sha256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\ '' , \ '' parent\ '' : \ '' 9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:03.4353208z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg libmesos_download_url=http : //downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\ '' , \ '' parent\ '' : \ '' 764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:03.0719423z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_folder=/usr/share/jenkins\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\ '' , \ '' parent\ '' : \ '' 7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\ '' , \ '' created\ '' : \ '' 2018-04-24t20:52:02.73463z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) workdir /tmp\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\ '' , \ '' parent\ '' : \ '' 35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\ '' , \ '' created\ '' : \ '' 2018-04-11t10:05:00.283278344z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:2874a36404a19c4075e62bf579a79bf730d317e628e80b03c676af4509481acc /usr/local/bin/install-plugins.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\ '' , \ '' parent\ '' : \ '' 9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:58.564052111z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:39d6085e6ad132734efabf90a5444f3bc74a21e8bf5a79f4d0176ac18bb98217 /usr/local/bin/plugins.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\ '' , \ '' parent\ '' : \ '' fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:56.647913351z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) entrypoint [ \\\ '' /sbin/tini\\\ '' \\\ '' -- \\\ '' \\\ '' /usr/local/bin/jenkins.sh\\\ '' ] \ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\ '' , \ '' parent\ '' : \ '' afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:54.736575307z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file : dc942ca949bb159f81bbc954773b3491e433d2d3e3ef90bac80ecf48a313c9c9 /bin/tini \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\ '' , \ '' parent\ '' : \ '' 64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:51.974150657z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:1a73810a97d134925c37b2276c894e0a9c92125cdd8c750aaf8ef15c3c20aa72 /usr/local/bin/jenkins.sh \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\ '' , \ '' parent\ '' : \ '' 0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:50.171056466z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:88dd96a27353c9d476981c3cfc6b39c95983c45083324afa7c8bddb682d91bff /usr/local/bin/jenkins-support \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\ '' , \ '' parent\ '' : \ '' d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:48.292041295z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) user jenkins\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\ '' , \ '' parent\ '' : \ '' c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:46.288406797z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env copy_reference_file_log=/var/jenkins_home/copy_reference_file.log\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\ '' , \ '' parent\ '' : \ '' 09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:44.37013921z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) expos 50000\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\ '' , \ '' parent\ '' : \ '' 3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:42.447771731z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) expos 8080\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\ '' , \ '' parent\ '' : \ '' 6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:40.453492565z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |9 jenkins_sha=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67 jenkins_url=http : //repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war tini_version=v0.16.1 agent_port=50000 gid=1000 group=jenkin http_port=8080 uid=1000 user=jenkin /bin/sh -c chown -r $ { user } \\\ '' $ jenkins_home\\\ '' /usr/share/jenkins/ref\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\ '' , \ '' parent\ '' : \ '' c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:37.42404848z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_uc_experimental=http : //updates.jenkins.io/experimental\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\ '' , \ '' parent\ '' : \ '' 9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:35.309385797z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_uc=http : //updates.jenkins.io\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\ '' , \ '' parent\ '' : \ '' 6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:33.341878374z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |9 jenkins_sha=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67 jenkins_url=http : //repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war tini_version=v0.16.1 agent_port=50000 gid=1000 group=jenkin http_port=8080 uid=1000 user=jenkin /bin/sh -c curl -fssl $ { jenkins_url } -o /usr/share/jenkins/jenkins.war \\u0026\\u0026 echo \\\ '' $ { jenkins_sha } /usr/share/jenkins/jenkins.war\\\ '' | sha256sum -c -\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\ '' , \ '' parent\ '' : \ '' 0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:28.72473862z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg jenkins_url=http : //repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\ '' , \ '' parent\ '' : \ '' 14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:26.621369421z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg jenkins_sha=2d71b8f87c8417f9303a73d52901a59678ee6c0eefcf7325efed6035ff39372a\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\ '' , \ '' parent\ '' : \ '' 3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:24.515479866z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_version=2.107.2\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\ '' , \ '' parent\ '' : \ '' 8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:22.485876008z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg jenkins_version\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\ '' , \ '' parent\ '' : \ '' 14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:20.518174508z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file : c84b91c835048a52bb864c1f4662607c56befe3c4b1520b0ea94633103a4554f /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovi \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\ '' , \ '' parent\ '' : \ '' f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:18.593424219z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |7 tini_version=v0.16.1 agent_port=50000 gid=1000 group=jenkin http_port=8080 uid=1000 user=jenkin /bin/sh -c curl -fssl http : //github.com/krallin/tini/releases/download/ $ { tini_vers } /tini-static- $ ( dpkg -- print-architectur ) -o /sbin/tini \\u0026\\u0026 curl -fssl http : //github.com/krallin/tini/releases/download/ $ { tini_vers } /tini-static- $ ( dpkg -- print-architectur ) .asc -o /sbin/tini.asc \\u0026\\u0026 gpg -- import /var/jenkins_home/tini_pub.gpg \\u0026\\u0026 gpg -- verifi /sbin/tini.asc \\u0026\\u0026 rm -rf /sbin/tini.asc /root/.gnupg \\u0026\\u0026 chmod +x /sbin/tini\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\ '' , \ '' parent\ '' : \ '' 8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:13.905006564z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) copi file:653491cb486e752a4c2b4b407a46ec75646a54eabb597634b25c7c2b82a31424 /var/jenkins_home/tini_pub.gpg \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\ '' , \ '' parent\ '' : \ '' 1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:11.747045116z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg tini_version=v0.16.1\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\ '' , \ '' parent\ '' : \ '' 9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:09.646844829z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |6 agent_port=50000 gid=1000 group=jenkin http_port=8080 uid=1000 user=jenkin /bin/sh -c mkdir -p /usr/share/jenkins/ref/init.groovy.d\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\ '' , \ '' parent\ '' : \ '' cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:05.986383436z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) volum [ /var/jenkins_hom ] \ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\ '' , \ '' parent\ '' : \ '' 8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:03.98242692z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' |6 agent_port=50000 gid=1000 group=jenkin http_port=8080 uid=1000 user=jenkin /bin/sh -c groupadd -g $ { gid } $ { group } \\u0026\\u0026 useradd -d \\\ '' $ jenkins_home\\\ '' -u $ { uid } -g $ { gid } -m -s /bin/bash $ { user } \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\ '' , \ '' parent\ '' : \ '' f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\ '' , \ '' created\ '' : \ '' 2018-04-11t10:04:00.815710832z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_slave_agent_port=50000\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\ '' , \ '' parent\ '' : \ '' 5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:58.893891854z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env jenkins_home=/var/jenkins_home\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\ '' , \ '' parent\ '' : \ '' b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:57.021756845z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg agent_port=50000\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\ '' , \ '' parent\ '' : \ '' d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:55.096596096z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg http_port=8080\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\ '' , \ '' parent\ '' : \ '' 35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:53.140848234z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg gid=1000\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\ '' , \ '' parent\ '' : \ '' d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:51.085212134z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg uid=1000\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\ '' , \ '' parent\ '' : \ '' edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:49.08677048z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg group=jenkins\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\ '' , \ '' parent\ '' : \ '' 27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:47.139089021z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) arg user=jenkins\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\ '' , \ '' parent\ '' : \ '' 558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\ '' , \ '' created\ '' : \ '' 2018-04-11t10:03:45.06746326z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c apt-get updat \\u0026\\u0026 apt-get instal -y git curl \\u0026\\u0026 rm -rf /var/lib/apt/lists/ * \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\ '' , \ '' parent\ '' : \ '' 180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\ '' , \ '' created\ '' : \ '' 2018-03-19t21:23:43.026367652z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c /var/lib/dpkg/info/ca-certificates-java.postinst configure\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\ '' , \ '' parent\ '' : \ '' 4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\ '' , \ '' created\ '' : \ '' 2018-03-19t21:23:40.069312316z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c set -ex ; \\t\\tif [ ! -d /usr/share/man/man1 ] ; \\t\\tmkdir -p /usr/share/man/man1 ; \\tfi ; \\t\\tapt-get updat ; \\tapt-get instal -y \\t\\topenjdk-8-jdk=\\\ '' $ java_debian_version\\\ '' \\t\\tca-certificates-java=\\\ '' $ ca_certificates_java_version\\\ '' \\t ; \\trm -rf /var/lib/apt/lists/ * ; \\t\\t [ \\\ '' $ ( readlink -f \\\ '' $ java_home\\\ '' ) \\\ '' = \\\ '' $ ( docker-java-hom ) \\\ '' ] ; \\t\\tupdate-altern -- get-select | awk -v home=\\\ '' $ ( readlink -f \\\ '' $ java_home\\\ '' ) \\\ '' 'index ( $ 3 , home ) == 1 { $ 2 = \\\ '' manual\\\ '' ; print | \\\ '' update-altern -- set-selections\\\ '' } ' ; \\tupdate-altern -- queri java | grep -q 'statu : manual'\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\ '' , \ '' parent\ '' : \ '' ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\ '' , \ '' created\ '' : \ '' 2018-03-19t21:22:53.380702822z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env ca_certificates_java_version=20170531+nmu1\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\ '' , \ '' parent\ '' : \ '' 7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\ '' , \ '' created\ '' : \ '' 2018-03-19t21:22:53.161529652z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env java_debian_version=8u162-b12-1~deb9u1\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\ '' , \ '' parent\ '' : \ '' 7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\ '' , \ '' created\ '' : \ '' 2018-03-19t21:22:52.921597489z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env java_version=8u162\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\ '' , \ '' parent\ '' : \ '' a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\ '' , \ '' created\ '' : \ '' 2018-03-14t11:09:02.54085877z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env java_home=/docker-java-home\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\ '' , \ '' parent\ '' : \ '' f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\ '' , \ '' created\ '' : \ '' 2018-03-14t11:09:02.292291489z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c ln -svt \\\ '' /usr/lib/jvm/java-8-openjdk- $ ( dpkg -- print-architectur ) \\\ '' /docker-java-home\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\ '' , \ '' parent\ '' : \ '' b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\ '' , \ '' created\ '' : \ '' 2018-03-14t11:09:01.580163972z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c { \\t\\techo ' # ! /bin/sh ' ; \\t\\techo 'set -e ' ; \\t\\techo ; \\t\\techo 'dirnam \\\ '' $ ( dirnam \\\ '' $ ( readlink -f \\\ '' $ ( javac || java ) \\\ '' ) \\\ '' ) \\\ '' ' ; \\t } \\u003 /usr/local/bin/docker-java-hom \\t\\u0026\\u0026 chmod +x /usr/local/bin/docker-java-home\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\ '' , \ '' parent\ '' : \ '' 800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\ '' , \ '' created\ '' : \ '' 2018-03-14t11:09:00.816087216z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) env lang=c.utf-8\ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\ '' , \ '' parent\ '' : \ '' 62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\ '' , \ '' created\ '' : \ '' 2018-03-14t11:09:00.593223495z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c apt-get updat \\u0026\\u0026 apt-get instal -y -- no-install-recommend \\t\\tbzip2 \\t\\tunzip \\t\\txz-util \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/ * \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\ '' , \ '' parent\ '' : \ '' 810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\ '' , \ '' created\ '' : \ '' 2018-03-13t23:56:55.333999982z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c apt-get updat \\u0026\\u0026 apt-get instal -y -- no-install-recommend \\t\\tbzr \\t\\tgit \\t\\tmercuri \\t\\topenssh-cli \\t\\tsubvers \\t\\t\\t\\tprocp \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/ * \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\ '' , \ '' parent\ '' : \ '' e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\ '' , \ '' created\ '' : \ '' 2018-03-13t23:56:22.934435097z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c set -ex ; \\tif ! command -v gpg \\u003 /dev/nul ; \\t\\tapt-get updat ; \\t\\tapt-get instal -y -- no-install-recommend \\t\\t\\tgnupg \\t\\t\\tdirmngr \\t\\t ; \\t\\trm -rf /var/lib/apt/lists/ * ; \\tfi\ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\ '' , \ '' parent\ '' : \ '' ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\ '' , \ '' created\ '' : \ '' 2018-03-13t23:56:19.194216172z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c apt-get updat \\u0026\\u0026 apt-get instal -y -- no-install-recommend \\t\\tca-certif \\t\\tcurl \\t\\twget \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/ * \ '' ] } } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\ '' , \ '' parent\ '' : \ '' 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\ '' , \ '' created\ '' : \ '' 2018-03-13t22:26:49.547884802z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) cmd [ \\\ '' bash\\\ '' ] \ '' ] } , \ '' throwaway\ '' : true } '' } , { `` v1compat '' : `` { \ '' id\ '' : \ '' 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\ '' , \ '' created\ '' : \ '' 2018-03-13t22:26:49.153534342z\ '' , \ '' container_config\ '' : { \ '' cmd\ '' : [ \ '' /bin/sh -c # ( nop ) add file : b380df301ccb5ca09f0d7cd5697ed402fa55f3e9bc5df2f4d489ba31f28de58a / \ '' ] } } '' } ] , `` signatur '' : [ { `` header '' : { `` jwk '' : { `` crv '' : `` p-256 '' , `` kid '' : `` jtgt : l32l : bi2g : tg3a : rlo2:6h6k : ozxc : hfyy : spzw : qxez : xnk3:2kal '' , `` kti '' : `` ec '' , `` x '' : `` q3qr-lnb0qyoiyfbhzf5v4gxgvp_driszyinemkb464 '' , `` '' : `` obzqusrherctdgdvxwor0zkij_b7gal9b20pwvthzf '' } , `` alg '' : `` es256 '' } , `` signatur '' : `` x6bvxe9thnyphivyh_0ge1blpxznecpbilpb5hbvi2339gsa5t4hae7gmalgklythjbjrnjiq_pqqrefmbpqza '' , `` protect '' : `` eyjmb3jtyxrmzw5ndggioju4odg5lcjmb3jtyxruywlsijoiq24wiiwidgltzsi6ijiwmtgtmtatmtvumtm6mdi6mjdain0 '' } ] } { noformat } thi relat : tri find extract layer layer dir , could find two : { noformat } error ( 130 ) -22:27:48-root @ int-agent89-mwst9 : /var/lib/mesos/slave/store/docker/lay # ls -alh | grep 'fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\|bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\|2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\|36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\|ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\|c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\|2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\|a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\|f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\|0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\|1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\|35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\|12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\|87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\|a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\|39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\|0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\|f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\|2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\|ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\|9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\|e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\|84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\|a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\|bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\|c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\|662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\|5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\|25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\|1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\|fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\|3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\|ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\|b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\|d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\|fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\|f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\|9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\|72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\|9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\|764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\|7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\|35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\|9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\|fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\|afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\|64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\|0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\|d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\|c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\|09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\|3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\|6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\|c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\|9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\|6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\|0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\|14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\|3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\|8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\|14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\|f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\|8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\|1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\|9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\|cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\|8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\|f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\|5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\|b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\|d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\|35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\|d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\|edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\|27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\|558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\|180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\|4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\|ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\|7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\|7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\|a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\|f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\|b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\|800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\|62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\|810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\|e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\|ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\|8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324' drwxr-xr-x . 3 root root 40 oct 15 10:23 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324 drwxr-xr-x . 3 root root 40 oct 15 10:23 ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182 { noformat } these two base layer download earlier imag . we still need figur one layer fetch finish . ( curl process tar process run stuck background )",MESOS-9320,5.0
uri disk profil adaptor could deadlock . the loop infinit : http : //github.com/apache/mesos/blob/1.7.0/src/resource_provider/storage/uri_disk_profile_adaptor.cpp # l61-l80,MESOS-9308,1.0
"nest contain launch could fail agent upgrad new cgroup subsystem . nest contain launch could fail agent upgrad new cgroup subsystem , new cgroup subsystem exist parent contain 's cgroup hierarchi .",MESOS-9295,5.0
"if framework loos oper inform reconcil acknowledg updat . normal , framework expect checkpoint agent id resourc provid id accept offer offeroper . from expect come requir v1 schedul api framework must provid agent id resourc provid id acknowledg offer oper statu updat . howev , expect break : 1. framework might lose checkpoint data ; longer rememb agent id resourc provid id 2. even framework checkpoint data , could sent stale updat : mayb origin ack sent meso lost , need re-ack . if framework delet checkpoint data send ack ( 's drop ) upon replay statu updat longer agent id resourc provid id oper . an easi remedi would add agent id resourc provid id operationstatu messag receiv schedul framework build proper ack updat , even n't access previous checkpoint inform . i 'm file bug 's way reliabl use offer oper statu api fix .",MESOS-9293,3.0
"slrp get stale checkpoint system crash . slrp checkpoint pend oper issu correspond csi call { { slave : :state : :checkpoint } } , write new checkpoint temporari file { { renam } } . howev , n't { { fsync } } , { { renam } } atom w.r.t . system crash . as result , oper process system crash , possibl csi call execut , slrp get back stale checkpoint reboot total n't know oper . to address problem , need ensur follow issu csi call : 1 . the temp file sync disk . 2 . the renam commit disk . a possibl solut { { fsync } } write temp file , anoth { { fsync } } checkpoint dir { { renam } } .",MESOS-9281,5.0
slrp clean plugin contain remov .,MESOS-9228,5.0
"if imag layer larg , imag pull may stuck due author token expir . the imag layer blob pull happen asynchron libprocess process . there chanc one layer get token thread switch anoth layer curl may take long . when origin layer curl resum , token alreadi expir ( e.g. , 60 second ) . { noformat } $ sudo cat /var/lib/mesos/slave/store/docker/staging/0gx64f/sha256\ : c75480ad9aafadef6c7faf829ede40cf2fa990c9308d6cd354d53041b01a7cda { `` error '' : [ { `` code '' : '' unauthor '' , '' messag '' : '' authent requir '' , '' detail '' : [ { `` type '' : '' repositori '' , '' class '' : '' '' , '' name '' : '' mesosphere/dapi '' , '' action '' : '' pull '' } ] } ] } { noformat } the impact task launch stuck subsequ task use imag would also stuck wait imag pull futur becom readi . pleas note issu like reproduc , unless busi system use imag contain larg layer .",MESOS-9221,5.0
remov rootf mount may fail ebusi . we observ product environ { code } fail destroy provis rootf destroy contain : collect fail : fail destroy overlay-mount rootf '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec ' : fail unmount '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec ' : devic resourc busi { code } consid fix issu use detach unmount unmount contain rootf . see mesos-3349 detail . the root caus `` devic resourc busi '' receiv rootf unmount still unknown . _update_ : the product environ cronjob scan filesystem build index ( updatedb mlocat ) . thi explain ebusi receiv ` unmount ` . _update_ : splunk 's scan ` /var/lib/meso ` could also sourc trigger .,MESOS-9196,5.0
"docker command executor may stuck infinit unkil loop . due chang http : //issues.apache.org/jira/browse/mesos-8574 , behavior docker command executor discard futur docker stop chang . if new killtask ( ) invok exist docker stop pend state , old one would call discard execut new one . thi ok case . howev , docker stop could take long ( depend grace period whether applic could handl sigterm ) . if framework retri killtask frequent grace period ( depend killpolici api , env var , agent flag ) , executor may stuck forev unkil task . becaus everytim docker stop finish , futur docker stop discard new incom killtask . we consid re-us grace period call discard ( ) pend docker stop futur .",MESOS-9191,3.0
"test ` storagelocalresourceprovidertest.root_createdestroydiskrecoveri ` flaki . the test flaki 1.7.x : { noformat } i0824 22:20:01.018494 4208 provider.cpp:1520 ] receiv destroy_disk oper `` ( uuid : 7aaadd15-1f6d-4d4e-9000-4c250495f7ba ) w0824 22:20:01.018517 4208 provider.cpp:3008 ] drop oper ( uuid : 7aaadd15-1f6d-4d4e-9000-4c250495f7ba ) : can appli oper reconcil storag pool ... i0824 22:20:01.086668 4209 master.cpp:9445 ] send offer [ 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-o4 ] framework 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-0000 ( default ) scheduler-0af22a76-f591-43ba-8470-f4b863292d61 @ 172.16.10.36:35916 .. / .. /src/tests/storage_local_resource_provider_tests.cpp:995 : failur mock function call time expect - return directli . function call : resourceoff ( 0x7ffe7ba8c240 , @ 0x7f04a09808c0 { 160-byte object < 98-7c 05-ad 04-7f 00-00 00-00 00-00 00-00 00-00 5f-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 05-00 00-00 05-00 00-00 10-a7 03-84 04-7f 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 ... 90-f2 08-84 04-7f 00-00 10-71 00-84 04-7f 00-00 40-71 00-84 04-7f 00-00 00-51 02-84 04-7f 00-00 c0-6a 03-84 04-7f 00-00 00-00 00-00 00-00 00-00 10-f0 00-84 04-7f 00-00 00-00 00-00 00-00 00-00 > } ) expect : call actual : call twice - over-satur activ { noformat } thi ` destry_disk ` race profil poll . if poll finish first , slrp start reconcil storag pool , drop certain incom oper reconcili .",MESOS-9190,2.0
add alloc benchmark allow multipl framework/ag profil . we want add test har allow us test alloc perform run multipl agent framework profil .,MESOS-9187,5.0
zookeep n't compil newer gcc due format error rr : http : //reviews.apache.org/r/68370/,MESOS-9170,2.0
"` uridiskprofileadaptor ` updat profil poll return non-ok http statu . current { { uridiskprofileadatpor } } receiv non-ok statu , e.g. , { { 404 not found } } , url poll , would still read respons bodi ( could empti malform ) updat profil matrix . the expect behavior skip poll retri later .",MESOS-9163,1.0
"mastertest.taskstatemetr flaki observ ubuntu 16.04 , cmake build : { code } mock function call time expect - return directli . function call : offer ( 0x7fffcf5518d0 , @ 0x7f64d805d440 48-byte object < c0-d3 39-0c 65-7f 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 04-00 00-00 d0-a1 08-d8 64-7f 00-00 > ) expect : call actual : call twice - over-satur activ { code } full log attach .",MESOS-9154,1.0
close file descriptor except whitelist_fd posix/subprocess . close file descriptor except whitelist_fd posix/subprocess ( current whitelist_fd honor yet ) . thi would avoid fd leak . pleas follow step commit make correspond chang : http : //issues.apache.org/jira/browse/mesos-8917 ? focusedcommentid=16522629 & page=com.atlassian.jira.plugin.system.issuetabpanel % 3acomment-tabpanel # comment-16522629,MESOS-9152,5.0
"contain stuck isol due fd leak when contain launch singl agent scale , one contain stuck isol could occasion happen . and contain becom un-destroy due container destroy alway wait isol ( ) finish continu . we add log debug issu : { noformat } aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.intern mesos-ag [ 2974 ] : i0810 17:23:28.050068 2995 collect.hpp:271 ] $ $ $ $ : awaitprocess wait invok processbas id : __await__ ( 26651 ) ; futur size : 3 ; futur : readi ; futur index : 2 ; readi count : 1 aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.intern mesos-ag [ 2974 ] : i0810 17:23:28.414436 2998 collect.hpp:271 ] $ $ $ $ : awaitprocess wait invok processbas id : __await__ ( 26651 ) ; futur size : 3 ; futur : readi ; futur index : 0 ; readi count : 2 { noformat } show await ( ) cni : :attach ( ) stuck second futur ( io : :read ( ) stdout ) . by look df stdout : { noformat } aug 10 17:23:27 ip-10-0-1-129.us-west-2.compute.intern mesos-ag [ 2974 ] : i0810 17:23:27.657501 2995 cni.cpp:1287 ] ! ! ! ! : start await plugin '/opt/mesosphere/active/mesos/libexec/mesos/mesos-cni-port-mapp ' finish contain 1c8abf4c-f71a-4704-9a73-1ab0dd709c62 pid '16644 ' ; stdout fd : 1781 ; stderr fd : 1800 { noformat } we found { noformat } core @ ip-10-0-1-129 ~ $ ps aux | grep mesos-ag core 1674 0.0 0.0 6704 864 pts/0 s+ 20:00 0:00 grep -- colour=auto mesos-ag root 2974 16.4 2.5 1211096 414048 ? ssl 17:02 29:11 /opt/mesosphere/packages/meso -- 61265af3be37861f26b657c1f9800293b86a0374/bin/mesos-ag core @ ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep 1781 l-wx -- -- -- . 1 root root 64 aug 10 19:38 1781 - > /var/lib/mesos/slave/meta/slaves/d3089315-8e34-40b4-b1f7-0ac6a624d7db-s0/frameworks/d3089315-8e34-40b4-b1f7-0ac6a624d7db-0000/executors/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/runs/38e9270d-ebda-4758-ad96-40c5b84bffdc/tasks/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/task.upd { noformat } { noformat } core @ ip-10-0-1-129 ~ $ ps aux | grep 27981 core 2201 0.0 0.0 6704 884 pts/0 s+ 20:06 0:00 grep -- colour=auto 27981 root 27981 0.0 0.0 1516 4 ? ss 17:25 0:00 sleep 10000 core @ ip-10-0-1-129 ~ $ cat /proc/s^c core @ ip-10-0-1-129 ~ $ sudo -s ip-10-0-1-129 core # ls -al /proc/27981/fd | grep 275230 lr-x -- -- -- . 1 root root 64 aug 10 20:05 1781 - > pipe : [ 275230 ] l-wx -- -- -- . 1 root root 64 aug 10 20:05 1787 - > pipe : [ 275230 ] { noformat } { noformat } core @ ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep pipe lr-x -- -- -- . 1 root root 64 aug 10 17:02 11 - > pipe : [ 49380 ] l-wx -- -- -- . 1 root root 64 aug 10 17:02 14 - > pipe : [ 49380 ] lr-x -- -- -- . 1 root root 64 aug 10 17:02 17 - > pipe : [ 48909 ] lr-x -- -- -- . 1 root root 64 aug 10 19:38 1708 - > pipe : [ 275089 ] l-wx -- -- -- . 1 root root 64 aug 10 19:38 1755 - > pipe : [ 275089 ] lr-x -- -- -- . 1 root root 64 aug 10 19:38 1787 - > pipe : [ 275230 ] l-wx -- -- -- . 1 root root 64 aug 10 17:02 19 - > pipe : [ 48909 ] { noformat } pipe 275230 held agent process sleep process time ! the reason leak possibl n't use ` pipe2 ` creat pipe ` o_cloexec ` subprocess : http : //github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp # l61 although set cloexec fd later : http : //github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp # l366-l373 there race fork happen ` pipe ( ) ` call , cloexec call later . thi like busi system ( explain 's hard repo issu launch lot contain singl box ) .",MESOS-9151,8.0
"test ` storagelocalresourceprovidertest.root_containerterminationmetr ` flaki . thi test flaki fail follow error : { noformat } .. / .. /src/tests/storage_local_resource_provider_tests.cpp:3167 fail wait 15sec pluginrestart { noformat } the actual error follow : { noformat } e0802 22:13:37.265038 8216 provider.cpp:1496 ] fail reconcil resourc provid b9379982-d990-4f63-8a5b-10edd4f5a1bb : collect fail : os error { noformat } the root caus slrp call { { listvolum } } { { getcapac } } start , plugin contain kill call ongo , grpc return { { os error } } lead slrp fail . thi flaki fix finish http : //issues.apache.org/jira/browse/mesos-8400 .",MESOS-9130,1.0
"virtualenv manag support directori buggi . when switch back forth python 2 3 , virtualenv get correctli reinstal .",MESOS-9075,2.0
tox n't run support virtualenv use python 3 mesos-style.pi,MESOS-9073,1.0
"default executor commit suicid receiv http respons launch_nested_contain call . if network problem ( e.g. , rout problem ) , possibl agent receiv { { launch_nested_contain } } call default executor launch nest contain , executor get http respons . thi would result task stuck { { task_start } } forev . we consid make default executor commit suicid receiv respons reason amount time .",MESOS-9052,3.0
"` update_st ` race ` update_operation_statu ` resourc provid . sinc resourc provid oper statu updat manag run differ actor , complet oper , ` update_operation_statu ` call may race ` update_st ` . when ` update_st ` arriv agent earlier , total resourc updat , termin statu complet oper ignor sinc known agent resourc provid . as result , ` update_operation_statu ` arriv later , agent tri appli oper , incorrect sinc total resourc alreadi updat .",MESOS-9010,2.0
"wire ` update_quota ` call . wire exist master , auth , registar , alloc piec togeth complet ` update_quota ` call . thi would enabl master capabl ` quota_v2 ` . thi also fix `` ignor zero resourc quota '' bug old quota implement , name : current , meso discard resourc object zero scalar valu pars resourc . thi mean quota set zero would ignor enforc . for exampl , role quota set `` cpu:10 ; mem:10 ; gpu:0 '' intend get gpu . due issu , alloc see quota `` cpu:10 ; mem:10 '' , quota gpu mean guarante no limit . thu gpu may still alloc role . with complet ` update_quota ` take map name , scalar valu , zero valu longer drop .",MESOS-8968,5.0
"python3/post-reviews.pi error due typeerror . { code : java } $ ./support/python3/post-reviews.pi run 'rbt post ' across ... 7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - ( head - > alexr/subscribers-health , private/ci/alexr/default ) sent task ( health ) check updat oper stream api . ( 2 minut ago ) creat diff : 7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - ( head - > alexr/subscribers-health , private/ci/alexr/default ) sent task ( health ) check updat oper stream api . press enter continu 'ctrl-c ' skip . traceback ( recent call last ) : file `` ./support/python3/post-reviews.pi '' , line 432 , < modul > main ( ) file `` ./support/python3/post-reviews.pi '' , line 365 , main sys.stdout.buffer.writ ( output ) typeerror : bytes-lik object requir , 'str ' { code } the review still get post .",MESOS-8954,1.0
quota guarante metric handl remov correctli . the quota guarante metric remov quota get remov : http : //github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp # l165-l174 the consequ metric hold initi valu get set subsequ remov / set expos via metric .,MESOS-8932,2.0
"autotool n't work newer openjdk version there three distinct issu modern java linux version : 1 . meso configur script expect ` libjvm.so ` ` $ java_home/jre/lib/ < arch > /server/libjvm.so ` , newer openjdk version , ` libjvm.so ` found ` $ java_home/lib/server/libjvm.so ` . 2 . on distro ( e.g. , ubuntu 18.04 ) , java_hom env var might miss . in case , configur abl comput look ` java ` ` javac ` path succe . howev , maven plugin requir java_hom set could fail 's found . { code : java } [ error ] fail execut goal org.apache.maven.plugin : maven-javadoc-plugin:2.8.1 : jar ( build-and-attach-javadoc ) project meso : mavenreportexcept : error creat archiv : unabl find javadoc command : the environ variabl java_hom correctli set . - > [ help 1 ] { code } becaus configur script gener automak variabl ` java_hom ` , simpli invok maven follow way fix issu : { code : java } java_home= $ java_hom mvn ... { code } these two behavior observ openjdk 1.11 ubuntu 18.04 i suspect behavior present distros/openjdk version . 3 . ` javah ` remov openjdk 1.10 . instead ` javac -h ` use replac . see [ http : //openjdk.java.net/jeps/313 ] detail .",MESOS-8921,3.0
"docker imag fetcher fail http/2 . { noformat } [ run ] imagealpine/provisionerdockertest.root_internet_curl_simplecommand/2 ... i0510 20:52:00.209815 25010 registry_puller.cpp:287 ] pull imag 'quay.io/coreos/alpine-sh ' 'docker-manifest : //quay.iocoreos/alpine-sh ? latest # http ' '/tmp/imagealpine_provisionerdockertest_root_internet_curl_simplecommand_2_wf7efm/store/docker/staging/qit1jn' e0510 20:52:00.756072 25003 slave.cpp:6176 ] contain '5eb869c5-555c-4dc9-a6ce-ddc2e7dbd01a ' executor 'ad9aa898-026e-47d8-bac6-0ff993ec5904 ' framework 7dbe7cd6-8ffe-4bcf-986a-17ba677b5a69-0000 fail start : fail decod http respons : decod fail http/2 200 server : nginx/1.13.12 date : fri , 11 may 2018 03:52:00 gmt content-typ : application/vnd.docker.distribution.manifest.v1+prettyjw content-length : 4486 docker-content-digest : sha256:61bd5317a92c3213cfe70e2b629098c51c50728ef48ff984ce929983889ed663 x-frame-opt : deni strict-transport-secur : max-age=63072000 ; preload ... { noformat } note curl say http version `` http/2 '' . thi happen modern curl automat negoti http/2 , docker fetcher n't prepar pars . { noformat } $ curl -i -- raw -l -s -s -o - 'http : //quay.io/coreos/alpine-sh ? latest # https' http/1.1 301 move perman content-typ : text/html date : fri , 11 may 2018 04:07:44 gmt locat : http : //quay.io/coreos/alpine-sh ? latest server : nginx/1.13.12 content-length : 186 connect : keep-al http/2 301 server : nginx/1.13.12 date : fri , 11 may 2018 04:07:45 gmt content-typ : text/html ; charset=utf-8 content-length : 287 locat : http : //quay.io/coreos/alpine-sh/ ? latest x-frame-opt : deni strict-transport-secur : max-age=63072000 ; preload { noformat }",MESOS-8907,3.0
"` uridiskprofileadaptor ` fail updat profil selector . the { { uridiskprofileadaptor } } ignor poll profil matrix poll one size current one : http : //github.com/apache/mesos/blob/1.5.x/src/resource_provider/storage/uri_disk_profile.cpp # l282-l286 { code : cxx } // profil ad , pars data size , // noth chang notif need sent . ( parsed.profile_matrix ( ) .size ( ) < = profilematrix.s ( ) ) { return ; } { code } howev , prevent profil selector updat , desir behavior .",MESOS-8906,2.0
resourceprovidermanagerhttpapitest.resubscriberesourceprovid flaki . thi test flaki ci : { noformat } .. / .. /src/tests/resource_provider_manager_tests.cpp:1114 : failur mock function call time expect - take default action specifi : .. / .. /src/tests/mesos.hpp:2972 : function call : subscrib ( @ 0x7f881c00aff0 32-byte object < 58-04 98-43 88-7f 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 e0-01 01-1c 88-7f 00-00 > ) expect : call actual : call twice - over-satur activ { noformat } thi differ http : //issues.apache.org/jira/browse/mesos-8315 .,MESOS-8874,1.0
storagelocalresourceprovidertest.root_zerosizeddisk flaki . thi test flaki ci : { noformat } .. / .. /src/tests/storage_local_resource_provider_tests.cpp:406 : failur valu : updateslave2- > has_resource_provid ( ) actual : fals expect : true { noformat },MESOS-8873,2.0
agent may fail recov agent die imag store cach checkpoint . { noformat } e0502 13:51:45.398555 10100 slave.cpp:7305 ] exit statu 1 : fail perform recoveri : collect fail : collect fail : collect fail : unexpect empti imag file '/var/lib/mesos/slave/store/docker/storedimages' { noformat } thi may happen agent die file creat content persist disk .,MESOS-8871,3.0
"consid valid resubscrib resourc provid chang name type the agent current use resourc provid 's name type construct e.g. , path persist resourc provid state recoveri . with like prevent resourc provid chang inform sinc might otherwis unabl recov success .",MESOS-8838,2.0
add test recoveri resourc provid manag registrar .,MESOS-8836,2.0
rp-relat api experiment . the new offer oper resourc provid api introduc meso 1.5.0 mark experiment .,MESOS-8787,1.0
operation_drop oper statu updat includ operation/framework id the agent includ operation/framework id oper statu updat sent respons reconcili request master . these statu updat oper statu : { { operation_drop } } .,MESOS-8784,3.0
transit pend oper operation_unreach agent remov . pend oper agent transit ` operation_unreach ` agent mark unreach . we also make sure pro-act send oper statu updat oper agent becom unreach . we also make sure send new oper updat if/when agent reconnect - perhap alreadi accomplish exist oper updat logic agent ?,MESOS-8783,3.0
"transit oper operation_gone_by_oper mark agent gone . the master transit oper state { { operation_gone_by_oper } } agent mark gone , send oper statu updat framework creat . we also remov { { master : :framework } } .",MESOS-8782,3.0
"meso master n't silent drop oper we make sure call place { { void master : :drop ( framework * , const offer : :oper & , const string & ) } } send statu updat oper id specifi . or make sure not send one , make method send one .",MESOS-8781,3.0
"agent resourc provid config api call idempot . there issu w.r.t . use current agent resourc provid config api call : 1 . { { update_resource_provider_config } } : if caller fail receiv http respons code , way retri oper without trigger rp restart . 2 . { { remove_resource_provider_config } } : if caller fail receiv http respons code , retri return 404 not found . but due mesos-7697 , way caller know 404 due previou success config remov . to address issu , make call idempot , return 200 ok caller retri . it would nice { { add_resource_provider_config } } also idempot consist .",MESOS-8742,2.0
"libprocess : deadlock process : :final sinc call [ ` libprocess : :final ( ) ` |http : //github.com/apache/mesos/blob/02ebf9986ab5ce883a71df72e9e3392a3e37e40e/src/slave/containerizer/mesos/io/switchboard_main.cpp # l157 ] return ioswitchboard 's main function , expect http respons go sent back client ioswitchboard termin . howev , [ adding|http : //reviews.apache.org/r/66147/ ] ` libprocess : :final ( ) ` seen ioswitchboard might get stuck ` libprocess : :final ( ) ` . see attach stacktrac .",MESOS-8729,5.0
"enabl resourc provid agent capabl default in 1.5.0 introduc resourc provid agent capabl e.g. , enabl modifi oper protocol . we enabl capabl default . if test explicitli depend agent fulli oper , adjust modifi protocol . it e.g. , enough wait { { dispatch } } agent 's recoveri method , instead one wait dedic { { updateslavemessag } } agent .",MESOS-8647,2.0
"termin task statu updat send 'docker inspect ' hung when agent process termin statu updat task , call { { containerizer- > updat ( ) } } contain forward updat : http : //github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/slave.cpp # l5509-l5514 in docker container , { { updat ( ) } } call { { docker : :inspect ( ) } } , mean inspect call hang , termin updat sent : http : //github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/containerizer/docker.cpp # l1714",MESOS-8605,3.0
"allow empti resourc provid selector ` uridiskprofileadaptor ` . current { { uridiskprofileadaptor } } , invalid profil resourc provid selector 0 resourc provid . howev , one put non-exist provid type name selector achiev effect , semant inconsist . we allow empti list resourc provid directli .",MESOS-8598,1.0
avoid failur invalid profil ` uridiskprofileadaptor ` we defens fail profil modul user provid invalid profil profil matrix .,MESOS-8592,1.0
test uridiskprofiletest.fetchfromhttp flaki . the { { uridiskprofiletest.fetchfromhttp } } test flaki debian 9 : { noformat } .. / .. /src/tests/disk_profile_tests.cpp:683 fail wait 15sec futur { noformat } i also run repetit got follow error log ( although test pass ) : { noformat } e0209 18:26:37.030012 7282 uri_disk_profile.cpp:220 ] fail pars result : fail pars diskprofilemap messag : invalid_argu : unexpect end string . expect valu . ^ { noformat },MESOS-8567,3.0
"default executor allow decreas escal grace period termin task the command executor support [ decreas escal grace period termin task|http : //github.com/apache/mesos/blob/c665dd6c22715fa941200020a8f7209f1f5b1ca1/src/launcher/executor.cpp # l800-l803 ] . for consist , also support default executor .",MESOS-8557,5.0
"test storagelocalresourceprovidertest.root_metr flaki the slrp metric test flaki agent might got two { { slaveregisteredmessag } } due retri logic registr , thu would send two { { updateslavemessag } } s. as result , futur wait messag readi plugin actual launch . thi lead race sigkil launch_contain test , kill happen slrp get connect plugin , slrp wait 1 minut give , long test wait second launch .",MESOS-8548,2.0
"the default executor wrongli indic task task group unhealthi the default executor set `` global '' [ unhealthy|http : //github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp # l1286 ] field { { true } } kill task due fail health check . when task task group exit , [ check|http : //github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp # l877-l882 ] `` global '' healthi field set task statu updat 's { { healthi } } field accordingli . thi mean unhealthi task belong task group kill , task statu updat task belong task group , contain healthi task , sent { { healthi } } field set { { fals } } .",MESOS-8543,3.0
"default executor n't wait statu updat ack 'd shut the default executor n't wait pend statu updat acknowledg shut , instead sleep one second termin : { code } void _shutdown ( ) { const durat durat = second ( 1 ) ; log ( info ) < < `` termin `` < < durat ; // todo ( qianzhang ) : remov hack sinc executor receiv // acknowledg statu updat . the executor termin // receiv ack termin statu updat . os : :sleep ( durat ) ; termin ( self ( ) ) ; } { code } the event handler exit upon receiv { { event : :acknowledg } } executor shut , task run anymor , pend statu updat acknowledg .",MESOS-8537,3.0
"the default executor n't retri kill initi the default executor might initi task kill due health check failur task task group fail . if kill call fail , executor wo n't retri , task get stuck kill state .",MESOS-8532,5.0
"some task statu updat sent default executor n't contain reason . the default executor n't set reason send { { task_kil } } , { { task_kil } } , { { task_fail } } task statu updat .",MESOS-8531,3.0
"default executor task get stuck kill state the default executor transit task { { task_kil } } mark contain kill issu { { kill_nested_contain } } call . if kill call fail , task get stuck { { task_kil } } , executor wo n't allow retri kill .",MESOS-8530,5.0
"when ` update_slav ` messag receiv , offer might rescind due race when agent enabl { { resource_provid } } capabl ( re- ) regist master send { { update_slav } } ( re- ) regist . in master , agent ad ( back ) alloc , soon 's ( re- ) regist , i.e . { { update_slav } } send . thi trigger alloc offer might get sent framework . when { { update_slav } } handl master , offer rescind , 're base outdat agent state . intern , alloc defer offer callback master ( { { master : :offer } } ) . in rare case { { update_slav } } messag might arriv time handler master call offer callback ( actual alloc took place ) . in case ( outdat ) offer still sent framework never rescind . here 's relev log line , discov work http : //reviews.apache.org/r/65045/ : { noformat } i0201 14:17:47.041093 242208768 hierarchical.cpp:1517 ] perform alloc 1 agent 704915n i0201 14:17:47.041738 242745344 master.cpp:7235 ] receiv updat agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-s0 slave ( 540 ) @ 172.18.8.20:60469 ( 172.18.8.20 ) total oversubscrib resourc { } i0201 14:17:47.042778 242745344 master.cpp:8808 ] send 1 offer framework 53c557e7-3161-449b-bacc-a4f8c02e78e7-0000 ( default ) scheduler-798f476b-b099-443e-bd3b-9e7333f29672 @ 172.18.8.20:60469 i0201 14:17:47.043102 243281920 sched.cpp:921 ] schedul : :resourceoff took 40444n i0201 14:17:47.043427 243818496 hierarchical.cpp:712 ] grew agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-s0 disk [ mount ] :200 ( total ) , { } ( use ) i0201 14:17:47.043643 243818496 hierarchical.cpp:669 ] agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-s0 ( 172.18.8.20 ) updat total resourc disk [ mount ] :200 ; cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] { noformat }",MESOS-8524,2.0
"slrp fail connect csi endpoint . after bump grpc bundl 1.8.3 , flaki slrp test caus slrp abl connect csi endpoint . the reason seem take longer grpc 1.8 prepar domain socket ( i.e. , time { { bind } } { { accept } } call longer ) , result , slrp talk csi plugin immedi socket file creat .",MESOS-8514,2.0
"uri disk profil adaptor consid plugin type profil . current , uri disk profil adaptor fetch uri , content contain profil matrix . howev , 's field profil matrix adaptor tell plugin type profil . we consid ad ` plugin_typ ` field ` csimanifest ` .",MESOS-8510,3.0
"test storagelocalresourceprovidertest.root_convertpreexistingvolum flaki observ intern ci ubuntu16.04 ssl grpc enabl , { noformat } .. / .. /src/tests/storage_local_resource_provider_tests.cpp:1898 expect : 2u which : 2 to equal : destroyed.s ( ) which : 1 { noformat }",MESOS-8474,2.0
"` launch_group ` failur tear default executor . the follow code default executor ( http : //github.com/apache/mesos/blob/12be4ba002f2f5ff314fbc16af51d095b0d90e56/src/launcher/default_executor.cpp # l525-l535 ) show ` launch_nested_contain ` call fail ( say , due fetcher failur ) , whole executor shut : { code : cpp } // check receiv 200 ok respons // ` launch_nested_contain ` call . shutdown executor // case . foreach ( const respons & respons , responses.get ( ) ) { ( response.cod ! = process : :http : :statu : :ok ) { log ( error ) < < `` receiv ' '' < < response.statu < < `` ' ( `` < < response.bodi < < `` ) launch child contain '' ; _shutdown ( ) ; return ; } } { code } thi expect user . instead , one would expect fail ` launch_group ` wo n't affect task group launch executor , similar case task failur take task group . we adjust semant make fail ` launch_group ` take executor affect task group .",MESOS-8468,5.0
"destroy executor might use ` slave : :publishresourc ( ) ` . in follow code [ http : //github.com/apache/mesos/blob/7b30b9ccd63dbcd3375e012dae6e2ffb9dc6a79f/src/slave/slave.cpp # l2652 : ] { code : cpp } publishresourc ( ) .then ( defer ( self ( ) , [ = ] { return containerizer- > updat ( executor- > containerid , executor- > allocatedresourc ( ) ) ; } ) ) { code } a destroy executor might dereferenc move { { framework.completedexecutor } } kick circular buffer . we refactor { { slave : :publishresourc ( ) } } use make code less fragil .",MESOS-8467,2.0
"clean endpoint socket contain daemon destroy wait . slrp use post-stop hook ask contain daemon clean endpoint socket plugin contain termin . howev , contain daemon destruct wait contain monitor contain termin , socket file remain , make slrp unabl recov . there might two solut : 1 . dure slrp recoveri , check plugin contain still run . 2 . start contain daemon wait phase .",MESOS-8429,3.0
"valid resourc provid config agent api call . current api return 200 ok config put resourc provid config directori , even config valid ( e.g. , n't specifi control plugin ) . we consid valid config call process .",MESOS-8425,2.0
"master 's updateslav handler correctli updat termin oper i creat test verifi oper statu updat resent master drop en rout ( mesos-8420 ) . the test follow : # creat volum raw disk resourc . # drop first ` updateoperationstatusmessag ` messag agent master , n't acknowledg master . # restart agent . # verifi agent resend oper statu updat . the good news agent resend oper statu updat , bad news trigger check failur crash master . here relev section log produc test : { noformat } [ run ] storagelocalresourceprovidertest.root_retryoperationstatusupdateafterrecoveri [ ... ] i0109 16:36:08.515882 24106 master.cpp:4284 ] process accept call offer : [ 046b3f21-6e97-4a56-9a13-773f7d481efd-o0 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 slave ( 2 ) @ 10.0.49.2:40681 ( core-dev ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( default ) scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1 @ 10.0.49.2:40681 i0109 16:36:08.516487 24106 master.cpp:5260 ] process create_volum oper sourc disk ( alloc : storag ) ( reserv : [ ( dynam , storag ) ] ) [ raw ( , volume-default ) ] :4096 framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( default ) scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1 @ 10.0.49.2:40681 agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 slave ( 2 ) @ 10.0.49.2:40681 ( core-dev ) i0109 16:36:08.518704 24106 master.cpp:10622 ] send oper `` ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 slave ( 2 ) @ 10.0.49.2:40681 ( core-dev ) i0109 16:36:08.521210 24130 provider.cpp:504 ] receiv apply_oper event i0109 16:36:08.521276 24130 provider.cpp:1368 ] receiv create_volum oper `` ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) i0109 16:36:08.523131 24432 test_csi_plugin.cpp:305 ] createvolumerequest ' { `` version '' : { `` minor '' :1 } , '' name '' : '' 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' , '' capacityrang '' : { `` requiredbyt '' : '' 4294967296 '' , '' limitbyt '' : '' 4294967296 '' } , '' volumecap '' : [ { `` mount '' : { } , '' accessmod '' : { `` mode '' : '' single_node_writ '' } } ] } ' i0109 16:36:08.525806 24152 provider.cpp:2635 ] appli convers 'disk ( alloc : storag ) ( reserv : [ ( dynam , storag ) ] ) [ raw ( , volume-default ) ] :4096 ' 'disk ( alloc : storag ) ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 ' oper ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) i0109 16:36:08.528725 24134 status_update_manager_process.hpp:152 ] receiv oper statu updat operation_finish ( statu uuid : 0c79cdf2-b89d-453b-bb62-57766e968dd0 ) oper uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ' agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 i0109 16:36:08.529207 24134 status_update_manager_process.hpp:929 ] checkpoint updat oper statu updat operation_finish ( statu uuid : 0c79cdf2-b89d-453b-bb62-57766e968dd0 ) oper uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ' agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 i0109 16:36:08.573177 24150 http.cpp:1185 ] http post /slave ( 2 ) /api/v1/resource_provid 10.0.49.2:53598 i0109 16:36:08.573974 24139 slave.cpp:7065 ] handl resourc provid messag 'update_operation_statu : ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( latest state : operation_finish , statu updat state : operation_finish ) ' i0109 16:36:08.574154 24139 slave.cpp:7409 ] updat state oper ' id ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( latest state : operation_finish , statu updat state : operation_finish ) i0109 16:36:08.574785 24139 slave.cpp:7249 ] forward statu updat oper id ( operation_uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 i0109 16:36:08.583748 24084 slave.cpp:931 ] agent termin i0109 16:36:08.584115 24144 master.cpp:1305 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 slave ( 2 ) @ 10.0.49.2:40681 ( core-dev ) disconnect [ ... ] i0109 16:36:08.655766 24140 slave.cpp:1378 ] re-regist master master @ 10.0.49.2:40681 i0109 16:36:08.655936 24117 task_status_update_manager.cpp:188 ] resum send task statu updat i0109 16:36:08.655995 24149 hierarchical.cpp:669 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 ( core-dev ) updat total resourc cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] i0109 16:36:08.656008 24140 slave.cpp:1423 ] forward agent updat { `` oper '' : { } , '' resource_version_uuid '' : { `` valu '' : '' icuakyo6tymmt2y9vyf6jg== '' } , '' slave_id '' : { `` valu '' : '' 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 '' } , '' update_oversubscribed_resourc '' : true } i0109 16:36:08.656121 24149 hierarchical.cpp:754 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 reactiv w0109 16:36:08.656481 24113 master.cpp:7277 ] ! ! ! ! updat slave messag : slave_id { valu : `` 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 '' } update_oversubscribed_resourc : true oper { } resource_version_uuid { valu : `` \211\313\200+ # \272o ) \214\267f=\277 ! z & '' } i0109 16:36:08.656637 24113 master.cpp:7320 ] receiv updat agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 slave ( 3 ) @ 10.0.49.2:40681 ( core-dev ) total oversubscrib resourc { } w0109 16:36:08.657387 24113 master.cpp:7704 ] perform explicit reconcili agent known oper 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 sinc present origin reconcili messag agent i0109 16:36:08.657917 24133 hierarchical.cpp:669 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 ( core-dev ) updat total resourc cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] w0109 16:36:08.658048 24125 manager.cpp:472 ] drop oper reconcili messag operation_uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 resourc provid 605b22f5-e39d-4d9f-950a-e7f44d202c01 subscrib i0109 16:36:08.658609 24143 container_daemon.cpp:119 ] launch contain 'org-apache-mesos-rp-local-storage-test -- org-apache-mesos-csi-test-slrp_test -- controller_service-node_service' [ ... ] i0109 16:36:08.689859 24130 provider.cpp:3066 ] send update_st call resourc 'disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 ' 1 oper agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 i0109 16:36:08.690449 24130 provider.cpp:1042 ] resourc provid 605b22f5-e39d-4d9f-950a-e7f44d202c01 readi state i0109 16:36:08.690491 24105 status_update_manager_process.hpp:385 ] resum oper statu updat manag i0109 16:36:08.690640 24105 status_update_manager_process.hpp:394 ] send oper statu updat operation_finish ( statu uuid : 0c79cdf2-b89d-453b-bb62-57766e968dd0 ) oper uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ' agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 i0109 16:36:08.693244 24131 http.cpp:1185 ] http post /slave ( 3 ) /api/v1/resource_provid 10.0.49.2:53606 i0109 16:36:08.693912 24140 http.cpp:1185 ] http post /slave ( 3 ) /api/v1/resource_provid 10.0.49.2:53606 i0109 16:36:08.693974 24115 manager.cpp:677 ] receiv update_st call resourc ' [ { `` disk '' : { `` sourc '' : { `` id '' : '' 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' , '' metadata '' : { `` label '' : [ { `` key '' : '' path '' , '' valu '' : '' \/tmp\/n5thz3\/test\/4gb-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } ] } , '' mount '' : { `` root '' : '' .\/csi\/org.apache.mesos.csi.test\/slrp_test\/mounts\/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } , '' profil '' : '' volume-default '' , '' type '' : '' mount '' } } , '' name '' : '' disk '' , '' provider_id '' : { `` valu '' : '' 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } , '' reserv '' : [ { `` role '' : '' storag '' , '' type '' : '' dynam '' } ] , '' scalar '' : { `` valu '' :4096.0 } , '' type '' : '' scalar '' } ] ' 1 oper resourc provid 605b22f5-e39d-4d9f-950a-e7f44d202c01 i0109 16:36:08.694897 24144 slave.cpp:7065 ] handl resourc provid messag 'update_st : 605b22f5-e39d-4d9f-950a-e7f44d202c01 disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096' i0109 16:36:08.695184 24144 slave.cpp:7182 ] forward new total resourc cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] ; disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 i0109 16:36:08.696467 24144 slave.cpp:7065 ] handl resourc provid messag 'update_operation_statu : ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( latest state : operation_finish , statu updat state : operation_finish ) ' i0109 16:36:08.696594 24144 slave.cpp:7409 ] updat state oper ' id ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( latest state : operation_finish , statu updat state : operation_finish ) i0109 16:36:08.696666 24144 slave.cpp:7249 ] forward statu updat oper id ( operation_uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 w0109 16:36:08.697093 24142 master.cpp:7277 ] ! ! ! ! updat slave messag : slave_id { valu : `` 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 '' } update_oversubscribed_resourc : fals oper { } resource_version_uuid { valu : `` \211\313\200+ # \272o ) \214\267f=\277 ! z & '' } resource_provid { provid { info { id { valu : `` 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } type : `` org.apache.mesos.rp.local.storag '' name : `` test '' default_reserv { role : `` storag '' type : dynam } storag { plugin { type : `` org.apache.mesos.csi.test '' name : `` slrp_test '' contain { [ ... ] } } } } total_resourc { name : `` disk '' type : scalar scalar { valu : 4096 } disk { sourc { type : mount mount { root : `` ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } id : `` 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' metadata { label { key : `` path '' valu : `` /tmp/n5thz3/test/4gb-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } } profil : `` volume-default '' } } provider_id { valu : `` 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } reserv { role : `` storag '' type : dynam } } oper { oper { framework_id { valu : `` 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 '' } slave_id { valu : `` 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 '' } info { type : create_volum create_volum { sourc { name : `` disk '' type : scalar scalar { valu : 4096 } disk { sourc { type : raw profil : `` volume-default '' } } allocation_info { role : `` storag '' } provider_id { valu : `` 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } reserv { role : `` storag '' type : dynam } } target_typ : mount } } latest_statu { state : operation_finish converted_resourc { name : `` disk '' type : scalar scalar { valu : 4096 } disk { sourc { type : mount mount { root : `` ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } id : `` 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' metadata { label { key : `` path '' valu : `` /tmp/n5thz3/test/4gb-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } } profil : `` volume-default '' } } allocation_info { role : `` storag '' } provider_id { valu : `` 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } reserv { role : `` storag '' type : dynam } } uuid { valu : `` \014y\315\362\270\235 ; \273bwvn\226\215\320 '' } } status { state : operation_finish converted_resourc { name : `` disk '' type : scalar scalar { valu : 4096 } disk { sourc { type : mount mount { root : `` ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } id : `` 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' metadata { label { key : `` path '' valu : `` /tmp/n5thz3/test/4gb-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 '' } } profil : `` volume-default '' } } allocation_info { role : `` storag '' } provider_id { valu : `` 605b22f5-e39d-4d9f-950a-e7f44d202c01 '' } reserv { role : `` storag '' type : dynam } } uuid { valu : `` \014y\315\362\270\235 ; \273bwvn\226\215\320 '' } } uuid { valu : `` \030\264\304\245\321bm\317\273 ! \241 < n\340\364\010 '' } } } resource_version_uuid { valu : `` m\250\313j\320\301ig\262\0164e\004\367\304\333 '' } } } i0109 16:36:08.700137 24142 master.cpp:10411 ] updat state oper `` ( uuid : 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ) framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 ( latest state : operation_finish , statu updat state : operation_finish ) i0109 16:36:08.700417 24146 hierarchical.cpp:669 ] agent 046b3f21-6e97-4a56-9a13-773f7d481efd-s0 ( core-dev ) updat total resourc disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 ; cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] f0109 16:36:08.700610 24142 master.cpp:11687 ] check_som ( resourc ) : disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 ; cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] contain disk ( reserv : [ ( dynam , storag ) ] ) [ raw ( , volume-default ) ] :4096 * * * check failur stack trace : * * * f0109 16:36:08.700896 24146 hierarchical.cpp:908 ] check_som ( updatedtot ) : disk ( reserv : [ ( dynam , storag ) ] ) [ mount ( 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 , volume-default ) : ./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 ] :4096 ; cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] contain disk ( reserv : [ ( dynam , storag ) ] ) [ raw ( , volume-default ) ] :4096 * * * check failur stack trace : * * * @ 0x7ff06d3bbe7e ( unknown ) @ 0x7ff06d3bbe7e ( unknown ) @ 0x7ff06d3bbddd ( unknown ) @ 0x7ff06d3bbddd ( unknown ) @ 0x7ff06d3bb7ee ( unknown ) @ 0x7ff06d3bb7ee ( unknown ) @ 0x7ff06d3be522 ( unknown ) @ 0x55c1c6c2be77 _ztsn6lambda12callableonceifvpn7process11processbaseeee10callablefnins_8internal7partializns1_8internal8dispatchins1_6futureist4listin5mesos5slave13qoscorrectionesaisf_eeeeeclins0_ifsi_veeeeesi_rkns1_4upideot_eulst10unique_ptrins1_7promiseish_eest14default_deleteisu_eeosm_s3_e_isx_sm_st12_placeholderili1eeeeee @ 0x7ff06d3be522 ( unknown ) @ 0x55c1c6c2be77 _ztsn6lambda12callableonceifvpn7process11processbaseeee10callablefnins_8internal7partializns1_8internal8dispatchins1_6futureist4listin5mesos5slave13qoscorrectionesaisf_eeeeeclins0_ifsi_veeeeesi_rkns1_4upideot_eulst10unique_ptrins1_7promiseish_eest14default_deleteisu_eeosm_s3_e_isx_sm_st12_placeholderili1eeeeee @ 0x7ff06b729277 ( unknown ) @ 0x55c1c6f3be8a _ztsn6lambda12callableonceifvrk6resultin5mesos2v117resource_provider5eventeeee10callablefnins_8internal7partializnk7process6futureis6_e7onreadyist5_bindifst7_mem_fnimsg_fbs8_eesg_st12_placeholderili1eeeebeerksg_ot_nsg_6prefereeulosq_s8_e_isq_so_eeee { noformat } we see slrp reregist agent , follow happen : # the agent send { { updateslav } } messag master includ convert resourc { { create_volum } } oper statu { { operation_finish } } . # the master updat agent 's resourc , includ volum creat oper . # the agent resend oper statu updat . # the master tri appli oper crash , alreadi updat agent 's resourc step # 2 .",MESOS-8422,5.0
"rp manag incorrectli set framework id lead check failur the resourc provid manag [ uncondit set framework id|http : //github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp # l637 ] forward oper statu updat agent . thi incorrect , exampl , resourc provid [ gener operation_drop updat reconciliation|http : //github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp # l1653-l1657 ] , lead protobuf error case sinc framework id 's requir { { valu } } field left unset .",MESOS-8419,1.0
"check failur tri recov nest contain framework checkpoint enabl . { noformat } i0108 23:05:25.313344 31743 slave.cpp:620 ] agent attribut : [ ] i0108 23:05:25.313832 31743 slave.cpp:629 ] agent hostnam : vagrant-ubuntu-wily-64 i0108 23:05:25.314916 31763 task_status_update_manager.cpp:181 ] paus send task statu updat i0108 23:05:25.323496 31766 state.cpp:66 ] recov state '/var/lib/mesos/slave/meta' i0108 23:05:25.323639 31766 state.cpp:724 ] no commit checkpoint resourc found '/var/lib/mesos/slave/meta/resources/resources.info' i0108 23:05:25.326169 31760 task_status_update_manager.cpp:207 ] recov task statu updat manag i0108 23:05:25.326954 31759 containerizer.cpp:674 ] recov container f0108 23:05:25.331529 31759 containerizer.cpp:919 ] check_som ( container- > directori ) : none * * * check failur stack trace : * * * @ 0x7f769dbc98bd googl : :logmessag : :fail ( ) @ 0x7f769dbc8c8e googl : :logmessag : :sendtolog ( ) @ 0x7f769dbc958d googl : :logmessag : :flush ( ) @ 0x7f769dbcca08 googl : :logmessagefat : :~logmessagefat ( ) @ 0x556cb4c2b937 _checkfat : :~_checkfat ( ) @ 0x7f769c5ac653 meso : :intern : :slave : :mesoscontainerizerprocess : :recov ( ) { noformat } if framework enabl checkpoint . it mean slave state checkpoint . but contain still checkpoint runtim dir , mean recov nest contain would caus check failur due parent 's sandbox dir unknown .",MESOS-8416,5.0
"use uniqu id csi plugin contain slrp . if agent crash abnorm runtim directori lost , standalon contain previous launch slrp consid orphan agent restart . sinc orphan contain clean asynchron , possibl cleanup race slrp launch new standalon contain instanc id . to avoid race , use uniqu id csi plugin contain .",MESOS-8399,3.0
slrp newvolumerecoveri launchtaskrecoveri test check failur . check failur manifest two slrp test resourc upgrade/downgrad introduc .,MESOS-8393,2.0
"resourc provider-cap agent correctli synchron checkpoint agent resourc reregistr for resourc provider-cap agent master re-send checkpoint resourc agent reregistr ; instead checkpoint resourc sent part { { reregisterslavemessag } } use . thi happen realiti . if e.g. , checkpoint offer oper fail agent fail checkpoint resourc would , expect , reflect agent , would still assum master . a workaround fail master would lead newli elect master bootstrap agent state { { reregisterslavemessag } } .",MESOS-8350,2.0
"when resourc provid driver disconnect , fail reconnect . if resourc provid manag close http connect resourc provid , resourc provid reconnect . for , resourc provid driver chang state `` disconnect '' , call { { disconnect } } callback use endpoint detector reconnect . thi n't work test environ { { constantendpointdetector } } use . while resourc provid notifi close http connect ( log { { end-of-fil receiv } } ) , never disconnect call { { disconnect } } callback . discard { { httpconnectionprocess : :detect } } { { httpconnectionprocess : :disconnect } } n't trigger { { onani } } callback futur . thi might problem { { httpconnectionprocess } } could relat test case use { { constantendpointdetector } } .",MESOS-8349,2.0
"resubscript resourc provid crash agent http connect n't close a resourc provid might resubscrib old http connect n't properli close . in case agent crashm , e.g. , follow log : { noformat } i1219 13:33:51.937295 128610304 manager.cpp:570 ] subscrib resourc provid { `` id '' : { `` valu '' : '' 8e71beef-796e-4bde-9257-952ed0f230a5 '' } , '' name '' : '' test '' , '' type '' : '' org.apache.mesos.rp.test '' } i1219 13:33:51.937443 128610304 manager.cpp:134 ] termin resourc provid 8e71beef-796e-4bde-9257-952ed0f230a5 i1219 13:33:51.937760 128610304 manager.cpp:134 ] termin resourc provid 8e71beef-796e-4bde-9257-952ed0f230a5 e1219 13:33:51.937851 129683456 http_connection.hpp:445 ] end-of-fil receiv i1219 13:33:51.937865 131293184 slave.cpp:7105 ] handl resourc provid messag 'disconnect : resourc provid 8e71beef-796e-4bde-9257-952ed0f230a5' i1219 13:33:51.937968 131293184 slave.cpp:7347 ] forward new total resourc cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] f1219 13:33:51.938052 132366336 manager.cpp:606 ] check fail : resourceproviders.subscribed.contain ( resourceproviderid ) * * * check failur stack trace : * * * e1219 13:33:51.938583 130756608 http_connection.hpp:445 ] end-of-fil receiv i1219 13:33:51.938987 129683456 hierarchical.cpp:669 ] agent 0019c3fa-28c5-43a9-88d0-709eee271c62-s0 ( 172.18.8.13 ) updat total resourc cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] @ 0x1125380ef googl : :logmessagefat : :~logmessagefat ( ) @ 0x112534ae9 googl : :logmessagefat : :~logmessagefat ( ) i1219 13:33:51.939131 129683456 hierarchical.cpp:1517 ] perform alloc 1 agent 61830n i1219 13:33:51.945793 2646795072 slave.cpp:927 ] agent termin i1219 13:33:51.945955 129146880 master.cpp:1305 ] agent 0019c3fa-28c5-43a9-88d0-709eee271c62-s0 slave ( 1 ) @ 172.18.8.13:64430 ( 172.18.8.13 ) disconnect i1219 13:33:51.945979 129146880 master.cpp:3364 ] disconnect agent 0019c3fa-28c5-43a9-88d0-709eee271c62-s0 slave ( 1 ) @ 172.18.8.13:64430 ( 172.18.8.13 ) i1219 13:33:51.946022 129146880 master.cpp:3383 ] deactiv agent 0019c3fa-28c5-43a9-88d0-709eee271c62-s0 slave ( 1 ) @ 172.18.8.13:64430 ( 172.18.8.13 ) i1219 13:33:51.946081 131293184 hierarchical.cpp:766 ] agent 0019c3fa-28c5-43a9-88d0-709eee271c62-s0 deactiv @ 0x115f2761d meso : :intern : :resourceprovidermanagerprocess : :subscrib ( ) : : $ _2 : :oper ( ) ( ) @ 0x115f2977d _zn5cpp176invokeizn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns2_14httpconnectionerkns1_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeeeedtclclsr3stde7forwardit_efp_espclsr3stde7forwardit0_efp0_eeeosg_dposh_ @ 0x115f29740 _zn6lambda8internal7partializn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns3_14httpconnectionerkns2_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeee13invoke_expandisc_nst3__15tupleijsg_eeensk_ijeeejlm0eeeedtclsr5cpp17e6invokeclsr3stde7forwardit_efp_espcl6expandclsr3stde3getixt2_eeclsr3stde7forwardit0_efp0_eeclsr3stde7forwardit1_efp2_eeeeosn_oso_n5cpp1416integer_sequenceimjxspt2_eeeeosp_ @ 0x115f296bb _zno6lambda8internal7partializn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns3_14httpconnectionerkns2_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeeeclijeeedtcl13invoke_expandcll_znst3__14moveirsc_eeonsj_16remove_referenceit_e4typeeosn_edtdefpt1fecll_znsk_irnsj_5tupleijsg_eeeeesq_sr_edtdefpt10bound_argsecvn5cpp1416integer_sequenceimjlm0eeee_eclsr3stde16forward_as_tuplespclsr3stde7forwardit_efp_eeeedposy_ @ 0x115f2965d _zn5cpp176invokein6lambda8internal7partializn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns5_14httpconnectionerkns4_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeeeejeeedtclclsr3stde7forwardit_efp_espclsr3stde7forwardit0_efp0_eeeosk_dposl_ @ 0x115f29631 _zn6lambda8internal6invokeiveclins0_7partializn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns6_14httpconnectionerkns5_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeeeejeeevot_dpot0_ @ 0x115f29526 _zno6lambda12callableonceifvvee10callablefnins_8internal7partializn5mesos8internal30resourceprovidermanagerprocess9subscribeerkns7_14httpconnectionerkns6_17resource_provider14call_subscribeee3 $ _2jn7process6futurei7nothingeeeeeeclev @ 0x10b6ca690 _zno6lambda12callableonceifvveeclev @ 0x10be09295 _zzn7process8internal8dispatchiveclin6lambda12callableonceifvveeeeevrkns_4upideot_enkulos7_pns_11processbaseee_clesd_sf_ @ 0x10be09180 _zn5cpp176invokeizn7process8internal8dispatchiveclin6lambda12callableonceifvveeeeevrkns1_4upideot_eulos9_pns1_11processbaseee_js9_sh_eeedtclclsr3stde7forwardisd_efp_espclsr3stde7forwardit0_efp0_eeese_dposj_ @ 0x10be0912b _zn6lambda8internal7partializn7process8internal8dispatchiveclins_12callableonceifvveeeeevrkns2_4upideot_eulos9_pns2_11processbaseee_js9_nst3__112placeholders4__phili1eeeee13invoke_expandisi_nsj_5tupleijs9_sm_eeensp_ijosh_eeejlm0elm1eeeedtclsr5cpp17e6invokeclsr3stde7forwardisd_efp_espcl6expandclsr3stde3getixt2_eeclsr3stde7forwardit0_efp0_eeclsr3stde7forwardit1_efp2_eeeese_ost_n5cpp1416integer_sequenceimjxspt2_eeeeosu_ @ 0x10be0905f _zno6lambda8internal7partializn7process8internal8dispatchiveclins_12callableonceifvveeeeevrkns2_4upideot_eulos9_pns2_11processbaseee_js9_nst3__112placeholders4__phili1eeeeeclijsh_eeedtcl13invoke_expandcll_znsj_4moveirsi_eeonsj_16remove_referenceisd_e4typeese_edtdefpt1fecll_znsp_irnsj_5tupleijs9_sm_eeeeesu_se_edtdefpt10bound_argsecvn5cpp1416integer_sequenceimjlm0elm1eeee_eclsr3stde16forward_as_tuplespclsr3stde7forwardit_efp_eeeedpos11_ @ 0x10be08f4d _zn5cpp176invokein6lambda8internal7partializn7process8internal8dispatchiveclins1_12callableonceifvveeeeevrkns4_4upideot_eulosb_pns4_11processbaseee_jsb_nst3__112placeholders4__phili1eeeeeejsj_eeedtclclsr3stde7forwardisf_efp_espclsr3stde7forwardit0_efp0_eeesg_dposq_ @ 0x10be08f11 _zn6lambda8internal6invokeiveclins0_7partializn7process8internal8dispatchiveclins_12callableonceifvveeeeevrkns5_4upideot_eulosc_pns5_11processbaseee_jsc_nst3__112placeholders4__phili1eeeeeejsk_eeevsh_dpot0_ @ 0x10be08d36 _zno6lambda12callableonceifvpn7process11processbaseeee10callablefnins_8internal7partializns1_8internal8dispatchiveclins0_ifvveeeeevrkns1_4upideot_eulose_s3_e_jse_nst3__112placeholders4__phili1eeeeeeecleos3_ @ 0x11fd64bc9 _zno6lambda12callableonceifvpn7process11processbaseeeecles3_ @ 0x11fd64a69 process : :processbas : :consum ( ) @ 0x11fe20ac4 _zno7process13dispatchevent7consumeepns_13eventconsumer @ 0x113c77819 process : :processbas : :serv ( ) @ 0x11fd5b8c9 process : :processmanag : :resum ( ) @ 0x11fe8260b process : :processmanag : :init_thread ( ) : : $ _1 : :oper ( ) ( ) @ 0x11fe82190 _znst3__114__thread_proxyins_5tupleijns_10unique_ptrins_15__thread_structens_14default_deleteis3_eeeezn7process14processmanager12init_threadseve3 $ _1eeeeepvsb_ @ 0x7fff64da56c1 _pthread_bodi @ 0x7fff64da556d _pthread_start @ 0x7fff64da4c5d thread_start abort trap : 6 { noformat } thi due race condit { { resource_provider/manager.cpp } } handl close http connect resourc provid . if resourc provid resubscrib old http connect still open , resourc provid manag close . thi unexpect trigger close new http connect result fail { { check } } .",MESOS-8346,2.0
"meso container properli handl old run contain we test upgrad scenario recent encount follow assert failur : { code } dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.693977 20810 http.cpp:3116 ] process launch_nested_container_sess call contain 'a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f' dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.695179 20807 containerizer.cpp:1169 ] tri chown '/var/lib/mesos/slave/slaves/aaf0a62f-a6eb-4c1d-80db-5fdd26fe8008-s12/frameworks/dcf5f8b5-86a8-44df-ac03-b39404239ad8-0377/executors/kafka__68baefd4-aa8c-4b97-a23e-eb6a73fa91f6/runs/a89b211a-4549-462d-9cc7-0ea2bac2f729/containers/1c262420-7525-4fee-99c1-aff4f66996bd/containers/check-a41362ae-13c6-4750-990e-a1a0b2792b5f ' user 'nobody' dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : w1212 16:45:42.695309 20807 containerizer.cpp:1198 ] can determin executor_info root contain 'a89b211a-4549-462d-9cc7-0ea2bac2f729 ' config recov . dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.695327 20807 containerizer.cpp:1203 ] start contain a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.695829 20807 containerizer.cpp:2932 ] transit state contain a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f provis prepar dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.700569 20811 systemd.cpp:98 ] assign child process '20941 ' 'mesos_executors.slice' dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.702945 20811 systemd.cpp:98 ] assign child process '20942 ' 'mesos_executors.slice' dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : i1212 16:45:42.706069 20806 switchboard.cpp:575 ] creat i/o switchboard server ( pid : 20943 ) listen socket file '/tmp/mesos-io-switchboard-74af71bb-2385-4dde-9762-94d0196124d3 ' contain a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : mesos-ag : /pkg/src/mesos/3rdparty/stout/include/stout/option.hpp:115 : t & option < t > : :get ( ) & [ t = meso : :slave : :containerconfig ] : assert ` issom ( ) ' fail . dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : * * * abort 1513097142 ( unix time ) tri `` date -d @ 1513097142 '' use gnu date * * * dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : pc : @ 0x7f472f2851f7 __gi_rais dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : * * * sigabrt ( @ 0x5134 ) receiv pid 20788 ( tid 0x7f472a2bf700 ) pid 20788 ; stack trace : * * * dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f6225e0 ( unknown ) dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f2851f7 __gi_rais dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f2868e8 __gi_abort dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f27e266 __assert_fail_bas dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f27e312 __gi___assert_fail dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f4731c481e3 _znr6optionin5mesos5slave15containerconfigee3getev.part.170 dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f4731c61c2d meso : :intern : :slave : :mesoscontainerizerprocess : :_launch ( ) dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f4731c7f403 _zn5cpp176invokeizn7process8dispatchin5mesos8internal5slave13containerizer12launchresultens5_25mesoscontainerizerprocesserkns3_11containeriderk6optionins3_5slave11containerioeerkst3mapissssst4lessissesaist4pairiksssseeerksc_issesb_sh_sr_su_eens1_6futureit_eerkns1_3pidit0_eemsz_fsx_t1_t2_t3_t4_eot5_ot6_ot7_ot8_eulst10unique_ptrins1_7promiseis7_eest14default_deleteis1j_eeos9_osf_osp_oss_pns1_11processbaseee_is1m_s9_sf_sp_ss_s1s_eeedtclcl7forwardisw_efp_espcl7forwardit0_efp0_eeeosw_dpos1u_ dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f4731c7f4f1 _zno6lambda12callableonceifvpn7process11processbaseeee10callablefnins_8internal7partializns1_8dispatchin5mesos8internal5slave13containerizer12launchresultensc_25mesoscontainerizerprocesserknsa_11containeriderk6optioninsa_5slave11containerioeerkst3mapissssst4lessissesaist4pairiksssseeerksj_issesi_so_sy_s11_eens1_6futureit_eerkns1_3pidit0_eems16_fs14_t1_t2_t3_t4_eot5_ot6_ot7_ot8_eulst10unique_ptrins1_7promiseise_eest14default_deleteis1q_eeosg_osm_osw_osz_s3_e_is1t_sg_sm_sw_sz_st12_placeholderili1eeeeeecleos3_ dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f47325dbb31 process : :processbas : :consum ( ) dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f47325ea882 process : :processmanag : :resum ( ) dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f47325efcf6 _znst6thread5_implist12_bind_simpleifzn7process14processmanager12init_threadseveulve_veee6_m_runev dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472fafa230 ( unknown ) dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f61ae25 start_thread dec 12 16:45:42 agent.hostnam mesos-ag [ 20788 ] : @ 0x7f472f34834d __clone dec 12 16:45:42 agent.hostnam systemd [ 1 ] : dcos-mesos-slave.servic : main process exit , code=kil , status=6/abrt dec 12 16:45:42 agent.hostnam systemd [ 1 ] : unit dcos-mesos-slave.servic enter fail state . dec 12 16:45:42 agent.hostnam systemd [ 1 ] : dcos-mesos-slave.servic fail . { code } look { { slave : :_launch } } , inde find unguard access parent contain 's { { containerconfig } } [ here|http : //github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp # l1716 ] . we recent [ ad checkpointing|http : //github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545 ] { { containerconfig } } meso container . it seem appropri handl upgrad , may old contain run expect recov { { containerconfig } } .",MESOS-8325,2.0
pass resourc provid inform master part updateslavemessag we extend { { updateslavemessag } } updat agent 's total resourc resourc provid possibl . we realiz need explicitli pass resourc provid detail ( : { { resourceproviderinfo } } ) master queri provid present certain agent . thi happen part { { updateslavemessag } } singl synchron channel use kind inform . we need adjust { { updateslavemessag } } requir . thi happen 1.5.0 get releas need deprec never realli use messag format .,MESOS-8312,5.0
"meso container gc set 'layer ' checkpoint layer id provision . { noformat } 11111 222222 333333 444444 11111 222222 333333 444444 i1129 23:24:45.469543 6592 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/mvgvc7/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/mvgvc7/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e/rootfs.overlay' i1129 23:24:45.473287 6592 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/mvgvc7/sha256 : b56ae66c29370df48e7377c8f9baa744a3958058a766793f821dadcb144a4647 rootf '/tmp/mesos/store/docker/staging/mvgvc7/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3/rootfs.overlay' i1129 23:24:45.582002 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs.overlay' i1129 23:24:45.589404 6595 metadata_manager.cpp:167 ] success cach imag 'alpine' i1129 23:24:45.590204 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs.overlay' i1129 23:24:45.595190 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs.overlay' i1129 23:24:45.599500 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs.overlay' i1129 23:24:45.602047 6597 provisioner.cpp:506 ] provis imag rootf '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35 ' contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 use overlay backend i1129 23:24:45.602751 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 rootf '/tmp/mesos/store/docker/staging/6zbc17/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs.overlay' i1129 23:24:45.603054 6596 overlay.cpp:168 ] creat symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/link ' - > '/tmp/xawq8y' i1129 23:24:45.604398 6596 overlay.cpp:196 ] provis imag rootf overlayf : 'lowerdir=/tmp/xawq8y/1 : /tmp/xawq8y/0 , upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/upperdir , workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/workdir' i1129 23:24:45.607802 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs.overlay' i1129 23:24:45.612139 6594 registry_puller.cpp:395 ] extract layer tar ball '/tmp/mesos/store/docker/staging/6zbc17/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/mesos/store/docker/staging/6zbc17/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs.overlay' i1129 23:24:45.612253 6593 containerizer.cpp:1369 ] checkpoint containerconfig '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/config' i1129 23:24:45.612298 6593 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 provis prepar i1129 23:24:45.625658 6596 containerizer.cpp:1838 ] launch 'mesos-container ' flag ' -- help= '' fals '' -- launch_info= '' { `` clone_namespac '' : [ 131072 ] , '' command '' : { `` shell '' : true , '' valu '' : '' sleep 1 '' } , '' environ '' : { `` variabl '' : [ { `` name '' : '' mesos_sandbox '' , '' type '' : '' valu '' , '' valu '' : '' \/mnt\/mesos\/sandbox '' } , { `` name '' : '' path '' , '' type '' : '' valu '' , '' valu '' : '' \/usr\/local\/sbin : \/usr\/local\/bin : \/usr\/sbin : \/usr\/bin : \/sbin : \/bin '' } , { `` name '' : '' mesos_container_ip '' , '' type '' : '' valu '' , '' valu '' : '' 10.0.2.15 '' } ] } , '' pre_exec_command '' : [ { `` argument '' : [ `` mesos-container '' , '' mount '' , '' -- help=fals '' , '' -- operation=make-rslav '' , '' -- path=\/ '' ] , '' shell '' : fals , '' valu '' : '' \/vagrant\/mesos\/build\/src\/mesos-container '' } , { `` argument '' : [ `` mount '' , '' -n '' , '' -- rbind '' , '' \/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-s0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5 '' , '' \/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35\/mnt\/mesos\/sandbox '' ] , '' shell '' : fals , '' valu '' : '' mount '' } ] , '' rootf '' : '' \/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35 '' , '' task_environ '' : { } , '' user '' : '' root '' , '' working_directori '' : '' \/mnt\/mesos\/sandbox '' } '' -- pipe_read= '' 12 '' -- pipe_write= '' 15 '' -- runtime_directory= '' /var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5 '' -- unshare_namespace_mnt= '' fals '' ' i1129 23:24:45.626317 6598 linux_launcher.cpp:438 ] launch nest contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 clone namespac clone_newn i1129 23:24:45.633211 6598 systemd.cpp:96 ] assign child process '6745 ' 'mesos_executors.slice' i1129 23:24:45.636270 6596 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 prepar isol i1129 23:24:45.691830 6597 metadata_manager.cpp:167 ] success cach imag 'mesosphere/inky' i1129 23:24:45.694399 6594 provisioner.cpp:506 ] provis imag rootf '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/rootfses/1187cc83-a23a-4390-9c28-092a7b7690b5 ' contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 use overlay backend i1129 23:24:45.694919 6596 overlay.cpp:168 ] creat symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/link ' - > '/tmp/gxhxit' i1129 23:24:45.695103 6596 overlay.cpp:196 ] provis imag rootf overlayf : 'lowerdir=/tmp/gxhxit/6 : /tmp/gxhxit/5 : /tmp/gxhxit/4 : /tmp/gxhxit/3 : /tmp/gxhxit/2 : /tmp/gxhxit/1 : /tmp/gxhxit/0 , upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/upperdir , workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/workdir' i1129 23:24:45.696255 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.696349 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.696449 6593 containerizer.cpp:1369 ] checkpoint containerconfig '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/config' i1129 23:24:45.696506 6593 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 provis prepar i1129 23:24:45.697865 6595 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.697918 6595 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.697968 6595 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.697999 6595 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.698025 6595 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.698050 6595 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.698076 6595 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.698104 6595 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.698129 6595 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.698894 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.698966 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.700333 6596 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.700394 6596 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.700412 6596 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.700428 6596 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.700441 6596 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.700454 6596 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.700467 6596 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.700480 6596 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.700495 6596 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.702491 6594 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.702554 6594 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.703707 6592 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.703783 6592 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.703812 6592 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.703816 6592 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.703816 6592 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.704164 6592 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.704208 6592 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.704255 6592 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.704285 6592 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.704814 6595 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.704861 6595 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.708112 6592 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.708204 6592 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.708238 6592 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.708264 6592 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.708375 6592 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.708407 6592 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.708472 6592 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.708514 6592 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.708545 6592 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.709048 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.709161 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.710321 6594 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.710382 6594 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.710412 6594 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.710436 6594 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.710458 6594 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.710480 6594 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.710522 6594 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.710551 6594 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.710590 6594 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.713176 6594 containerizer.cpp:1838 ] launch 'mesos-container ' flag ' -- help= '' fals '' -- launch_info= '' { `` clone_namespac '' : [ 131072 ] , '' command '' : { `` shell '' : true , '' valu '' : '' sleep 100000 '' } , '' environ '' : { `` variabl '' : [ { `` name '' : '' mesos_sandbox '' , '' type '' : '' valu '' , '' valu '' : '' \/mnt\/mesos\/sandbox '' } , { `` name '' : '' home '' , '' type '' : '' valu '' , '' valu '' : '' \/ '' } , { `` name '' : '' path '' , '' type '' : '' valu '' , '' valu '' : '' \/usr\/local\/sbin : \/usr\/local\/bin : \/usr\/sbin : \/usr\/bin : \/sbin : \/bin '' } , { `` name '' : '' mesos_container_ip '' , '' type '' : '' valu '' , '' valu '' : '' 10.0.2.15 '' } ] } , '' pre_exec_command '' : [ { `` argument '' : [ `` mesos-container '' , '' mount '' , '' -- help=fals '' , '' -- operation=make-rslav '' , '' -- path=\/ '' ] , '' shell '' : fals , '' valu '' : '' \/vagrant\/mesos\/build\/src\/mesos-container '' } , { `` argument '' : [ `` mount '' , '' -n '' , '' -- rbind '' , '' \/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-s0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3 '' , '' \/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5\/mnt\/mesos\/sandbox '' ] , '' shell '' : fals , '' valu '' : '' mount '' } ] , '' rootf '' : '' \/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5 '' , '' task_environ '' : { } , '' user '' : '' root '' , '' working_directori '' : '' \/mnt\/mesos\/sandbox '' } '' -- pipe_read= '' 13 '' -- pipe_write= '' 14 '' -- runtime_directory= '' /var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3 '' -- unshare_namespace_mnt= '' fals '' ' i1129 23:24:45.713954 6597 linux_launcher.cpp:438 ] launch nest contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 clone namespac clone_newn i1129 23:24:45.721781 6597 systemd.cpp:96 ] assign child process '6775 ' 'mesos_executors.slice' i1129 23:24:45.725494 6594 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 prepar isol i1129 23:24:45.791635 6595 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 isol fetch i1129 23:24:45.791880 6591 fetcher.cpp:379 ] start fetch uri contain : 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 , directori : /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-s0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:45.792626 6591 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 fetch run 11111 222222 333333 444444 i1129 23:24:45.807262 6592 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.807375 6592 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:45.808658 6591 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:45.808843 6591 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:45.808869 6591 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:45.808897 6591 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:45.808962 6591 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:45.808990 6591 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:45.809012 6591 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:45.809036 6591 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:45.809057 6591 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:45.893280 6596 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 isol fetch i1129 23:24:45.893523 6596 fetcher.cpp:379 ] start fetch uri contain : 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 , directori : /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-s0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3 i1129 23:24:45.894335 6594 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 fetch run i1129 23:24:45.902606 6598 process.cpp:3503 ] handl http event process 'slave ( 1 ) ' path : '/slave ( 1 ) /api/v1/executor' i1129 23:24:45.903908 6597 process.cpp:3503 ] handl http event process 'slave ( 1 ) ' path : '/slave ( 1 ) /api/v1' i1129 23:24:45.904618 6597 process.cpp:3503 ] handl http event process 'slave ( 1 ) ' path : '/slave ( 1 ) /api/v1' i1129 23:24:45.908681 6597 http.cpp:1185 ] http post /slave ( 1 ) /api/v1 10.0.2.15:57620 i1129 23:24:45.909113 6597 http.cpp:1185 ] http post /slave ( 1 ) /api/v1 10.0.2.15:57622 i1129 23:24:45.909708 6597 http.cpp:2589 ] process wait_nested_contain call contain '3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5' i1129 23:24:45.910148 6597 http.cpp:2589 ] process wait_nested_contain call contain '3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3' i1129 23:24:45.938350 6596 process.cpp:3503 ] handl http event process 'slave ( 1 ) ' path : '/slave ( 1 ) /api/v1/executor' i1129 23:24:45.938781 6596 http.cpp:1185 ] http post /slave ( 1 ) /api/v1/executor 10.0.2.15:57564 i1129 23:24:45.939141 6596 slave.cpp:4584 ] handl statu updat task_run ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.940809 6591 http.cpp:1185 ] http post /slave ( 1 ) /api/v1/executor 10.0.2.15:57564 i1129 23:24:45.941130 6591 slave.cpp:4584 ] handl statu updat task_run ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.942906 6591 task_status_update_manager.cpp:328 ] receiv task statu updat task_run ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.943076 6591 task_status_update_manager.cpp:383 ] forward task statu updat task_run ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 agent i1129 23:24:45.943322 6595 slave.cpp:5067 ] forward updat task_run ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 master @ 127.0.0.1:5050 i1129 23:24:45.943332 6591 task_status_update_manager.cpp:328 ] receiv task statu updat task_run ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.943506 6591 task_status_update_manager.cpp:383 ] forward task statu updat task_run ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 agent i1129 23:24:45.943717 6595 slave.cpp:4960 ] task statu updat manag success handl statu updat task_run ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.943905 6595 slave.cpp:5067 ] forward updat task_run ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 master @ 127.0.0.1:5050 i1129 23:24:45.943905 6595 slave.cpp:4960 ] task statu updat manag success handl statu updat task_run ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.992866 6591 task_status_update_manager.cpp:401 ] receiv task statu updat acknowledg ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.993261 6595 slave.cpp:3868 ] task statu updat manag success handl statu updat acknowledg ( uuid : 6ba2641b-ff6a-4ac2-9123-23579534147d ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.993968 6593 task_status_update_manager.cpp:401 ] receiv task statu updat acknowledg ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:45.994295 6598 slave.cpp:3868 ] task statu updat manag success handl statu updat acknowledg ( uuid : f22fe2c5-2a2e-43c9-9f22-a7da351bab08 ) task 2 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 11111 222222 333333 444444 i1129 23:24:46.808684 6592 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:46.808876 6592 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer i1129 23:24:46.810683 6593 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:46.810751 6593 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:46.810781 6593 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:46.810808 6593 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:46.810834 6593 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:46.810860 6593 store.cpp:531 ] layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' retain imag store cach i1129 23:24:46.810885 6593 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:46.810911 6593 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:46.810937 6593 store.cpp:531 ] layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' retain imag store cach i1129 23:24:47.057590 6596 containerizer.cpp:2775 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 exit i1129 23:24:47.057667 6596 containerizer.cpp:2324 ] destroy contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 run state i1129 23:24:47.057695 6596 containerizer.cpp:2926 ] transit state contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 run destroy i1129 23:24:47.058082 6596 linux_launcher.cpp:514 ] ask destroy contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:47.059027 6596 linux_launcher.cpp:560 ] use freezer destroy cgroup mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:47.060667 6596 cgroups.cpp:3058 ] freez cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:47.062700 6597 cgroups.cpp:1413 ] success froze cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 1.838336m i1129 23:24:47.064627 6592 cgroups.cpp:3076 ] thaw cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:47.066498 6598 cgroups.cpp:1442 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 1.642752m i1129 23:24:47.071521 6592 provisioner.cpp:648 ] destroy contain rootf '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35 ' contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 i1129 23:24:47.098203 6596 overlay.cpp:296 ] remov temporari directori '/tmp/xawq8i ' point '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links' i1129 23:24:47.100265 6591 containerizer.cpp:2613 ] checkpoint termin state nest contain 's runtim directori '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/termination' i1129 23:24:47.107206 6594 process.cpp:3503 ] handl http event process 'slave ( 1 ) ' path : '/slave ( 1 ) /api/v1/executor' i1129 23:24:47.151911 6594 http.cpp:1185 ] http post /slave ( 1 ) /api/v1/executor 10.0.2.15:57564 i1129 23:24:47.152243 6594 slave.cpp:4584 ] handl statu updat task_finish ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.154391 6594 task_status_update_manager.cpp:328 ] receiv task statu updat task_finish ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.154578 6594 task_status_update_manager.cpp:383 ] forward task statu updat task_finish ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 agent i1129 23:24:47.154810 6593 slave.cpp:5067 ] forward updat task_finish ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 master @ 127.0.0.1:5050 i1129 23:24:47.157249 6593 slave.cpp:4960 ] task statu updat manag success handl statu updat task_finish ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.385977 6592 task_status_update_manager.cpp:401 ] receiv task statu updat acknowledg ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.386334 6592 task_status_update_manager.cpp:538 ] clean statu updat stream task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.387459 6592 slave.cpp:3868 ] task statu updat manag success handl statu updat acknowledg ( uuid : 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44 ) task 1 framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 i1129 23:24:47.387568 6592 slave.cpp:8457 ] complet task 1 11111 222222 333333 444444 i1129 23:24:47.818768 6595 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:47.824591 6598 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:47.824724 6598 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:47.824753 6598 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:47.824767 6598 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:47.824782 6598 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:47.824861 6598 store.cpp:550 ] mark layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' gc renam '/tmp/mesos/store/docker/layers/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06 ' '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784' i1129 23:24:47.824918 6598 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:47.824960 6598 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach i1129 23:24:47.825088 6598 store.cpp:550 ] mark layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' gc renam '/tmp/mesos/store/docker/layers/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3 ' '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040' i1129 23:24:47.825389 6598 store.cpp:577 ] delet path '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040' i1129 23:24:47.829903 6598 store.cpp:584 ] delet '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040' i1129 23:24:47.829980 6598 store.cpp:577 ] delet path '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784' i1129 23:24:47.830047 6598 store.cpp:584 ] delet '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784' 11111 222222 333333 444444 i1129 23:24:48.829519 6595 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:48.831161 6598 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:48.831225 6598 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:48.831248 6598 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:48.831266 6598 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:48.831284 6598 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:48.831302 6598 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:48.831321 6598 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach 11111 222222 333333 444444 i1129 23:24:49.830904 6597 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:49.832487 6597 store.cpp:531 ] layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' retain imag store cach i1129 23:24:49.832584 6597 store.cpp:531 ] layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' retain imag store cach i1129 23:24:49.833329 6597 store.cpp:531 ] layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' retain imag store cach i1129 23:24:49.833367 6597 store.cpp:531 ] layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' retain imag store cach i1129 23:24:49.833387 6597 store.cpp:531 ] layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' retain imag store cach i1129 23:24:49.833406 6597 store.cpp:531 ] layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' retain imag store cach i1129 23:24:49.833425 6597 store.cpp:531 ] layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' retain imag store cach { noformat } pleas neglect debug log like '111111 ' . to reproduc issu , continu trigger imag gc . the log scenario launch two nest contain . one sleep 1 second , anoth sleep forev . thi relat patch : http : //github.com/apache/mesos/commit/e273efe6976434858edb85bbcf367a02e963a467 # diff-a3593ed0ebd2b205775f7f04d9b5afe7 the root caus set 'layer ' checkpoint layer id provision . the log prove : { noformat } i1129 23:24:45.698894 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 checkpoint layer i1129 23:24:45.698966 6596 provisioner.cpp:714 ] contain 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 checkpoint layer { noformat }",MESOS-8280,3.0
"support imag prune meso container provision . implement imag prune container provision , use mark sweep garbag collect unus layer .",MESOS-8249,13.0
"unifi container auto backend check xf ftype overlayf backend . use xf back filesystem unifi container , ` ftype ` equal 1 use overlay fs backend . add detect auto backend logic os ( like cento 7.2 ) xf ftype=0 default . http : //docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/",MESOS-8121,3.0
"v1 role-rel endpoint need reflect hierarch account . with introduct hierarch role , role-rel endpoint need updat provid aggreg account inform . for exampl , inform mani resourc alloc `` /eng '' includ resourc alloc `` /eng/frontend '' `` /eng/backend '' , sinc quota guarante limit also appli aggreg . thi also affect ui display , exampl 'role ' tab .",MESOS-8069,8.0
"agent master race updat agent state . in { { 2af9a5b07dc80151154264e974d03f56a1c25838 } } introduc use { { updateslavemessag } } agent inform master current total resourc . current trigger messag agent registr reregistr . thi race oper appli master commun via { { checkpointresourcesmessag } } . exampl : 1 . agent ( { { cpus:4 ( \ * ) } } regist . 2 . master trigger appli oper agent 's resourc , e.g. , reserv : { { cpus:4 ( \ * ) - > cpus:4 ( a ) } } . the master appli oper current view agent 's resourc send agent { { checkpointresourcesmessag } } agent persist result . 3 . the agent send master { { updateslavemessag } } , e.g. , { { cpus:4 ( \ * ) } } sinc n't receiv { { checkpointresourcesmessag } } yet . 4 . the master process { { updateslavemessag } } updat view agent 's resourc { { cpus:4 ( \ * ) } } . 5 . the agent process { { checkpointresourcesmessag } } updat view resourc { { cpus:4 ( a ) } } . 6 . the agent master inconsist view agent 's resourc .",MESOS-8058,2.0
"oom due libeventsslsocket send incorrectli return 0 shutdown . libeventsslsocket return 0 send incorrectli , lead caller send data twice ! see : http : //github.com/apache/mesos/blob/1.3.1/3rdparty/libprocess/src/libevent_ssl_socket.cpp # l396-l398 in particular case , 's possibl caller keep get back 0 loop infinit , blow memori oom process . one exampl send occur shutdown : { code } test_f ( ssltest , shutdownthensend ) { clock : :paus ( ) ; tri < socket > server = setup_serv ( { { `` libprocess_ssl_en '' , `` true '' } , { `` libprocess_ssl_key_fil '' , key_path ( ) .string ( ) } , { `` libprocess_ssl_cert_fil '' , certificate_path ( ) .string ( ) } } ) ; assert_som ( server ) ; assert_som ( server.get ( ) .address ( ) ) ; assert_som ( server.get ( ) .address ( ) .get ( ) .hostnam ( ) ) ; futur < socket > socket = server.get ( ) .accept ( ) ; clock : :settl ( ) ; expect_tru ( socket.ispend ( ) ) ; tri < socket > client = socket : :creat ( socketimpl : :kind : :ssl ) ; assert_som ( client ) ; await_assert_readi ( client- > connect ( server- > address ( ) .get ( ) ) ) ; await_assert_readi ( socket ) ; expect_som ( socket ( socket.get ( ) ) .shutdown ( ) ) ; // thi loop forev ! await_fail ( socket ( socket.get ( ) ) .send ( `` hello world '' ) ) ; } { code }",MESOS-7934,5.0
"fix commun old master new agent . for re-registr , agent current send resourc task executor master `` post-reservation-refin '' format , incompat pre-1.4 master . we chang agent alway downgrad resourc `` pre-reservation-refin '' format , master uncondit upgrad resourc `` post-reservation-refin '' format .",MESOS-7922,2.0
"non-checkpoint framework 's task mark lost agent disconnect . current , framework checkpoint disabl task run agent agent disconnect master , master mark task lost remov memori . the assumpt agent disconnect termin . howev , 's possibl disconnect occur due transient loss connect agent re-connect never termin . thi case violat assumpt unknown task master : `` ` void master : :reconcileknownslav ( slave * slave , const vector < executorinfo > & executor , const vector < task > & task ) { ... // todo ( bmahler ) : there 's implicit assumpt slave // task unknown master . thi _should_ // case sinc causal relationship : // slave remov task - > master remov task // add error log violat assumpt ! `` ` as result , task would remain agent master would know ! a appropri action would : # when agent disconnect , mark task unreach . # # if framework partit awar , show last known task state . # # if framework partit awar , let know 's unreach . # if agent re-connect : # # and agent restart , let non-checkpoint framework know task gone/lost . # # if agent still hold task , task restor reachabl . # if agent get remov : # # for partit awar non-checkpoint framework , let know task unreach . # # for non partit awar non-checkpoint framework , let know task lost kill agent come back .",MESOS-7911,5.0
"meso master rescind in-flight offer regist agent new mainten schedul post subset slave we run meso 1.1.0 product . we use custom autoscal scale meso cluster . while scale cluster , autoscal make post request meso master /maintenance/schedul endpoint set slave move mainten mode . thi forc meso master rescind in-flight offer * slave * cluster . if schedul accept one offer , get task_lost statu updat back task . we also see ( http : //gist.github.com/sagar8192/8858e7cb59a23e8e1762a27571824118 ) log line meso master log . after read code ( ref : http : //github.com/apache/mesos/blob/master/src/master/master.cpp # l6772 ) , appear offer get rescind slave . i sure expect behavior , make sens resourc slave mark mainten reclaim . * experi : * to verifi actual happen , i check master branch ( sha : a31dd52ab71d2a529b55cd9111ec54acf7550d ) ad log line ( http : //gist.github.com/sagar8192/42ca055720549c5ff3067b1e6c7c68b3 ) . built binari start meso master 2 agent process . use basic python framework launch docker contain slave . verifi exist schedul slave use ` curl 10.40.19.239:5050/maintenance/statu ` . post mainten schedul one slave ( http : //gist.github.com/sagar8192/fb65170240dd32a53f27e6985c549df0 ) start meso framework . * log : * mesos-mast : http : //gist.github.com/sagar8192/91888419fdf8284e33ebd58351131203 mesos-slave1 : http : //gist.github.com/sagar8192/3a83364b1f5ffc63902a80c728647f31 mesos-slave2 : http : //gist.github.com/sagar8192/1b341ef2271dde11d276974a27109426 meso framework : http : //gist.github.com/sagar8192/bcd4b37dba03bde0a942b5b972004e8a i think meso rescind offer invers offer slave mark mainten ( drain mode ) .",MESOS-7882,3.0
"master store old resourc format registri we intend master store intern resourc represent new , post-reservation-refin format . howev , [ persist regist agent registrar|http : //github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp # l5861-l5876 ] , master convert resourc ; agent provid resourc pre-reservation-refin format , resourc store as-i . thi mean recoveri , agent master 's { { slaves.recov } } map { { slaveinfo.resourc } } pre-reservation-refin format . we updat master convert resourc persist registri .",MESOS-7851,3.0
"sandbox_path volum ownership set correctli . thi issu expos use sandbox_path volum support share volum nest contain one task group . here scenario : the agent process run 'root ' user , framework user set 'nobodi ' . no matter commandinfo user set , non-root user access sandbox_path volum ( e.g. , parent sandbox_path volum writabl nest contain ) . thi sourc path parent sandbox level creat agent process ( aka root case ) . while oper respons guarante nest contain permiss write sandbox path volum parent 's sandbox , guarante sourc path creat parent 's sandbox set ownership sandbox 's ownership .",MESOS-7830,3.0
"current approach pars protobuf enum json support upgrad to use protobuf enum backward compat way , [ suggest protobuf mail list|http : //groups.google.com/forum/ # ! msg/protobuf/nhujbfdygmy/pf294zmi2bij ] use option enum field includ unknown valu first entri enum list ( and/or explicitli specifi default ) . thi handl case pars protobuf messag serial string , handl case pars protobuf messag json . e.g. , i access master endpoint inexist enum { { xxx } } , i get error : { code } $ curl -x post -h `` content-typ : application/json '' -d ' { `` type '' : `` xxx '' } ' 127.0.0.1:5050/api/v1 fail convert json call protobuf : fail find enum 'xxx ' % { code } in { { call } } protobuf messag , enum { { type } } alreadi default valu { { unknown } } ( see [ here|http : //github.com/apache/mesos/blob/1.3.0/include/mesos/v1/master/master.proto # l45 ] detail ) field { { call.typ } } option , curl command still fail . the root caus , code [ here|http : //github.com/apache/mesos/blob/1.3.0/3rdparty/stout/include/stout/protobuf.hpp # l449 : l454 ] tri get enum valu string `` xxx '' , fail sinc enum valu correspond `` xxx '' .",MESOS-7828,3.0
"mesos-execut incorrect exampl taskinfo help string { { mesos-execut } } document task defin via json { noformat } { `` name '' : `` name task '' , `` task_id '' : { `` valu '' : `` id task '' } , `` agent_id '' : { `` valu '' : `` '' } , `` resourc '' : [ { `` name '' : `` cpu '' , `` type '' : `` scalar '' , `` scalar '' : { `` valu '' : 0.1 } , `` role '' : `` * '' } , { `` name '' : `` mem '' , `` type '' : `` scalar '' , `` scalar '' : { `` valu '' : 32 } , `` role '' : `` * '' } ] , `` command '' : { `` valu '' : `` sleep 1000 '' } } { noformat } if one actual use exampl task definit one get { noformat } % ./build/src/mesos-execut -- master=127.0.0.1:5050 -- task=task.json warn : log initgooglelog ( ) written stderr w0719 17:08:17.909696 3291313088 parse.hpp:114 ] specifi absolut filenam read command line option without use 'file : // deprec remov futur releas . simpli ad 'file : // ' begin path elimin warn . [ warn ] kq_init : detect broken kqueue ; use . : undefin error : 0 i0719 17:08:17.919190 119246848 scheduler.cpp:184 ] version : 1.4.0 i0719 17:08:17.923991 119783424 scheduler.cpp:470 ] new master detect master @ 127.0.0.1:5050 subscrib id bb0d36b4-fee0-4412-9cd9-1fa4e330355c-0000 f0719 17:08:18.137984 119783424 resources.cpp:1081 ] check fail : ! resource.has_rol ( ) * * * check failur stack trace : * * * @ 0x101d65f5f googl : :logmessagefat : :~logmessagefat ( ) @ 0x101d62609 googl : :logmessagefat : :~logmessagefat ( ) @ 0x1016ef3a3 meso : :v1 : :resourc : :isempti ( ) @ 0x1016ed267 meso : :v1 : :resourc : :add ( ) @ 0x1016f05af meso : :v1 : :resourc : :operator+= ( ) @ 0x1016f08fb meso : :v1 : :resourc : :resourc ( ) @ 0x100c0d89f commandschedul : :offer ( ) @ 0x100c085e4 commandschedul : :receiv ( ) @ 0x100c0ae06 _zzn7process8dispatchi16commandschedulernst3__15queuein5mesos2v19scheduler5eventens2_5dequeis7_ns2_9allocatoris7_eeeeeesc_eevrkns_3pidit_eemse_fvt0_et1_enkulpns_11processbaseee_clesn_ @ 0x101ce5a21 process : :processbas : :visit ( ) @ 0x101ce3747 process : :processmanag : :resum ( ) @ 0x101d0e243 _znst3__114__thread_proxyins_5tupleijns_10unique_ptrins_15__thread_structens_14default_deleteis3_eeeezn7process14processmanager12init_threadseve3 $ _0eeeeepvsb_ @ 0x7fffbb5d693b _pthread_bodi @ 0x7fffbb5d6887 _pthread_start @ 0x7fffbb5d608d thread_start [ 1 ] 73521 abort ./build/src/mesos-execut -- master=127.0.0.1:5050 -- task=task.json { noformat } remov resourc role field allow task execut .",MESOS-7805,1.0
"fs : :list drop path compon window fs : :list ( /foo/bar/ * .txt ) return a.txt , b.txt , /foo/bar/a.txt , /foo/bar/b.txt thi break zookeep test window .",MESOS-7803,2.0
"copy-n-past error slave/main.cpp cover diagnos copy-n-past error { { slave/main.cpp } } ( http : //scan5.coverity.com/reports.htm # v10074/p10429/fileinstanceid=120155401 & defectinstanceid=33592186 & mergeddefectid=1414687+1+com ) , { noformat } 323 } els ( flags.ip6.issom ( ) ) { cid 1414687 ( # 1 1 ) : copy-past error ( copy_paste_error ) copy_paste_error : ip flags.ip look like copy-past error . should say ip6 instead ? 324 os : :setenv ( `` libprocess_ip6 '' , flags.ip.get ( ) ) ; 325 } { noformat } we check incorrect ip valu ( check { { ip6 } } , use { { ip } } ) , seem extrem like intend use { { flags.ip6 } } .",MESOS-7772,1.0
"persist volum might mount sandbox volum whose sourc target persist volum . thi issu meso container . if sourc sandbox volum rel path , 'll creat directori sandbox isol : :prepar method : http : //github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp # l480-l485 and , 'll tri mount persist volum . howev , todo code : http : //github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp # l726-l739 we 'll skip mount persist volum . that caus silent failur . thi import workaround suggest folk solv mesos-4016 use addit sandbox volum .",MESOS-7770,3.0
"libprocess initi bind random port -- ip specifi when run current [ head|http : //github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25 ] , { noformat : title=without -- ip } ./mesos-master.sh -- work_dir=/tmp/mesos-test1 ... i0707 14:14:05.927870 5820 master.cpp:438 ] master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 ( < host > ) start < addr > :36839 { noformat } { noformat : title=with -- ip } ./mesos-master.sh -- ip= < addr > -- work_dir=/tmp/mesos-test1 i0707 14:09:56.851483 5729 master.cpp:438 ] master 963e0f42-9767-4629-8e3d-02c6ab6ad225 ( < host > ) start < addr > :5050 { noformat } it would great caught tests/ci .",MESOS-7769,1.0
mastertest.killunknowntask faill due bug ` net : :ipv4 : :ani ( ) ` see follow failur run ` mastertest.killunknowntask ` : `` ` i0706 14:08:20.724071 25596 sched.cpp:1041 ] schedul : :statusupd took 19411n [ libprotobuf fatal google/protobuf/message_lite.cc:294 ] check fail : isiniti ( ) : ca n't serial messag type `` mesos.scheduler.cal '' miss requir field : acknowledge.slave_id.valu libprocess : scheduler-5cca230e-e4c9-466e-b2cd-bde7b7d7ed71 @ 127.0.0.1:44650 termin due check fail : isiniti ( ) : ca n't serial messag type `` mesos.scheduler.cal '' miss requir field : acknowledge.slave_id.valuei0706 14:08:20.724196 25570 sched.cpp:2021 ] ask stop driver `` ` look introduc bug creat ` net : :ipv4 ` class . the ` ani ` method class return ` inaddr_loopback ` instead ` inaddr_ani ` . thi end caus weird issu term connect . we need fix ` net : :ipv4 : :ani ` return ` inaddr_ani ` .,MESOS-7765,1.0
"master 's agent remov rate limit also appli agent unreach . current , implement partit awar re-us { { -- agent_removal_rate_limit } } mark agent unreach . thi mean partit awar framework expos agent remov rate limit , rather would like see inform immedi impos rate limit . rather wait non-partition-awar support remov ( may occur long time ) per mesos-5948 , instead fix implement unreach get gate behind agent remov rate limit . mark bug sinc user 's perspect n't behav expect , separ flag rate limit unreach mark , like unreach mark need rate limit , sinc intent framework impos rate limit replac task .",MESOS-7721,3.0
"test master reject request creat refin reserv non-cap agent . thi test done manual , write test . similar { { createoperationvalidationtest.agenthierarchicalrolecap } } .",MESOS-7715,2.0
"fix agent downgrad reserv refin the agent code partial support downgrad agent correctli . the checkpoint resourc done correctli , resourc within { { slaveinfo } } messag well task executor also need downgrad correctli convert back recoveri .",MESOS-7714,8.0
"prevent non-reservation_refin framework refin reserv . we output `` endpoint '' format endpoint backward compat extern tool . a framework abl use result endpoint pass back meso , sinc result produc meso . thi especi applic v1 api . we also allow `` pre-reservation-refin '' format exist `` resourc file '' written format , still usabl without modif . thi probabl flexibl howev , sinc framework without reservation_refin capabl could make refin reserv use `` post-reservation-refin '' format , although would n't offer resourc . it still seem undesir anyon run , consid ad sensibl restrict .",MESOS-7705,3.0
"docker imag univers container work workdir miss rootf . hello , use follow docker imag recent quay.io/spinnaker/front50 : master http : //quay.io/repository/spinnaker/front50 here link dockerfil http : //github.com/spinnaker/front50/blob/master/dockerfil sourc { color : blue } from java:8 maintain delivery-engin @ netflix.com copi . workdir/ workdir workdir run gradle_user_home=cach ./gradlew builddeb -x test & & \ dpkg -i ./front50-web/build/distributions/ * .deb & & \ cd .. & & \ rm -rf workdir cmd [ `` /opt/front50/bin/front50 '' ] { color } the imag work fine docker container , univers container show follow stderr . `` fail chdir current work directori '/workdir ' : no file directori '' the problem come fact dockerfil creat workdir later remov creat dir part run . the docker container problem docker run -ti -- rm quay.io/spinnaker/front50 : master bash get work dir , univers container fail error . thank help , michael",MESOS-7652,3.0
"introduc heartbeat mechan v1 http executor < - > agent commun . current , heartbeat executor < - > agent commun . thi especi problemat scenario ipfilt enabl sinc default conntrack keep aliv timeout 5 day . when timeout elaps , executor n't get notifi via socket disconnect agent process restart . the executor would get kill n't re-regist agent recoveri process complet . enabl applic level heartbeat tcp keepal 's possibl way fix issu . we also updat executor api document explain new behavior .",MESOS-7564,5.0
command check via agent lead flaki test . test reli command check via agent flaki apach ci . here exampl one fail run : http : //pastebin.com/g2mpgyzu,MESOS-7500,8.0
"provision recov alway assum 'rootfs ' dir exist . the meso agent would restart due mani reason ( e.g. , disk full ) . alway assum provision 'rootfs ' dir exist would block agent recov . { noformat } fail perform recoveri : collect fail : unabl list rootfs belong contain a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 : unabl list backend directori : fail opendir '/var/lib/mesos/slave/provisioner/containers/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/backends/overlay/rootfs ' : no file directori { noformat } thi issu may occur due race remov provision contain dir agent restart : { noformat } may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.058349 11441 linux_launcher.cpp:429 ] launch contain a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 clone namespac clone_newn | clone_newpid may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.072191 11441 systemd.cpp:96 ] assign child process '11577 ' 'mesos_executors.slice' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.075932 11439 containerizer.cpp:1592 ] checkpoint contain 's fork pid 11577 '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-s34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008/executors/node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05/runs/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/pids/forked.pid' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.081516 11438 linux_launcher.cpp:429 ] launch contain 03a57a37-eede-46ec-8420-dda3cc54e2e0 clone namespac clone_newn | clone_newpid may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.083516 11438 systemd.cpp:96 ] assign child process '11579 ' 'mesos_executors.slice' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.087345 11444 containerizer.cpp:1592 ] checkpoint contain 's fork pid 11579 '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-s34/frameworks/36a25adb-4ea2-49d3-a195-448cff1dc146-0002/executors/66897/runs/03a57a37-eede-46ec-8420-dda3cc54e2e0/pids/forked.pid' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : w0505 02:14:32.213049 11440 fetcher.cpp:896 ] begin fetcher log ( stderr sandbox ) contain 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac run command : /opt/mesosphere/packages/meso -- aaedd03eee0d57f5c0d49c74ff1e5721862cad98/libexec/mesos/mesos-fetch may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.006201 11561 fetcher.cpp:531 ] fetcher info : { `` cache_directori '' : '' \/tmp\/mesos\/fetch\/slaves\/36a25adb-4ea2-49d3-a195-448cff1dc146-s34\/root '' , '' item '' : [ { `` action '' : '' bypass_cach '' , '' uri '' : { `` extract '' : true , '' valu '' : '' http : \/\/downloads.mesosphere.com\/libmesos-bundle\/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz '' } } , { `` action '' : '' bypass_cach '' , may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.009678 11561 fetcher.cpp:442 ] fetch uri 'http : //downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.009693 11561 fetcher.cpp:283 ] fetch directli sandbox directori may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.009711 11561 fetcher.cpp:220 ] fetch uri 'http : //downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz' may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.009723 11561 fetcher.cpp:163 ] download resourc 'http : //downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz ' '/var/lib/mesos/slave/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-s34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011/executors/hello__91922a16-889e-4e94-9dab-9f6754f091de/ may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : fail fetch 'http : //downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz ' : error download resourc : fail write receiv data disk/appl may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : end fetcher log contain 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : e0505 02:14:32.213114 11440 fetcher.cpp:558 ] fail run mesos-fetch : fail fetch uri contain '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac ' exit statu : 256 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : e0505 02:14:32.213351 11444 slave.cpp:4642 ] contain '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac ' executor 'hello__91922a16-889e-4e94-9dab-9f6754f091d ' framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011 fail start : fail fetch uri contain '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac ' exit statu : 256 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.213614 11443 containerizer.cpp:2071 ] destroy contain 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac fetch state may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.213977 11443 linux_launcher.cpp:505 ] ask destroy contain 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.214757 11443 linux_launcher.cpp:548 ] use freezer destroy cgroup mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.216047 11444 cgroups.cpp:2692 ] freez cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.218407 11443 cgroups.cpp:1405 ] success froze cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac 2.326016m may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.220391 11445 cgroups.cpp:2710 ] thaw cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.222124 11445 cgroups.cpp:1434 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac 1.693952m may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : e0505 02:14:32.239018 11441 fetcher.cpp:558 ] fail run mesos-fetch : fail creat 'stdout ' file : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : e0505 02:14:32.239162 11442 slave.cpp:4642 ] contain 'a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 ' executor 'node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05 ' framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008 fail start : fail creat 'stdout ' file : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.239284 11445 containerizer.cpp:2071 ] destroy contain a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 fetch state may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.239390 11444 linux_launcher.cpp:505 ] ask destroy contain a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.240103 11444 linux_launcher.cpp:548 ] use freezer destroy cgroup mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.241353 11440 cgroups.cpp:2692 ] freez cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.243120 11444 cgroups.cpp:1405 ] success froze cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 1.726976m may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.245045 11440 cgroups.cpp:2710 ] thaw cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.246800 11440 cgroups.cpp:1434 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 1.715968m may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.285477 11438 slave.cpp:1625 ] got assign task 'dse-1-agent__720d6f09-9d60-4667-b224-abcd495e0e58 ' framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0009 may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : f0505 02:14:32.296481 11438 slave.cpp:6381 ] check_som ( state : :checkpoint ( path , info ) ) : fail creat temporari file : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : * * * check failur stack trace : * * * may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : @ 0x7f5856be857d googl : :logmessag : :fail ( ) may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : @ 0x7f5856bea3ad googl : :logmessag : :sendtolog ( ) may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : @ 0x7f5856be816c googl : :logmessag : :flush ( ) may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : @ 0x7f5856beaca9 googl : :logmessagefat : :~logmessagefat ( ) may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : @ 0x7f5855e4b5e9 _checkfat : :~_checkfat ( ) may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.314082 11445 containerizer.cpp:2434 ] contain 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac exit may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.314826 11440 containerizer.cpp:2434 ] contain a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 exit may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 17142 ] : fail write : no space left devic may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316660 11439 container_assigner.cpp:101 ] unregist container_id [ valu : `` 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac '' ] . may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316761 11474 container_assigner_strategy.cpp:202 ] close ephemeral-port reader contain [ valu : `` 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac '' ] endpoint [ 198.51.100.1:34273 ] . may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316804 11474 container_reader_impl.cpp:38 ] trigger containerread shutdown may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316833 11474 sync_util.hpp:39 ] dispatch wait < =5 ticket 7 : ~containerreaderimpl : shutdown may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316769 11439 container_assigner.cpp:101 ] unregist container_id [ valu : `` a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 '' ] . may 05 02:14:32 ip-172-31-7-83.us-west-2.compute.intern mesos-ag [ 11432 ] : i0505 02:14:32.316864 11474 container_read { noformat } in provision recov , list contain rootfs , possibl 'rootfs ' dir exist . becaus possibl race provision destroy agent restart . for instanc , provision destroy contain dir agent restart . due os : :rmdir ( ) recurs travers ft tree , possibl 'rootfs ' dir remov other ( e.g. , scratch dir ) . current , return error 'rootfs ' dir exist , block agent recoveri . we skip 'rootfs ' exist .",MESOS-7471,2.0
"doubl free corrupt use parallel test runner i observ follow use parallel test runner : { noformat } /home/bmahler/git/mesos/build/ .. /support/mesos-gtest-runner.pi -- sequential= * root_ * ./mesos-test .. * * * error ` /home/bmahler/git/mesos/build/src/.libs/mesos-test ' : doubl free corrupt ( ) : 0x00007fa818001310 * * * ======= backtrac : ========= /usr/lib64/libc.so.6 ( +0x7c503 ) [ 0x7fa87f27e503 ] /usr/lib64/libsasl2.so.3 ( +0x866d ) [ 0x7fa880f0d66d ] /usr/lib64/libsasl2.so.3 ( sasl_dispose+0x3b ) [ 0x7fa880f1075b ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn5mesos8internal8cram_md527crammd5authenticateeprocessd1ev+0x5d ) [ 0x7fa88708f67d ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn5mesos8internal8cram_md527crammd5authenticateeprocessd0ev+0x18 ) [ 0x7fa88708f734 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn5mesos8internal8cram_md520crammd5authenticateed1ev+0xfb ) [ 0x7fa88708a065 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn5mesos8internal8cram_md520crammd5authenticateed0ev+0x18 ) [ 0x7fa88708a0b4 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn5mesos8internal5slave5slave13_authenticateev+0x67 ) [ 0x7fa8879ff579 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zzn7process8dispatchin5mesos8internal5slave5slaveeeevrkns_3pidit_eems6_fvveenkulpns_11processbaseee_clesd_+0xe2 ) [ 0x7fa887a60b7a ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchin5mesos8internal5slave5slaveeeevrkns0_3pidit_eemsa_fvveeuls2_e_e9_m_invokeerkst9_any_datas2_+0x37 ) [ 0x7fa887aa0ef ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _znkst8functionifvpn7process11processbaseeeecles2_+0x49 ) [ 0x7fa8888d1177 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn7process11processbase5visiterkns_13dispatchevente+0x2f ) [ 0x7fa8888b5063 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _znk7process13dispatchevent5visitepns_12eventvisitore+0x2 ) [ 0x7fa8888c0422 ] /home/bmahler/git/mesos/build/src/.libs/mesos-test ( _zn7process11processbase5serveerkns_5evente+0x2 ) [ 0xb088c8 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( _zn7process14processmanager6resumeepns_11processbasee+0x525 ) [ 0x7fa8888b10d5 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( +0x5f1a880 ) [ 0x7fa8888ad880 ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( +0x5f2ca8a ) [ 0x7fa8888bfa8a ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( +0x5f2c9ce ) [ 0x7fa8888bf9c ] /home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so ( +0x5f2c958 ) [ 0x7fa8888bf958 ] /usr/lib64/libstdc++.so.6 ( +0xb5230 ) [ 0x7fa87fb90230 ] /usr/lib64/libpthread.so.0 ( +0x7dc5 ) [ 0x7fa88040ddc5 ] /usr/lib64/libc.so.6 ( clone+0x6d ) [ 0x7fa87f2f973d ] { noformat } not sure reproduc , appear occur authent path agent .",MESOS-7438,2.0
"registri puller fetch manifest googl gcr : 403 forbidden . when registri puller pull repositori googl 's gce contain registri , '403 forbidden ' error occur instead 401 fetch manifest .",MESOS-7431,5.0
"run docker imag meso contain runtim without ` linux/filesystem ` isol enabl render host unus if i run pod ( use marathon 1.4.2 ) meso agent flag ( also ) , overlay filesystem replac system root mount , effect render host unus reboot . flag : - { { -- container meso , docker } } - { { -- image_provid appc , docker } } - { { -- isol cgroups/cpu , cgroups/mem , docker/runtim } } pod definit marathon : { code : java } { `` id '' : `` /simplepod '' , `` scale '' : { `` kind '' : `` fix '' , `` instanc '' : 1 } , `` contain '' : [ { `` name '' : `` sleep1 '' , `` exec '' : { `` command '' : { `` shell '' : `` sleep 1000 '' } } , `` resourc '' : { `` cpu '' : 0.1 , `` mem '' : 32 } , `` imag '' : { `` id '' : `` alpin '' , `` kind '' : `` docker '' } } ] , `` network '' : [ { `` mode '' : `` host '' } ] } { code } meso probabl check avoid replac system root mount point startup launch time .",MESOS-7374,3.0
"fail pull imag nexu registri due signatur miss . i ’ tri launch docker contain univers container , meso 1.2.0 . but get error “ fail pars imag manifest : docker v2 imag manifest valid fail : ‘ signatur ’ field size must least one ” . and i switch docker container , app start normal . we work privat docker registri v2 back nexu repositori manag 3.1.0 { code } cat /etc/mesos-slave/docker_registri http : //docker.company.ru cat /etc/mesos-slave/docker_config { '' auth '' : { '' docker.company.ru '' : { '' auth '' : `` ........ '' } } } { code } here agent 's log : { code } i0405 22:00:49.860234 44856 slave.cpp:4346 ] receiv ping slave-observ ( 7 ) @ 10.34.1.31:5050 i0405 22:00:50.327030 44865 slave.cpp:1625 ] got assign task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.327785 44865 slave.cpp:1785 ] launch task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.329324 44865 paths.cpp:547 ] tri chown '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-s1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff ' user 'dockdata' i0405 22:00:50.329607 44865 slave.cpp:6896 ] checkpoint executorinfo '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-s1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/executor.info' i0405 22:00:50.330531 44865 slave.cpp:6472 ] launch executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 resourc cpu ( * ) ( alloc : general_marathon_service_rol ) :0.1 ; mem ( * ) ( alloc : general_marathon_service_rol ) :32 work directori '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-s1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff' i0405 22:00:50.331244 44865 slave.cpp:6919 ] checkpoint taskinfo '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-s1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff/tasks/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/task.info' i0405 22:00:50.331568 44862 docker.cpp:1106 ] skip non-dock contain i0405 22:00:50.331822 44865 slave.cpp:2118 ] queu task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.331966 44865 slave.cpp:884 ] success attach file '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-s1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff' i0405 22:00:50.332582 44861 containerizer.cpp:993 ] start contain f82f5f69-87a3-4586-b4cc-b91d285dcaff executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.333286 44862 metadata_manager.cpp:168 ] look imag 'docker.company.ru/company-infra/kafka:0.10.2.0-16' i0405 22:00:50.333627 44879 registry_puller.cpp:247 ] pull imag 'docker.company.ru/company-infra/kafka:0.10.2.0-16 ' 'docker-manifest : //docker.company.rucompany-infra/kafka ? 0.10.2.0-16 # http ' '/export/intssd/mesos-slave/docker-store/staging/av2yko' e0405 22:00:50.834630 44872 slave.cpp:4642 ] contain 'f82f5f69-87a3-4586-b4cc-b91d285dcaff ' executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 fail start : fail pars imag manifest : docker v2 imag manifest valid fail : 'signatur ' field size must least one i0405 22:00:50.835008 44853 containerizer.cpp:2069 ] destroy contain f82f5f69-87a3-4586-b4cc-b91d285dcaff provis state i0405 22:00:50.835127 44853 containerizer.cpp:2124 ] wait provision complet provis destroy contain f82f5f69-87a3-4586-b4cc-b91d285dcaff i0405 22:00:50.835273 44844 provisioner.cpp:484 ] ignor destroy request unknown contain f82f5f69-87a3-4586-b4cc-b91d285dcaff i0405 22:00:50.836199 44837 slave.cpp:4754 ] executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 termin unknown statu i0405 22:00:50.837193 44837 slave.cpp:3816 ] handl statu updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 @ 0.0.0.0:0 e0405 22:00:50.837766 44846 slave.cpp:4097 ] fail updat resourc contain f82f5f69-87a3-4586-b4cc-b91d285dcaff executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 ' run task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 statu updat termin task , destroy contain : contain found w0405 22:00:50.837962 44865 composing.cpp:630 ] attempt destroy unknown contain f82f5f69-87a3-4586-b4cc-b91d285dcaff i0405 22:00:50.838018 44877 status_update_manager.cpp:323 ] receiv statu updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.838081 44877 status_update_manager.cpp:500 ] creat statusupd stream task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.838560 44877 status_update_manager.cpp:832 ] checkpoint updat statu updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.838708 44877 status_update_manager.cpp:377 ] forward updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 agent i0405 22:00:50.838860 44878 slave.cpp:4256 ] forward updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 master @ 10.34.1.31:5050 i0405 22:00:50.839059 44878 slave.cpp:4150 ] statu updat manag success handl statu updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.848595 44866 status_update_manager.cpp:395 ] receiv statu updat acknowledg ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.848696 44866 status_update_manager.cpp:832 ] checkpoint ack statu updat task_fail ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.848801 44866 status_update_manager.cpp:531 ] clean statu updat stream task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.849365 44850 slave.cpp:3105 ] statu updat manag success handl statu updat acknowledg ( uuid : efd419db-5350-48bf-b612-8e5b5685b9a0 ) task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 i0405 22:00:50.849431 44850 slave.cpp:6875 ] complet task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 { code }",MESOS-7350,2.0
updat resourc proto storag resourc provid . storag resourc provid support requir number chang { { resourc } } proto : * support { { raw } } { { block } } type { { resourc : :diskinfo : :sourc } } * { { resourceproviderid } } resourc * { { resourc : :diskinfo : :sourc : :path } } { { option } } .,MESOS-7312,3.0
"unifi container provis docker imag error copi backend error occur specif docker imag copi backend , 1.0.2 1.2.0 . it work well overlay backend 1.2.0 . { quot } i0321 09:36:07.308830 27613 paths.cpp:528 ] tri chown '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-s102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct : transcoding_test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7 ' user 'root' i0321 09:36:07.319628 27613 slave.cpp:5703 ] launch executor ct : transcoding_test_114489497_1490060156172:3 framework 20151223-150303-2677017098-5050-30032-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-s102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct : transcoding_test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' i0321 09:36:07.321436 27615 containerizer.cpp:781 ] start contain '7e518538-7b56-4b14-a3c9-bee43c669bd7 ' executor 'ct : transcoding_test_114489497_1490060156172:3 ' framework '20151223-150303-2677017098-5050-30032-0000' i0321 09:36:37.902195 27600 provisioner.cpp:294 ] provis imag rootf '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9 ' contain 7e518538-7b56-4b14-a3c9-bee43c669bd7 * e0321 09:36:58.707718 27606 slave.cpp:4000 ] contain '7e518538-7b56-4b14-a3c9-bee43c669bd7 ' executor 'ct : transcoding_test_114489497_1490060156172:3 ' framework 20151223-150303-2677017098-5050-30032-0000 fail start : collect fail : fail copi layer : cp : creat regular file ‘ /data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9/usr/bin/python ’ : text file busi * i0321 09:36:58.707991 27608 containerizer.cpp:1622 ] destroy contain '7e518538-7b56-4b14-a3c9-bee43c669bd7' i0321 09:36:58.708468 27607 provisioner.cpp:434 ] destroy contain rootf '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9 ' contain 7e518538-7b56-4b14-a3c9-bee43c669bd7 { quot } docker imag privat one , tri reproduc bug sampl dockerfil possibl .",MESOS-7280,2.0
"unifi container support docker registri version < 2.3. file ` src/uri/fetchers/docker.cpp ` `` ` option < string > contenttyp = response.headers.get ( `` content-typ '' ) ; ( contenttype.issom ( ) & & ! string : :startswith ( contenttype.get ( ) , `` application/vnd.docker.distribution.manifest.v1 '' ) ) { return failur ( `` unsupport manifest mime type : `` + contenttype.get ( ) ) ; } `` ` docker fetcher check contenttyp strictli , docker registri version < 2.3 return manifest contenttyp ` application/json ` , lead failur like ` e0321 13:27:27.572402 40370 slave.cpp:4650 ] contain 'xxx ' executor 'xxx ' framework xxx fail start : unsupport manifest mime type : application/json ; charset=utf-8 ` .",MESOS-7272,2.0
support pull imag alicloud privat registri . the imag puller via curl n't work i 'm specifi imag name : registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75 400 bad request but docker pull success bq . docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75,MESOS-7251,2.0
"task launch via default executor access disk resourc volum . current , task task group tri access volum specifi disk resourc ( e.g. , persist volum ) , n't access sinc mount root contain ( executor ) . thi happen due mechan specifi resourc child contain yet . henc , default resourc ( e.g. , disk ) ad root contain . a possibl solut set map manual default executor use { { sandbox_path } } volum sourc type give child contain access volum mount parent contain . thi best workaround ideal solut would tackl part mesos-7207 .",MESOS-7225,3.0
"http health check n't work meso run -- docker_mesos_imag when run mesos-slav option `` docker_mesos_imag '' like : { code } -- master=zk : //standalone:2181/meso -- containerizers=dock , meso -- executor_registration_timeout=5min -- hostname=standalon -- ip=0.0.0.0 -- docker_stop_timeout=5sec -- gc_delay=1day -- docker_socket=/var/run/docker.sock -- no-systemd_enable_support -- work_dir=/tmp/meso -- docker_mesos_image=panteras/paas-in-a-box:0.4.0 { code } contain start option `` pid : host '' like : { code } net : host privileg : true pid : host { code } exampl marathon job , use mesos_http check like : { code } { `` id '' : `` python-example-st '' , `` cmd '' : `` python3 -m http.server 8080 '' , `` mem '' : 16 , `` cpu '' : 0.1 , `` instanc '' : 2 , `` contain '' : { `` type '' : `` docker '' , `` docker '' : { `` imag '' : `` python : alpin '' , `` network '' : `` bridg '' , `` portmap '' : [ { `` containerport '' : 8080 , `` hostport '' : 0 , `` protocol '' : `` tcp '' } ] } } , `` env '' : { `` service_nam '' : `` python '' } , `` healthcheck '' : [ { `` path '' : `` / '' , `` portindex '' : 0 , `` protocol '' : `` mesos_http '' , `` graceperiodsecond '' : 30 , `` intervalsecond '' : 10 , `` timeoutsecond '' : 30 , `` maxconsecutivefailur '' : 3 } ] } { code } i see error like : { code } f0306 07:41:58.844293 35 health_checker.cpp:94 ] fail enter net namespac task ( pid : '13527 ' ) : pid 13527 exist * * * check failur stack trace : * * * @ 0x7f51770b0c1d googl : :logmessag : :fail ( ) @ 0x7f51770b29d0 googl : :logmessag : :sendtolog ( ) @ 0x7f51770b0803 googl : :logmessag : :flush ( ) @ 0x7f51770b33f9 googl : :logmessagefat : :~logmessagefat ( ) @ 0x7f517647ce46 _znst17_function_handlerifivezn5mesos8internal6health14clonewithsetnserkst8functionis0_e6optioniierkst6vectorinst7__cxx1112basic_stringicst11char_traitsicesaiceeesaisg_eeeulve_e9_m_invokeerkst9_any_data @ 0x7f517647bf2b meso : :intern : :health : :clonewithsetn ( ) @ 0x7f517648374b std : :_function_handl < > : :_m_invok ( ) @ 0x7f5177068167 process : :intern : :clonechild ( ) @ 0x7f5177065c32 process : :subprocess ( ) @ 0x7f5176481a9d meso : :intern : :health : :healthcheckerprocess : :_httphealthcheck ( ) @ 0x7f51764831f7 meso : :intern : :health : :healthcheckerprocess : :_healthcheck ( ) @ 0x7f517701f38c process : :processbas : :visit ( ) @ 0x7f517702c8b3 process : :processmanag : :resum ( ) @ 0x7f517702fb77 _znst6thread5_implist12_bind_simpleifzn7process14processmanager12init_threadseveut_veee6_m_runev @ 0x7f51754ddc80 ( unknown ) @ 0x7f5174cf06ba start_thread @ 0x7f5174a2682d ( unknown ) i0306 07:41:59.077986 9 health_checker.cpp:199 ] ignor failur health check still grace period { code } look like option docker_mesos_imag make , newli start meso job use `` pid host '' option mother contain start , pid namespac ( n't matter mother contain start `` pid host '' never abl find pid )",MESOS-7210,3.0
"persist volum ownership set root task run non-root user i ’ run docker contain univers container , meso 1.1.0. switch_user=tru , isolator=filesystem/linux , docker/runtim . contain launch marathon , “ user ” : ” someappus ” . i ’ want use persist volum , ’ expos contain root user permiss even root folder creat someppus ownership ( look like meso chown folder ) . log contain : { code } i0305 22:51:36.414655 10175 slave.cpp:1701 ] launch task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a ' framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 i0305 22:51:36.415118 10175 paths.cpp:536 ] tri chown '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-s10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a ' user 'root' i0305 22:51:36.422992 10175 slave.cpp:6179 ] launch executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a ' framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-s10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a' i0305 22:51:36.424278 10175 slave.cpp:1987 ] queu task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a ' executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a ' framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 i0305 22:51:36.424347 10158 docker.cpp:1000 ] skip non-dock contain i0305 22:51:36.425639 10142 containerizer.cpp:938 ] start contain e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a ' framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 i0305 22:51:36.428725 10166 provisioner.cpp:294 ] provis imag rootf '/export/intssd/mesos-slave/workdir/provisioner/containers/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/backends/copy/rootfses/0e2181e9-1bf2-42d4-8cb0-ee70e466c3a ' contain e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a i0305 22:51:42.981240 10149 linux.cpp:695 ] chang ownership persist volum '/export/intssd/mesos-slave/data/volumes/roles/general_marathon_service_role/md_hdfs_journ # data # 23f813aa-01dd-11e7-a012-0242ce94d92a ' uid 0 gid 0 i0305 22:51:42.986593 10136 linux_launcher.cpp:421 ] launch contain e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a clone namespac clone_newn { code } { code } ls -la /export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-s10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/ drwxr-xr-x 3 someappus someappgroup 4096 22:51 . drwxr-xr-x 3 root root 4096 22:51 .. drwxr-xr-x 2 root root 4096 22:51 data -rw-r -- r -- 1 root root 169 22:51 stderr -rw-r -- r -- 1 root root 183012 23:00 stdout { code }",MESOS-7208,3.0
add document debug api oper api doc,MESOS-7188,1.0
"agent valid nest contain id exceed certain length . thi relat mesos-691 . sinc nest contain id gener executor , agent verifi length exceed certain length .",MESOS-7168,3.0
"the agent may flap machin reboot due provision recov . after agent machin reboot , agent work dir surviv ( e.g. , /var/lib/meso ) contain runtim directori gone ( empti slavest well ) , provision recov ( ) would get segfault case break semant child contain alway clean parent contain . thi particular case happen machin reboot provision directori surviv . { noformat } f0217 01:10:18.423238 30099 provisioner.cpp:504 ] check fail : entry.par ( ) ! = containerid fail destroy contain 1 sinc nest contain 1.2 destroy yet * * * check failur stack trace : * * * @ 0x7fceb444121d googl : :logmessag : :fail ( ) @ 0x7fceb44405e googl : :logmessag : :sendtolog ( ) @ 0x7fceb4440e googl : :logmessag : :flush ( ) @ 0x7fceb4444368 googl : :logmessagefat : :~logmessagefat ( ) @ 0x7fceb36137f9 meso : :intern : :slave : :provisionerprocess : :destroy ( ) @ 0x7fceb36126f0 meso : :intern : :slave : :provisionerprocess : :recov ( ) @ 0x7fceb3637fc6 _zzn7process8dispatchi7nothingn5mesos8internal5slave18provisionerprocesserk7hashsetins2_11containeridest4hashis7_est8equal_tois7_eesc_eens_6futureit_eerkns_3pidit0_eemsj_fsh_t1_et2_enkulpns_11processbaseee_cless_ @ 0x7fceb3637bc2 _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchi7nothingn5mesos8internal5slave18provisionerprocesserk7hashsetins6_11containeridest4hashisb_est8equal_toisb_eesg_eens0_6futureit_eerkns0_3pidit0_eemsn_fsl_t1_et2_euls2_e_e9_m_invokeerkst9_any_dataos2_ @ 0x7fceb43848e4 std : :function < > : :oper ( ) ( ) @ 0x7fceb436baf4 process : :processbas : :visit ( ) @ 0x7fceb43e5fd process : :dispatchev : :visit ( ) @ 0x9e4101 process : :processbas : :serv ( ) @ 0x7fceb4369007 process : :processmanag : :resum ( ) @ 0x7fceb4377a8c process : :processmanag : :init_thread ( ) : : $ _2 : :oper ( ) ( ) @ 0x7fceb4377995 _znst12_bind_simpleifzn7process14processmanager12init_threadseve3 $ _2vee9_m_invokeijeeevst12_index_tupleijxspt_ee @ 0x7fceb4377965 std : :_bind_simpl < > : :oper ( ) ( ) @ 0x7fceb437793c std : :thread : :_impl < > : :_m_run ( ) @ 0x7fceadefa030 ( unknown ) @ 0x7fcead70b6aa start_thread @ 0x7fcead440e9d ( unknown ) { noformat } the provision directori suppos contain runtim directori . howev , backward compat . we chang deprec cycl . for , three option : 1. make provision : :destroy ( ) recurs . 2. sort contain recoveri guarante ` child parent ` semant . 3. remov check-failur sinc provision dir remov eventu end ( recommend ) . recommend ( 1 ) .",MESOS-7152,5.0
"wrap ioswitchboard.connect ( ) dispatch sinc ioswitchboard implement mesosisolatorprocess , api call automat dispatch onto underli process isol wrapper . howev , ioswitchboard also includ addit connect ( ) call access isol wrapper . as , need wrap dispatch call manual .",MESOS-7144,1.0
"quota exceed due coarse-grain offer techniqu . the current implement quota alloc alloc entir avail resourc agent tri satisfi quota . what mean quota exceed size agent . thi especi problemat larg machin , consid 48 core , 512 gb memori server role given 4 core 4gb memori . given current approach , send offer entir 48 core 512 gb memori ! thi ticket perform fine grain offer alloc exceed quota .",MESOS-7099,5.0
"libprocess test fail use libev 2.1.8 run { { libprocess-test } } meso compil { { -- enable-libev -- enable-ssl } } oper system use libev 2.1.8 , ssl relat test fail like { noformat } [ run ] ssltest.sslsocket i0207 15:20:46.017881 2528580544 openssl.cpp:419 ] ca file path unspecifi ! note : set ca file path libprocess_ssl_ca_file= < filepath > i0207 15:20:46.017904 2528580544 openssl.cpp:424 ] ca directori path unspecifi ! note : set ca directori path libprocess_ssl_ca_dir= < dirpath > i0207 15:20:46.017918 2528580544 openssl.cpp:429 ] will verifi peer certif ! note : set libprocess_ssl_verify_cert=1 enabl peer certif verif i0207 15:20:46.017923 2528580544 openssl.cpp:435 ] will verifi peer certif present ! note : set libprocess_ssl_require_cert=1 requir peer certif verif warn : log initgooglelog ( ) written stderr i0207 15:20:46.033001 2528580544 openssl.cpp:419 ] ca file path unspecifi ! note : set ca file path libprocess_ssl_ca_file= < filepath > i0207 15:20:46.033179 2528580544 openssl.cpp:424 ] ca directori path unspecifi ! note : set ca directori path libprocess_ssl_ca_dir= < dirpath > i0207 15:20:46.033196 2528580544 openssl.cpp:429 ] will verifi peer certif ! note : set libprocess_ssl_verify_cert=1 enabl peer certif verif i0207 15:20:46.033201 2528580544 openssl.cpp:435 ] will verifi peer certif present ! note : set libprocess_ssl_require_cert=1 requir peer certif verif .. / .. / .. /3rdparty/libprocess/src/tests/ssl_tests.cpp:257 : failur fail wait 15sec socket ( socket.get ( ) ) .recv ( ) [ fail ] ssltest.sslsocket ( 15196 ms ) { noformat } test fail { noformat } ssltest.sslsocket ssltest.noverifybadca ssltest.verifycertif ssltest.protocolmismatch ssltest.ecdhesupport ssltest.peeraddress ssltest.httpsget ssltest.httpspost ssltest.silentsocket ssltest.shutdownthensend sslverifyipadd/ssltest.basicsameprocess/0 , getparam ( ) = `` fals '' sslverifyipadd/ssltest.basicsameprocess/1 , getparam ( ) = `` true '' sslverifyipadd/ssltest.basicsameprocessunix/0 , getparam ( ) = `` fals '' sslverifyipadd/ssltest.basicsameprocessunix/1 , getparam ( ) = `` true '' sslverifyipadd/ssltest.requirecertificate/0 , getparam ( ) = `` fals '' sslverifyipadd/ssltest.requirecertificate/1 , getparam ( ) = `` true '' { noformat }",MESOS-7076,8.0
"the linux filesystem isol set mode ownership host volum . if host path rel path , linux filesystem isol set mode ownership host volum sinc allow non-root user write volum . note case share host fileysystem ( without rootf ) .",MESOS-7069,2.0
"ioswitchboard fd leak container launch fail -- lead deadlock if contain launch path fail actual launch contain , fd alloc contain ioswitchboard isol leak . thi lead deadlock destroy path ioswitchboard shutdown fd alloc contain close . sinc switchboard n't shutdown , futur return 'cleanup ( ) ' function never satisfi . we need gener purpos method close ioswitchboard fd fail launch path .",MESOS-7050,2.0
send sigkil sigterm ioswitchboard contain termin . thi follow mesos-6664,MESOS-7042,2.0
"updat framework author support multipl role current master assum framework singl role , see { { master : :authorizeframework } } . thi code updat support framework multipl role . in particular get author framework 's princip regist framework 's role .",MESOS-7022,3.0
"chang ` environment.variable.valu ` requir option to prepar futur work enabl modular fetch secret , chang { { environment.variable.valu } } field { { requir } } { { option } } . thi way , field left empti fill secret fetch modul .",MESOS-6991,2.0
"fix boost random gener initi window seed_rng : :seed_rng produc expect result window sinc use ` /dev/urandom ` file . 0:005 > k # child-sp retaddr call site 00 00000049 ` 22dfc108 00007ff6 ` 5193822f kernel32 ! createfilew ... 0e 00000049 ` 22dfc660 00007ff6 ` 502228fd mesos_ag ! boost : :uuid : :detail : :seed_rng : :seed_rng+0x3d [ : \repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80 ] 0f 00000049 ` 22dfc690 00007ff6 ` 502591e3 mesos_ag ! boost : :uuid : :detail : :seed < boost : :random : :mersenne_twister_engin < unsign int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253 > > +0x4d [ : \repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246 ] 10 00000049 ` 22dfc790 00007ff6 ` 50395518 mesos_ag ! boost : :uuid : :basic_random_gener < boost : :random : :mersenne_twister_engin < unsign int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253 > > : :basic_random_gener < boost : :random : :mersenne_twister_engin < unsign int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253 > > +0xd3 [ : \repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50 ] 11 00000049 ` 22dfc800 00007ff6 ` 500ad140 mesos_ag ! id : :uuid : :random+0x78 [ : \repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49 ] 12 00000049 ` 22dfc870 00007ff6 ` 5007ff55 mesos_ag ! meso : :intern : :slave : :framework : :launchexecutor+0x70 [ : \repositories\mesoswin\src\slave\slave.cpp @ 6301 ] 13 00000049 ` 22dfd520 00007ff6 ` 502a0a35 mesos_ag ! meso : :intern : :slave : :slave : :_run+0x2455 [ : \repositories\mesoswin\src\slave\slave.cpp @ 1990 ] ... 0:005 > du @ rcx 000001d7 ` cc55fb60 `` /dev/urandom ''",MESOS-6973,1.0
"launch two task docker imag simultan may caus stage dir never clean if user launch two task docker imag simultan ( e.g. , run { { mesos-executor } } twice docker imag ) , stage directori second task never clean , like : { code } └── store └── docker ├── layer │ ... ├── stage │ └── a6rxwc └── storedimag { code }",MESOS-6950,2.0
"improv health check valid . the `` gener '' field also valid ( i.e. , ` timeout_second ` ) , similar 's done http : //reviews.apache.org/r/55458/",MESOS-6916,2.0
"zero health check timeout interpret liter . current zero health check timeout interpret liter , help sinc health check even get chanc finish . we suggest fix behaviour interpret zero { { durat : :max ( ) } } effect render timeout infinit .",MESOS-6908,1.0
"add test framework upgrad multi-rol capabl . framework upgrad multi-rol capabl long framework 's role remain . we consid framework role unchang * framework previous n't specifi { { role } } { { roles= ( ) } } , * framework previous { { role=a } } { { roles= ( a ) } } .",MESOS-6900,2.0
"reconsid process creation primit window window notion process hierarchi unix , kill group process requir us make sure process contain job object , act someth like cgroup . thi particularli import decid kill task , way reliabl unless process 'd like kill job object . thi caus us number issu ; big reason need fork command executor , reason task current unkil default executor . as clean issu , need think care process govern semant meso , map reliabl , simpl window implement .",MESOS-6892,5.0
"transit window away ` os : :killtre ` . window robust notion process hierarchi unix , thu function like ` os : :killtre ` alway critic limit semant mismatch unix window . we transit away function , replac someth similar kill cgroup .",MESOS-6868,3.0
"contain exec possibl task belong task group { { launchnestedcontainersess } } current requir parent contain executor ( http : //github.com/apache/mesos/blob/f89f28724f5837ff414dc6cc84e1afb63f3306e5/src/slave/http.cpp # l2189-l2211 ) . thi work command task , task contain id executor contain id . but wo n't work pod task whose contain id differ executor ’ contain id . in order resolv ticket , need allow launch child contain arbitrari level .",MESOS-6864,5.0
faulttolerancetest.frameworkreregist flaki observ intern ci : { noformat } [ 21:27:38 ] : [ step 11/11 ] /mnt/teamcity/work/4240ba9ddd0997c3/src/tests/fault_tolerance_tests.cpp:892 : failur [ 21:27:38 ] : [ step 11/11 ] valu : framework.valu [ `` registered_tim '' ] .a ( ) .a ( ) [ 21:27:38 ] : [ step 11/11 ] actual : 1482442093 [ 21:27:38 ] : [ step 11/11 ] expect : static_cast ( registertime.sec ( ) ) [ 21:27:38 ] : [ step 11/11 ] which : 1482442094 { noformat } look like anoth instanc mesos-4695 .,MESOS-6837,3.0
consecutive_failur 0 == 1 healthcheck . when defin healthcheck consecutive_failures=0 one would expect meso never kill task notifi failur . what seem happen instead meso handl consecutive_failures=0 consecutive_failures=1 kill task 1 failur . sinc 0 n't 1 seem bug result unexpect behaviour .,MESOS-6833,3.0
"mesos-this-captur clang-tidi check fals posit the { { mesos-this-captur } } clang-tidi check incorrectli trigger code , http : //github.com/apache/mesos/blob/d2117362349ab4c383045720f77d42b2d9fd6871/src/slave/containerizer/mesos/io/switchboard.cpp # l1487 we tighten matcher avoid trigger construct .",MESOS-6824,2.0
"enabl glog stack trace call thing like ` abort ` window current window build , call ` abort ` ( etc . ) simpli bail , stack trace . thi highli undesir . stack trace import oper cluster product . we work enabl behavior , includ possibl work glog add support current nativ support .",MESOS-6815,5.0
"check unreach task cach task id collis launch as discuss mesos-6785 , possibl crash master launch task reus id unreachable/partit task . a complet solut problem quit involv , increment improv easi : see task launch oper , reject launch attempt task id collid id per-framework { { unreachabletask } } cach . thi n't catch situat id reus , better noth .",MESOS-6805,2.0
"ssl socket lose byte case eof dure recent work ssl-enabl test libprocess ( mesos-5966 ) , discov bug { { libeventsslsocketimpl } } , wherein socket either fail receiv eof , lose data eof receiv . the { { libeventsslsocketimpl : :event_callback ( short event ) } } method immedi set pend { { recvrequest } } 's promis zero upon receipt eof . howev , time promis set , may actual data wait read libev . upon receipt eof , attempt read socket 's bufferev first ensur n't lose data previous receiv socket .",MESOS-6802,3.0
"ssl socket 's 'shutdown ( ) ' method broken we recent uncov two issu { { libeventsslsocketimpl : :shutdown } } method : * the introduct shutdown method paramet [ commit|http : //reviews.apache.org/r/54113/ ] mean implement 's method longer overrid default implement . in addit fix implement method 's signatur , add { { overrid } } specifi socket implement ' method ensur n't happen futur . * the { { libeventsslsocketimpl : :shutdown } } function actual shutdown ssl socket . the proper function shutdown ssl socket { { ssl_shutdown } } , call implement 's destructor . we move { { shutdown ( ) } } time method return , socket actual shutdown .",MESOS-6789,2.0
"ioswitchboardtest.killswitchboardcontainerdestroy flaki { noformat } [ run ] ioswitchboardtest.killswitchboardcontainerdestroy i1212 13:57:02.641043 2211 containerizer.cpp:220 ] use isol : posix/cpu , filesystem/posix , network/cni w1212 13:57:02.641438 2211 backend.cpp:76 ] fail creat 'overlay ' backend : overlaybackend requir root privileg , run user nrc w1212 13:57:02.641559 2211 backend.cpp:76 ] fail creat 'bind ' backend : bindbackend requir root privileg i1212 13:57:02.642822 2268 containerizer.cpp:594 ] recov container i1212 13:57:02.643975 2253 provisioner.cpp:253 ] provision recoveri complet i1212 13:57:02.644953 2255 containerizer.cpp:986 ] start contain 09e87380-00ab-4987-83c9-fa1c5d86717f executor 'executor ' framework i1212 13:57:02.647004 2245 switchboard.cpp:430 ] alloc pseudo termin '/dev/pts/54 ' contain 09e87380-00ab-4987-83c9-fa1c5d86717f i1212 13:57:02.652305 2245 switchboard.cpp:596 ] creat i/o switchboard server ( pid : 2705 ) listen socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a ' contain 09e87380-00ab-4987-83c9-fa1c5d86717f i1212 13:57:02.655513 2267 launcher.cpp:133 ] fork child pid '2706 ' contain '09e87380-00ab-4987-83c9-fa1c5d86717f' i1212 13:57:02.655732 2267 containerizer.cpp:1621 ] checkpoint contain 's fork pid 2706 '/tmp/ioswitchboardtest_killswitchboardcontainerdestroyed_me5crx/meta/slaves/frameworks/executors/executor/runs/09e87380-00ab-4987-83c9-fa1c5d86717f/pids/forked.pid' i1212 13:57:02.726306 2265 containerizer.cpp:2463 ] contain 09e87380-00ab-4987-83c9-fa1c5d86717f exit i1212 13:57:02.726352 2265 containerizer.cpp:2100 ] destroy contain 09e87380-00ab-4987-83c9-fa1c5d86717f run state e1212 13:57:02.726495 2243 switchboard.cpp:861 ] unexpect termin i/o switchboard server : 'ioswitchboard ' exit signal : kill contain 09e87380-00ab-4987-83c9-fa1c5d86717f i1212 13:57:02.726563 2265 launcher.cpp:149 ] ask destroy contain 09e87380-00ab-4987-83c9-fa1c5d86717f e1212 13:57:02.783607 2228 switchboard.cpp:799 ] fail remov unix domain socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a ' contain '09e87380-00ab-4987-83c9-fa1c5d86717f ' : no file directori .. / .. /mesos/src/tests/containerizer/io_switchboard_tests.cpp:661 : failur valu : wait.get ( ) - > reason ( ) .size ( ) == 1 actual : fals expect : true * * * abort 1481579822 ( unix time ) tri `` date -d @ 1481579822 '' use gnu date * * * pc : @ 0x1bf16d0 test : :unittest : :addtestpartresult ( ) * * * sigsegv ( @ 0x0 ) receiv pid 2211 ( tid 0x7faed7d078c0 ) pid 0 ; stack trace : * * * @ 0x7faecf855100 ( unknown ) @ 0x1bf16d0 test : :unittest : :addtestpartresult ( ) @ 0x1be6247 test : :intern : :asserthelp : :operator= ( ) @ 0x19ed751 meso : :intern : :test : :ioswitchboardtest_killswitchboardcontainerdestroyed_test : :testbodi ( ) @ 0x1c0ed8c test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1c09e74 test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x1beb505 test : :test : :run ( ) @ 0x1bebc88 test : :testinfo : :run ( ) @ 0x1bec2c test : :testcas : :run ( ) @ 0x1bf2ba8 test : :intern : :unittestimpl : :runalltest ( ) @ 0x1c0f9b1 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1c0a9f2 test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x1bf18ee test : :unittest : :run ( ) @ 0x11bc9e3 run_all_test ( ) @ 0x11bc599 main @ 0x7faece663b15 __libc_start_main @ 0xa9c219 ( unknown ) { noformat }",MESOS-6784,1.0
"the 'http : :connect ( address ) ' alway use default_kind ( ) socket even ssl undesir . the 'http : :connect ( address ) ' variant 'http : :connect ( ) ' current support ssl . howev , ssl enabl , default 'socket : :creat ( ) ' call use 'default_kind ( ) ' socket set ssl . thi caus problem 'connect ( ) ' becuus creat socket 'kind ' ssl without way overrid .",MESOS-6775,1.0
reach unreach statement < path > /mesos/src/slave/containerizer/mesos/launch.cpp:766 thi error messag pop unexpect place ( e.g . run launch_nested_container_sess invalid command pass ) . we like remov unreach ( ) statement 's obvious reachabl case command tri launch found .,MESOS-6767,1.0
ioswitchboardservertest.attachoutput check failur run multipl time . i easili repo issu dev centos7 box follow command : { noformat } glog_v=1 bin/mesos-tests.sh -- gtest_filter=ioswitchboardservertest.attachoutput -- verbos -- gtest_repeat=2 .... [ ========== ] run 1 test 1 test case . [ -- -- -- -- -- ] global test environ set-up . [ -- -- -- -- -- ] 1 test ioswitchboardservertest [ run ] ioswitchboardservertest.attachoutput i1208 10:46:31.574084 41813 poll_socket.cpp:209 ] socket error send : broken pipe /home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:265 : failur ( respons ) .failur ( ) : disconnect /home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:266 : failur ( respons ) .failur ( ) : disconnect f1208 10:46:31.574919 41751 future.hpp:1137 ] check fail : ! isfail ( ) futur : :get ( ) state == fail : disconnect * * * check failur stack trace : * * * @ 0x7fc3f35a633a googl : :logmessag : :fail ( ) @ 0x7fc3f35a6299 googl : :logmessag : :sendtolog ( ) @ 0x7fc3f35a5caa googl : :logmessag : :flush ( ) @ 0x7fc3f35a89d googl : :logmessagefat : :~logmessagefat ( ) @ 0xb6a352 process : :futur < > : :get ( ) @ 0x1a050f meso : :intern : :test : :ioswitchboardservertest_attachoutput_test : :testbodi ( ) @ 0x1c54ce2 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1c4fe00 test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x1c31491 test : :test : :run ( ) @ 0x1c31c14 test : :testinfo : :run ( ) @ 0x1c3225a test : :testcas : :run ( ) @ 0x1c38b34 test : :intern : :unittestimpl : :runalltest ( ) @ 0x1c55907 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1c50948 test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x1c3787a test : :unittest : :run ( ) @ 0x11cc653 run_all_test ( ) @ 0x11cc209 main @ 0x7fc3ecb61b15 __libc_start_main @ 0xab5e89 ( unknown ) abort ( core dump ) { noformat },MESOS-6759,3.0
"i/o switchboard deal case reap server fail . current , n't deal reap failur , .",MESOS-6756,3.0
"i/o switchboard inherit agent environ variabl . sinc libexec binari own meso . agent might environ variabl ( e.g. , ld_library_path ) need io switchboard server process .",MESOS-6748,1.0
"ioswitchboard n't properli flush data attach_container_output current close write end connect pipe exit switchboard , n't wait read flush exit . thi caus data get drop sinc process may exit reader flush . the current code : { noformat } void ioswitchboardserverprocess : :final ( ) { foreach ( httpconnect & connect , outputconnect ) { connection.clos ( ) ; } ( failure.issom ( ) ) { promise.fail ( failure- > messag ) ; } els { promise.set ( noth ( ) ) ; } } { noformat } we chang : { noformat } void ioswitchboardserverprocess : :final ( ) { foreach ( httpconnect & connect , outputconnect ) { connection.clos ( ) ; connection.clos ( ) .await ( ) ; } ( failure.issom ( ) ) { promise.fail ( failure- > messag ) ; } els { promise.set ( noth ( ) ) ; } } { noformat }",MESOS-6746,1.0
"the agent synchron ioswitchboard determin readi accept incom connect . current , agent way know ioswitchboard start readi listen incom connect . we add support synchron agent figur . the implement block launch path contain , rather incom connect ioswitchboard { { connect ( ) } } call .",MESOS-6737,2.0
"creat test filter stout test use ` symlink ` window , fail run admin",MESOS-6731,3.0
"reserv oper valid reserv resourc role resourc allocationinfo role when dynam reserv valid , current logic make sure reserv resourc role framework role ( see [ src/master/validation.cpp|http : //github.com/apache/mesos/blob/0228fa74c25f450478a6a5a42e1ca384c26db8bd/src/master/validation.cpp # l1539-l1544 ] ) : { code } ( frameworkrole.issom ( ) & & resource.rol ( ) ! = frameworkrole.get ( ) ) { return error ( `` a reserv oper attempt resourc role '' `` ' '' + resource.rol ( ) + `` ' , framework reserv '' `` resourc role ' '' + frameworkrole.get ( ) + `` ' '' ) ; } { code } with multi-rol framework , valid reserv resourc role resourc alloc role . pleas make sure distinguish dynam reserv framework http endpoint . if dynam reserv trigger framework , need valid . if done http endpoint , need valid role .",MESOS-6730,3.0
"check ` preferredtoolarchitectur ` set ` x64 ` window build if variabl set build , caus linker occasion hang forev , due msvc toolchain bug linker . we make easi develop check . if variabl set , display error messag explain .",MESOS-6720,2.0
should destroy debug contain agent recoveri . we need add support destroy debug contain agent recoveri . right contain stick around forev ( run complet ) .,MESOS-6718,3.0
port ` slave_recovery_tests.cpp ` http : //reviews.apache.org/r/65408/,MESOS-6713,3.0
remov unix domain socket path ioswitchboard : :cleanup we current leak unix domain socket file creat switchboard ` /tmp ` directori . we need clean properli .,MESOS-6689,1.0
"ioswitchboard recov spawn server pid agent restart we need proper recoveri io switchboard server pid across agent restart . as , agent restart way recov pid .",MESOS-6688,2.0
"duplic imag layer id may make backend fail mount rootf . some imag ( e.g. , 'mesosphere/inki ' ) may contain duplic layer id manifest , may caus backend unabl mount rootf ( e.g. , 'auf ' backend ) . we make sure layer path return 'imageinfo ' uniqu . here exampl manifest 'mesosphere/inki ' : { noformat } [ 20:13:08 ] w : [ step 10/10 ] `` name '' : `` mesosphere/inki '' , [ 20:13:08 ] w : [ step 10/10 ] `` tag '' : `` latest '' , [ 20:13:08 ] w : [ step 10/10 ] `` architectur '' : `` amd64 '' , [ 20:13:08 ] w : [ step 10/10 ] `` fslayer '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] , [ 20:13:08 ] w : [ step 10/10 ] `` histori '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' parent\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.247988044z\ '' , \ '' container\ '' : \ '' ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) cmd [ inki ] \ '' ] , \ '' image\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' parent\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.068514721z\ '' , \ '' container\ '' : \ '' 696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) maintain support @ mesosphere.io\ '' ] , \ '' image\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' ] , \ '' image\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' parent\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.990887725z\ '' , \ '' container\ '' : \ '' bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) cmd [ /bin/sh ] \ '' ] , \ '' image\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' ] , \ '' image\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' parent\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.692528634z\ '' , \ '' container\ '' : \ '' fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) add file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 /\ '' ] , \ '' image\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : null , \ '' image\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :2433303 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' parent\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.589531476z\ '' , \ '' container\ '' : \ '' f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) maintain jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' ] , \ '' image\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : null , \ '' image\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' comment\ '' : \ '' import -\ '' , \ '' created\ '' : \ '' 2013-06-13t14:03:50.821769-07:00\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' \ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : null , \ '' cmd\ '' : null , \ '' image\ '' : \ '' \ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : null , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.4.0\ '' , \ '' architecture\ '' : \ '' x86_64\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] , [ 20:13:08 ] w : [ step 10/10 ] `` schemavers '' : 1 , [ 20:13:08 ] w : [ step 10/10 ] `` signatur '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` header '' : { [ 20:13:08 ] w : [ step 10/10 ] `` jwk '' : { [ 20:13:08 ] w : [ step 10/10 ] `` crv '' : `` p-256 '' , [ 20:13:08 ] w : [ step 10/10 ] `` kid '' : `` 4ayn : kh32 : gjjd : i6bx : sjaz : a3ec : p7ic:7o7c:22zq:3z5o:75vq:3qot '' , [ 20:13:08 ] w : [ step 10/10 ] `` kti '' : `` ec '' , [ 20:13:08 ] w : [ step 10/10 ] `` x '' : `` o8bvruwnpxkzdgoo2wq7ehqzcvyhvuoovjqgextrylu '' , [ 20:13:08 ] w : [ step 10/10 ] `` '' : `` dchygr0cbi-fzzqypqm16qkfefumqctk0rqme-q5gma '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] `` alg '' : `` es256 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] `` signatur '' : `` f3faob4xpt0puw9tiptxae_zpae0pdm2imxaeacmjbbf6lb-sufpvge4iqz1co0voijeyvub1g1lv_a5nnj5zg '' , [ 20:13:08 ] w : [ step 10/10 ] `` protect '' : `` eyjmb3jtyxrmzw5ndggiojeznza3lcjmb3jtyxruywlsijoiq24wiiwidgltzsi6ijiwmtytmdgtmdvumja6mtm6mddain0 '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] [ 20:13:08 ] w : [ step 10/10 ] } ' { noformat } these two layer id total ident : { noformat } [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , { noformat } it would make backend ( e.g. , auf ) fail mount rootf due invalid argument . { noformat } [ 20:13:08 ] w : [ step 10/10 ] e0805 20:13:08.614994 23432 slave.cpp:4029 ] contain 'f2c1fd6d-4d11-45cd-a916-e4d73d226451 ' executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 fail start : fail mount rootf '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77 ' auf : invalid argument { noformat } we make sure vector layer path pass backend contain uniqu layer path .",MESOS-6654,3.0
"overlayf backend may fail mount rootf contain imag imag volum specifi . depend mesos-6000 , use symlink shorten overlayf mount argument . howev , one imag need provis ( e.g. , contain imag specifi imag volum specifi contain ) , symlink ... /backends/overlay/link would fail creat sinc exist alreadi . here simpl log hard code overlayf default backend : { noformat } [ 07:02:45 ] : [ step 10/10 ] [ run ] nesting/volumeimageisolatortest.root_imageinvolumewithrootfilesystem/0 [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.416021 2919 containerizer.cpp:207 ] use isol : filesystem/linux , volume/imag , docker/runtim , network/cni [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.419312 2919 linux_launcher.cpp:150 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 07:02:46 ] : [ step 10/10 ] e1127 07:02:46.425336 2919 shell.hpp:107 ] command 'hadoop version 2 > & 1 ' fail ; output : [ 07:02:46 ] : [ step 10/10 ] sh : 1 : hadoop : found [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.425379 2919 fetcher.cpp:69 ] skip uri fetcher plugin 'hadoop ' could creat : fail creat hdf client : fail execut 'hadoop version 2 > & 1 ' ; command either found exit non-zero exit statu : 127 [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.425452 2919 local_puller.cpp:94 ] creat local puller docker registri '/tmp/r6ouei/registry' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.427258 2934 containerizer.cpp:956 ] start contain 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 executor 'test_executor ' framework [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.427592 2938 metadata_manager.cpp:167 ] look imag 'test_image_rootfs' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.427774 2936 local_puller.cpp:147 ] untar imag 'test_image_rootf ' '/tmp/r6ouei/registry/test_image_rootfs.tar ' '/tmp/r6ouei/store/staging/9krdz2' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.512070 2933 local_puller.cpp:167 ] the repositori json file imag 'test_image_rootf ' ' { `` test_image_rootf '' : { `` latest '' : '' 815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346 '' } } ' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.512279 2933 local_puller.cpp:295 ] extract layer tar ball '/tmp/r6ouei/store/staging/9krdz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar rootf '/tmp/r6ouei/store/staging/9krdz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617442 2937 metadata_manager.cpp:155 ] success cach imag 'test_image_rootfs' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617908 2938 provisioner.cpp:286 ] imag layer : 1 [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617925 2938 provisioner.cpp:296 ] should hit [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617949 2938 provisioner.cpp:315 ] ! ! ! ! : bind [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617959 2938 provisioner.cpp:315 ] ! ! ! ! : overlay [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617967 2938 provisioner.cpp:315 ] ! ! ! ! : copi [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.617974 2938 provisioner.cpp:318 ] provis imag rootf '/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7 ' contain 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 use overlay backend [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.618408 2936 overlay.cpp:175 ] creat symlink '/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/link ' - > '/tmp/dq3blt' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.618472 2936 overlay.cpp:203 ] provis imag rootf overlayf : 'lowerdir=/tmp/dq3blt/0 , upperdir=/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/upperdir , workdir=/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/workdir' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.619098 2933 linux.cpp:451 ] ignor imag volum contain 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.619745 2938 metadata_manager.cpp:167 ] look imag 'test_image_volume' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.619925 2937 local_puller.cpp:147 ] untar imag 'test_image_volum ' '/tmp/r6ouei/registry/test_image_volume.tar ' '/tmp/r6ouei/store/staging/2gnljo' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.713526 2935 local_puller.cpp:167 ] the repositori json file imag 'test_image_volum ' ' { `` test_image_volum '' : { `` latest '' : '' 815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346 '' } } ' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.713726 2935 local_puller.cpp:295 ] extract layer tar ball '/tmp/r6ouei/store/staging/2gnljo/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar rootf '/tmp/r6ouei/store/staging/2gnljo/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.818696 2937 metadata_manager.cpp:155 ] success cach imag 'test_image_volume' [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819169 2934 provisioner.cpp:286 ] imag layer : 1 [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819188 2934 provisioner.cpp:296 ] should hit [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819221 2934 provisioner.cpp:315 ] ! ! ! ! : bind [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819232 2934 provisioner.cpp:315 ] ! ! ! ! : overlay [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819236 2934 provisioner.cpp:315 ] ! ! ! ! : copi [ 07:02:46 ] : [ step 10/10 ] i1127 07:02:46.819241 2934 provisioner.cpp:318 ] provis imag rootf '/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/baf632b3-29c5-45e4-9d2e-6f3a2bdd9759 ' contain 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 use overlay backend [ 07:02:46 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/volume_image_isolator_tests.cpp:214 : failur [ 07:02:46 ] : [ step 10/10 ] ( launch ) .failur ( ) : fail creat symlink '/mnt/teamcity/temp/buildtmp/nesting_volumeimageisolatortest_root_imageinvolumewithrootfilesystem_0_1fmo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/link ' - > '/tmp/6dj9ig' [ 07:02:46 ] : [ step 10/10 ] [ fail ] nesting/volumeimageisolatortest.root_imageinvolumewithrootfilesystem/0 , getparam ( ) = fals ( 919 ms ) { noformat } we differenci link differ provis imag .",MESOS-6653,3.0
"expos contain id containerstatu dockercontainer . current , contain id expos mesoscontainer . we make consist dockercontainer .",MESOS-6625,2.0
"ssl downgrad path check-fail use temporari persist socket the code path downgrad socket ssl non-ssl includ code : { code } // if address temporari link . ( temps.count ( address [ to_fd ] ) > 0 ) { temp [ address [ to_fd ] ] = to_fd ; // no need eras 're chang valu , key. } // if address persist link . ( persists.count ( address [ to_fd ] ) > 0 ) { persist [ address [ to_fd ] ] = to_fd ; // no need eras 're chang valu , key. } { code } http : //github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp # l2311-l2321 it possibl libprocess hold temporari persist socket address . thi happen messag first sent ( { { processbas : :send } } ) , link establish ( { { processbas : :link } } ) . when target message/link non-ssl socket , temporari persist socket go downgrad path . if temporari socket present persist socket creat , code remap temporari persist socket address ( remap persist socket ) . thi lead check failur socket use close later : * { code } bool persist = persists.count ( address ) > 0 ; bool temp = temps.count ( address ) > 0 ; ( persist || temp ) { int = persist ? persist [ address ] : temp [ address ] ; check ( sockets.count ( ) > 0 ) ; socket = sockets.at ( ) ; { code } http : //github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp # l1942 * { code } ( dispose.count ( ) > 0 ) { // thi either temporari socket creat 's // socket receiv data possibl // send http respons back . clean either way . ( addresses.count ( ) > 0 ) { const address & address = address [ ] ; check ( temps.count ( address ) > 0 & & temp [ address ] == ) ; temps.eras ( address ) ; { code } http : //github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp # l2044",MESOS-6621,3.0
"improv task manag unreach task scenario : # framework start non-partition-awar task t agent a # agent a partit . task t mark `` complet task '' { { framework } } struct master , part { { framework : :removetask } } . # agent a re-regist master . the task run a re-ad respect framework master run task . # in { { master : :\_reregisterslav } } , master send { { shutdownframeworkmessag } } non-partition-awar framework run agent . the master { { removetask } } task manag one framework , result call { { framework : :removetask } } , add _another_ task { { completed_task } } . note { { completed_task } } attempt detect/suppress duplic , result two element { { completed_task } } collect . similar problem occur partition-awar task run partit agent re-regist : result task { { task } } list _and_ task { { completed_task } } list . possibl fixes/chang : * ad task { { completed_task } } list agent becom partit debat ; certainli partition-awar task , task `` complet '' . we might consid ad `` { { unreachable_task } } '' list http endpoint . * regardless whether continu use { { completed_task } } add new collect , ensur consist data structur agent re-registr .",MESOS-6619,8.0
some test use hardcod port number . dockercontainerizertest.root_docker_notransitionfromkillingtorun mani healthchecktest use hardcod port number . thi creat fals failur test run parallel machin . it appear instead use random port number .,MESOS-6618,3.0
shutdown complet framework unreach agent re-regist we current shutdown complet framework agent re-regist master alreadi regist ( mesos-633 ) . we also shutdown complet framework unreach agent re-regist . thi distinct difficult problem shut complet framework master failov ( mesos-4659 ) .,MESOS-6602,5.0
"mesoscontainerizer/defaultexecutortest.killtask/0 fail asf ci { noformat : title= } [ run ] mesoscontainerizer/defaultexecutortest.killtask/0 i1110 01:20:11.482097 29700 cluster.cpp:158 ] creat default 'local ' author i1110 01:20:11.485241 29700 leveldb.cpp:174 ] open db 2.774513m i1110 01:20:11.486237 29700 leveldb.cpp:181 ] compact db 953614n i1110 01:20:11.486299 29700 leveldb.cpp:196 ] creat db iter 24739n i1110 01:20:11.486325 29700 leveldb.cpp:202 ] seek begin db 2300n i1110 01:20:11.486344 29700 leveldb.cpp:271 ] iter 0 key db 378n i1110 01:20:11.486399 29700 replica.cpp:776 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i1110 01:20:11.486933 29733 recover.cpp:451 ] start replica recoveri i1110 01:20:11.487289 29733 recover.cpp:477 ] replica empti statu i1110 01:20:11.488503 29721 replica.cpp:673 ] replica empti statu receiv broadcast recov request __req_res__ ( 7318 ) @ 172.17.0.3:52462 i1110 01:20:11.488855 29727 recover.cpp:197 ] receiv recov respons replica empti statu i1110 01:20:11.489398 29729 recover.cpp:568 ] updat replica statu start i1110 01:20:11.490223 29723 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 575135n i1110 01:20:11.490284 29732 master.cpp:380 ] master d28fbae1-c3dc-45fa-8384-32ab9395a975 ( 3a31be8bf679 ) start 172.17.0.3:52462 i1110 01:20:11.490317 29732 master.cpp:382 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticate_http_readonly= '' true '' -- authenticate_http_readwrite= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/k50x7x/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_gc_interval= '' 15min '' -- registry_max_agent_age= '' 2week '' -- registry_max_agent_count= '' 102400 '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' fals '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-1.2.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/k50x7x/master '' -- zk_session_timeout= '' 10sec '' i1110 01:20:11.490696 29732 master.cpp:432 ] master allow authent framework regist i1110 01:20:11.490712 29732 master.cpp:446 ] master allow authent agent regist i1110 01:20:11.490720 29732 master.cpp:459 ] master allow authent http framework regist i1110 01:20:11.490730 29732 credentials.hpp:37 ] load credenti authent '/tmp/k50x7x/credentials' i1110 01:20:11.490281 29723 replica.cpp:320 ] persist replica statu start i1110 01:20:11.491210 29732 master.cpp:504 ] use default 'crammd5 ' authent i1110 01:20:11.491225 29720 recover.cpp:477 ] replica start statu i1110 01:20:11.491394 29732 http.cpp:895 ] use default 'basic ' http authent realm 'mesos-master-readonly' i1110 01:20:11.491621 29732 http.cpp:895 ] use default 'basic ' http authent realm 'mesos-master-readwrite' i1110 01:20:11.491770 29732 http.cpp:895 ] use default 'basic ' http authent realm 'mesos-master-scheduler' i1110 01:20:11.491937 29732 master.cpp:584 ] author enabl i1110 01:20:11.492276 29725 whitelist_watcher.cpp:77 ] no whitelist given i1110 01:20:11.492310 29723 hierarchical.cpp:149 ] initi hierarch alloc process i1110 01:20:11.492569 29721 replica.cpp:673 ] replica start statu receiv broadcast recov request __req_res__ ( 7319 ) @ 172.17.0.3:52462 i1110 01:20:11.492830 29719 recover.cpp:197 ] receiv recov respons replica start statu i1110 01:20:11.493371 29720 recover.cpp:568 ] updat replica statu vote i1110 01:20:11.494002 29721 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 367673n i1110 01:20:11.494032 29721 replica.cpp:320 ] persist replica statu vote i1110 01:20:11.494218 29734 recover.cpp:582 ] success join paxo group i1110 01:20:11.494469 29734 recover.cpp:466 ] recov process termin i1110 01:20:11.495633 29733 master.cpp:2033 ] elect lead master ! i1110 01:20:11.495685 29733 master.cpp:1560 ] recov registrar i1110 01:20:11.495880 29720 registrar.cpp:329 ] recov registrar i1110 01:20:11.496842 29730 log.cpp:553 ] attempt start writer i1110 01:20:11.498610 29725 replica.cpp:493 ] replica receiv implicit promis request __req_res__ ( 7320 ) @ 172.17.0.3:52462 propos 1 i1110 01:20:11.499179 29725 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 524192n i1110 01:20:11.499213 29725 replica.cpp:342 ] persist promis 1 i1110 01:20:11.500258 29726 coordinator.cpp:238 ] coordin attempt fill miss posit i1110 01:20:11.501874 29731 replica.cpp:388 ] replica receiv explicit promis request __req_res__ ( 7321 ) @ 172.17.0.3:52462 posit 0 propos 2 i1110 01:20:11.502413 29731 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 484138n i1110 01:20:11.502457 29731 replica.cpp:708 ] persist action nop posit 0 i1110 01:20:11.503885 29720 replica.cpp:537 ] replica receiv write request posit 0 __req_res__ ( 7322 ) @ 172.17.0.3:52462 i1110 01:20:11.503985 29720 leveldb.cpp:436 ] read posit leveldb took 56800n i1110 01:20:11.504534 29720 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 467426n i1110 01:20:11.504566 29720 replica.cpp:708 ] persist action nop posit 0 i1110 01:20:11.505470 29721 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i1110 01:20:11.505988 29721 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 479078n i1110 01:20:11.506021 29721 replica.cpp:708 ] persist action nop posit 0 i1110 01:20:11.506706 29732 log.cpp:569 ] writer start end posit 0 i1110 01:20:11.508041 29734 leveldb.cpp:436 ] read posit leveldb took 50010n i1110 01:20:11.509210 29733 registrar.cpp:362 ] success fetch registri ( 0b ) 13.068032m i1110 01:20:11.509356 29733 registrar.cpp:461 ] appli 1 oper 27124n ; attempt updat registri i1110 01:20:11.510251 29732 log.cpp:577 ] attempt append 168 byte log i1110 01:20:11.510457 29724 coordinator.cpp:348 ] coordin attempt write append action posit 1 i1110 01:20:11.511355 29728 replica.cpp:537 ] replica receiv write request posit 1 __req_res__ ( 7323 ) @ 172.17.0.3:52462 i1110 01:20:11.511828 29728 leveldb.cpp:341 ] persist action ( 187 byte ) leveldb took 423890n i1110 01:20:11.511859 29728 replica.cpp:708 ] persist action append posit 1 i1110 01:20:11.512572 29734 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i1110 01:20:11.513051 29734 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 368122n i1110 01:20:11.513087 29734 replica.cpp:708 ] persist action append posit 1 i1110 01:20:11.514302 29726 registrar.cpp:506 ] success updat registri 4.862976m i1110 01:20:11.514503 29726 registrar.cpp:392 ] success recov registrar i1110 01:20:11.514593 29728 log.cpp:596 ] attempt truncat log 1 i1110 01:20:11.514760 29730 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i1110 01:20:11.515249 29723 master.cpp:1676 ] recov 0 agent registri ( 129b ) ; allow 10min agent re-regist i1110 01:20:11.515534 29722 hierarchical.cpp:176 ] skip recoveri hierarch alloc : noth recov i1110 01:20:11.516068 29722 replica.cpp:537 ] replica receiv write request posit 2 __req_res__ ( 7324 ) @ 172.17.0.3:52462 i1110 01:20:11.516619 29722 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 497823n i1110 01:20:11.516652 29722 replica.cpp:708 ] persist action truncat posit 2 i1110 01:20:11.517526 29734 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i1110 01:20:11.518040 29734 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 384129n i1110 01:20:11.518111 29734 leveldb.cpp:399 ] delet ~1 key leveldb took 39398n i1110 01:20:11.518138 29734 replica.cpp:708 ] persist action truncat posit 2 i1110 01:20:11.525027 29700 containerizer.cpp:201 ] use isol : posix/cpu , posix/mem , filesystem/posix , network/cni w1110 01:20:11.525806 29700 backend.cpp:76 ] fail creat 'auf ' backend : aufsbackend requir root privileg , run user meso w1110 01:20:11.526018 29700 backend.cpp:76 ] fail creat 'bind ' backend : bindbackend requir root privileg i1110 01:20:11.527331 29700 cluster.cpp:435 ] creat default 'local ' author i1110 01:20:11.528741 29725 slave.cpp:208 ] meso agent start ( 571 ) @ 172.17.0.3:52462 i1110 01:20:11.528789 29725 slave.cpp:209 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http_readonly= '' true '' -- authenticate_http_readwrite= '' fals '' -- authenticatee= '' crammd5 '' -- authentication_backoff_factor= '' 1sec '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher= '' posix '' -- launcher_dir= '' /mesos/mesos-1.2.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_executors_per_framework= '' 150 '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- runtime_dir= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd '' i1110 01:20:11.529228 29725 credentials.hpp:86 ] load credenti authent '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/credential' i1110 01:20:11.529305 29700 scheduler.cpp:176 ] version : 1.2.0 i1110 01:20:11.529436 29725 slave.cpp:346 ] agent use credenti : test-princip i1110 01:20:11.529464 29725 credentials.hpp:37 ] load credenti authent '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/http_credentials' i1110 01:20:11.529747 29725 http.cpp:895 ] use default 'basic ' http authent realm 'mesos-agent-readonly' i1110 01:20:11.529855 29729 scheduler.cpp:469 ] new master detect master @ 172.17.0.3:52462 i1110 01:20:11.529884 29729 scheduler.cpp:478 ] wait 0n initi re- ( connect ) attempt master i1110 01:20:11.531039 29725 slave.cpp:533 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i1110 01:20:11.531113 29725 slave.cpp:541 ] agent attribut : [ ] i1110 01:20:11.531126 29725 slave.cpp:546 ] agent hostnam : 3a31be8bf679 i1110 01:20:11.532897 29723 state.cpp:57 ] recov state '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/meta' i1110 01:20:11.533222 29727 status_update_manager.cpp:203 ] recov statu updat manag i1110 01:20:11.533269 29721 scheduler.cpp:353 ] connect master http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.533627 29734 containerizer.cpp:557 ] recov container i1110 01:20:11.534519 29725 scheduler.cpp:235 ] send subscrib call http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.535482 29732 provisioner.cpp:253 ] provision recoveri complet i1110 01:20:11.535652 29734 process.cpp:3570 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i1110 01:20:11.535815 29724 slave.cpp:5411 ] finish recoveri i1110 01:20:11.536440 29724 slave.cpp:5585 ] queri resourc estim oversubscrib resourc i1110 01:20:11.536898 29721 slave.cpp:915 ] new master detect master @ 172.17.0.3:52462 i1110 01:20:11.536906 29731 status_update_manager.cpp:177 ] paus send statu updat i1110 01:20:11.536941 29721 slave.cpp:974 ] authent master master @ 172.17.0.3:52462 i1110 01:20:11.537076 29721 slave.cpp:985 ] use default cram-md5 authenticate i1110 01:20:11.537214 29733 http.cpp:391 ] http post /master/api/v1/schedul 172.17.0.3:54635 i1110 01:20:11.537353 29719 authenticatee.cpp:121 ] creat new client sasl connect i1110 01:20:11.537256 29721 slave.cpp:947 ] detect new master i1110 01:20:11.537591 29733 master.cpp:2329 ] receiv subscript request http framework 'default' i1110 01:20:11.537611 29721 slave.cpp:5599 ] receiv oversubscrib resourc { } resourc estim i1110 01:20:11.537701 29733 master.cpp:2069 ] author framework princip 'test-princip ' receiv offer role ' * ' i1110 01:20:11.538077 29733 master.cpp:6745 ] authent slave ( 571 ) @ 172.17.0.3:52462 i1110 01:20:11.538208 29732 authenticator.cpp:414 ] start authent session crammd5-authenticate ( 1121 ) @ 172.17.0.3:52462 i1110 01:20:11.538291 29733 master.cpp:2427 ] subscrib framework 'default ' checkpoint disabl capabl [ ] i1110 01:20:11.538508 29731 authenticator.cpp:98 ] creat new server sasl connect i1110 01:20:11.538782 29720 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 i1110 01:20:11.538823 29720 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' i1110 01:20:11.539227 29730 hierarchical.cpp:275 ] ad framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.539317 29722 master.hpp:2161 ] send heartbeat d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.539331 29730 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:11.539696 29730 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:11.539818 29730 hierarchical.cpp:1286 ] perform alloc 0 agent 554795n i1110 01:20:11.540354 29720 authenticator.cpp:204 ] receiv sasl authent start i1110 01:20:11.540361 29719 scheduler.cpp:675 ] enqueu event subscrib receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.540438 29720 authenticator.cpp:326 ] authent requir step i1110 01:20:11.540750 29720 authenticatee.cpp:259 ] receiv sasl authent step i1110 01:20:11.541038 29721 authenticator.cpp:232 ] receiv sasl authent step i1110 01:20:11.541081 29721 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : '3a31be8bf679 ' server fqdn : '3a31be8bf679 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i1110 01:20:11.541112 29721 auxprop.cpp:181 ] look auxiliari properti ' * userpassword' i1110 01:20:11.541147 29719 scheduler.cpp:675 ] enqueu event heartbeat receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.541178 29721 auxprop.cpp:181 ] look auxiliari properti ' * cmusaslsecretcram-md5' i1110 01:20:11.541260 29721 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : '3a31be8bf679 ' server fqdn : '3a31be8bf679 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i1110 01:20:11.541285 29721 auxprop.cpp:131 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i1110 01:20:11.541307 29721 auxprop.cpp:131 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i1110 01:20:11.541342 29721 authenticator.cpp:318 ] authent success i1110 01:20:11.541517 29733 authenticatee.cpp:299 ] authent success i1110 01:20:11.541586 29720 master.cpp:6775 ] success authent princip 'test-princip ' slave ( 571 ) @ 172.17.0.3:52462 i1110 01:20:11.541826 29721 authenticator.cpp:432 ] authent session cleanup crammd5-authenticate ( 1121 ) @ 172.17.0.3:52462 i1110 01:20:11.542129 29730 slave.cpp:1069 ] success authent master master @ 172.17.0.3:52462 i1110 01:20:11.542362 29730 slave.cpp:1483 ] will retri registr 9.532818m necessari i1110 01:20:11.542577 29733 master.cpp:5154 ] regist agent slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) id d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 i1110 01:20:11.543083 29731 registrar.cpp:461 ] appli 1 oper 60476n ; attempt updat registri i1110 01:20:11.543926 29729 log.cpp:577 ] attempt append 337 byte log i1110 01:20:11.544077 29723 coordinator.cpp:348 ] coordin attempt write append action posit 3 i1110 01:20:11.545238 29731 replica.cpp:537 ] replica receiv write request posit 3 __req_res__ ( 7325 ) @ 172.17.0.3:52462 i1110 01:20:11.546116 29731 leveldb.cpp:341 ] persist action ( 356 byte ) leveldb took 825474n i1110 01:20:11.546169 29731 replica.cpp:708 ] persist action append posit 3 i1110 01:20:11.547427 29725 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i1110 01:20:11.547969 29725 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 483290n i1110 01:20:11.548005 29725 replica.cpp:708 ] persist action append posit 3 i1110 01:20:11.550129 29732 registrar.cpp:506 ] success updat registri 6.962944m i1110 01:20:11.550396 29726 log.cpp:596 ] attempt truncat log 3 i1110 01:20:11.550614 29720 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i1110 01:20:11.551375 29723 slave.cpp:4263 ] receiv ping slave-observ ( 531 ) @ 172.17.0.3:52462 i1110 01:20:11.551326 29734 master.cpp:5225 ] regist agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i1110 01:20:11.551720 29730 replica.cpp:537 ] replica receiv write request posit 4 __req_res__ ( 7326 ) @ 172.17.0.3:52462 i1110 01:20:11.551892 29723 slave.cpp:1115 ] regist master master @ 172.17.0.3:52462 ; given agent id d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 i1110 01:20:11.551975 29723 fetcher.cpp:86 ] clear fetcher cach i1110 01:20:11.552170 29732 hierarchical.cpp:485 ] ad agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 ( 3a31be8bf679 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : { } ) i1110 01:20:11.552338 29721 status_update_manager.cpp:184 ] resum send statu updat i1110 01:20:11.552486 29730 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 709727n i1110 01:20:11.552655 29723 slave.cpp:1138 ] checkpoint slaveinfo '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/meta/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/slave.info' i1110 01:20:11.552609 29730 replica.cpp:708 ] persist action truncat posit 4 i1110 01:20:11.553383 29731 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i1110 01:20:11.553409 29723 slave.cpp:1175 ] forward total oversubscrib resourc { } i1110 01:20:11.553653 29732 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:11.553671 29727 master.cpp:5624 ] receiv updat agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) total oversubscrib resourc { } i1110 01:20:11.553755 29732 hierarchical.cpp:1309 ] perform alloc agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 1.528714m i1110 01:20:11.553975 29731 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 525057n i1110 01:20:11.554072 29731 leveldb.cpp:399 ] delet ~2 key leveldb took 59750n i1110 01:20:11.554065 29732 hierarchical.cpp:555 ] agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 ( 3a31be8bf679 ) updat oversubscrib resourc { } ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) i1110 01:20:11.554105 29731 replica.cpp:708 ] persist action truncat posit 4 i1110 01:20:11.554260 29732 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:11.554314 29732 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:11.554345 29727 master.cpp:6574 ] send 1 offer framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:11.554379 29732 hierarchical.cpp:1309 ] perform alloc agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 239597n i1110 01:20:11.556370 29724 scheduler.cpp:675 ] enqueu event offer receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.559177 29730 scheduler.cpp:235 ] send accept call http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.560282 29734 process.cpp:3570 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i1110 01:20:11.561323 29733 http.cpp:391 ] http post /master/api/v1/schedul 172.17.0.3:54634 i1110 01:20:11.562417 29733 master.cpp:3581 ] process accept call offer : [ d28fbae1-c3dc-45fa-8384-32ab9395a975-o0 ] agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:11.562584 29733 master.cpp:3173 ] author framework princip 'test-princip ' launch task c96eb523-0365-49b2-8b3b-78976ff28797 i1110 01:20:11.563097 29733 master.cpp:3173 ] author framework princip 'test-princip ' launch task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba i1110 01:20:11.567248 29733 master.cpp:8337 ] ad task c96eb523-0365-49b2-8b3b-78976ff28797 resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 ( 3a31be8bf679 ) i1110 01:20:11.567651 29733 master.cpp:8337 ] ad task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 ( 3a31be8bf679 ) i1110 01:20:11.567845 29733 master.cpp:4438 ] launch task group { 08848440-4c0e-4ad6-a0a9-b5947c5d21ba , c96eb523-0365-49b2-8b3b-78976ff28797 } framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) resourc cpu ( * ) :0.2 ; mem ( * ) :64 ; disk ( * ) :64 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:11.568495 29724 slave.cpp:1547 ] got assign task group contain task [ c96eb523-0365-49b2-8b3b-78976ff28797 , 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.569128 29729 hierarchical.cpp:1018 ] recov cpu ( * ) :1.7 ; mem ( * ) :928 ; disk ( * ) :928 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :0.3 ; mem ( * ) :96 ; disk ( * ) :96 ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.569211 29729 hierarchical.cpp:1055 ] framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 filter agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 5sec i1110 01:20:11.570461 29724 slave.cpp:1709 ] launch task group contain task [ c96eb523-0365-49b2-8b3b-78976ff28797 , 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.571297 29724 paths.cpp:530 ] tri chown '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06 ' user 'mesos' i1110 01:20:11.580168 29724 slave.cpp:6319 ] launch executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 work directori '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' i1110 01:20:11.580930 29734 containerizer.cpp:940 ] start contain a283035b-25d3-4b48-b59a-964e5a4dfa06 executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.581110 29724 slave.cpp:2031 ] queu task group contain task [ c96eb523-0365-49b2-8b3b-78976ff28797 , 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.581214 29724 slave.cpp:868 ] success attach file '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' i1110 01:20:11.585572 29722 containerizer.cpp:1480 ] launch 'mesos-container ' flag ' -- command= '' { `` argument '' : [ `` mesos-default-executor '' , '' -- launcher_dir=\/mesos\/mesos-1.2.0\/_build\/src '' ] , '' shell '' : fals , '' valu '' : '' \/mesos\/mesos-1.2.0\/_build\/src\/mesos-default-executor '' } '' -- help= '' fals '' -- pipe_read= '' 60 '' -- pipe_write= '' 61 '' -- pre_exec_commands= '' [ ] '' -- runtime_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06 '' -- unshare_namespace_mnt= '' fals '' -- user= '' meso '' -- working_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06 '' ' i1110 01:20:11.588587 29722 launcher.cpp:127 ] fork child pid '10191 ' contain 'a283035b-25d3-4b48-b59a-964e5a4dfa06' i1110 01:20:11.592996 29734 fetcher.cpp:345 ] start fetch uri contain : a283035b-25d3-4b48-b59a-964e5a4dfa06 , directori : /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06 i1110 01:20:11.777189 10229 executor.cpp:189 ] version : 1.2.0 i1110 01:20:11.786099 29719 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1/executor' i1110 01:20:11.787382 29722 http.cpp:277 ] http post /slave ( 571 ) /api/v1/executor 172.17.0.3:54638 i1110 01:20:11.787693 29722 slave.cpp:3086 ] receiv subscrib request http executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.790436 29730 slave.cpp:2276 ] send queu task group task group contain task [ c96eb523-0365-49b2-8b3b-78976ff28797 , 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( via http ) i1110 01:20:11.795663 10221 default_executor.cpp:130 ] receiv subscrib event i1110 01:20:11.797111 10221 default_executor.cpp:134 ] subscrib executor 3a31be8bf679 i1110 01:20:11.797611 10221 default_executor.cpp:130 ] receiv launch_group event i1110 01:20:11.801981 29729 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.802435 29729 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.803306 29723 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54640 i1110 01:20:11.803452 29723 http.cpp:353 ] process call launch_nested_contain i1110 01:20:11.803827 29727 containerizer.cpp:1685 ] start nest contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d i1110 01:20:11.803865 29723 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54640 i1110 01:20:11.803978 29723 http.cpp:353 ] process call launch_nested_contain i1110 01:20:11.804236 29727 containerizer.cpp:1709 ] tri chown '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d ' user 'mesos' i1110 01:20:11.814858 29727 containerizer.cpp:1685 ] start nest contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 i1110 01:20:11.815129 29727 containerizer.cpp:1709 ] tri chown '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 ' user 'mesos' i1110 01:20:11.824666 29727 containerizer.cpp:1480 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' sleep 1000 '' } '' -- help= '' fals '' -- pipe_read= '' 63 '' -- pipe_write= '' 64 '' -- pre_exec_commands= '' [ ] '' -- runtime_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d '' -- unshare_namespace_mnt= '' fals '' -- user= '' meso '' -- working_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d '' ' i1110 01:20:11.826855 29727 launcher.cpp:127 ] fork child pid '10240 ' contain 'a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d' i1110 01:20:11.828918 29727 containerizer.cpp:1480 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' sleep 1000 '' } '' -- help= '' fals '' -- pipe_read= '' 65 '' -- pipe_write= '' 66 '' -- pre_exec_commands= '' [ ] '' -- runtime_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 '' -- unshare_namespace_mnt= '' fals '' -- user= '' meso '' -- working_directory= '' /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 '' ' i1110 01:20:11.831428 29727 launcher.cpp:127 ] fork child pid '10241 ' contain 'a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611' i1110 01:20:11.834421 29731 fetcher.cpp:345 ] start fetch uri contain : a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d , directori : /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d i1110 01:20:11.837882 29731 fetcher.cpp:345 ] start fetch uri contain : a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 , directori : /tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611 i1110 01:20:11.847651 10227 default_executor.cpp:470 ] success launch child contain [ a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d , a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 ] task [ c96eb523-0365-49b2-8b3b-78976ff28797 , 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] i1110 01:20:11.849225 29728 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1/executor' i1110 01:20:11.850085 10226 default_executor.cpp:546 ] wait child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d task 'c96eb523-0365-49b2-8b3b-78976ff28797' i1110 01:20:11.850145 29734 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1/executor' i1110 01:20:11.850405 10226 default_executor.cpp:546 ] wait child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' i1110 01:20:11.850746 29726 http.cpp:277 ] http post /slave ( 571 ) /api/v1/executor 172.17.0.3:54639 i1110 01:20:11.851114 29726 slave.cpp:3740 ] handl statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.851552 29726 http.cpp:277 ] http post /slave ( 571 ) /api/v1/executor 172.17.0.3:54639 i1110 01:20:11.851727 29726 slave.cpp:3740 ] handl statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.852295 29726 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.852826 29726 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.853938 29724 status_update_manager.cpp:323 ] receiv statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.854076 29724 status_update_manager.cpp:500 ] creat statusupd stream task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.854460 29726 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54641 i1110 01:20:11.854559 29726 http.cpp:353 ] process call wait_nested_contain i1110 01:20:11.854610 29724 status_update_manager.cpp:377 ] forward updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent i1110 01:20:11.855126 29724 status_update_manager.cpp:323 ] receiv statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.855190 29724 status_update_manager.cpp:500 ] creat statusupd stream task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.855200 29726 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54642 i1110 01:20:11.855409 29726 http.cpp:353 ] process call wait_nested_contain i1110 01:20:11.855608 29724 status_update_manager.cpp:377 ] forward updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent i1110 01:20:11.855803 29726 slave.cpp:4181 ] forward updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 master @ 172.17.0.3:52462 i1110 01:20:11.856199 29726 slave.cpp:4075 ] statu updat manag success handl statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.856346 29725 master.cpp:5760 ] statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:11.856439 29725 master.cpp:5822 ] forward statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.856598 29726 slave.cpp:4181 ] forward updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 master @ 172.17.0.3:52462 i1110 01:20:11.856828 29726 slave.cpp:4075 ] statu updat manag success handl statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.856998 29725 master.cpp:7715 ] updat state task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_run , statu updat state : task_run ) i1110 01:20:11.857322 29725 master.cpp:5760 ] statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:11.857386 29725 master.cpp:5822 ] forward statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.857787 29725 master.cpp:7715 ] updat state task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_run , statu updat state : task_run ) i1110 01:20:11.858124 10226 default_executor.cpp:130 ] receiv acknowledg event i1110 01:20:11.858530 29725 scheduler.cpp:675 ] enqueu event updat receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.859519 29732 scheduler.cpp:675 ] enqueu event updat receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.859676 10233 default_executor.cpp:130 ] receiv acknowledg event .. / .. /src/tests/default_executor_tests.cpp:338 : failur valu : runningupdate1- > statu ( ) .task_id ( ) actual : 08848440-4c0e-4ad6-a0a9-b5947c5d21ba expect : taskinfo1.task_id ( ) which : c96eb523-0365-49b2-8b3b-78976ff28797 .. / .. /src/tests/default_executor_tests.cpp:342 : failur valu : runningupdate2- > statu ( ) .task_id ( ) actual : c96eb523-0365-49b2-8b3b-78976ff28797 expect : taskinfo2.task_id ( ) which : 08848440-4c0e-4ad6-a0a9-b5947c5d21ba i1110 01:20:11.861587 29733 scheduler.cpp:235 ] send acknowledg call http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.861948 29733 scheduler.cpp:235 ] send acknowledg call http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.862280 29733 scheduler.cpp:235 ] send kill call http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:11.862632 29721 process.cpp:3570 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i1110 01:20:11.863528 29719 http.cpp:391 ] http post /master/api/v1/schedul 172.17.0.3:54634 i1110 01:20:11.863664 29719 master.cpp:4870 ] process acknowledg call 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 i1110 01:20:11.864003 29732 status_update_manager.cpp:395 ] receiv statu updat acknowledg ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 w1110 01:20:11.864294 29732 status_update_manager.cpp:769 ] unexpect statu updat acknowledg ( receiv 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 , expect d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 e1110 01:20:11.864575 29726 slave.cpp:3015 ] fail handl statu updat acknowledg ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 : duplic acknowledg i1110 01:20:11.864804 29723 process.cpp:3570 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i1110 01:20:11.865231 29723 process.cpp:3570 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i1110 01:20:11.866297 29722 http.cpp:391 ] http post /master/api/v1/schedul 172.17.0.3:54634 i1110 01:20:11.866420 29722 master.cpp:4870 ] process acknowledg call d91c7deb-4646-4b4e-ba1a-5650a256e8d2 task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 i1110 01:20:11.867036 29726 status_update_manager.cpp:395 ] receiv statu updat acknowledg ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.867076 29722 http.cpp:391 ] http post /master/api/v1/schedul 172.17.0.3:54634 i1110 01:20:11.867291 29722 master.cpp:4762 ] tell agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) kill task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) w1110 01:20:11.867297 29726 status_update_manager.cpp:769 ] unexpect statu updat acknowledg ( receiv d91c7deb-4646-4b4e-ba1a-5650a256e8d2 , expect 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.867449 29733 slave.cpp:2344 ] ask kill task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 e1110 01:20:11.867710 29733 slave.cpp:3015 ] fail handl statu updat acknowledg ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 : duplic acknowledg i1110 01:20:11.869391 10228 default_executor.cpp:130 ] receiv kill event i1110 01:20:11.869498 10228 default_executor.cpp:810 ] receiv kill task 'c96eb523-0365-49b2-8b3b-78976ff28797' i1110 01:20:11.869544 10228 default_executor.cpp:694 ] shut i1110 01:20:11.870112 10221 default_executor.cpp:782 ] kill child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d i1110 01:20:11.870338 10221 default_executor.cpp:782 ] kill child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 i1110 01:20:11.870965 29729 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.871399 29730 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1' i1110 01:20:11.871984 29723 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54643 i1110 01:20:11.872088 29723 http.cpp:353 ] process call kill_nested_contain i1110 01:20:11.872284 29726 containerizer.cpp:1973 ] destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d run state i1110 01:20:11.872340 29723 http.cpp:277 ] http post /slave ( 571 ) /api/v1 172.17.0.3:54643 i1110 01:20:11.872416 29723 http.cpp:353 ] process call kill_nested_contain i1110 01:20:11.872597 29726 launcher.cpp:143 ] ask destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d i1110 01:20:11.877090 29726 containerizer.cpp:1973 ] destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 run state i1110 01:20:11.877320 29726 launcher.cpp:143 ] ask destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 i1110 01:20:11.962539 29729 containerizer.cpp:2336 ] contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d exit i1110 01:20:11.963811 29733 provisioner.cpp:324 ] ignor destroy request unknown contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d i1110 01:20:11.963851 29729 containerizer.cpp:2336 ] contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 exit i1110 01:20:11.964437 29729 containerizer.cpp:2252 ] checkpoint termin state nest contain 's runtim directori '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d/termination' i1110 01:20:11.965940 29728 provisioner.cpp:324 ] ignor destroy request unknown contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 i1110 01:20:11.966202 29732 containerizer.cpp:2252 ] checkpoint termin state nest contain 's runtim directori '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_wvf7jj/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611/termination' i1110 01:20:11.970046 10231 default_executor.cpp:663 ] success wait child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d task 'c96eb523-0365-49b2-8b3b-78976ff28797 ' state task_kil i1110 01:20:11.970501 10231 default_executor.cpp:663 ] success wait child contain a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba ' state task_kil i1110 01:20:11.970559 10231 default_executor.cpp:768 ] termin 1sec i1110 01:20:11.971288 29723 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1/executor' i1110 01:20:11.972218 29728 http.cpp:277 ] http post /slave ( 571 ) /api/v1/executor 172.17.0.3:54639 i1110 01:20:11.972488 29728 slave.cpp:3740 ] handl statu updat task_kil ( uuid : 48f76bc6-3855-40fc-b22b-e26b1048fc89 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.974179 29719 process.cpp:3570 ] handl http event process 'slave ( 571 ) ' path : '/slave ( 571 ) /api/v1/executor' i1110 01:20:11.975229 29726 status_update_manager.cpp:323 ] receiv statu updat task_kil ( uuid : 48f76bc6-3855-40fc-b22b-e26b1048fc89 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.975278 29729 http.cpp:277 ] http post /slave ( 571 ) /api/v1/executor 172.17.0.3:54639 i1110 01:20:11.975517 29729 slave.cpp:3740 ] handl statu updat task_kil ( uuid : a3be1364-0830-4dd3-8be5-2bbbafde1029 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.976048 29729 slave.cpp:4075 ] statu updat manag success handl statu updat task_kil ( uuid : 48f76bc6-3855-40fc-b22b-e26b1048fc89 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.978132 29720 status_update_manager.cpp:323 ] receiv statu updat task_kil ( uuid : a3be1364-0830-4dd3-8be5-2bbbafde1029 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:11.978482 29725 slave.cpp:4075 ] statu updat manag success handl statu updat task_kil ( uuid : a3be1364-0830-4dd3-8be5-2bbbafde1029 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:12.494274 29725 hierarchical.cpp:1880 ] filter offer cpu ( * ) :1.7 ; mem ( * ) :928 ; disk ( * ) :928 ; port ( * ) : [ 31000-32000 ] agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:12.494357 29725 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:12.494402 29725 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:12.494491 29725 hierarchical.cpp:1286 ] perform alloc 1 agent 915780n i1110 01:20:13.071280 29734 containerizer.cpp:2336 ] contain a283035b-25d3-4b48-b59a-964e5a4dfa06 exit i1110 01:20:13.071339 29734 containerizer.cpp:1973 ] destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06 run state i1110 01:20:13.071746 29734 launcher.cpp:143 ] ask destroy contain a283035b-25d3-4b48-b59a-964e5a4dfa06 i1110 01:20:13.076637 29723 provisioner.cpp:324 ] ignor destroy request unknown contain a283035b-25d3-4b48-b59a-964e5a4dfa06 i1110 01:20:13.077929 29721 slave.cpp:4672 ] executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 exit statu 0 i1110 01:20:13.078433 29732 master.cpp:5884 ] executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) : exit statu 0 i1110 01:20:13.078538 29732 master.cpp:7840 ] remov executor 'default ' resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:13.079448 29730 hierarchical.cpp:1018 ] recov cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :0.2 ; mem ( * ) :64 ; disk ( * ) :64 ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:13.081049 29723 scheduler.cpp:675 ] enqueu event failur receiv http : //172.17.0.3:52462/master/api/v1/schedul gmock warn : uninterest mock function call - return directli . function call : failur ( 0x7fff1aa9a950 , @ 0x2ab91c02dd10 48-byte object < 90-62 27-ec b8-2a 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-ce 01-1c b9-2a 00-00 70-ce 02-1c b9-2a 00-00 00-00 00-00 b8-2a 00-00 > ) stack trace : i1110 01:20:13.496551 29730 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:13.496680 29730 hierarchical.cpp:1286 ] perform alloc 1 agent 1.498625m i1110 01:20:13.497339 29729 master.cpp:6574 ] send 1 offer framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:13.499797 29721 scheduler.cpp:675 ] enqueu event offer receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:14.497707 29732 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:14.497795 29732 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:14.497895 29732 hierarchical.cpp:1286 ] perform alloc 1 agent 410313n i1110 01:20:15.499423 29728 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:15.499526 29728 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:15.499658 29728 hierarchical.cpp:1286 ] perform alloc 1 agent 547651n i1110 01:20:16.500463 29729 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:16.500581 29729 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:16.500699 29729 hierarchical.cpp:1286 ] perform alloc 1 agent 505442n i1110 01:20:17.502176 29727 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:17.502262 29727 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:17.502367 29727 hierarchical.cpp:1286 ] perform alloc 1 agent 464526n i1110 01:20:18.503680 29723 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:18.503762 29723 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:18.503851 29723 hierarchical.cpp:1286 ] perform alloc 1 agent 425163n i1110 01:20:19.505476 29723 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:19.505586 29723 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:19.505705 29723 hierarchical.cpp:1286 ] perform alloc 1 agent 590762n i1110 01:20:20.507310 29724 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:20.507390 29724 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:20.507477 29724 hierarchical.cpp:1286 ] perform alloc 1 agent 411721n i1110 01:20:21.508368 29729 hierarchical.cpp:1694 ] no alloc perform i1110 01:20:21.508458 29729 hierarchical.cpp:1789 ] no invers offer send ! i1110 01:20:21.508564 29729 hierarchical.cpp:1286 ] perform alloc 1 agent 432440n w1110 01:20:21.855908 29728 status_update_manager.cpp:478 ] resend statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.856066 29728 status_update_manager.cpp:377 ] forward updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent i1110 01:20:21.856652 29734 slave.cpp:4181 ] forward updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 master @ 172.17.0.3:52462 w1110 01:20:21.857002 29727 status_update_manager.cpp:478 ] resend statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.857069 29727 status_update_manager.cpp:377 ] forward updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent i1110 01:20:21.857378 29733 master.cpp:5760 ] statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.857475 29733 master.cpp:5822 ] forward statu updat task_run ( uuid : 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 ) task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.857662 29722 slave.cpp:4181 ] forward updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 master @ 172.17.0.3:52462 i1110 01:20:21.858206 29733 master.cpp:7715 ] updat state task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_kil , statu updat state : task_run ) i1110 01:20:21.858988 29733 master.cpp:5760 ] statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.859259 29733 master.cpp:5822 ] forward statu updat task_run ( uuid : d91c7deb-4646-4b4e-ba1a-5650a256e8d2 ) task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.859647 29725 hierarchical.cpp:1018 ] recov cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1.9 ; mem ( * ) :992 ; disk ( * ) :992 ; port ( * ) : [ 31000-32000 ] ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.859845 29725 scheduler.cpp:675 ] enqueu event updat receiv http : //172.17.0.3:52462/master/api/v1/schedul i1110 01:20:21.859979 29733 master.cpp:7715 ] updat state task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_kil , statu updat state : task_run ) i1110 01:20:21.860970 29729 hierarchical.cpp:1018 ] recov cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1.8 ; mem ( * ) :960 ; disk ( * ) :960 ; port ( * ) : [ 31000-32000 ] ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.861687 29725 scheduler.cpp:675 ] enqueu event updat receiv http : //172.17.0.3:52462/master/api/v1/schedul .. / .. /src/tests/default_executor_tests.cpp:400 : failur valu : killedupdate1- > statu ( ) .state ( ) actual : task_run expect : task_kil i1110 01:20:21.864666 29721 master.cpp:1297 ] framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) disconnect i1110 01:20:21.864765 29721 master.cpp:2918 ] disconnect framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:21.864820 29721 master.cpp:2942 ] deactiv framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:21.865016 29732 hierarchical.cpp:386 ] deactiv framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 w1110 01:20:21.865586 29721 master.hpp:2264 ] master attempt send messag disconnect framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) w1110 01:20:21.865691 29721 master.hpp:2270 ] unabl send event framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) : connect close i1110 01:20:21.865777 29721 master.cpp:1310 ] give framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) 0n failov i1110 01:20:21.866277 29728 hierarchical.cpp:1018 ] recov cpu ( * ) :1.8 ; mem ( * ) :960 ; disk ( * ) :960 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : { } ) agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.867295 29733 master.cpp:6426 ] framework failov timeout , remov framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:21.867328 29733 master.cpp:7170 ] remov framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( default ) i1110 01:20:21.867539 29733 master.cpp:7715 ] updat state task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_kil , statu updat state : task_kil ) i1110 01:20:21.867559 29731 slave.cpp:2575 ] ask shut framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 master @ 172.17.0.3:52462 i1110 01:20:21.867617 29731 slave.cpp:2600 ] shut framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.867585 29733 master.cpp:7811 ] remov task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.867707 29731 slave.cpp:4776 ] clean executor 'default ' framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( via http ) i1110 01:20:21.867904 29733 master.cpp:7715 ] updat state task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ( latest state : task_kil , statu updat state : task_kil ) i1110 01:20:21.868042 29729 gc.cpp:55 ] schedul '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06 ' gc 6.99998999732444day futur i1110 01:20:21.867939 29733 master.cpp:7811 ] remov task c96eb523-0365-49b2-8b3b-78976ff28797 resourc cpu ( * ) :0.1 ; mem ( * ) :32 ; disk ( * ) :32 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.868232 29731 slave.cpp:4864 ] clean framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.868252 29729 gc.cpp:55 ] schedul '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default ' gc 6.99998999732444day futur i1110 01:20:21.868422 29725 status_update_manager.cpp:285 ] close statu updat stream framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.868484 29725 status_update_manager.cpp:531 ] clean statu updat stream task c96eb523-0365-49b2-8b3b-78976ff28797 framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.868777 29721 gc.cpp:55 ] schedul '/tmp/mesoscontainerizer_defaultexecutortest_killtask_0_8sxovd/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-s0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 ' gc 6.99998999732444day futur i1110 01:20:21.868926 29720 hierarchical.cpp:337 ] remov framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.869235 29725 status_update_manager.cpp:531 ] clean statu updat stream task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 i1110 01:20:21.870568 29730 slave.cpp:787 ] agent termin i1110 01:20:21.870795 29732 master.cpp:1258 ] agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) disconnect i1110 01:20:21.870825 29732 master.cpp:2977 ] disconnect agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.870930 29732 master.cpp:2996 ] deactiv agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 slave ( 571 ) @ 172.17.0.3:52462 ( 3a31be8bf679 ) i1110 01:20:21.871158 29726 hierarchical.cpp:584 ] agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 deactiv i1110 01:20:21.876986 29733 master.cpp:1097 ] master termin i1110 01:20:21.877754 29724 hierarchical.cpp:517 ] remov agent d28fbae1-c3dc-45fa-8384-32ab9395a975-s0 [ fail ] mesoscontainerizer/defaultexecutortest.killtask/0 , getparam ( ) = `` meso '' ( 10406 ms ) { noformat }",MESOS-6569,2.0
"parallel test run respect gtest_filt normal , use { { gtest_filt } } control test run { { make check } } . howev , n't current work meso configur { { -- enable-parallel-test-execut } } .",MESOS-6516,2.0
"use 'geteuid ( ) ' root privileg check . current , part code meso check root privileg use os : :user ( ) compar `` root '' , suffici , sinc compar real user . when peopl chang meso binari 'setuid root ' , process may right permiss execut . we check effect user id instead code .",MESOS-6504,3.0
posixrlimitsisolatortest.taskexceedinglimit fail os x thi test consist fail os x : { code } 31-7e9c-4248-acfd-21634150a657 @ 172.28.128.1:64864 agent 52cc4957-1a39-4d66-ace6-5622fac3b85e-s0 .. / .. /src/tests/containerizer/posix_rlimits_isolator_tests.cpp:120 : failur valu : statusfailed- > state ( ) actual : task_finish expect : task_fail { code },MESOS-6459,2.0
"role quota assign `` game '' system receiv excess resourc . the current implement quota alloc attempt satisfi resourc quota role , far exceed quota assign role . for exampl , role quota { { \ [ 30,20,10\ ] } } , consum : { { \ [ ∞ , ∞ , 10\ ] } } { { \ [ ∞ , 20 , ∞\ ] } } { { \ [ 30 , ∞ , ∞\ ] } } resourc quota vector satisfi stop alloc agent 's resourc role ! as first step prevent game , could consid quota satisfi resourc vector quota satisfi . thi approach work reason well resourc requir present everi agent ( cpu , mem , disk ) . howev , n't work well resourc option / present agent ( e.g . gpu ) ( a.k.a . non-ubiquit / scarc resourc ) . for would need determin agent resourc satisfi quota prior perform alloc .",MESOS-6432,5.0
the python linter n't rebuild virtual environ lint `` pip-requirements.txt '' chang we need detect `` pip-requirements.txt '' chang rebuild virtual environ .,MESOS-6430,2.0
"report new partition_awar task status http endpoint at minimum , { { /state-summari } } endpoint need updat .",MESOS-6388,1.0
"'mesos-container launch ' inherit agent environ variabl . if dynam librari agent depend store non standard locat , oper start agent use ld_library_path . when actual fork/exec 'mesos-container launch ' helper , need make sure inherit agent 's environ variabl . otherwis , might throw link error . thi make sens 's meso control process . howev , helper actual fork/exec user contain ( executor ) , need make sure strip agent environ variabl . the tricki case default executor command executor . these two control meso well , also want agent environ variabl . we need somehow distinguish custom executor case .",MESOS-6323,5.0
"agent fail kill empti parent contain i launch pod use marathon , led launch task group meso agent . the pod spec flaw , led marathon repeatedli re-launch multipl instanc task group . after went minut , i told marathon scale app 0 instanc , send { { task_kil } } one task task group . after , meso agent report { { task_kil } } statu updat 3 task pod , hit { { /contain } } endpoint agent reveal executor contain task group still run . here task group launch agent : { code } slave.cpp:1696 ] launch task group contain task [ test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask1 , test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask2 , test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clienttask ] framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 { code } executor contain start : { code } mesos-ag [ 2994 ] : i1006 20:23:27.407563 3094 containerizer.cpp:965 ] start contain bf38ff09-3da1-487a-8926-1f4cc88bce32 executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601 ' framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 { code } output show { { task_kil } } updat one task group : { code } mesos-ag [ 2994 ] : i1006 20:23:28.728224 3097 slave.cpp:2283 ] ask kill task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask1 framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 mesos-ag [ 2994 ] : w1006 20:23:28.728304 3097 slave.cpp:2364 ] transit state task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask1 framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 task_kil executor regist mesos-ag [ 2994 ] : i1006 20:23:28.728659 3097 slave.cpp:3609 ] handl statu updat task_kil ( uuid : 1cb8197a-7829-4a05-9cb1-14ba97519c42 ) task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask1 framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 @ 0.0.0.0:0 mesos-ag [ 2994 ] : i1006 20:23:28.728817 3097 slave.cpp:3609 ] handl statu updat task_kil ( uuid : e377e9fb-6466-4ce5-b32a-37d840b9f87c ) task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthtask2 framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 @ 0.0.0.0:0 mesos-ag [ 2994 ] : i1006 20:23:28.728912 3097 slave.cpp:3609 ] handl statu updat task_kil ( uuid : 24d44b25-ea52-43a1-afdb-6c04389879d2 ) task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clienttask framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 @ 0.0.0.0:0 { code } howev , grep log executor 's id , last line mention : { code } slave.cpp:3080 ] creat marker file http base executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601 ' framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 ( via http ) path '/var/lib/mesos/slave/meta/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-s0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32/http.marker' { code } seem executor never exit . if hit agent 's { { /contain } } endpoint , get output includ executor contain : { code } { `` container_id '' : `` bf38ff09-3da1-487a-8926-1f4cc88bce32 '' , `` executor_id '' : `` instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601 '' , `` executor_nam '' : `` '' , `` framework_id '' : `` 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 '' , `` sourc '' : `` '' , `` statist '' : { `` cpus_limit '' : 0.1 , `` cpus_nr_period '' : 17 , `` cpus_nr_throttl '' : 11 , `` cpus_system_time_sec '' : 0.02 , `` cpus_throttled_time_sec '' : 0.784142448 , `` cpus_user_time_sec '' : 0.09 , `` disk_limit_byt '' : 10485760 , `` disk_used_byt '' : 20480 , `` mem_anon_byt '' : 11337728 , `` mem_cache_byt '' : 0 , `` mem_critical_pressure_count '' : 0 , `` mem_file_byt '' : 0 , `` mem_limit_byt '' : 33554432 , `` mem_low_pressure_count '' : 0 , `` mem_mapped_file_byt '' : 0 , `` mem_medium_pressure_count '' : 0 , `` mem_rss_byt '' : 11337728 , `` mem_swap_byt '' : 0 , `` mem_total_byt '' : 12013568 , `` mem_unevictable_byt '' : 0 , `` timestamp '' : 1475792290.12373 } , `` statu '' : { `` executor_pid '' : 9068 , `` network_info '' : [ { `` ip_address '' : [ { `` ip_address '' : `` 9.0.1.34 '' , `` protocol '' : `` ipv4 '' } ] , `` label '' : { } , `` name '' : `` dco '' , `` port_map '' : [ { `` container_port '' : 8080 , `` host_port '' : 24758 , `` protocol '' : `` tcp '' } , { `` container_port '' : 8081 , `` host_port '' : 24759 , `` protocol '' : `` tcp '' } ] } ] } } , { code } look output { { ps } } agent , inde locat executor process : { code } $ ps aux | grep 9068 root 9068 0.0 0.1 96076 25380 ? ss 20:23 0:00 mesos-container launch -- command= { `` argument '' : [ `` mesos-default-executor '' ] , '' shell '' : fals , '' user '' : '' root '' , '' valu '' : '' \/opt\/mesosphere\/packages\/meso -- 3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-default-executor '' } -- help=fals -- pipe_read=49 -- pipe_write=50 -- pre_exec_commands= [ { `` argument '' : [ `` mesos-container '' , '' mount '' , '' -- help=fals '' , '' -- operation=make-rslav '' , '' -- path=\/ '' ] , '' shell '' : fals , '' valu '' : '' \/opt\/mesosphere\/packages\/meso -- 3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-container '' } , { `` shell '' : true , '' valu '' : '' ifconfig lo '' } ] -- runtime_directory=/var/run/mesos/containers/bf38ff09-3da1-487a-8926-1f4cc88bce32 -- unshare_namespace_mnt=fals -- user=root -- working_directory=/var/lib/mesos/slave/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-s0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32 core 20406 0.0 0.0 6764 1020 pts/1 s+ 23:05 0:00 grep -- colour=auto 9068 $ sudo cat /proc/9068/task/9068/children 9330 $ ps aux | grep 9330 root 9330 0.0 0.2 498040 32944 ? sl 20:23 0:00 mesos-default-executor root 19330 0.0 0.0 0 0 ? s 22:49 0:00 [ kworker/0:2 ] core 20573 0.0 0.0 6764 888 pts/1 s+ 23:07 0:00 grep -- colour=auto 9330 { code } look executor 's log , find stdout : { code } execut pre-exec command ' { `` argument '' : [ `` mesos-container '' , '' mount '' , '' -- help=fals '' , '' -- operation=make-rslav '' , '' -- path=\/ '' ] , '' shell '' : fals , '' valu '' : '' \/opt\/mesosphere\/packages\/meso -- 3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-container '' } ' execut pre-exec command ' { `` shell '' : true , '' valu '' : '' ifconfig lo '' } ' { code } stderr : { code } i1006 20:23:28.817401 9330 executor.cpp:189 ] version : 1.1.0 i1006 20:23:28.906349 9352 default_executor.cpp:123 ] receiv subscrib event i1006 20:23:28.908797 9352 default_executor.cpp:127 ] subscrib executor 10.0.0.133 { code } with short executor log , seem possibl agent receiv { { task_kil } } task sent executor , agent remov task data structur without termin parent contain .",MESOS-6322,3.0
"implement clang-tidi check catch incorrect flag hierarchi class need alway use { { virtual } } inherit deriv { { flagsbas } } . also , order compos deriv flag inherit virtual . some exampl : { code } struct a : virtual flagsbas { } ; // ok struct b : flagsbas { } ; // error struct c : a { } ; // error { code } we implement clang-tidi checker catch wrong inherit issu .",MESOS-6320,3.0
"agent recoveri fail nest contain launch after launch nest contain use docker imag , i restart agent ran task group saw follow agent log recoveri : { code } oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : i1001 01:45:10.813596 4640 status_update_manager.cpp:203 ] recov statu updat manag oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : i1001 01:45:10.813622 4640 status_update_manager.cpp:211 ] recov executor 'instance-testvolume.02c26bce-8778-11e6-9ff3-7a3cd7c1568 ' framework 118ca38d-daee-4b2d-b584-b5581738a3dd-0000 oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : i1001 01:45:10.814249 4639 docker.cpp:745 ] recov docker contain oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : i1001 01:45:10.815294 4642 containerizer.cpp:581 ] recov container oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : fail perform recoveri : collect fail : unabl list rootfs belong contain a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53 : unabl list contain directori : fail opendir '/var/lib/mesos/slave/provisioner/containers/a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53/backend ' : no file directori oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : to remedi follow : oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : step 1 : rm -f /var/lib/mesos/slave/meta/slaves/latest oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : thi ensur agent n't recov old live executor . oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.intern mesos-ag [ 4629 ] : step 2 : restart agent . { code } agent continu restart fashion . attach marathon app definit i use launch task group .",MESOS-6302,3.0
"recurs destroy mesoscontainer problemat . when recurs destroy , return collect futur nest contain destroy . intead , fail correspond termin return termin nest contain destroy fail . in addit , remov 'contain ' struct intern map destroy nest contain fail . thi ensur top level contain proceed destroy nest contain fail destroy .",MESOS-6301,3.0
a destroy nest contain reflect parent contain 's children map . we updat parent contain 's children map 's nest contain termin .,MESOS-6300,2.0
"healthchecktest.healthytaskviahttpwithouttyp fail distro . i see consist failur test intern ci * * distro , specif cento 6 , ubuntu 14 , 15 , 16 . the sourc health check failur alway : { { curl } } connect target : { noformat } receiv task health updat , healthi : fals w0929 17:22:05.270992 2730 health_checker.cpp:204 ] health check fail 1 time consecut : http health check fail : curl return exit statu 7 : curl : ( 7 ) could n't connect host i0929 17:22:05.273634 26850 slave.cpp:3609 ] handl statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 executor ( 1 ) @ 172.30.2.20:58660 i0929 17:22:05.274178 26844 status_update_manager.cpp:323 ] receiv statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 i0929 17:22:05.274226 26844 status_update_manager.cpp:377 ] forward updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 agent i0929 17:22:05.274314 26845 slave.cpp:4026 ] forward updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 master @ 172.30.2.20:38955 i0929 17:22:05.274415 26845 slave.cpp:3920 ] statu updat manag success handl statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 i0929 17:22:05.274436 26845 slave.cpp:3936 ] send acknowledg statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 executor ( 1 ) @ 172.30.2.20:58660 i0929 17:22:05.274534 26849 master.cpp:5661 ] statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 agent 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-s0 slave ( 77 ) @ 172.30.2.20:38955 ( ip-172-30-2-20.mesosphere.io ) .. / .. /src/tests/health_check_tests.cpp:1398 : failur i0929 17:22:05.274567 26849 master.cpp:5723 ] forward statu updat task_run ( uuid : f5408ac9-f6ba-447f-b3d7-9dce44384ff ) task aa0792d3-8d85-4c32-bd04-56a9b552ebda health state unhealthi framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 valu : statushealth.get ( ) .healthi ( ) actual : fals expect : true i0929 17:22:05.274636 26849 master.cpp:7560 ] updat state task aa0792d3-8d85-4c32-bd04-56a9b552ebda framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 ( latest state : task_run , statu updat state : task_run ) i0929 17:22:05.274829 26844 sched.cpp:1025 ] schedul : :statusupd took 43297n receiv shutdown event { noformat }",MESOS-6293,3.0
"support nest contain logger meso container . current , two issu meso container use logger nest contaienr : 1 . an empti executorinfo pass logger launch nest contain , would potenti break logger modul modul tri access requir proto field ( e.g. , executorid ) . 2 . the logger reocver nest contain yet mesoscontainer : :recov .",MESOS-6290,2.0
"the default executor maintain launcher_dir . both command docker executor requir { { launcher_dir } } provid flag . thi directori contain meso binari , e.g . tcp checker necessari tcp health check . the default executor obtain somehow ( flag , env var ) maintain directori health checker use .",MESOS-6288,3.0
"meso container figur correct sandbox directori nest launch . current meso container take sandbox directori agent . ideal , nest sandbox dir figur container . and need pass agent . we remov ` directori ` paramet nest launch interfac .",MESOS-6263,3.0
default executor kill task task group task exit non-zero exit statu . the default restart polici task group kill activ contain task termin non-zero exit statu code . the default executor need honor default polici .,MESOS-6262,3.0
"compos container need properli handl nest contain launch right agent start -- containerizers= '' docker , meso '' , nest contain launch fail compos container n't implement nest ` launch ` method . thi result use default ` launch ` method defin base class , return error .",MESOS-6260,3.0
cni isol ` check ` ` resolv.conf ` ` rootcontainerdir `,MESOS-6259,1.0
"driver base schedul perform explicit acknowledg acknowledg updat http base executor . it seem agent code set { { statusupd } } - > { { slave_id } } set { { taskstatu } } - > { { slave_id } } 's alreadi set . on driver , receiv statu updat explicit ack enabl , would pass { { taskstatu } } schedul . but , schedul way ack updat due { { slave_id } } present . note , implicit acknowledg still work sinc use { { slave_id } } { { statusupd } } . henc , never notic test use implicit acknowledg driver .",MESOS-6245,3.0
"page_s declar ppc64le when compil meso ppc64le , get error { code } .. / .. /src/slave/containerizer/mesos/isolators/gpu/isolator.cpp -fpic -dpic -o slave/containerizer/mesos/isolators/gpu/.libs/libmesos_no_3rdparty_la-isolator.o .. / .. /src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp : in member function 'virtual process : :futur < noth > meso : :intern : :slave : :memorysubsystem : :updat ( const meso : :containerid & , const meso : :resourc & ) ' : .. / .. /src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp:230:55 : error : 'page_s ' declar scope byte initiallimit ( static_cast < uint64_t > ( long_max / page_s * page_s ) ) ; ^ { code }",MESOS-6217,1.0
"health check grace period cover failur happen first success . current , health check librari [ ignor * * failures|http : //github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/health-check/health_checker.cpp # l192-l197 ] task ’ start ( technic health check librari initi ) [ grace period ends|http : //github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/include/mesos/v1/mesos.proto # l403 ] . thi behaviour mislead . onc health check succe first time , grace period rule failur appli . for exampl , grace period set 10 minut , task becom healthi 1 minut fail 2 minut , failur treat normal failur consequ .",MESOS-6170,5.0
"remov stout 's set type stout provid { { set } } type wrap { { std : :set } } . as addit provid new constructor , { code } set ( const t & t1 ) ; set ( const t & t1 , const t & t2 ) ; set ( const t & t1 , const t & t2 , const t & t3 ) ; set ( const t & t1 , const t & t2 , const t & t3 , const t & t4 ) ; { code } simplifi creation { { set } } ( four ) known element . c++11 brought { { std : :initializer_list } } use creat { { std : :set } } arbitrari number element , appear possibl retir { { set } } .",MESOS-6159,1.0
"clean queu task task group kill launch . we properli clean queu task group one task kill i.e . http : //github.com/apache/mesos/blob/aaa353acc515c0435a859113c9ee236247b51169/src/slave/slave.cpp # l6554 , clean queu task n't go around clean queu task group . also , would great add test similar exercis { { pend } } task workflow i.e . { { slavetest.killtaskgroupbetweenruntaskpart } } { { queuedtask } } .",MESOS-6154,2.0
"resourc leak slave.cpp . cover detect follow resourc leak : { code } 1 . condit this- > queuedtasks.contain ( taskid ) , take true branch . 6547 ( queuedtasks.contain ( taskid ) ) { 2 . condit termin , take true branch . 6548 ( termin ) { 3. alloc_fn : storag return alloc function oper new . [ note : the sourc code implement function overridden builtin model . ] 4. var_assign : assign : task = storag return new meso : :task ( meso : :intern : :protobuf : :createtask ( this- > queuedtasks.at ( taskid ) , meso : :taskstat const ( status- > state ( ) ) , this- > frameworkid ) ) . 6549 task * task = new task ( protobuf : :createtask ( 6550 queuedtasks.at ( taskid ) , 6551 status.st ( ) , 6552 frameworkid ) ) ; 6553 6554 queuedtasks.eras ( taskid ) ; 6555 6556 // thi might queu task belong task group . 6557 // if , need updat task belong task group . 6558 option < taskgroupinfo > taskgroup = getqueuedtaskgroup ( taskid ) ; 6559 5 . condit taskgroup.issom ( ) , take true branch . 6560 ( taskgroup.issom ( ) ) { 6 . no element left taskgroup- > task ( ) , leav loop . 6561 foreach ( const taskinfo & task_ , taskgroup- > task ( ) ) { 6562 task * task = new task ( 6563 protobuf : :createtask ( task_ , status.st ( ) , frameworkid ) ) ; 6564 6565 tasks.push_back ( task ) ; 6566 } 7 . fall end statement . 6567 } els { 6568 tasks.push_back ( task ) ; 6569 } cid 1372871 ( # 1 1 ) : resourc leak ( resource_leak ) 8. leaked_storag : variabl task go scope leak storag point . 6570 } els { 6571 return error ( `` can send non-termin updat queu task '' ) ; { code } http : //scan5.coverity.com/reports.htm # v39597/p10429/fileinstanceid=98881751 & defectinstanceid=28450463",MESOS-6153,1.0
"resourc leak libevent_ssl_socket.cpp . cover detect follow resourc leak . imo { code } ( fd == -1 ) { code } { code } ( owned_fd == -1 ) { code } . { code } // duplic file descriptor libev take ownership 754 // control lifecycl separ . 755 // 756 // todo ( josephw ) : we avoid duplic file descriptor 757 // futur version libev . in libev version 2.1.2 later , 758 // may use ` evbuffer_file_segment_new ` ` evbuffer_add_file_seg ` 759 // instead ` evbuffer_add_fil ` . 3. open_fn : return handl open dup . 4. var_assign : assign : owned_fd = handl return dup ( fd ) . 760 int owned_fd = dup ( fd ) ; cid 1372873 : argument neg ( reverse_neg ) [ select issu ] 5 . condit fd == -1 , take true branch . 761 ( fd == -1 ) { cid 1372872 ( # 1 1 ) : resourc leak ( resource_leak ) 6. leaked_handl : handl variabl owned_fd go scope leak handl . 762 return failur ( errnoerror ( `` fail duplic file descriptor '' ) ) ; 763 } { code } http : //scan5.coverity.com/reports.htm # v39597/p10429/fileinstanceid=98881747 & defectinstanceid=28450468",MESOS-6152,2.0
"framework may reserv arbitrari role . the master valid resourc reserv request role framework regist . as result , framework may reserv resourc arbitrari role . i 've modifi role [ { { reservethenunreserv } } test|http : //github.com/apache/mesos/blob/bca600cf5602ed8227d91af9f73d689da14ad786/src/tests/reservation_tests.cpp # l117 ] `` yoyo '' observ follow test 's log : { noformat } i0908 18:35:43.379122 2138112 master.cpp:3362 ] process accept call offer : [ dfaf67e6-7c1c-4988-b427-c49842cb7bb7-o0 ] agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-s0 slave ( 1 ) @ 10.200.181.237:60116 ( alexr.railnet.train ) framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 ( default ) scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da @ 10.200.181.237:60116 i0908 18:35:43.379170 2138112 master.cpp:3022 ] author princip 'test-princip ' reserv resourc 'cpu ( yoyo , test-princip ) :1 ; mem ( yoyo , test-princip ) :512' i0908 18:35:43.379678 2138112 master.cpp:3642 ] appli reserv oper resourc cpu ( yoyo , test-princip ) :1 ; mem ( yoyo , test-princip ) :512 framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 ( default ) scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da @ 10.200.181.237:60116 agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-s0 slave ( 1 ) @ 10.200.181.237:60116 ( alexr.railnet.train ) i0908 18:35:43.379767 2138112 master.cpp:7341 ] send checkpoint resourc cpu ( yoyo , test-princip ) :1 ; mem ( yoyo , test-princip ) :512 agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-s0 slave ( 1 ) @ 10.200.181.237:60116 ( alexr.railnet.train ) i0908 18:35:43.380273 3211264 slave.cpp:2497 ] updat checkpoint resourc cpu ( yoyo , test-princip ) :1 ; mem ( yoyo , test-princip ) :512 i0908 18:35:43.380574 2674688 hierarchical.cpp:760 ] updat alloc framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-s0 cpu ( * ) :1 ; mem ( * ) :512 ; disk ( * ) :470841 ; port ( * ) : [ 31000-32000 ] port ( * ) : [ 31000-32000 ] ; cpu ( yoyo , test-princip ) :1 ; disk ( * ) :470841 ; mem ( yoyo , test-princip ) :512 reserv oper { noformat }",MESOS-6142,3.0
"tcp health check portabl . mesos-3567 introduc depend `` bash '' tcp health check , undesir . we implement portabl solut tcp health check .",MESOS-6119,3.0
consid support tcp half-open check . a tcp half-open check complet tcp handshak henc test task notifi someon connect . thi usual perform complet tcp connect .,MESOS-6116,8.0
"deprec use health check without set type when send task launch use 1.0.x proto legaci ( non-http ) api , task healthcheck defin reject ( task_error ) 'type ' field set . thi field mark option proto avail 1.1.0 , requir order keep meso v1 api compat promis . for backward compat temporarili allow use case command health check set without type .",MESOS-6110,3.0
"potenti fd doubl close libev 's implement ` sendfil ` . repro copi : http : //reviews.apache.org/r/51509/ it possibl make master check fail repeatedli hit web ui reload static asset : 1 ) past lot text ( 16kb ) text ` src/webui/master/static/home.html ` . the text , reliabl repro . 2 ) start master ssl enabl : { code } libprocess_ssl_enabled=tru libprocess_ssl_key_file=key.pem libprocess_ssl_cert_file=cert.pem bin/mesos-master.sh -- work_dir=/tmp/mast { code } 3 ) run two instanc python script repeatedli : { code } import socket import ssl = ssl.wrap_socket ( socket.socket ( ) ) s.connect ( ( `` localhost '' , 5050 ) ) s.sendal ( `` '' '' get /static/home.html http/1.1 user-ag : foobar host : localhost:5050 accept : * / * connect : keep-al '' '' '' ) # the http part respons print s.recv ( 1000 ) { code } i.e . { code } python test.pi ; : ; done & python test.pi ; : ; done { code }",MESOS-6104,3.0
"unabl launch contain cni network coreo coreo ` /etc/host ` . current , ` network/cni ` isol , n't see ` /etc/host ` host filesystem n't bind mount contain ` host ` file target ` command executor ` . on distro coreo fail contain launch sinc ` libprocess ` initi ` command executor ` fail caus ca n't resolv ` hostnam ` . we creat ` /etc/host ` ` /etc/hostnam ` file absent host filesystem sinc creat file affect name resolut host network namespac , allow ` /etc/host ` file bind mount correctli allow name resolut contain network namespac well .",MESOS-6052,1.0
"auf backend support imag numer layer . thi issu expos unit test ` root_curl_internet_dockerdefaultentryptregistrypul ` manual specifi ` bind ` backend . most like mount auf specif option limit string length . { noformat } [ 20:13:07 ] : [ step 10/10 ] [ run ] dockerruntimeisolatortest.root_curl_internet_dockerdefaultentryptregistrypul [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.615844 23416 cluster.cpp:155 ] creat default 'local ' author [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.624106 23416 leveldb.cpp:174 ] open db 8.148813m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627252 23416 leveldb.cpp:181 ] compact db 3.126629m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627275 23416 leveldb.cpp:196 ] creat db iter 4410n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627282 23416 leveldb.cpp:202 ] seek begin db 763n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627287 23416 leveldb.cpp:271 ] iter 0 key db 491n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627301 23416 replica.cpp:776 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627563 23434 recover.cpp:451 ] start replica recoveri [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.627800 23437 recover.cpp:477 ] replica empti statu [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628113 23431 replica.cpp:673 ] replica empti statu receiv broadcast recov request __req_res__ ( 5852 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628243 23430 recover.cpp:197 ] receiv recov respons replica empti statu [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628365 23437 recover.cpp:568 ] updat replica statu start [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628744 23432 master.cpp:375 ] master dd755a55-0dd1-4d2d-9a49-812a666015cb ( ip-172-30-2-138.mesosphere.io ) start 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628758 23432 master.cpp:377 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticate_http_readonly= '' true '' -- authenticate_http_readwrite= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/ozhdiq/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/ozhdiq/mast '' -- zk_session_timeout= '' 10sec '' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628893 23432 master.cpp:427 ] master allow authent framework regist [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628900 23432 master.cpp:441 ] master allow authent agent regist [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628902 23432 master.cpp:454 ] master allow authent http framework regist [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628906 23432 credentials.hpp:37 ] load credenti authent '/tmp/ozhdiq/credentials' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.628999 23432 master.cpp:499 ] use default 'crammd5 ' authent [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629041 23432 http.cpp:883 ] use default 'basic ' http authent realm 'mesos-master-readonly' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629114 23432 http.cpp:883 ] use default 'basic ' http authent realm 'mesos-master-readwrite' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629166 23432 http.cpp:883 ] use default 'basic ' http authent realm 'mesos-master-scheduler' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629231 23432 master.cpp:579 ] author enabl [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629290 23434 whitelist_watcher.cpp:77 ] no whitelist given [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629302 23430 hierarchical.cpp:151 ] initi hierarch alloc process [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629921 23433 master.cpp:1851 ] elect lead master ! [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629933 23433 master.cpp:1547 ] recov registrar [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.629992 23436 registrar.cpp:332 ] recov registrar [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.630861 23435 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.358536m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.630877 23435 replica.cpp:320 ] persist replica statu start [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.630924 23435 recover.cpp:477 ] replica start statu [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.631178 23435 replica.cpp:673 ] replica start statu receiv broadcast recov request __req_res__ ( 5853 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.631285 23435 recover.cpp:197 ] receiv recov respons replica start statu [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.631433 23436 recover.cpp:568 ] updat replica statu vote [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.633391 23433 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.912156m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.633409 23433 replica.cpp:320 ] persist replica statu vote [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.633438 23433 recover.cpp:582 ] success join paxo group [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.633479 23433 recover.cpp:466 ] recov process termin [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.633635 23435 log.cpp:553 ] attempt start writer [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.634021 23432 replica.cpp:493 ] replica receiv implicit promis request __req_res__ ( 5854 ) @ 172.30.2.138:44256 propos 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.636034 23432 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.995908m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.636049 23432 replica.cpp:342 ] persist promis 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.636239 23432 coordinator.cpp:238 ] coordin attempt fill miss posit [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.636672 23432 replica.cpp:388 ] replica receiv explicit promis request __req_res__ ( 5855 ) @ 172.30.2.138:44256 posit 0 propos 2 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.637307 23432 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 614745n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.637318 23432 replica.cpp:708 ] persist action nop posit 0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.637668 23432 replica.cpp:537 ] replica receiv write request posit 0 __req_res__ ( 5856 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.637692 23432 leveldb.cpp:436 ] read posit leveldb took 10680n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.638314 23432 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 610038n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.638325 23432 replica.cpp:708 ] persist action nop posit 0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.638569 23436 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.640446 23436 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.856131m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.640461 23436 replica.cpp:708 ] persist action nop posit 0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.640645 23437 log.cpp:569 ] writer start end posit 0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.640940 23430 leveldb.cpp:436 ] read posit leveldb took 11341n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.641152 23430 registrar.cpp:365 ] success fetch registri ( 0b ) 11.14496m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.641185 23430 registrar.cpp:464 ] appli 1 oper 5010n ; attempt updat registri [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.641381 23434 log.cpp:577 ] attempt append 209 byte log [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.641425 23430 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.641706 23434 replica.cpp:537 ] replica receiv write request posit 1 __req_res__ ( 5857 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.642320 23434 leveldb.cpp:341 ] persist action ( 228 byte ) leveldb took 596016n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.642333 23434 replica.cpp:708 ] persist action append posit 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.642608 23435 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644492 23435 leveldb.cpp:341 ] persist action ( 230 byte ) leveldb took 1.868216m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644507 23435 replica.cpp:708 ] persist action append posit 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644716 23432 registrar.cpp:509 ] success updat registri 3.512064m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644759 23432 registrar.cpp:395 ] success recov registrar [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644811 23431 log.cpp:596 ] attempt truncat log 1 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644879 23433 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644949 23430 master.cpp:1655 ] recov 0 agent registri ( 170b ) ; allow 10min agent re-regist [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.644959 23437 hierarchical.cpp:178 ] skip recoveri hierarch alloc : noth recov [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.645247 23431 replica.cpp:537 ] replica receiv write request posit 2 __req_res__ ( 5858 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.645884 23431 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 618643n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.645896 23431 replica.cpp:708 ] persist action truncat posit 2 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.646080 23437 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.648093 23437 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.995217m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.648118 23437 leveldb.cpp:399 ] delet ~1 key leveldb took 10026n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.648125 23437 replica.cpp:708 ] persist action truncat posit 2 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.649564 23416 containerizer.cpp:200 ] use isol : docker/runtim , filesystem/linux , network/cni [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.652878 23416 linux_launcher.cpp:101 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 20:13:07 ] w : [ step 10/10 ] e0805 20:13:07.656265 23416 shell.hpp:106 ] command 'hadoop version 2 > & 1 ' fail ; output : [ 20:13:07 ] w : [ step 10/10 ] sh : 1 : hadoop : found [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.656286 23416 fetcher.cpp:62 ] skip uri fetcher plugin 'hadoop ' could creat : fail creat hdf client : fail execut 'hadoop version 2 > & 1 ' ; command either found exit non-zero exit statu : 127 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.656338 23416 registry_puller.cpp:111 ] creat registri puller docker registri 'http : //registry-1.docker.io' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.657330 23416 linux.cpp:148 ] bind mount '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn ' make share mount [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663147 23416 cluster.cpp:434 ] creat default 'local ' author [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663566 23436 slave.cpp:198 ] meso agent start ( 506 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663583 23436 slave.cpp:199 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http_readonly= '' true '' -- authenticate_http_readwrite= '' true '' -- authenticatee= '' crammd5 '' -- authentication_backoff_factor= '' 1sec '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/ozhdiq/stor '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/http_credenti '' -- image_providers= '' docker '' -- initialize_driver_logging= '' true '' -- isolation= '' docker/runtim , filesystem/linux '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn '' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663796 23436 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/credential' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663868 23436 slave.cpp:336 ] agent use credenti : test-princip [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663882 23436 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/http_credentials' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.663969 23436 http.cpp:883 ] use default 'basic ' http authent realm 'mesos-agent-readonly' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664010 23436 http.cpp:883 ] use default 'basic ' http authent realm 'mesos-agent-readwrite' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664225 23416 sched.cpp:226 ] version : 1.1.0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664423 23435 sched.cpp:330 ] new master detect master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664451 23435 sched.cpp:396 ] authent master master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664428 23436 slave.cpp:519 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664458 23435 sched.cpp:403 ] use default cram-md5 authenticate [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664463 23436 slave.cpp:527 ] agent attribut : [ ] [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664470 23436 slave.cpp:532 ] agent hostnam : ip-172-30-2-138.mesosphere.io [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664588 23437 authenticatee.cpp:121 ] creat new client sasl connect [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664810 23437 master.cpp:5900 ] authent scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664873 23437 authenticator.cpp:414 ] start authent session crammd5-authenticate ( 1028 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.664939 23432 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/meta' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665006 23431 authenticator.cpp:98 ] creat new server sasl connect [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665024 23435 status_update_manager.cpp:203 ] recov statu updat manag [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665174 23434 containerizer.cpp:527 ] recov container [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665201 23431 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665221 23431 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665266 23431 authenticator.cpp:204 ] receiv sasl authent start [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665303 23431 authenticator.cpp:326 ] authent requir step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665347 23431 authenticatee.cpp:259 ] receiv sasl authent step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665436 23431 authenticator.cpp:232 ] receiv sasl authent step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665457 23431 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-138.mesosphere.io ' server fqdn : 'ip-172-30-2-138.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665465 23431 auxprop.cpp:181 ] look auxiliari properti ' * userpassword' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665482 23431 auxprop.cpp:181 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665494 23431 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-138.mesosphere.io ' server fqdn : 'ip-172-30-2-138.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665503 23431 auxprop.cpp:131 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665510 23431 auxprop.cpp:131 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665524 23431 authenticator.cpp:318 ] authent success [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665575 23436 authenticatee.cpp:299 ] authent success [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665596 23435 authenticator.cpp:432 ] authent session cleanup crammd5-authenticate ( 1028 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665624 23431 master.cpp:5930 ] success authent princip 'test-princip ' scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665705 23436 sched.cpp:502 ] success authent master master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665715 23436 sched.cpp:820 ] send subscrib call master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665751 23436 sched.cpp:853 ] will retri registr 188.601026m necessari [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665796 23437 master.cpp:2425 ] receiv subscrib call framework 'default ' scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665817 23437 master.cpp:1887 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.665998 23430 master.cpp:2501 ] subscrib framework default checkpoint disabl capabl [ ] [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666132 23432 hierarchical.cpp:271 ] ad framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666148 23434 sched.cpp:743 ] framework regist dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666154 23432 hierarchical.cpp:1548 ] no alloc perform [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666173 23432 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666177 23434 sched.cpp:757 ] schedul : :regist took 11084n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666189 23432 hierarchical.cpp:1192 ] perform alloc 0 agent 43102n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666486 23431 metadata_manager.cpp:205 ] no imag load disk . docker provision imag storag path '/tmp/ozhdiq/store/storedimag ' exist [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666558 23436 provisioner.cpp:255 ] provision recoveri complet [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666677 23435 slave.cpp:4872 ] finish recoveri [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666831 23435 slave.cpp:5044 ] queri resourc estim oversubscrib resourc [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666919 23435 slave.cpp:895 ] new master detect master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666929 23435 slave.cpp:954 ] authent master master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666931 23436 status_update_manager.cpp:177 ] paus send statu updat [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666944 23435 slave.cpp:965 ] use default cram-md5 authenticate [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.666982 23435 slave.cpp:927 ] detect new master [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667006 23431 authenticatee.cpp:121 ] creat new client sasl connect [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667014 23435 slave.cpp:5058 ] receiv oversubscrib resourc resourc estim [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667162 23431 master.cpp:5900 ] authent slave ( 506 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667225 23434 authenticator.cpp:414 ] start authent session crammd5-authenticate ( 1029 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667275 23434 authenticator.cpp:98 ] creat new server sasl connect [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667418 23434 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667436 23434 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667492 23436 authenticator.cpp:204 ] receiv sasl authent start [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667515 23436 authenticator.cpp:326 ] authent requir step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667546 23436 authenticatee.cpp:259 ] receiv sasl authent step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667592 23436 authenticator.cpp:232 ] receiv sasl authent step [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667603 23436 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-138.mesosphere.io ' server fqdn : 'ip-172-30-2-138.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667610 23436 auxprop.cpp:181 ] look auxiliari properti ' * userpassword' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667619 23436 auxprop.cpp:181 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667630 23436 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-138.mesosphere.io ' server fqdn : 'ip-172-30-2-138.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667639 23436 auxprop.cpp:131 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667642 23436 auxprop.cpp:131 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667652 23436 authenticator.cpp:318 ] authent success [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667688 23436 authenticatee.cpp:299 ] authent success [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667713 23432 authenticator.cpp:432 ] authent session cleanup crammd5-authenticate ( 1029 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667733 23434 master.cpp:5930 ] success authent princip 'test-princip ' slave ( 506 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667783 23437 slave.cpp:1049 ] success authent master master @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667836 23437 slave.cpp:1455 ] will retri registr 4.197236m necessari [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.667901 23436 master.cpp:4554 ] regist agent slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) id dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.668021 23430 registrar.cpp:464 ] appli 1 oper 13306n ; attempt updat registri [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.668269 23433 log.cpp:577 ] attempt append 395 byte log [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.668329 23434 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.668622 23433 replica.cpp:537 ] replica receiv write request posit 3 __req_res__ ( 5859 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.669297 23433 leveldb.cpp:341 ] persist action ( 414 byte ) leveldb took 658552n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.669309 23433 replica.cpp:708 ] persist action append posit 3 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.669589 23432 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.672566 23432 leveldb.cpp:341 ] persist action ( 416 byte ) leveldb took 2.962622m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.672580 23432 replica.cpp:708 ] persist action append posit 3 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.672866 23435 registrar.cpp:509 ] success updat registri 4.822784m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.672936 23434 log.cpp:596 ] attempt truncat log 3 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673001 23437 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673110 23436 master.cpp:4623 ] regist agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673152 23432 hierarchical.cpp:478 ] ad agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 ( ip-172-30-2-138.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673174 23430 slave.cpp:3739 ] receiv ping slave-observ ( 465 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673254 23430 slave.cpp:1095 ] regist master master @ 172.30.2.138:44256 ; given agent id dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673266 23430 fetcher.cpp:86 ] clear fetcher cach [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673288 23433 replica.cpp:537 ] replica receiv write request posit 4 __req_res__ ( 5860 ) @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673317 23432 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673333 23432 hierarchical.cpp:1215 ] perform alloc agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 160981n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673358 23432 status_update_manager.cpp:184 ] resum send statu updat [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673435 23437 master.cpp:5729 ] send 1 offer framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673467 23430 slave.cpp:1118 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/meta/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/slave.info' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673566 23437 sched.cpp:917 ] schedul : :resourceoff took 40919n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673607 23430 slave.cpp:1155 ] forward total oversubscrib resourc [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673710 23437 master.cpp:5006 ] receiv updat agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) total oversubscrib resourc [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673781 23437 hierarchical.cpp:542 ] agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 ( ip-172-30-2-138.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673823 23437 hierarchical.cpp:1548 ] no alloc perform [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673830 23437 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.673838 23437 hierarchical.cpp:1215 ] perform alloc agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 31940n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674163 23435 master.cpp:3346 ] process accept call offer : [ dd755a55-0dd1-4d2d-9a49-812a666015cb-o0 ] agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674186 23435 master.cpp:2981 ] author framework princip 'test-princip ' launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674538 23437 master.cpp:7451 ] ad task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 resourc cpu ( * ) :1 ; mem ( * ) :128 agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674564 23437 master.cpp:3835 ] launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 resourc cpu ( * ) :1 ; mem ( * ) :128 agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674665 23430 slave.cpp:1495 ] got assign task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674713 23436 hierarchical.cpp:924 ] recov cpu ( * ) :1 ; mem ( * ) :896 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :128 ) agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674736 23436 hierarchical.cpp:961 ] framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 filter agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 5sec [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.674866 23430 slave.cpp:1614 ] launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.675107 23430 paths.cpp:536 ] tri chown '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451 ' user 'root' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.678246 23433 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 4.916164m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.678267 23433 replica.cpp:708 ] persist action truncat posit 4 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.678629 23436 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679050 23430 slave.cpp:5764 ] launch executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679200 23430 slave.cpp:1840 ] queu task 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679219 23437 containerizer.cpp:786 ] start contain 'f2c1fd6d-4d11-45cd-a916-e4d73d226451 ' executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679234 23430 slave.cpp:848 ] success attach file '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679435 23430 metadata_manager.cpp:167 ] look imag 'mesosphere/inky' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.679572 23430 registry_puller.cpp:236 ] pull imag 'mesosphere/inki ' 'docker-manifest : //registry-1.docker.io:443mesosphere/inki ? latest # http ' '/tmp/ozhdiq/store/staging/hbsybx' [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.680943 23436 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.14361m [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.681073 23436 leveldb.cpp:399 ] delet ~2 key leveldb took 60273n [ 20:13:07 ] w : [ step 10/10 ] i0805 20:13:07.681112 23436 replica.cpp:708 ] persist action truncat posit 4 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104004 23431 registry_puller.cpp:259 ] the manifest imag 'mesosphere/inki ' ' { [ 20:13:08 ] w : [ step 10/10 ] `` name '' : `` mesosphere/inki '' , [ 20:13:08 ] w : [ step 10/10 ] `` tag '' : `` latest '' , [ 20:13:08 ] w : [ step 10/10 ] `` architectur '' : `` amd64 '' , [ 20:13:08 ] w : [ step 10/10 ] `` fslayer '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] , [ 20:13:08 ] w : [ step 10/10 ] `` histori '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\ '' , \ '' parent\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.407713553z\ '' , \ '' container\ '' : \ '' 5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) entrypoint [ echo ] \ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : [ \ '' echo\ '' ] , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\ '' , \ '' parent\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.247988044z\ '' , \ '' container\ '' : \ '' ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) cmd [ inki ] \ '' ] , \ '' image\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' inky\ '' ] , \ '' image\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\ '' , \ '' parent\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' created\ '' : \ '' 2014-08-15t00:31:36.068514721z\ '' , \ '' container\ '' : \ '' 696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) maintain support @ mesosphere.io\ '' ] , \ '' image\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 1.1.2\ '' , \ '' author\ '' : \ '' support @ mesosphere.io\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' ] , \ '' image\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\ '' , \ '' parent\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.990887725z\ '' , \ '' container\ '' : \ '' bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) cmd [ /bin/sh ] \ '' ] , \ '' image\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' ] , \ '' image\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\ '' , \ '' parent\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.692528634z\ '' , \ '' container\ '' : \ '' fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) add file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 /\ '' ] , \ '' image\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : null , \ '' image\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :2433303 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\ '' , \ '' parent\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' created\ '' : \ '' 2014-06-05t00:05:35.589531476z\ '' , \ '' container\ '' : \ '' f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) maintain jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' ] , \ '' image\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.10.0\ '' , \ '' author\ '' : \ '' jrme petazzoni \\u003cjerom @ docker.com\\u003e\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' f7d939e68b5a\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : [ \ '' home=/\ '' , \ '' path=/usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin\ '' ] , \ '' cmd\ '' : null , \ '' image\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : [ ] , \ '' labels\ '' : null } , \ '' architecture\ '' : \ '' amd64\ '' , \ '' os\ '' : \ '' linux\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' id\ '' : \ '' 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\ '' , \ '' comment\ '' : \ '' import -\ '' , \ '' created\ '' : \ '' 2013-06-13t14:03:50.821769-07:00\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' \ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' portspecs\ '' : null , \ '' exposedports\ '' : null , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : null , \ '' cmd\ '' : null , \ '' image\ '' : \ '' \ '' , \ '' volumes\ '' : null , \ '' volumedriver\ '' : \ '' \ '' , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' networkdisabled\ '' : fals , \ '' macaddress\ '' : \ '' \ '' , \ '' onbuild\ '' : null , \ '' labels\ '' : null } , \ '' docker_version\ '' : \ '' 0.4.0\ '' , \ '' architecture\ '' : \ '' x86_64\ '' , \ '' size\ '' :0 } \n '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] , [ 20:13:08 ] w : [ step 10/10 ] `` schemavers '' : 1 , [ 20:13:08 ] w : [ step 10/10 ] `` signatur '' : [ [ 20:13:08 ] w : [ step 10/10 ] { [ 20:13:08 ] w : [ step 10/10 ] `` header '' : { [ 20:13:08 ] w : [ step 10/10 ] `` jwk '' : { [ 20:13:08 ] w : [ step 10/10 ] `` crv '' : `` p-256 '' , [ 20:13:08 ] w : [ step 10/10 ] `` kid '' : `` 4ayn : kh32 : gjjd : i6bx : sjaz : a3ec : p7ic:7o7c:22zq:3z5o:75vq:3qot '' , [ 20:13:08 ] w : [ step 10/10 ] `` kti '' : `` ec '' , [ 20:13:08 ] w : [ step 10/10 ] `` x '' : `` o8bvruwnpxkzdgoo2wq7ehqzcvyhvuoovjqgextrylu '' , [ 20:13:08 ] w : [ step 10/10 ] `` '' : `` dchygr0cbi-fzzqypqm16qkfefumqctk0rqme-q5gma '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] `` alg '' : `` es256 '' [ 20:13:08 ] w : [ step 10/10 ] } , [ 20:13:08 ] w : [ step 10/10 ] `` signatur '' : `` f3faob4xpt0puw9tiptxae_zpae0pdm2imxaeacmjbbf6lb-sufpvge4iqz1co0voijeyvub1g1lv_a5nnj5zg '' , [ 20:13:08 ] w : [ step 10/10 ] `` protect '' : `` eyjmb3jtyxrmzw5ndggiojeznza3lcjmb3jtyxruywlsijoiq24wiiwidgltzsi6ijiwmtytmdgtmdvumja6mtm6mddain0 '' [ 20:13:08 ] w : [ step 10/10 ] } [ 20:13:08 ] w : [ step 10/10 ] ] [ 20:13:08 ] w : [ step 10/10 ] } ' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104116 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104130 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104138 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104146 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104151 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104158 23431 registry_puller.cpp:369 ] fetch blob 'sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 ' layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104164 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.104171 23431 registry_puller.cpp:369 ] fetch blob 'sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 ' layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 ' imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.504564 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.507129 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.508962 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.510915 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.512848 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.515400 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 rootf '/tmp/ozhdiq/store/staging/hbsybx/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.517390 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.519486 23436 registry_puller.cpp:306 ] extract layer tar ball '/tmp/ozhdiq/store/staging/hbsybx/sha256 : a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 rootf '/tmp/ozhdiq/store/staging/hbsybx/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.606955 23434 metadata_manager.cpp:155 ] success cach imag 'mesosphere/inky' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.607501 23436 provisioner.cpp:312 ] provis imag rootf '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77 ' contain f2c1fd6d-4d11-45cd-a916-e4d73d226451 use 'auf ' backend [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.607787 23434 aufs.cpp:152 ] provis imag rootf auf : 'dirs=/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/scratch/427b7851-bf82-4553-80f3-da2d42cede77/workdir : /tmp/ozhdiq/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootf : /tmp/ozhdiq/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootf : /tmp/ozhdiq/store/layers/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootf : /tmp/ozhdiq/store/layers/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootf : /tmp/ozhdiq/store/layers/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootf : /tmp/ozhdiq/store/layers/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootf : /tmp/ozhdiq/store/layers/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootf : /tmp/ozhdiq/store/layers/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs' [ 20:13:08 ] w : [ step 10/10 ] e0805 20:13:08.614994 23432 slave.cpp:4029 ] contain 'f2c1fd6d-4d11-45cd-a916-e4d73d226451 ' executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 fail start : fail mount rootf '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77 ' auf : invalid argument [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.615058 23436 containerizer.cpp:1637 ] destroy contain 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.615072 23436 containerizer.cpp:1640 ] wait provision complet contain 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.615279 23435 provisioner.cpp:455 ] destroy contain rootf '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77 ' contain f2c1fd6d-4d11-45cd-a916-e4d73d226451 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616097 23430 slave.cpp:4135 ] executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 termin unknown statu [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616173 23430 slave.cpp:3264 ] handl statu updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 @ 0.0.0.0:0 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616320 23435 slave.cpp:6104 ] termin task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 [ 20:13:08 ] w : [ step 10/10 ] w0805 20:13:08.616402 23432 containerizer.cpp:1466 ] ignor updat unknown contain : f2c1fd6d-4d11-45cd-a916-e4d73d226451 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616528 23433 status_update_manager.cpp:323 ] receiv statu updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616545 23433 status_update_manager.cpp:500 ] creat statusupd stream task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616750 23433 status_update_manager.cpp:377 ] forward updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 agent [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616827 23431 slave.cpp:3657 ] forward updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 master @ 172.30.2.138:44256 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.616936 23431 slave.cpp:3551 ] statu updat manag success handl statu updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617010 23433 master.cpp:5141 ] statu updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617032 23433 master.cpp:5203 ] forward statu updat task_fail ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617079 23433 master.cpp:6845 ] updat state task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( latest state : task_fail , statu updat state : task_fail ) [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617187 23435 sched.cpp:1025 ] schedul : :statusupd took 57204n [ 20:13:08 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/runtime_isolator_tests.cpp:309 : failur [ 20:13:08 ] : [ step 10/10 ] valu : statusrunning- > state ( ) [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617234 23436 hierarchical.cpp:924 ] recov cpu ( * ) :1 ; mem ( * ) :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] : [ step 10/10 ] actual : task_fail [ 20:13:08 ] : [ step 10/10 ] expect : task_run [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617281 23432 master.cpp:4266 ] process acknowledg call 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617311 23432 master.cpp:6911 ] remov task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 resourc cpu ( * ) :1 ; mem ( * ) :128 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617450 23430 status_update_manager.cpp:395 ] receiv statu updat acknowledg ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617480 23430 status_update_manager.cpp:531 ] clean statu updat stream task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617545 23430 slave.cpp:2650 ] statu updat manag success handl statu updat acknowledg ( uuid : 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 ) task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617561 23430 slave.cpp:6145 ] complet task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617575 23430 slave.cpp:4246 ] clean executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617660 23435 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451 ' gc 6.99999285160889day futur [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617688 23430 slave.cpp:4334 ] clean framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617708 23435 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12 ' gc 6.9999928509363day futur [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617748 23434 status_update_manager.cpp:285 ] close statu updat stream framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.617772 23434 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/dockerruntimeisolatortest_root_curl_internet_dockerdefaultentryptregistrypuller_cl0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-s0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ' gc 6.99999285021926day futur [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.630481 23432 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.630504 23432 hierarchical.cpp:1192 ] perform alloc 1 agent 155186n [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.630609 23430 master.cpp:5729 ] send 1 offer framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:08 ] w : [ step 10/10 ] i0805 20:13:08.630728 23430 sched.cpp:917 ] schedul : :resourceoff took 13371n [ 20:13:09 ] w : [ step 10/10 ] i0805 20:13:09.631413 23437 hierarchical.cpp:1548 ] no alloc perform [ 20:13:09 ] w : [ step 10/10 ] i0805 20:13:09.631450 23437 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:09 ] w : [ step 10/10 ] i0805 20:13:09.631465 23437 hierarchical.cpp:1192 ] perform alloc 1 agent 202676n [ 20:13:10 ] w : [ step 10/10 ] i0805 20:13:10.631609 23435 hierarchical.cpp:1548 ] no alloc perform [ 20:13:10 ] w : [ step 10/10 ] i0805 20:13:10.631640 23435 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:10 ] w : [ step 10/10 ] i0805 20:13:10.631655 23435 hierarchical.cpp:1192 ] perform alloc 1 agent 102058n [ 20:13:11 ] w : [ step 10/10 ] i0805 20:13:11.632261 23431 hierarchical.cpp:1548 ] no alloc perform [ 20:13:11 ] w : [ step 10/10 ] i0805 20:13:11.632294 23431 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:11 ] w : [ step 10/10 ] i0805 20:13:11.632308 23431 hierarchical.cpp:1192 ] perform alloc 1 agent 112653n [ 20:13:12 ] w : [ step 10/10 ] i0805 20:13:12.632477 23433 hierarchical.cpp:1548 ] no alloc perform [ 20:13:12 ] w : [ step 10/10 ] i0805 20:13:12.632510 23433 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:12 ] w : [ step 10/10 ] i0805 20:13:12.632525 23433 hierarchical.cpp:1192 ] perform alloc 1 agent 144467n [ 20:13:13 ] w : [ step 10/10 ] i0805 20:13:13.633517 23430 hierarchical.cpp:1548 ] no alloc perform [ 20:13:13 ] w : [ step 10/10 ] i0805 20:13:13.633549 23430 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:13 ] w : [ step 10/10 ] i0805 20:13:13.633563 23430 hierarchical.cpp:1192 ] perform alloc 1 agent 111395n [ 20:13:14 ] w : [ step 10/10 ] i0805 20:13:14.633985 23436 hierarchical.cpp:1548 ] no alloc perform [ 20:13:14 ] w : [ step 10/10 ] i0805 20:13:14.634018 23436 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:14 ] w : [ step 10/10 ] i0805 20:13:14.634048 23436 hierarchical.cpp:1192 ] perform alloc 1 agent 132707n [ 20:13:15 ] w : [ step 10/10 ] i0805 20:13:15.634266 23430 hierarchical.cpp:1548 ] no alloc perform [ 20:13:15 ] w : [ step 10/10 ] i0805 20:13:15.634299 23430 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:15 ] w : [ step 10/10 ] i0805 20:13:15.634313 23430 hierarchical.cpp:1192 ] perform alloc 1 agent 103933n [ 20:13:16 ] w : [ step 10/10 ] i0805 20:13:16.635295 23431 hierarchical.cpp:1548 ] no alloc perform [ 20:13:16 ] w : [ step 10/10 ] i0805 20:13:16.635330 23431 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:16 ] w : [ step 10/10 ] i0805 20:13:16.635346 23431 hierarchical.cpp:1192 ] perform alloc 1 agent 115517n [ 20:13:17 ] w : [ step 10/10 ] i0805 20:13:17.635922 23436 hierarchical.cpp:1548 ] no alloc perform [ 20:13:17 ] w : [ step 10/10 ] i0805 20:13:17.635958 23436 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:17 ] w : [ step 10/10 ] i0805 20:13:17.635973 23436 hierarchical.cpp:1192 ] perform alloc 1 agent 109700n [ 20:13:18 ] w : [ step 10/10 ] i0805 20:13:18.636693 23437 hierarchical.cpp:1548 ] no alloc perform [ 20:13:18 ] w : [ step 10/10 ] i0805 20:13:18.636728 23437 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:18 ] w : [ step 10/10 ] i0805 20:13:18.636744 23437 hierarchical.cpp:1192 ] perform alloc 1 agent 123133n [ 20:13:19 ] w : [ step 10/10 ] i0805 20:13:19.637589 23432 hierarchical.cpp:1548 ] no alloc perform [ 20:13:19 ] w : [ step 10/10 ] i0805 20:13:19.637624 23432 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:19 ] w : [ step 10/10 ] i0805 20:13:19.637639 23432 hierarchical.cpp:1192 ] perform alloc 1 agent 118581n [ 20:13:20 ] w : [ step 10/10 ] i0805 20:13:20.638517 23431 hierarchical.cpp:1548 ] no alloc perform [ 20:13:20 ] w : [ step 10/10 ] i0805 20:13:20.638550 23431 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:20 ] w : [ step 10/10 ] i0805 20:13:20.638566 23431 hierarchical.cpp:1192 ] perform alloc 1 agent 107979n [ 20:13:21 ] w : [ step 10/10 ] i0805 20:13:21.639577 23435 hierarchical.cpp:1548 ] no alloc perform [ 20:13:21 ] w : [ step 10/10 ] i0805 20:13:21.639612 23435 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:21 ] w : [ step 10/10 ] i0805 20:13:21.639628 23435 hierarchical.cpp:1192 ] perform alloc 1 agent 126299n [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.640533 23430 hierarchical.cpp:1548 ] no alloc perform [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.640566 23430 hierarchical.cpp:1643 ] no invers offer send ! [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.640581 23430 hierarchical.cpp:1192 ] perform alloc 1 agent 106384n [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.667985 23437 slave.cpp:5044 ] queri resourc estim oversubscrib resourc [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.668124 23434 slave.cpp:5058 ] receiv oversubscrib resourc resourc estim [ 20:13:22 ] w : [ step 10/10 ] i0805 20:13:22.674278 23433 slave.cpp:3739 ] receiv ping slave-observ ( 465 ) @ 172.30.2.138:44256 [ 20:13:23 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/runtime_isolator_tests.cpp:311 : failur [ 20:13:23 ] : [ step 10/10 ] fail wait 15sec statusfinish [ 20:13:23 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/runtime_isolator_tests.cpp:301 : failur [ 20:13:23 ] : [ step 10/10 ] actual function call count n't match expect_cal ( sched , statusupd ( & driver , _ ) ) ... [ 20:13:23 ] : [ step 10/10 ] expect : call twice [ 20:13:23 ] : [ step 10/10 ] actual : call - unsatisfi activ [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.618680 23433 master.cpp:1284 ] framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 disconnect [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.618721 23433 master.cpp:2726 ] disconnect framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.618737 23433 master.cpp:2750 ] deactiv framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.618883 23434 hierarchical.cpp:382 ] deactiv framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:23 ] w : [ step 10/10 ] w0805 20:13:23.618918 23433 master.hpp:2131 ] master attempt send messag disconnect framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.618963 23433 master.cpp:1297 ] give framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 0n failov [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619046 23434 hierarchical.cpp:924 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619258 23416 slave.cpp:767 ] agent termin [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619321 23432 master.cpp:1245 ] agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) disconnect [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619336 23432 master.cpp:2785 ] disconnect agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619371 23432 master.cpp:2804 ] deactiv agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 slave ( 506 ) @ 172.30.2.138:44256 ( ip-172-30-2-138.mesosphere.io ) [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.619431 23432 hierarchical.cpp:571 ] agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 deactiv [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.620216 23435 master.cpp:5581 ] framework failov timeout , remov framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.620232 23435 master.cpp:6316 ] remov framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 ( default ) scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba @ 172.30.2.138:44256 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.620357 23433 hierarchical.cpp:333 ] remov framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.621464 23416 master.cpp:1092 ] master termin [ 20:13:23 ] w : [ step 10/10 ] i0805 20:13:23.621561 23433 hierarchical.cpp:510 ] remov agent dd755a55-0dd1-4d2d-9a49-812a666015cb-s0 [ 20:13:23 ] : [ step 10/10 ] [ fail ] dockerruntimeisolatortest.root_curl_internet_dockerdefaultentryptregistrypul ( 16012 ms ) { noformat }",MESOS-6001,3.0
"re-evalu libev ssl socket eof semant libprocess while debug issu relat libprocess finalization/reiniti , [ ~bmahler ] point libprocess n't strictli adher expect behavior unix socket eof receiv . if socket receiv eof , mean writer end close write end socket . howev , end may still interest read . libprocess current treat receiv eof { { shutdown ( ) } } call socket , end close read write ( see [ here|http : //github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/libevent_ssl_socket.cpp # l349-l360 ] [ here|http : //github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp # l692-l697 ] ) . we consid chang eof semant { { socket } } object close match unix socket .",MESOS-5999,3.0
"ssl socket check fail socket receiv eof while write test mesos-3753 , i encount bug [ check|http : //github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp # l708 ] fail end test bodi , object stack frame destroy . after ad debug log output , i produc follow : { code } i0804 08:32:33.263211 273793024 libevent_ssl_socket.cpp:681 ] * * * send ( ) 17 i0804 08:32:33.263209 273256448 process.cpp:2970 ] clean __limiter__ ( 3 ) @ 127.0.0.1:55688 i0804 08:32:33.263263 275939328 libevent_ssl_socket.cpp:152 ] * * * initi ( ) : 14 i0804 08:32:33.263206 272719872 process.cpp:2865 ] resum ( 61 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.263261952+00:00 i0804 08:32:33.263327 275939328 libevent_ssl_socket.cpp:584 ] * * * recv ( ) 14 i0804 08:32:33.263337 272719872 hierarchical.cpp:571 ] agent e2a49340-34ec-403f-a5a4-15e29c4a2434-s0 deactiv i0804 08:32:33.263322 275402752 process.cpp:2865 ] resum help @ 127.0.0.1:55688 2016-08-04 15:32:33.263343104+00:00 i0804 08:32:33.263510 275939328 libevent_ssl_socket.cpp:322 ] * * * event_callback ( bev ) i0804 08:32:33.263536 275939328 libevent_ssl_socket.cpp:353 ] * * * event_callback check eof/connected/error : 19 i0804 08:32:33.263592 275939328 libevent_ssl_socket.cpp:159 ] * * * shutdown ( ) : 19 i0804 08:32:33.263622 1985901312 process.cpp:3170 ] donat thread ( 87 ) @ 127.0.0.1:55688 wait i0804 08:32:33.263639 274329600 process.cpp:2865 ] resum __http__ ( 12 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.263653888+00:00 i0804 08:32:33.263659 1985901312 process.cpp:2865 ] resum ( 87 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.263671040+00:00 i0804 08:32:33.263730 1985901312 process.cpp:2970 ] clean ( 87 ) @ 127.0.0.1:55688 i0804 08:32:33.263741 275939328 libevent_ssl_socket.cpp:322 ] * * * event_callback ( bev ) i0804 08:32:33.263736 274329600 process.cpp:2970 ] clean __http__ ( 12 ) @ 127.0.0.1:55688 i0804 08:32:33.263778 275939328 libevent_ssl_socket.cpp:353 ] * * * event_callback check eof/connected/error : 17 i0804 08:32:33.263818 275939328 libevent_ssl_socket.cpp:159 ] * * * shutdown ( ) : 17 i0804 08:32:33.263839 272183296 process.cpp:2865 ] resum help @ 127.0.0.1:55688 2016-08-04 15:32:33.263857920+00:00 i0804 08:32:33.263933 273793024 process.cpp:2865 ] resum __gc__ @ 127.0.0.1:55688 2016-08-04 15:32:33.263951104+00:00 i0804 08:32:33.264034 275939328 libevent_ssl_socket.cpp:681 ] * * * send ( ) 17 i0804 08:32:33.264020 272719872 process.cpp:2865 ] resum __http__ ( 11 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.264041984+00:00 i0804 08:32:33.264036 274329600 process.cpp:2865 ] resum status-update-manag ( 3 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.264056064+00:00 i0804 08:32:33.264071 272719872 process.cpp:2970 ] clean __http__ ( 11 ) @ 127.0.0.1:55688 i0804 08:32:33.264088 274329600 process.cpp:2970 ] clean status-update-manag ( 3 ) @ 127.0.0.1:55688 i0804 08:32:33.264086 275939328 libevent_ssl_socket.cpp:721 ] * * * send socket : 17 , data : 0 i0804 08:32:33.264112 272183296 process.cpp:2865 ] resum ( 89 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.264126976+00:00 i0804 08:32:33.264118 275402752 process.cpp:2865 ] resum help @ 127.0.0.1:55688 2016-08-04 15:32:33.264144896+00:00 i0804 08:32:33.264149 272183296 process.cpp:2970 ] clean ( 89 ) @ 127.0.0.1:55688 i0804 08:32:33.264202 275939328 libevent_ssl_socket.cpp:281 ] * * * send_callback ( bev ) i0804 08:32:33.264400 273793024 process.cpp:3170 ] donat thread ( 86 ) @ 127.0.0.1:55688 wait i0804 08:32:33.264413 273256448 process.cpp:2865 ] resum ( 76 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.264428032+00:00 i0804 08:32:33.296268 275939328 libevent_ssl_socket.cpp:300 ] * * * send_callback ( ) : 17 i0804 08:32:33.296419 273256448 process.cpp:2970 ] clean ( 76 ) @ 127.0.0.1:55688 i0804 08:32:33.296357 273793024 process.cpp:2865 ] resum ( 86 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.296414976+00:00 i0804 08:32:33.296464 273793024 process.cpp:2970 ] clean ( 86 ) @ 127.0.0.1:55688 i0804 08:32:33.296497 275939328 libevent_ssl_socket.cpp:104 ] * * * releas ssl socket i0804 08:32:33.296517 275939328 libevent_ssl_socket.cpp:106 ] * * * releas ssl socket : 19 i0804 08:32:33.296515 274329600 process.cpp:2865 ] resum help @ 127.0.0.1:55688 2016-08-04 15:32:33.296532992+00:00 i0804 08:32:33.296550 275939328 libevent_ssl_socket.cpp:721 ] * * * send socket : 17 , data : 0 i0804 08:32:33.296583 273793024 process.cpp:2865 ] resum ( 77 ) @ 127.0.0.1:55688 2016-08-04 15:32:33.296616960+00:00 f0804 08:32:33.296623 275939328 libevent_ssl_socket.cpp:723 ] check fail : 'self- > send_request.get ( ) ' must non null * * * check failur stack trace : * * * { code } the { { send ( ) 17 } } line indic begin { { send ( ) } } ssl socket use fd 17 . { { shutdown ( ) : 17 } } indic begin { { shutdown ( ) } } socket , { { send socket : 17 } } indic execut lambda { { send ( ) } } event loop . sinc { { shutdown ( ) } } call call { { send ( ) } } execut lambda , look like { { socket } } destroy lambda could run . it 's unclear would happen , sinc { { send ( ) } } 's lambda captur share copi socket 's { { } } pointer order keep aliv .",MESOS-5986,3.0
"nvidiavolum error binari miss we current error binari tri add volum found host filesystem . howev , semant want . by design , list binari * may * exist filesystem want put volum , binari * must * exist . we simpli skip unfound binari move next one instead error .",MESOS-5982,2.0
"healthcheck decid kill task stop perform health check . current , { { healthcheck } } librari decid task kill base health statu . moreov , stop check health . thi seem unfortun , 's executor / framework decid kill task health check .",MESOS-5963,5.0
all non-root test fail gpu machin a recent addit ensur { { nvidiavolum : :creat ( ) } } ran root broke non-root test gpu machin . the reason uncondit creat volum long detect { { nvml.isavail ( ) } } fail allow creat volum root permiss . we fix ad proper condit determin / creat volum base combin { { \-\-container } } { { \-\-isol } } flag .,MESOS-5959,2.0
"nvidiavolum : :creat ( ) check root creat volum without root , creat nvidia volum { { /var/run/meso } } mount { { tmpf } } case need overrid { { noexec } } current file system .",MESOS-5945,2.0
"unabl run `` scratch '' dockerfil unifi container . it possibl run docker contain base upon `` scratch '' contain . setup : meso 1.0.0 follow meso set : { code : none } echo 'docker ' | sudo tee /etc/mesos-slave/image_provid echo 'filesystem/linux , docker/runtim ' | sudo tee /etc/mesos-slave/isol { code } recreat : from master slave , run : { code : none } mesos-execut -- command='echo ok ' -- docker_image=hello-seattl -- master=localhost:5050 -- name=test { code } effect : the contain crash messag meso report ca n't mount folder x/y/z . e.g . ca n't mount /tmp . thi mean ca n't run contain `` fat '' contain ( i.e . one full os ) . e.g . error : bq . fail enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd ' : /tmp chroot existi0729 07:49:56.753474 4362 exec.cpp:413 ] executor ask shutdown expect : run without issu . use case : we use scratch base contain static binari keep imag size . thi common practic .",MESOS-5927,3.0
"ubuntu 14.04 lt gpu isol `` /run '' directori noexec in ubuntu 14.04 lt mount /run directori noexec . it affect { { /var/run/mesos/isolators/gpu/nvidia_352.63/bin } } directori meso gpu isol depend . { { bill @ billz : /var/run $ mount | grep noexec proc /proc type proc ( rw , noexec , nosuid , nodev ) sysf /si type sysf ( rw , noexec , nosuid , nodev ) devpt /dev/pt type devpt ( rw , noexec , nosuid , gid=5 , mode=0620 ) tmpf /run type tmpf ( rw , noexec , nosuid , size=10 % , mode=0755 ) } } the /var/run link /run : { { bill @ billz : /var $ total 52 drwxr-xr-x 13 root root 4096 may 5 20:00 ./ drwxr-xr-x 27 root root 4096 jul 14 17:29 .. / lrwxrwxrwx 1 root root 9 may 5 19:50 lock - > /run/lock/ drwxrwxr-x 19 root syslog 4096 jul 28 08:00 log/ drwxr-xr-x 2 root root 4096 aug 4 2015 opt/ lrwxrwxrwx 1 root root 4 may 5 19:50 run - > /run/ } } current work around mount without noexec : { { sudo mount -o remount , exec /run } }",MESOS-5923,3.0
"strict/registrartest.updatequota/0 flaki observ asf ci ( http : //builds.apache.org/job/mesos/buildtool=autotool , compiler=clang , configuration= -- verbos % 20 -- enable-libev % 20 -- enable-ssl , environment=glog_v=1 % 20mesos_verbose=1 , os=ubuntu:14.04 , label_exp= ( docker % 7c % 7chadoop ) & & ( ! ubuntu-us1 ) & & ( ! ubuntu-6 ) /2539/consoleful ) . log file attach . note might uncov due recent remov { { os : :sleep } } { { clock : :settl } } .",MESOS-5878,3.0
document mesos_sandbox executor env variabl . and document differ mesos_directori .,MESOS-5864,2.0
"logrot containerlogg modul rotat log run root ` -- switch_us ` . the logrot containerlogg modul run agent 's user . in case , { { root } } . when { { logrot } } run root , addit check configur file must pass ( root { { logrot } } need secur non-root modif configur ) : http : //github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c # l807-l815 log rotat fail follow scenario : 1 ) the agent run { { -- switch_us } } ( default : true ) 2 ) a task launch non-root user specifi 3 ) the logrot modul spawn companion process ( root ) creat { { stdout } } , { { stderr } } , { { stdout.logrotate.conf } } , { { stderr.logrotate.conf } } file ( root ) . thi step race next step . 4 ) the meso container fetcher { { chown } } task 's sandbox non-root user . includ file creat . 5 ) when { { logrot } } run , skip non-root configur file . thi mean file rotat . -- -- fix : the logrot modul 's companion process call { { setuid } } { { setgid } } .",MESOS-5856,3.0
"cmake build need gener protobuf build libmeso the exist cmake list place protobuf level meso sourc : http : //github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/cmakelists.txt # l415 thi incorrect , protobuf chang need regener build . note : autotool build , done compil protobuf { { libmeso } } , build { { libmesos_no_3rdparti } } : http : //github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/makefile.am # l1304-l1305",MESOS-5852,2.0
modular network replicated_log current replicated_log reli zookeep coordin elect . thi done network abstract _zookeepernetwork_ . we need modular part order enabl replicated_log use master contender/detector modul .,MESOS-5828,8.0
"masterapitest.subscrib flaki thi test seem flaki , although mac os x cento 7 error bit differ . on mac os x : { noformat } [ run ] contenttype/masterapitest.subscribe/0 i0708 11:42:48.474665 1927435008 cluster.cpp:155 ] creat default 'local ' author i0708 11:42:48.480677 1927435008 leveldb.cpp:174 ] open db 5727u i0708 11:42:48.481494 1927435008 leveldb.cpp:181 ] compact db 722u i0708 11:42:48.481541 1927435008 leveldb.cpp:196 ] creat db iter 19u i0708 11:42:48.481572 1927435008 leveldb.cpp:202 ] seek begin db 9u i0708 11:42:48.481587 1927435008 leveldb.cpp:271 ] iter 0 key db 7u i0708 11:42:48.481617 1927435008 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0708 11:42:48.482030 350982144 recover.cpp:451 ] start replica recoveri i0708 11:42:48.482203 350982144 recover.cpp:477 ] replica empti statu i0708 11:42:48.484107 348299264 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 3780 ) @ 127.0.0.1:50325 i0708 11:42:48.484318 350982144 recover.cpp:197 ] receiv recov respons replica empti statu i0708 11:42:48.484750 348835840 master.cpp:382 ] master e055d60c-05ff-487e-82da-d0a43e52605c ( localhost ) start 127.0.0.1:50325 i0708 11:42:48.484850 349908992 recover.cpp:568 ] updat replica statu start i0708 11:42:48.484788 348835840 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /private/tmp/sn2kf4/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /private/tmp/sn2kf4/mast '' -- zk_session_timeout= '' 10sec '' w0708 11:42:48.485263 348835840 master.cpp:387 ] * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * master bound loopback interfac ! can commun remot schedul agent . you might want set ' -- ip ' flag routabl ip address . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * i0708 11:42:48.485291 348835840 master.cpp:434 ] master allow authent framework regist i0708 11:42:48.485314 348835840 master.cpp:448 ] master allow authent agent regist i0708 11:42:48.485335 348835840 master.cpp:461 ] master allow authent http framework regist i0708 11:42:48.485347 348835840 credentials.hpp:37 ] load credenti authent '/private/tmp/sn2kf4/credentials' i0708 11:42:48.485373 349372416 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 397u i0708 11:42:48.485414 349372416 replica.cpp:320 ] persist replica statu start i0708 11:42:48.485608 350982144 recover.cpp:477 ] replica start statu i0708 11:42:48.485749 348835840 master.cpp:506 ] use default 'crammd5 ' authent i0708 11:42:48.485852 348835840 master.cpp:578 ] use default 'basic ' http authent i0708 11:42:48.486018 348835840 master.cpp:658 ] use default 'basic ' http framework authent i0708 11:42:48.486140 348835840 master.cpp:705 ] author enabl i0708 11:42:48.486486 350982144 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 3783 ) @ 127.0.0.1:50325 i0708 11:42:48.486758 352055296 recover.cpp:197 ] receiv recov respons replica start statu i0708 11:42:48.487176 350982144 recover.cpp:568 ] updat replica statu vote i0708 11:42:48.487576 352055296 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 300u i0708 11:42:48.487658 352055296 replica.cpp:320 ] persist replica statu vote i0708 11:42:48.487736 350982144 recover.cpp:582 ] success join paxo group i0708 11:42:48.487951 350982144 recover.cpp:466 ] recov process termin i0708 11:42:48.489441 348835840 master.cpp:1973 ] the newli elect leader master @ 127.0.0.1:50325 id e055d60c-05ff-487e-82da-d0a43e52605c i0708 11:42:48.489518 348835840 master.cpp:1986 ] elect lead master ! i0708 11:42:48.489545 348835840 master.cpp:1673 ] recov registrar i0708 11:42:48.489637 350982144 registrar.cpp:332 ] recov registrar i0708 11:42:48.490120 351518720 log.cpp:553 ] attempt start writer i0708 11:42:48.491161 350445568 replica.cpp:493 ] replica receiv implicit promis request ( 3784 ) @ 127.0.0.1:50325 propos 1 i0708 11:42:48.491461 350445568 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 252u i0708 11:42:48.491528 350445568 replica.cpp:342 ] persist promis 1 i0708 11:42:48.492337 348299264 coordinator.cpp:238 ] coordin attempt fill miss posit i0708 11:42:48.493482 349372416 replica.cpp:388 ] replica receiv explicit promis request ( 3785 ) @ 127.0.0.1:50325 posit 0 propos 2 i0708 11:42:48.493854 349372416 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 283u i0708 11:42:48.493904 349372416 replica.cpp:712 ] persist action 0 i0708 11:42:48.495302 348299264 replica.cpp:537 ] replica receiv write request posit 0 ( 3786 ) @ 127.0.0.1:50325 i0708 11:42:48.495455 348299264 leveldb.cpp:436 ] read posit leveldb took 45u i0708 11:42:48.495761 348299264 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 261u i0708 11:42:48.495803 348299264 replica.cpp:712 ] persist action 0 i0708 11:42:48.496484 350445568 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0708 11:42:48.496795 350445568 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 255u i0708 11:42:48.496857 350445568 replica.cpp:712 ] persist action 0 i0708 11:42:48.496896 350445568 replica.cpp:697 ] replica learn nop action posit 0 i0708 11:42:48.497445 350982144 log.cpp:569 ] writer start end posit 0 i0708 11:42:48.498523 350982144 leveldb.cpp:436 ] read posit leveldb took 80u i0708 11:42:48.499307 349908992 registrar.cpp:365 ] success fetch registri ( 0b ) 9.63712m i0708 11:42:48.499464 349908992 registrar.cpp:464 ] appli 1 oper 36u ; attempt updat 'registry' i0708 11:42:48.499953 351518720 log.cpp:577 ] attempt append 159 byte log i0708 11:42:48.500088 350982144 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0708 11:42:48.500880 348299264 replica.cpp:537 ] replica receiv write request posit 1 ( 3787 ) @ 127.0.0.1:50325 i0708 11:42:48.501186 348299264 leveldb.cpp:341 ] persist action ( 178 byte ) leveldb took 259u i0708 11:42:48.501231 348299264 replica.cpp:712 ] persist action 1 i0708 11:42:48.501786 351518720 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0708 11:42:48.502118 351518720 leveldb.cpp:341 ] persist action ( 180 byte ) leveldb took 311u i0708 11:42:48.502260 351518720 replica.cpp:712 ] persist action 1 i0708 11:42:48.502305 351518720 replica.cpp:697 ] replica learn append action posit 1 i0708 11:42:48.503475 349908992 registrar.cpp:509 ] success updat 'registri ' 3.944192m i0708 11:42:48.503909 349908992 registrar.cpp:395 ] success recov registrar i0708 11:42:48.504003 350982144 log.cpp:596 ] attempt truncat log 1 i0708 11:42:48.504250 349372416 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0708 11:42:48.504546 350445568 master.cpp:1781 ] recov 0 agent registri ( 121b ) ; allow 10min agent re-regist i0708 11:42:48.506022 352055296 replica.cpp:537 ] replica receiv write request posit 2 ( 3788 ) @ 127.0.0.1:50325 i0708 11:42:48.506479 352055296 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 320u i0708 11:42:48.506513 352055296 replica.cpp:712 ] persist action 2 i0708 11:42:48.506978 351518720 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0708 11:42:48.507155 351518720 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 169u i0708 11:42:48.507237 351518720 leveldb.cpp:399 ] delet ~1 key leveldb took 37u i0708 11:42:48.507264 351518720 replica.cpp:712 ] persist action 2 i0708 11:42:48.507285 351518720 replica.cpp:697 ] replica learn truncat action posit 2 i0708 11:42:48.521363 1927435008 cluster.cpp:432 ] creat default 'local ' author i0708 11:42:48.522498 350982144 slave.cpp:205 ] agent start 119 ) @ 127.0.0.1:50325 i0708 11:42:48.522538 350982144 slave.cpp:206 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authentication_backoff_factor= '' 1sec '' -- authorizer= '' local '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /users/zhitao/uber/sync/zhitao-mesos1.dev.uber.com/home/uber/mesos/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- version= '' fals '' -- work_dir= '' /var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx '' w0708 11:42:48.522903 350982144 slave.cpp:209 ] * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * agent bound loopback interfac ! can commun remot master ( ) . you might want set ' -- ip ' flag routabl ip address . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * i0708 11:42:48.522922 350982144 credentials.hpp:86 ] load credenti authent '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/credential' w0708 11:42:48.522965 1927435008 scheduler.cpp:157 ] * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * schedul driver bound loopback interfac ! can commun remot master ( ) . you might want set 'libprocess_ip ' environ variabl use routabl ip address . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * i0708 11:42:48.522992 1927435008 scheduler.cpp:172 ] version : 1.0.0 i0708 11:42:48.523066 350982144 slave.cpp:343 ] agent use credenti : test-princip i0708 11:42:48.523092 350982144 credentials.hpp:37 ] load credenti authent '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/http_credentials' i0708 11:42:48.523334 350982144 slave.cpp:395 ] use default 'basic ' http authent i0708 11:42:48.523973 352055296 scheduler.cpp:461 ] new master detect master @ 127.0.0.1:50325 i0708 11:42:48.524050 350982144 slave.cpp:594 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0708 11:42:48.524196 350982144 slave.cpp:602 ] agent attribut : [ ] i0708 11:42:48.524224 350982144 slave.cpp:607 ] agent hostnam : localhost i0708 11:42:48.525522 350445568 state.cpp:57 ] recov state '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/meta' i0708 11:42:48.525853 350445568 status_update_manager.cpp:200 ] recov statu updat manag i0708 11:42:48.526165 350445568 slave.cpp:4856 ] finish recoveri i0708 11:42:48.527223 349372416 status_update_manager.cpp:174 ] paus send statu updat i0708 11:42:48.527231 352055296 slave.cpp:969 ] new master detect master @ 127.0.0.1:50325 i0708 11:42:48.527276 352055296 slave.cpp:1028 ] authent master master @ 127.0.0.1:50325 i0708 11:42:48.527328 352055296 slave.cpp:1039 ] use default cram-md5 authenticate i0708 11:42:48.527561 352055296 slave.cpp:1001 ] detect new master i0708 11:42:48.527582 348299264 authenticatee.cpp:121 ] creat new client sasl connect i0708 11:42:48.528666 349908992 master.cpp:6006 ] authent slave ( 119 ) @ 127.0.0.1:50325 i0708 11:42:48.528880 352055296 authenticator.cpp:98 ] creat new server sasl connect i0708 11:42:48.529089 350445568 http.cpp:381 ] http post /master/api/v1/schedul 127.0.0.1:50918 i0708 11:42:48.529233 350445568 master.cpp:2272 ] receiv subscript request http framework 'default' i0708 11:42:48.529261 350445568 master.cpp:2012 ] author framework princip 'test-princip ' receiv offer role ' * ' i0708 11:42:48.529323 352055296 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 i0708 11:42:48.529357 352055296 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' i0708 11:42:48.529417 352055296 authenticator.cpp:204 ] receiv sasl authent start i0708 11:42:48.529503 352055296 authenticator.cpp:326 ] authent requir step i0708 11:42:48.529561 352055296 master.cpp:2370 ] subscrib framework 'default ' checkpoint disabl capabl [ ] i0708 11:42:48.529721 349908992 authenticatee.cpp:259 ] receiv sasl authent step i0708 11:42:48.530005 348835840 authenticator.cpp:232 ] receiv sasl authent step i0708 11:42:48.530241 348835840 authenticator.cpp:318 ] authent success i0708 11:42:48.530254 350445568 hierarchical.cpp:271 ] ad framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:42:48.530900 349908992 authenticatee.cpp:299 ] authent success i0708 11:42:48.531186 350982144 master.cpp:6036 ] success authent princip 'test-princip ' slave ( 119 ) @ 127.0.0.1:50325 i0708 11:42:48.531657 348299264 slave.cpp:1123 ] success authent master master @ 127.0.0.1:50325 i0708 11:42:48.531935 349372416 master.cpp:4676 ] regist agent slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) id e055d60c-05ff-487e-82da-d0a43e52605c-s0 i0708 11:42:48.532304 349908992 registrar.cpp:464 ] appli 1 oper 55u ; attempt updat 'registry' i0708 11:42:48.532908 348835840 log.cpp:577 ] attempt append 326 byte log i0708 11:42:48.533015 352055296 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0708 11:42:48.533641 349372416 replica.cpp:537 ] replica receiv write request posit 3 ( 3798 ) @ 127.0.0.1:50325 i0708 11:42:48.533867 349372416 leveldb.cpp:341 ] persist action ( 345 byte ) leveldb took 186u i0708 11:42:48.533917 349372416 replica.cpp:712 ] persist action 3 i0708 11:42:48.537066 349908992 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0708 11:42:48.538169 349908992 leveldb.cpp:341 ] persist action ( 347 byte ) leveldb took 914u i0708 11:42:48.538226 349908992 replica.cpp:712 ] persist action 3 i0708 11:42:48.538255 349908992 replica.cpp:697 ] replica learn append action posit 3 i0708 11:42:48.539247 352055296 registrar.cpp:509 ] success updat 'registri ' 6.895104m i0708 11:42:48.539302 348299264 log.cpp:596 ] attempt truncat log 3 i0708 11:42:48.539393 348299264 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0708 11:42:48.539798 348835840 master.cpp:4745 ] regist agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0708 11:42:48.539881 348299264 hierarchical.cpp:478 ] ad agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 ( localhost ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0708 11:42:48.539901 349908992 slave.cpp:1169 ] regist master master @ 127.0.0.1:50325 ; given agent id e055d60c-05ff-487e-82da-d0a43e52605c-s0 i0708 11:42:48.540287 350445568 status_update_manager.cpp:181 ] resum send statu updat i0708 11:42:48.540501 351518720 replica.cpp:537 ] replica receiv write request posit 4 ( 3799 ) @ 127.0.0.1:50325 i0708 11:42:48.540583 352055296 master.cpp:5835 ] send 1 offer framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:42:48.540798 351518720 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 247u i0708 11:42:48.540868 351518720 replica.cpp:712 ] persist action 4 i0708 11:42:48.540895 349908992 slave.cpp:1229 ] forward total oversubscrib resourc i0708 11:42:48.541035 352055296 master.cpp:5128 ] receiv updat agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) total oversubscrib resourc i0708 11:42:48.541291 349908992 hierarchical.cpp:542 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 ( localhost ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) i0708 11:42:48.541630 350982144 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0708 11:42:48.541911 350982144 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 189u i0708 11:42:48.541965 350982144 leveldb.cpp:399 ] delet ~2 key leveldb took 28u i0708 11:42:48.541987 350982144 replica.cpp:712 ] persist action 4 i0708 11:42:48.542006 350982144 replica.cpp:697 ] replica learn truncat action posit 4 i0708 11:42:48.544836 352055296 http.cpp:381 ] http post /master/api/v1 127.0.0.1:50920 i0708 11:42:48.544884 352055296 http.cpp:484 ] process call subscrib i0708 11:42:48.545382 352055296 master.cpp:7599 ] ad subscrib : a85e7341-ac15-4f18-9021-1a2efa326442 list activ subscrib i0708 11:42:48.550048 348835840 http.cpp:381 ] http post /master/api/v1/schedul 127.0.0.1:50919 i0708 11:42:48.550339 348835840 master.cpp:3468 ] process accept call offer : [ e055d60c-05ff-487e-82da-d0a43e52605c-o0 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:42:48.550390 348835840 master.cpp:3106 ] author framework princip 'test-princip ' launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 w0708 11:42:48.551434 348835840 validation.cpp:650 ] executor default task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 use less cpu ( none ) minimum requir ( 0.01 ) . pleas updat executor , mandatori futur releas . w0708 11:42:48.551477 348835840 validation.cpp:662 ] executor default task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 use less memori ( none ) minimum requir ( 32mb ) . pleas updat executor , mandatori futur releas . i0708 11:42:48.551803 348835840 master.cpp:7565 ] ad task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 ( localhost ) i0708 11:42:48.551949 348835840 master.cpp:3957 ] launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:42:48.552151 352055296 slave.cpp:1569 ] got assign task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:42:48.552592 352055296 slave.cpp:1688 ] launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:42:48.553282 352055296 paths.cpp:528 ] tri chown '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-s0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26 ' user 'zhitao' i0708 11:42:48.566201 352055296 slave.cpp:5748 ] launch executor default framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 resourc work directori '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-s0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' i0708 11:42:48.567876 352055296 executor.cpp:188 ] version : 1.0.0 i0708 11:42:48.568428 352055296 slave.cpp:1914 ] queu task 'd94e54c0-8c89-43bd-be2f-adeb8cf70cb1 ' executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 e0708 11:42:48.571115 352591872 process.cpp:2104 ] fail shutdown socket fd 254 : socket connect w0708 11:42:48.570768 352055296 executor.cpp:739 ] drop subscrib : executor state disconnect gmock warn : uninterest mock function call - return directli . function call : disconnect ( 0x7fad21fcebf0 ) stack trace : .. / .. /src/tests/api_tests.cpp:1537 : failur fail wait 15sec event e0708 11:43:03.556205 352591872 process.cpp:2104 ] fail shutdown socket fd 235 : socket connect e0708 11:43:03.556584 352591872 process.cpp:2104 ] fail shutdown socket fd 223 : socket connect i0708 11:43:03.557134 349908992 master.cpp:1410 ] framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) disconnect i0708 11:43:03.557176 349908992 master.cpp:2851 ] disconnect framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:43:03.557209 349908992 master.cpp:2875 ] deactiv framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:43:03.557415 349908992 master.cpp:1423 ] give framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) 0n failov i0708 11:43:03.557456 348835840 hierarchical.cpp:382 ] deactiv framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:43:03.557878 350445568 master.cpp:5687 ] framework failov timeout , remov framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:43:03.557945 350445568 master.cpp:6422 ] remov framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( default ) i0708 11:43:03.558076 352055296 slave.cpp:2292 ] ask shut framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 master @ 127.0.0.1:50325 i0708 11:43:03.558106 352055296 slave.cpp:2317 ] shut framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:43:03.558131 352055296 slave.cpp:4481 ] shut executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 w0708 11:43:03.558147 352055296 slave.hpp:768 ] unabl send event executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 : unknown connect type i0708 11:43:03.558188 350445568 master.cpp:6959 ] updat state task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 ( latest state : task_kil , statu updat state : task_kil ) i0708 11:43:03.558507 350445568 master.cpp:7025 ] remov task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:43:03.558709 350445568 master.cpp:7054 ] remov executor 'default ' resourc framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:43:03.559051 349372416 hierarchical.cpp:333 ] remov framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:43:03.567955 350982144 slave.cpp:4163 ] executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 exit statu 0 i0708 11:43:03.568176 350982144 slave.cpp:4267 ] clean executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 w0708 11:43:03.568258 348299264 master.cpp:5369 ] ignor unknown exit executor 'default ' framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:43:03.568584 348299264 gc.cpp:55 ] schedul '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-s0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26 ' gc 6.99999342143407day futur i0708 11:43:03.568864 350982144 slave.cpp:4355 ] clean framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:43:03.568879 352055296 gc.cpp:55 ] schedul '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-s0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default ' gc 6.99999341739556day futur i0708 11:43:03.569056 350445568 status_update_manager.cpp:282 ] close statu updat stream framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 i0708 11:43:03.569247 350982144 slave.cpp:841 ] agent termin i0708 11:43:03.569239 348835840 gc.cpp:55 ] schedul '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/t/contenttype_masterapitest_subscribe_0_vapndx/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-s0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000 ' gc 6.99999341315852day futur i0708 11:43:03.569524 350982144 master.cpp:1371 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) disconnect i0708 11:43:03.569577 350982144 master.cpp:2910 ] disconnect agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:43:03.569767 350982144 master.cpp:2929 ] deactiv agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 slave ( 119 ) @ 127.0.0.1:50325 ( localhost ) i0708 11:43:03.570020 349372416 hierarchical.cpp:571 ] agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 deactiv .. / .. /src/tests/api_tests.cpp:1509 : failur actual function call count n't match expect_cal ( * executor , acknowledg ( _ , _ ) ) ... expect : call actual : never call - unsatisfi activ .. / .. /src/tests/api_tests.cpp:1505 : failur actual function call count n't match expect_cal ( * executor , launch ( _ , _ ) ) ... expect : call actual : never call - unsatisfi activ .. / .. /src/tests/api_tests.cpp:1503 : failur actual function call count n't match expect_cal ( * executor , subscrib ( _ , _ ) ) ... expect : call actual : never call - unsatisfi activ .. / .. /src/tests/api_tests.cpp:1496 : failur actual function call count n't match expect_cal ( * schedul , updat ( _ , _ ) ) ... expect : call twice actual : never call - unsatisfi activ i0708 11:43:03.572598 1927435008 master.cpp:1218 ] master termin i0708 11:43:03.572844 352055296 hierarchical.cpp:510 ] remov agent e055d60c-05ff-487e-82da-d0a43e52605c-s0 [ fail ] contenttype/masterapitest.subscribe/0 , getparam ( ) = application/x-protobuf ( 15105 ms ) { noformat } on cento 7 { noformat } [ run ] contenttype/masterapitest.subscribe/0 i0708 15:42:16.042171 29138 cluster.cpp:155 ] creat default 'local ' author i0708 15:42:16.154358 29138 leveldb.cpp:174 ] open db 111.818825m i0708 15:42:16.197175 29138 leveldb.cpp:181 ] compact db 42.714984m i0708 15:42:16.197293 29138 leveldb.cpp:196 ] creat db iter 32582n i0708 15:42:16.197324 29138 leveldb.cpp:202 ] seek begin db 4050n i0708 15:42:16.197343 29138 leveldb.cpp:271 ] iter 0 key db 538n i0708 15:42:16.197417 29138 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0708 15:42:16.198655 29157 recover.cpp:451 ] start replica recoveri i0708 15:42:16.199364 29161 recover.cpp:477 ] replica empti statu i0708 15:42:16.200865 29161 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 16431 ) @ 172.17.0.3:34502 i0708 15:42:16.201282 29158 recover.cpp:197 ] receiv recov respons replica empti statu i0708 15:42:16.203222 29160 recover.cpp:568 ] updat replica statu start i0708 15:42:16.204633 29158 master.cpp:382 ] master 2aea5b7f-ec9f-4fda-8f34-877d8adf064f ( 0382d073a49a ) start 172.17.0.3:34502 i0708 15:42:16.204675 29158 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/lu916i/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-1.1.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/lu916i/mast '' -- zk_session_timeout= '' 10sec '' i0708 15:42:16.205265 29158 master.cpp:434 ] master allow authent framework regist i0708 15:42:16.205283 29158 master.cpp:448 ] master allow authent agent regist i0708 15:42:16.205294 29158 master.cpp:461 ] master allow authent http framework regist i0708 15:42:16.205307 29158 credentials.hpp:37 ] load credenti authent '/tmp/lu916i/credentials' i0708 15:42:16.205705 29158 master.cpp:506 ] use default 'crammd5 ' authent i0708 15:42:16.205940 29158 master.cpp:578 ] use default 'basic ' http authent i0708 15:42:16.206192 29158 master.cpp:658 ] use default 'basic ' http framework authent i0708 15:42:16.206374 29158 master.cpp:705 ] author enabl i0708 15:42:16.206866 29172 hierarchical.cpp:151 ] initi hierarch alloc process i0708 15:42:16.207018 29172 whitelist_watcher.cpp:77 ] no whitelist given i0708 15:42:16.210026 29165 master.cpp:1973 ] the newli elect leader master @ 172.17.0.3:34502 id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f i0708 15:42:16.210187 29165 master.cpp:1986 ] elect lead master ! i0708 15:42:16.210330 29165 master.cpp:1673 ] recov registrar i0708 15:42:16.210577 29171 registrar.cpp:332 ] recov registrar i0708 15:42:16.239378 29160 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 35.540287m i0708 15:42:16.239485 29160 replica.cpp:320 ] persist replica statu start i0708 15:42:16.239938 29161 recover.cpp:477 ] replica start statu i0708 15:42:16.242017 29165 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 16434 ) @ 172.17.0.3:34502 i0708 15:42:16.242527 29167 recover.cpp:197 ] receiv recov respons replica start statu i0708 15:42:16.243140 29167 recover.cpp:568 ] updat replica statu vote i0708 15:42:16.281746 29167 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 38.318978m i0708 15:42:16.281828 29167 replica.cpp:320 ] persist replica statu vote i0708 15:42:16.282094 29170 recover.cpp:582 ] success join paxo group i0708 15:42:16.282440 29170 recover.cpp:466 ] recov process termin i0708 15:42:16.283365 29170 log.cpp:553 ] attempt start writer i0708 15:42:16.285605 29167 replica.cpp:493 ] replica receiv implicit promis request ( 16435 ) @ 172.17.0.3:34502 propos 1 i0708 15:42:16.315435 29167 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 29.761608m i0708 15:42:16.315528 29167 replica.cpp:342 ] persist promis 1 i0708 15:42:16.317147 29159 coordinator.cpp:238 ] coordin attempt fill miss posit i0708 15:42:16.318914 29160 replica.cpp:388 ] replica receiv explicit promis request ( 16436 ) @ 172.17.0.3:34502 posit 0 propos 2 i0708 15:42:16.348886 29160 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 29.896283m i0708 15:42:16.349161 29160 replica.cpp:712 ] persist action 0 i0708 15:42:16.350939 29170 replica.cpp:537 ] replica receiv write request posit 0 ( 16437 ) @ 172.17.0.3:34502 i0708 15:42:16.351029 29170 leveldb.cpp:436 ] read posit leveldb took 42967n i0708 15:42:16.382378 29170 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 31.28917m i0708 15:42:16.382464 29170 replica.cpp:712 ] persist action 0 i0708 15:42:16.383646 29169 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0708 15:42:16.415894 29169 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 32.189511m i0708 15:42:16.416015 29169 replica.cpp:712 ] persist action 0 i0708 15:42:16.416056 29169 replica.cpp:697 ] replica learn nop action posit 0 i0708 15:42:16.417312 29168 log.cpp:569 ] writer start end posit 0 i0708 15:42:16.418628 29167 leveldb.cpp:436 ] read posit leveldb took 56748n i0708 15:42:16.420019 29165 registrar.cpp:365 ] success fetch registri ( 0b ) 209.31712m i0708 15:42:16.420155 29165 registrar.cpp:464 ] appli 1 oper 30566n ; attempt updat 'registry' i0708 15:42:16.420994 29172 log.cpp:577 ] attempt append 168 byte log i0708 15:42:16.421149 29157 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0708 15:42:16.422169 29162 replica.cpp:537 ] replica receiv write request posit 1 ( 16438 ) @ 172.17.0.3:34502 i0708 15:42:16.457743 29162 leveldb.cpp:341 ] persist action ( 187 byte ) leveldb took 35.505294m i0708 15:42:16.457844 29162 replica.cpp:712 ] persist action 1 i0708 15:42:16.459228 29172 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0708 15:42:16.495947 29172 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 36.653391m i0708 15:42:16.496048 29172 replica.cpp:712 ] persist action 1 i0708 15:42:16.496091 29172 replica.cpp:697 ] replica learn append action posit 1 i0708 15:42:16.497947 29172 registrar.cpp:509 ] success updat 'registri ' 77.703936m i0708 15:42:16.498132 29172 registrar.cpp:395 ] success recov registrar i0708 15:42:16.498169 29171 log.cpp:596 ] attempt truncat log 1 i0708 15:42:16.498294 29162 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0708 15:42:16.498668 29171 master.cpp:1781 ] recov 0 agent registri ( 129b ) ; allow 10min agent re-regist i0708 15:42:16.498919 29162 hierarchical.cpp:178 ] skip recoveri hierarch alloc : noth recov i0708 15:42:16.499577 29171 replica.cpp:537 ] replica receiv write request posit 2 ( 16439 ) @ 172.17.0.3:34502 i0708 15:42:16.521065 29171 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 21.423468m i0708 15:42:16.521160 29171 replica.cpp:712 ] persist action 2 i0708 15:42:16.522766 29171 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0708 15:42:16.546223 29171 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 23.402601m i0708 15:42:16.546380 29171 leveldb.cpp:399 ] delet ~1 key leveldb took 70830n i0708 15:42:16.546411 29171 replica.cpp:712 ] persist action 2 i0708 15:42:16.546445 29171 replica.cpp:697 ] replica learn truncat action posit 2 i0708 15:42:16.560467 29138 cluster.cpp:432 ] creat default 'local ' author i0708 15:42:16.565003 29162 slave.cpp:205 ] agent start 449 ) @ 172.17.0.3:34502 i0708 15:42:16.565520 29138 scheduler.cpp:172 ] version : 1.1.0 i0708 15:42:16.565150 29162 slave.cpp:206 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authentication_backoff_factor= '' 1sec '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/contenttype_masterapitest_subscribe_0_be660m/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/contenttype_masterapitest_subscribe_0_be660m/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' true '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /tmp/contenttype_masterapitest_subscribe_0_be660m/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-1.1.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/contenttype_masterapitest_subscribe_0_be660m '' i0708 15:42:16.566128 29162 credentials.hpp:86 ] load credenti authent '/tmp/contenttype_masterapitest_subscribe_0_be660m/credential' i0708 15:42:16.566423 29162 slave.cpp:343 ] agent use credenti : test-princip i0708 15:42:16.566520 29171 scheduler.cpp:461 ] new master detect master @ 172.17.0.3:34502 i0708 15:42:16.566543 29162 credentials.hpp:37 ] load credenti authent '/tmp/contenttype_masterapitest_subscribe_0_be660m/http_credentials' i0708 15:42:16.566557 29171 scheduler.cpp:470 ] wait 0n initi re- ( connect ) attempt master i0708 15:42:16.566838 29162 slave.cpp:395 ] use default 'basic ' http authent i0708 15:42:16.568023 29162 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0708 15:42:16.568527 29162 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0708 15:42:16.569443 29162 slave.cpp:594 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0708 15:42:16.569535 29162 slave.cpp:602 ] agent attribut : [ ] i0708 15:42:16.569552 29162 slave.cpp:607 ] agent hostnam : 0382d073a49a i0708 15:42:16.571897 29165 state.cpp:57 ] recov state '/tmp/contenttype_masterapitest_subscribe_0_be660m/meta' i0708 15:42:16.572376 29165 status_update_manager.cpp:200 ] recov statu updat manag i0708 15:42:16.572638 29165 slave.cpp:4856 ] finish recoveri i0708 15:42:16.573194 29165 slave.cpp:5028 ] queri resourc estim oversubscrib resourc i0708 15:42:16.574082 29165 slave.cpp:969 ] new master detect master @ 172.17.0.3:34502 i0708 15:42:16.574111 29165 slave.cpp:1028 ] authent master master @ 172.17.0.3:34502 i0708 15:42:16.574174 29165 slave.cpp:1039 ] use default cram-md5 authenticate i0708 15:42:16.574213 29162 status_update_manager.cpp:174 ] paus send statu updat i0708 15:42:16.574323 29165 slave.cpp:1001 ] detect new master i0708 15:42:16.574525 29165 authenticatee.cpp:121 ] creat new client sasl connect i0708 15:42:16.574851 29160 slave.cpp:5042 ] receiv oversubscrib resourc resourc estim i0708 15:42:16.575621 29164 scheduler.cpp:349 ] connect master http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.577546 29164 scheduler.cpp:231 ] send subscrib call http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.579020 29168 master.cpp:6006 ] authent slave ( 449 ) @ 172.17.0.3:34502 i0708 15:42:16.579133 29165 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 926 ) @ 172.17.0.3:34502 i0708 15:42:16.579236 29168 process.cpp:3322 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0708 15:42:16.579448 29157 authenticator.cpp:98 ] creat new server sasl connect i0708 15:42:16.579684 29165 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 i0708 15:42:16.579722 29165 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' i0708 15:42:16.579831 29165 authenticator.cpp:204 ] receiv sasl authent start i0708 15:42:16.579910 29165 authenticator.cpp:326 ] authent requir step i0708 15:42:16.580013 29165 authenticatee.cpp:259 ] receiv sasl authent step i0708 15:42:16.580111 29165 authenticator.cpp:232 ] receiv sasl authent step i0708 15:42:16.580143 29165 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : '0382d073a49a ' server fqdn : '0382d073a49a ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0708 15:42:16.580157 29165 auxprop.cpp:181 ] look auxiliari properti ' * userpassword' i0708 15:42:16.580196 29165 auxprop.cpp:181 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0708 15:42:16.580227 29165 auxprop.cpp:109 ] request lookup properti user : 'test-princip ' realm : '0382d073a49a ' server fqdn : '0382d073a49a ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0708 15:42:16.580240 29165 auxprop.cpp:131 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0708 15:42:16.580251 29165 auxprop.cpp:131 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0708 15:42:16.580271 29165 authenticator.cpp:318 ] authent success i0708 15:42:16.580420 29165 authenticatee.cpp:299 ] authent success i0708 15:42:16.580525 29165 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 926 ) @ 172.17.0.3:34502 i0708 15:42:16.580840 29165 slave.cpp:1123 ] success authent master master @ 172.17.0.3:34502 i0708 15:42:16.581131 29165 slave.cpp:1529 ] will retri registr 814473n necessari i0708 15:42:16.581560 29168 master.cpp:6036 ] success authent princip 'test-princip ' slave ( 449 ) @ 172.17.0.3:34502 i0708 15:42:16.581795 29168 master.cpp:4676 ] regist agent slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 i0708 15:42:16.583050 29162 registrar.cpp:464 ] appli 1 oper 131284n ; attempt updat 'registry' i0708 15:42:16.584233 29170 slave.cpp:1529 ] will retri registr 27.411836m necessari i0708 15:42:16.584384 29158 master.cpp:4664 ] ignor regist agent messag slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) admiss alreadi progress i0708 15:42:16.585019 29168 log.cpp:577 ] attempt append 337 byte log i0708 15:42:16.585113 29162 http.cpp:381 ] http post /master/api/v1/schedul 172.17.0.3:33142 i0708 15:42:16.585156 29159 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0708 15:42:16.585417 29162 master.cpp:2272 ] receiv subscript request http framework 'default' i0708 15:42:16.585486 29162 master.cpp:2012 ] author framework princip 'test-princip ' receiv offer role ' * ' i0708 15:42:16.586509 29159 replica.cpp:537 ] replica receiv write request posit 3 ( 16448 ) @ 172.17.0.3:34502 i0708 15:42:16.587302 29168 master.cpp:2370 ] subscrib framework 'default ' checkpoint disabl capabl [ ] i0708 15:42:16.588059 29170 master.hpp:2010 ] send heartbeat 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.588745 29168 hierarchical.cpp:271 ] ad framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.588819 29168 hierarchical.cpp:1537 ] no alloc perform i0708 15:42:16.588851 29168 hierarchical.cpp:1632 ] no invers offer send ! i0708 15:42:16.588910 29168 hierarchical.cpp:1172 ] perform alloc 0 agent 138375n i0708 15:42:16.593391 29162 scheduler.cpp:662 ] enqueu event subscrib receiv http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.594115 29162 scheduler.cpp:662 ] enqueu event heartbeat receiv http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.612622 29162 slave.cpp:1529 ] will retri registr 35.186867m necessari i0708 15:42:16.613113 29169 master.cpp:4664 ] ignor regist agent messag slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) admiss alreadi progress i0708 15:42:16.621047 29159 leveldb.cpp:341 ] persist action ( 356 byte ) leveldb took 34.409256m i0708 15:42:16.621134 29159 replica.cpp:712 ] persist action 3 i0708 15:42:16.622661 29159 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0708 15:42:16.646806 29159 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 24.085822m i0708 15:42:16.646906 29159 replica.cpp:712 ] persist action 3 i0708 15:42:16.646986 29159 replica.cpp:697 ] replica learn append action posit 3 i0708 15:42:16.649273 29157 registrar.cpp:509 ] success updat 'registri ' 66.121984m i0708 15:42:16.649538 29167 slave.cpp:1529 ] will retri registr 111.475397m necessari i0708 15:42:16.649603 29158 log.cpp:596 ] attempt truncat log 3 i0708 15:42:16.649811 29157 master.cpp:4664 ] ignor regist agent messag slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) admiss alreadi progress i0708 15:42:16.650069 29160 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0708 15:42:16.650713 29160 slave.cpp:3760 ] receiv ping slave-observ ( 404 ) @ 172.17.0.3:34502 i0708 15:42:16.650879 29160 slave.cpp:1169 ] regist master master @ 172.17.0.3:34502 ; given agent id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 i0708 15:42:16.651007 29160 fetcher.cpp:86 ] clear fetcher cach i0708 15:42:16.651065 29158 hierarchical.cpp:478 ] ad agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 ( 0382d073a49a ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0708 15:42:16.651480 29160 slave.cpp:1192 ] checkpoint slaveinfo '/tmp/contenttype_masterapitest_subscribe_0_be660m/meta/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/slave.info' i0708 15:42:16.651499 29166 status_update_manager.cpp:181 ] resum send statu updat i0708 15:42:16.650825 29157 master.cpp:4745 ] regist agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0708 15:42:16.651847 29158 hierarchical.cpp:1632 ] no invers offer send ! i0708 15:42:16.652433 29158 hierarchical.cpp:1195 ] perform alloc agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 1.317746m i0708 15:42:16.651897 29172 replica.cpp:537 ] replica receiv write request posit 4 ( 16450 ) @ 172.17.0.3:34502 i0708 15:42:16.653264 29157 master.cpp:5835 ] send 1 offer framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.654682 29160 slave.cpp:1229 ] forward total oversubscrib resourc i0708 15:42:16.656188 29165 master.cpp:5128 ] receiv updat agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) total oversubscrib resourc i0708 15:42:16.656200 29164 scheduler.cpp:662 ] enqueu event offer receiv http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.656708 29159 hierarchical.cpp:542 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 ( 0382d073a49a ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) i0708 15:42:16.657385 29159 hierarchical.cpp:1537 ] no alloc perform i0708 15:42:16.657438 29159 hierarchical.cpp:1632 ] no invers offer send ! i0708 15:42:16.657519 29159 hierarchical.cpp:1195 ] perform alloc agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 516462n i0708 15:42:16.660909 29163 process.cpp:3322 ] handl http event process 'master ' path : '/master/api/v1' i0708 15:42:16.661958 29163 http.cpp:381 ] http post /master/api/v1 172.17.0.3:33143 i0708 15:42:16.662125 29163 http.cpp:484 ] process call subscrib i0708 15:42:16.663280 29164 master.cpp:7599 ] ad subscrib : 726edf8d-ad3d-4d08-9243-de3dc2df5f4a list activ subscrib i0708 15:42:16.671409 29161 scheduler.cpp:231 ] send accept call http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.672615 29165 process.cpp:3322 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0708 15:42:16.676375 29169 http.cpp:381 ] http post /master/api/v1/schedul 172.17.0.3:33141 i0708 15:42:16.677199 29169 master.cpp:3468 ] process accept call offer : [ 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-o0 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.677291 29169 master.cpp:3106 ] author framework princip 'test-princip ' launch task d8bd1ba3-055a-4420-820c-8e85fdde7c08 w0708 15:42:16.679435 29169 validation.cpp:650 ] executor default task d8bd1ba3-055a-4420-820c-8e85fdde7c08 use less cpu ( none ) minimum requir ( 0.01 ) . pleas updat executor , mandatori futur releas . w0708 15:42:16.679492 29169 validation.cpp:662 ] executor default task d8bd1ba3-055a-4420-820c-8e85fdde7c08 use less memori ( none ) minimum requir ( 32mb ) . pleas updat executor , mandatori futur releas . i0708 15:42:16.680003 29169 master.cpp:7573 ] notifi activ subscrib task_ad event i0708 15:42:16.680454 29172 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 27.707387m i0708 15:42:16.680685 29172 replica.cpp:712 ] persist action 4 i0708 15:42:16.681685 29168 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0708 15:42:16.680449 29169 master.cpp:7565 ] ad task d8bd1ba3-055a-4420-820c-8e85fdde7c08 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 ( 0382d073a49a ) i0708 15:42:16.682688 29169 master.cpp:3957 ] launch task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.683289 29171 slave.cpp:1569 ] got assign task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.683903 29171 slave.cpp:1688 ] launch task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.684563 29171 paths.cpp:528 ] tri chown '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3 ' user 'mesos' i0708 15:42:16.699834 29171 slave.cpp:5748 ] launch executor default framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 resourc work directori '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' i0708 15:42:16.702018 29171 executor.cpp:188 ] version : 1.1.0 i0708 15:42:16.702541 29171 slave.cpp:1914 ] queu task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08 ' executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.702777 29171 slave.cpp:922 ] success attach file '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' i0708 15:42:16.704201 29169 executor.cpp:389 ] connect agent i0708 15:42:16.704911 29159 executor.cpp:290 ] send subscrib call http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.706003 29170 process.cpp:3322 ] handl http event process 'slave ( 449 ) ' path : '/slave ( 449 ) /api/v1/executor' i0708 15:42:16.706897 29157 http.cpp:270 ] http post /slave ( 449 ) /api/v1/executor 172.17.0.3:33144 i0708 15:42:16.707108 29157 slave.cpp:2735 ] receiv subscrib request http executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.707819 29168 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 26.083403m i0708 15:42:16.707986 29168 leveldb.cpp:399 ] delet ~2 key leveldb took 117548n i0708 15:42:16.708031 29168 replica.cpp:712 ] persist action 4 i0708 15:42:16.708076 29168 replica.cpp:697 ] replica learn truncat action posit 4 i0708 15:42:16.708082 29157 slave.cpp:2079 ] send queu task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08 ' executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( via http ) i0708 15:42:16.710268 29163 executor.cpp:707 ] enqueu event subscrib receiv http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.712131 29169 executor.cpp:707 ] enqueu event launch receiv http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.713174 29172 executor.cpp:290 ] send updat call http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.713984 29170 process.cpp:3322 ] handl http event process 'slave ( 449 ) ' path : '/slave ( 449 ) /api/v1/executor' i0708 15:42:16.714614 29170 http.cpp:270 ] http post /slave ( 449 ) /api/v1/executor 172.17.0.3:33145 i0708 15:42:16.714753 29170 slave.cpp:3285 ] handl statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.715451 29172 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.715498 29172 status_update_manager.cpp:497 ] creat statusupd stream task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.715996 29172 status_update_manager.cpp:374 ] forward updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent i0708 15:42:16.716584 29172 slave.cpp:3678 ] forward updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 master @ 172.17.0.3:34502 i0708 15:42:16.716956 29172 slave.cpp:3572 ] statu updat manag success handl statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.717159 29171 master.cpp:5273 ] statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.717265 29171 master.cpp:5321 ] forward statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.718080 29168 executor.cpp:707 ] enqueu event acknowledg receiv http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.718299 29171 master.cpp:7573 ] notifi activ subscrib task_upd event i0708 15:42:16.719683 29171 master.cpp:6959 ] updat state task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( latest state : task_run , statu updat state : task_run ) i0708 15:42:16.720386 29171 scheduler.cpp:662 ] enqueu event updat receiv http : //172.17.0.3:34502/master/api/v1/schedul i0708 15:42:16.726471 29164 hierarchical.cpp:1537 ] no alloc perform w0708 15:42:16.726742 29163 status_update_manager.cpp:475 ] resend statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.726840 29163 status_update_manager.cpp:374 ] forward updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent i0708 15:42:16.727401 29163 slave.cpp:3678 ] forward updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 master @ 172.17.0.3:34502 i0708 15:42:16.727880 29163 master.cpp:5273 ] statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.728035 29163 master.cpp:5321 ] forward statu updat task_run ( uuid : a93b73fb-5289-4aec-80e9-1cad44bec619 ) task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.728570 29163 master.cpp:6959 ] updat state task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( latest state : task_run , statu updat state : task_run ) i0708 15:42:16.728003 29164 hierarchical.cpp:1632 ] no invers offer send ! i0708 15:42:16.730080 29164 hierarchical.cpp:1172 ] perform alloc 1 agent 3.858387m i0708 15:42:16.751055 29160 master.cpp:1410 ] framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) disconnect i0708 15:42:16.751116 29160 master.cpp:2851 ] disconnect framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.751149 29160 master.cpp:2875 ] deactiv framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.751242 29160 master.cpp:1423 ] give framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) 0n failov i0708 15:42:16.751602 29160 hierarchical.cpp:382 ] deactiv framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.755091 29157 master.cpp:5687 ] framework failov timeout , remov framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.755164 29157 master.cpp:6422 ] remov framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( default ) i0708 15:42:16.755425 29157 master.cpp:7573 ] notifi activ subscrib task_upd event i0708 15:42:16.755795 29157 master.cpp:6959 ] updat state task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( latest state : task_kil , statu updat state : task_kil ) i0708 15:42:16.756032 29166 slave.cpp:2292 ] ask shut framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 master @ 172.17.0.3:34502 i0708 15:42:16.756093 29166 slave.cpp:2317 ] shut framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.756172 29166 slave.cpp:4481 ] shut executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( via http ) i0708 15:42:16.757699 29157 master.cpp:7025 ] remov task d8bd1ba3-055a-4420-820c-8e85fdde7c08 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.758779 29161 hierarchical.cpp:924 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.759784 29161 executor.cpp:707 ] enqueu event shutdown receiv http : //172.17.0.3:34502/slave ( 449 ) /api/v1/executor i0708 15:42:16.761289 29157 master.cpp:7054 ] remov executor 'default ' resourc framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.763124 29157 hierarchical.cpp:333 ] remov framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.777029 29163 slave.cpp:4163 ] executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 exit statu 0 i0708 15:42:16.777218 29163 slave.cpp:4267 ] clean executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ( via http ) i0708 15:42:16.777710 29163 slave.cpp:4355 ] clean framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.778026 29163 gc.cpp:55 ] schedul '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3 ' gc 6.99999163714074day futur w0708 15:42:16.778028 29167 master.cpp:5369 ] ignor unknown exit executor 'default ' framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.778195 29157 status_update_manager.cpp:282 ] close statu updat stream framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.778239 29163 gc.cpp:55 ] schedul '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default ' gc 6.99999163714074day futur i0708 15:42:16.778257 29157 status_update_manager.cpp:528 ] clean statu updat stream task d8bd1ba3-055a-4420-820c-8e85fdde7c08 framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 i0708 15:42:16.778327 29163 gc.cpp:55 ] schedul '/tmp/contenttype_masterapitest_subscribe_0_be660m/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 ' gc 6.99999163714074day futur i0708 15:42:16.797328 29138 slave.cpp:841 ] agent termin i0708 15:42:16.799114 29165 master.cpp:1371 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) disconnect i0708 15:42:16.800149 29165 master.cpp:2910 ] disconnect agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.800727 29165 master.cpp:2929 ] deactiv agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 slave ( 449 ) @ 172.17.0.3:34502 ( 0382d073a49a ) i0708 15:42:16.801389 29165 hierarchical.cpp:571 ] agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 deactiv .. / .. /src/tests/api_tests.cpp:1496 : failur actual function call count n't match expect_cal ( * schedul , updat ( _ , _ ) ) ... expect : call twice actual : call - unsatisfi activ i0708 15:42:16.806820 29138 master.cpp:1218 ] master termin i0708 15:42:16.807718 29160 hierarchical.cpp:510 ] remov agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-s0 [ fail ] contenttype/masterapitest.subscribe/0 , getparam ( ) = application/x-protobuf ( 780 ms ) { noformat }",MESOS-5812,3.0
"cni isol prepar network relat /etc/ * file contain use host mode specifi contain imag . current , cni isol ignor contain want join host network ( i.e. , specifi networkinfo ) . howev , contain specifi contain imag , need make sure access host /etc/ * file . we perform bind mount contain . thi also docker contain run host mode .",MESOS-5806,5.0
miss licens inform bundl nvml header see summari,MESOS-5766,1.0
"when start agent ` -- resourc ` , gpu resourc fraction so far , gpu resourc fraction , integ valu allow . but start agent { { \-\-resources='gpu:1.2 ' } } , also work without warn error . and webui gpu resourc ` 1.2 ` .",MESOS-5742,1.0
"command executor health check work task specifi contain imag . sinc launch task pivot_root , longer access mesos-health-check binari . the solut refactor health check librari ( libprocess ) depend underli filesystem . one note strive keep command executor task mount namespac meso cli tool need find mount namespac task . it need find correspond pid executor . thi statement * arguabl * , see comment .",MESOS-5727,5.0
"ssl downgrad support leak socket close_wait statu repro step : 1 ) start master : { code } bin/mesos-master.sh -- work_dir=/tmp/mast { code } 2 ) start agent ssl downgrad enabl : { code } # taken http : //mesos.apache.org/documentation/latest/ssl/ openssl genrsa -des3 -f4 -passout pass : some_password -out key.pem 4096 openssl req -new -x509 -passin pass : some_password -day 365 -key key.pem -out cert.pem ssl_key_file=key.pem ssl_cert_file=cert.pem ssl_enabled=tru ssl_support_downgrade=tru sudo -e bin/mesos-agent.sh -- master=localhost:5050 -- work_dir=/tmp/ag { code } 3 ) start framework launch lot executor , one anoth : { code } sudo src/balloon-framework -- master=localhost:5050 -- task_memory=64mb -- task_memory_usage_limit=256mb -- long_run { code } 4 ) check fd , repeatedli { code } sudo lsof -i | grep meso | grep close_wait | wc -l { code } the number socket { { close_wait } } increas linearli number launch executor .",MESOS-5691,5.0
"port map isol may fail 'isol ' method . port map isol may return failur isol method , symlink network namespac handl use containerid alreadi exist . we overwrit symlink exist . thi affect coupl test failur : { noformat } portmappingisolatortest.root_toomanycontain portmappingisolatortest.root_containerarpextern portmappingisolatortest.root_containercmpintern portmappingisolatortest.root_nc_hosttocontainertcp { noformat } here exampl failur test log : { noformat } [ 00:28:37 ] : [ step 10/10 ] [ run ] portmappingisolatortest.root_toomanycontain [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.046444 24846 port_mapping_tests.cpp:229 ] use eth0 public interfac [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.046728 24846 port_mapping_tests.cpp:237 ] use lo loopback interfac [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.058758 24846 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; ephemeral_port : [ 30001-30999 ] ; port : [ 31000-32000 ] [ 00:28:37 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.059711 24846 port_mapping.cpp:1557 ] use eth0 public interfac [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.059998 24846 port_mapping.cpp:1582 ] use lo loopback interfac [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061126 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061172 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061206 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_wmem = '4096 16384 4194304' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061256 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_synack_retri = ' 5' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061297 24846 port_mapping.cpp:1869 ] /proc/sys/net/core/rmem_max = '212992' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061331 24846 port_mapping.cpp:1869 ] /proc/sys/net/core/somaxconn = '128' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061360 24846 port_mapping.cpp:1869 ] /proc/sys/net/core/wmem_max = '212992' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061390 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_rmem = '4096 87380 6291456' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061419 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_tim = '7200' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061450 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061480 24846 port_mapping.cpp:1869 ] /proc/sys/net/core/netdev_max_backlog = '1000' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061511 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061540 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_prob = ' 9' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061569 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.061599 24846 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_retries2 = '15' [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.069964 24846 linux_launcher.cpp:101 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.070144 24846 resources.cpp:572 ] pars resourc json fail : port : [ 31000-31499 ] [ 00:28:37 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.070677 24867 port_mapping.cpp:2512 ] use non-ephemer port { [ 31000,31500 ) } ephemer port [ 30208,30720 ) contain container1 executor `` [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.071688 24846 linux_launcher.cpp:281 ] clone child process flag = clone_newn | clone_newnet [ 00:28:37 ] w : [ step 10/10 ] i0606 00:28:37.084079 24863 port_mapping.cpp:2576 ] bind mount '/proc/11997/ns/net ' '/run/netns/11997 ' contain container1 [ 00:28:37 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/port_mapping_tests.cpp:1438 : failur [ 00:28:37 ] : [ step 10/10 ] ( isolator.get ( ) - > isol ( containerid1 , pid.get ( ) ) ) .failur ( ) : fail symlink network namespac handl '/var/run/mesos/netns/container1 ' - > '/run/netns/11997 ' : file exist [ 00:28:37 ] : [ step 10/10 ] [ fail ] portmappingisolatortest.root_toomanycontain ( 57 ms ) { noformat }",MESOS-5674,3.0
"port map isol may caus segfault bind mount root exist . a check need port map isol bind mount root . otherwis , non-exist port-map bind mount root may caus segment fault case . here test log : { noformat } [ 00:57:42 ] : [ step 10/10 ] [ -- -- -- -- -- ] 11 test portmappingisolatortest [ 00:57:42 ] : [ step 10/10 ] [ run ] portmappingisolatortest.root_nc_containertocontainertcp [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.723029 24841 port_mapping_tests.cpp:229 ] use eth0 public interfac [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.723348 24841 port_mapping_tests.cpp:237 ] use lo loopback interfac [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.735090 24841 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; ephemeral_port : [ 30001-30999 ] ; port : [ 31000-32000 ] [ 00:57:42 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.736006 24841 port_mapping.cpp:1557 ] use eth0 public interfac [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.736331 24841 port_mapping.cpp:1582 ] use lo loopback interfac [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737501 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737545 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737578 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_wmem = '4096 16384 4194304' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737608 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_synack_retri = ' 5' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737637 24841 port_mapping.cpp:1869 ] /proc/sys/net/core/rmem_max = '212992' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737666 24841 port_mapping.cpp:1869 ] /proc/sys/net/core/somaxconn = '128' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737694 24841 port_mapping.cpp:1869 ] /proc/sys/net/core/wmem_max = '212992' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737720 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_rmem = '4096 87380 6291456' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737746 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_tim = '7200' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737772 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737798 24841 port_mapping.cpp:1869 ] /proc/sys/net/core/netdev_max_backlog = '1000' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737828 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737854 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_keepalive_prob = ' 9' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737879 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512' [ 00:57:42 ] w : [ step 10/10 ] i0604 00:57:42.737905 24841 port_mapping.cpp:1869 ] /proc/sys/net/ipv4/tcp_retries2 = '15' [ 00:57:42 ] w : [ step 10/10 ] f0604 00:57:42.737968 24841 port_mapping_tests.cpp:448 ] check_som ( isol ) : fail get realpath bind mount root '/var/run/netn ' : not found [ 00:57:42 ] w : [ step 10/10 ] * * * check failur stack trace : * * * [ 00:57:42 ] w : [ step 10/10 ] @ 0x7f8bd52583d2 googl : :logmessag : :fail ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x7f8bd525832b googl : :logmessag : :sendtolog ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x7f8bd5257d21 googl : :logmessag : :flush ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x7f8bd525ab92 googl : :logmessagefat : :~logmessagefat ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0xa62171 _checkfat : :~_checkfat ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x1931b17 meso : :intern : :test : :portmappingisolatortest_root_nc_containertocontainertcp_test : :testbodi ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19e17b6 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19dc864 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19bd2ae test : :test : :run ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19bda66 test : :testinfo : :run ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19be0b7 test : :testcas : :run ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19c4bf5 test : :intern : :unittestimpl : :runalltest ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19e247d test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19dd3a4 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0x19c38d1 test : :unittest : :run ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0xfd28cb run_all_test ( ) [ 00:57:42 ] w : [ step 10/10 ] @ 0xfd24b1 main [ 00:57:42 ] w : [ step 10/10 ] @ 0x7f8bceb89580 __libc_start_main [ 00:57:42 ] w : [ step 10/10 ] @ 0xa607c9 _start [ 00:57:43 ] w : [ step 10/10 ] /mnt/teamcity/temp/agenttmp/custom_script659125926639545396 : line 3 : 24841 abort ( core dump ) glog_v=1 ./bin/mesos-tests.sh -- verbos -- gtest_filter= '' $ gtest_filt '' [ 00:57:43 ] w : [ step 10/10 ] process exit code 134 { noformat }",MESOS-5673,3.0
"memorypressuremesostest.cgroups_root_statist flaki . { noformat } [ 00:48:29 ] : [ step 10/10 ] [ run ] memorypressuremesostest.cgroups_root_statist [ 00:48:29 ] w : [ step 10/10 ] 1+0 record [ 00:48:29 ] w : [ step 10/10 ] 1+0 record [ 00:48:29 ] w : [ step 10/10 ] 1048576 byte ( 1.0 mb ) copi , 0.000517638 , 2.0 gb/ [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.000998 25413 cluster.cpp:155 ] creat default 'local ' author [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.020459 25413 leveldb.cpp:174 ] open db 19.338463m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.022897 25413 leveldb.cpp:181 ] compact db 2.416906m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.022919 25413 leveldb.cpp:196 ] creat db iter 4037n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.022927 25413 leveldb.cpp:202 ] seek begin db 769n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.022932 25413 leveldb.cpp:271 ] iter 0 key db 390n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.022944 25413 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.023272 25432 recover.cpp:451 ] start replica recoveri [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.023425 25434 recover.cpp:477 ] replica empti statu [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.023748 25434 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 19361 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.023849 25429 recover.cpp:197 ] receiv recov respons replica empti statu [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024019 25435 recover.cpp:568 ] updat replica statu start [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024338 25432 master.cpp:382 ] master 0e92ffa4-4f26-4cea-84d3-9c67612de1bd ( ip-172-30-2-56.mesosphere.io ) start 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024348 25432 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/jbjy5p/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/jbjy5p/mast '' -- zk_session_timeout= '' 10sec '' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024502 25432 master.cpp:434 ] master allow authent framework regist [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024508 25432 master.cpp:448 ] master allow authent agent regist [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024513 25432 master.cpp:461 ] master allow authent http framework regist [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024516 25432 credentials.hpp:37 ] load credenti authent '/tmp/jbjy5p/credentials' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024603 25432 master.cpp:506 ] use default 'crammd5 ' authent [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024644 25432 master.cpp:578 ] use default 'basic ' http authent [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024701 25432 master.cpp:658 ] use default 'basic ' http framework authent [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024770 25432 master.cpp:705 ] author enabl [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024883 25435 whitelist_watcher.cpp:77 ] no whitelist given [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.024885 25434 hierarchical.cpp:142 ] initi hierarch alloc process [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.025539 25433 master.cpp:1969 ] the newli elect leader master @ 172.30.2.56:53790 id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.025555 25433 master.cpp:1982 ] elect lead master ! [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.025560 25433 master.cpp:1669 ] recov registrar [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.025611 25432 registrar.cpp:332 ] recov registrar [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.026397 25431 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.288187m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.026438 25431 replica.cpp:320 ] persist replica statu start [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.026486 25431 recover.cpp:477 ] replica start statu [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.026793 25432 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 19364 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.026897 25429 recover.cpp:197 ] receiv recov respons replica start statu [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.027031 25428 recover.cpp:568 ] updat replica statu vote [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.028960 25432 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.874668m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.028975 25432 replica.cpp:320 ] persist replica statu vote [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.029007 25432 recover.cpp:582 ] success join paxo group [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.029047 25432 recover.cpp:466 ] recov process termin [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.029209 25430 log.cpp:553 ] attempt start writer [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.029614 25429 replica.cpp:493 ] replica receiv implicit promis request ( 19365 ) @ 172.30.2.56:53790 propos 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.031486 25429 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.850474m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.031502 25429 replica.cpp:342 ] persist promis 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.031726 25431 coordinator.cpp:238 ] coordin attempt fill miss posit [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.032245 25428 replica.cpp:388 ] replica receiv explicit promis request ( 19366 ) @ 172.30.2.56:53790 posit 0 propos 2 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.034101 25428 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.831441m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.034117 25428 replica.cpp:712 ] persist action 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.034561 25433 replica.cpp:537 ] replica receiv write request posit 0 ( 19367 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.034589 25433 leveldb.cpp:436 ] read posit leveldb took 10586n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.036419 25433 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.817267m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.036434 25433 replica.cpp:712 ] persist action 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.036679 25429 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.038661 25429 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.96521m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.038677 25429 replica.cpp:712 ] persist action 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.038682 25429 replica.cpp:697 ] replica learn nop action posit 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.038839 25435 log.cpp:569 ] writer start end posit 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039198 25433 leveldb.cpp:436 ] read posit leveldb took 10572n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039412 25433 registrar.cpp:365 ] success fetch registri ( 0b ) 13.778944m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039448 25433 registrar.cpp:464 ] appli 1 oper 4778n ; attempt updat 'registry' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039643 25428 log.cpp:577 ] attempt append 205 byte log [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039696 25432 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.039945 25430 replica.cpp:537 ] replica receiv write request posit 1 ( 19368 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.041738 25430 leveldb.cpp:341 ] persist action ( 224 byte ) leveldb took 1.771112m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.041754 25430 replica.cpp:712 ] persist action 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.041977 25432 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.043805 25432 leveldb.cpp:341 ] persist action ( 226 byte ) leveldb took 1.810425m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.043820 25432 replica.cpp:712 ] persist action 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.043825 25432 replica.cpp:697 ] replica learn append action posit 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044040 25430 registrar.cpp:509 ] success updat 'registri ' 4.556032m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044100 25430 registrar.cpp:395 ] success recov registrar [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044124 25428 log.cpp:596 ] attempt truncat log 1 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044215 25431 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044244 25430 master.cpp:1777 ] recov 0 agent registri ( 166b ) ; allow 10min agent re-regist [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044317 25433 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.044497 25433 replica.cpp:537 ] replica receiv write request posit 2 ( 19369 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.046368 25433 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.851883m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.046383 25433 replica.cpp:712 ] persist action 2 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.046583 25430 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.048426 25430 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.821628m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.048455 25430 leveldb.cpp:399 ] delet ~1 key leveldb took 14283n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.048463 25430 replica.cpp:712 ] persist action 2 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.048468 25430 replica.cpp:697 ] replica learn truncat action posit 2 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.055145 25413 containerizer.cpp:203 ] use isol : cgroups/mem , filesystem/posix , network/cni [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.058349 25413 linux_launcher.cpp:101 ] use /cgroup/freez freezer hierarchi linux launcher [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069301 25413 cluster.cpp:432 ] creat default 'local ' author [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069707 25431 slave.cpp:203 ] agent start 485 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069718 25431 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572 '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' cgroups/mem '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p '' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069916 25431 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/credential' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069967 25431 slave.cpp:341 ] agent use credenti : test-princip [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.069984 25431 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/http_credentials' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070050 25431 slave.cpp:393 ] use default 'basic ' http authent [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070127 25431 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 00:48:30 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070282 25431 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070309 25431 slave.cpp:600 ] agent attribut : [ ] [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070314 25431 slave.cpp:605 ] agent hostnam : ip-172-30-2-56.mesosphere.io [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070484 25413 sched.cpp:224 ] version : 1.0.0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070667 25433 sched.cpp:328 ] new master detect master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070711 25429 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/meta' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070749 25433 sched.cpp:394 ] authent master master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070758 25433 sched.cpp:401 ] use default cram-md5 authenticate [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070793 25430 status_update_manager.cpp:200 ] recov statu updat manag [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070904 25432 authenticatee.cpp:121 ] creat new client sasl connect [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.070914 25430 containerizer.cpp:518 ] recov container [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071049 25432 master.cpp:5943 ] authent scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071105 25428 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 984 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071164 25434 authenticator.cpp:98 ] creat new server sasl connect [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071241 25434 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071254 25434 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071292 25434 authenticator.cpp:204 ] receiv sasl authent start [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071336 25434 authenticator.cpp:326 ] authent requir step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071374 25434 authenticatee.cpp:259 ] receiv sasl authent step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071553 25434 authenticator.cpp:232 ] receiv sasl authent step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071574 25434 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-56 ' server fqdn : 'ip-172-30-2-56 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071586 25434 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071594 25434 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071604 25434 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-56 ' server fqdn : 'ip-172-30-2-56 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071615 25434 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071619 25434 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071630 25434 authenticator.cpp:318 ] authent success [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071684 25428 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 984 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071687 25431 authenticatee.cpp:299 ] authent success [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071704 25434 master.cpp:5973 ] success authent princip 'test-princip ' scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071826 25431 sched.cpp:484 ] success authent master master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071841 25431 sched.cpp:800 ] send subscrib call master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071954 25431 sched.cpp:833 ] will retri registr 731.385085m necessari [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.071996 25434 master.cpp:2539 ] receiv subscrib call framework 'default ' scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072013 25434 master.cpp:2008 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072180 25430 master.cpp:2615 ] subscrib framework default checkpoint disabl capabl [ ] [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072305 25429 hierarchical.cpp:264 ] ad framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072326 25429 hierarchical.cpp:1488 ] no alloc perform [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072335 25429 hierarchical.cpp:1583 ] no invers offer send ! [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072347 25429 hierarchical.cpp:1139 ] perform alloc 0 agent 26673n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072351 25431 provisioner.cpp:253 ] provision recoveri complet [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072371 25430 sched.cpp:723 ] framework regist 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072403 25430 sched.cpp:737 ] schedul : :regist took 11852n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072587 25433 slave.cpp:4840 ] finish recoveri [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072760 25433 slave.cpp:5012 ] queri resourc estim oversubscrib resourc [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072865 25431 status_update_manager.cpp:174 ] paus send statu updat [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072893 25432 slave.cpp:962 ] new master detect master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072906 25432 slave.cpp:1024 ] authent master master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072917 25432 slave.cpp:1035 ] use default cram-md5 authenticate [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072948 25432 slave.cpp:997 ] detect new master [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072976 25432 slave.cpp:5026 ] receiv oversubscrib resourc resourc estim [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.072974 25435 authenticatee.cpp:121 ] creat new client sasl connect [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073099 25434 master.cpp:5943 ] authent slave ( 485 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073142 25434 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 985 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073213 25431 authenticator.cpp:98 ] creat new server sasl connect [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073268 25431 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073287 25431 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073320 25431 authenticator.cpp:204 ] receiv sasl authent start [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073353 25431 authenticator.cpp:326 ] authent requir step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073390 25431 authenticatee.cpp:259 ] receiv sasl authent step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073444 25435 authenticator.cpp:232 ] receiv sasl authent step [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073460 25435 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-56 ' server fqdn : 'ip-172-30-2-56 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073465 25435 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073472 25435 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073477 25435 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-56 ' server fqdn : 'ip-172-30-2-56 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073480 25435 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073484 25435 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073493 25435 authenticator.cpp:318 ] authent success [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073534 25431 authenticatee.cpp:299 ] authent success [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073561 25435 master.cpp:5973 ] success authent princip 'test-princip ' slave ( 485 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073590 25433 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 985 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073698 25431 slave.cpp:1103 ] success authent master master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073742 25431 slave.cpp:1506 ] will retri registr 17.704164m necessari [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073786 25434 master.cpp:4653 ] regist agent slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.073874 25434 registrar.cpp:464 ] appli 1 oper 9493n ; attempt updat 'registry' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.074077 25430 log.cpp:577 ] attempt append 390 byte log [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.074152 25432 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.074385 25431 replica.cpp:537 ] replica receiv write request posit 3 ( 19391 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.076269 25431 leveldb.cpp:341 ] persist action ( 409 byte ) leveldb took 1.86243m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.076284 25431 replica.cpp:712 ] persist action 3 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.076551 25434 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078383 25434 leveldb.cpp:341 ] persist action ( 411 byte ) leveldb took 1.815955m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078398 25434 replica.cpp:712 ] persist action 3 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078404 25434 replica.cpp:697 ] replica learn append action posit 3 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078703 25432 registrar.cpp:509 ] success updat 'registri ' 4.813056m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078745 25429 log.cpp:596 ] attempt truncat log 3 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078806 25433 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078909 25431 master.cpp:4721 ] regist agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078928 25428 slave.cpp:3742 ] receiv ping slave-observ ( 439 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.078991 25430 hierarchical.cpp:473 ] ad agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 ( ip-172-30-2-56.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079001 25428 slave.cpp:1147 ] regist master master @ 172.30.2.56:53790 ; given agent id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079020 25428 fetcher.cpp:86 ] clear fetcher cach [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079093 25430 hierarchical.cpp:1583 ] no invers offer send ! [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079111 25430 hierarchical.cpp:1162 ] perform alloc agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 100093n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079150 25435 replica.cpp:537 ] replica receiv write request posit 4 ( 19392 ) @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079233 25429 master.cpp:5772 ] send 1 offer framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079259 25434 status_update_manager.cpp:181 ] resum send statu updat [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079263 25428 slave.cpp:1170 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/meta/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0/slave.info' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079396 25428 slave.cpp:1207 ] forward total oversubscrib resourc [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079427 25429 sched.cpp:897 ] schedul : :resourceoff took 25735n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079448 25428 master.cpp:5066 ] receiv updat agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) total oversubscrib resourc [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079608 25413 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:256 ; disk:1024 [ 00:48:30 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079612 25434 hierarchical.cpp:531 ] agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 ( ip-172-30-2-56.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079645 25434 hierarchical.cpp:1488 ] no alloc perform [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079651 25434 hierarchical.cpp:1583 ] no invers offer send ! [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079660 25434 hierarchical.cpp:1162 ] perform alloc agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 26873n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079957 25428 master.cpp:3457 ] process accept call offer : [ 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-o0 ] agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.079979 25428 master.cpp:3095 ] author framework princip 'test-princip ' launch task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080334 25432 master.hpp:178 ] ad task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 ( ip-172-30-2-56.mesosphere.io ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080365 25432 master.cpp:3946 ] launch task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080495 25429 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 ) agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080507 25428 slave.cpp:1546 ] got assign task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080528 25429 hierarchical.cpp:928 ] framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 filter agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 5sec [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080602 25428 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 00:48:30 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080718 25428 slave.cpp:1665 ] launch task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.080747 25428 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 00:48:30 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.081048 25428 paths.cpp:528 ] tri chown '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 ' user 'root' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.082818 25435 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 3.508394m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.082859 25435 replica.cpp:712 ] persist action 4 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.083400 25435 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085247 25435 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.827229m [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085294 25435 leveldb.cpp:399 ] delet ~2 key leveldb took 30113n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085304 25435 replica.cpp:712 ] persist action 4 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085310 25435 replica.cpp:697 ] replica learn truncat action posit 4 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085690 25428 slave.cpp:5729 ] launch executor 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085846 25428 slave.cpp:1891 ] queu task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085849 25429 containerizer.cpp:777 ] start contain '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 ' executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' framework '0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.085898 25428 slave.cpp:915 ] success attach file '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.087308 25428 mem.cpp:602 ] start listen oom event contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.087671 25428 mem.cpp:722 ] start listen low memori pressur event contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.088007 25428 mem.cpp:722 ] start listen medium memori pressur event contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.088412 25428 mem.cpp:722 ] start listen critic memori pressur event contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.088750 25428 mem.cpp:353 ] updat 'memory.soft_limit_in_byt ' 288mb contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.089221 25428 mem.cpp:388 ] updat 'memory.limit_in_byt ' 288mb contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.089759 25430 containerizer.cpp:1271 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' \/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor '' } '' -- commands= '' { `` command '' : [ ] } '' -- help= '' fals '' -- pipe_read= '' 110 '' -- pipe_write= '' 111 '' -- sandbox= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_statistics_af5x0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 '' -- user= '' root '' ' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.089825 25430 linux_launcher.cpp:281 ] clone child process flag = [ 00:48:30 ] w : [ step 10/10 ] warn : log initgooglelog ( ) written stderr [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.153952 10096 process.cpp:1060 ] libprocess initi 172.30.2.56:34658 8 worker thread [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.156230 10096 logging.cpp:199 ] log stderr [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.157129 10096 exec.cpp:161 ] version : 1.0.0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.157197 10125 exec.cpp:211 ] executor start : executor ( 1 ) @ 172.30.2.56:34658 pid 10096 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.157687 25431 slave.cpp:2879 ] got registr executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 executor ( 1 ) @ 172.30.2.56:34658 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.158280 10129 exec.cpp:236 ] executor regist agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.158689 25433 mem.cpp:353 ] updat 'memory.soft_limit_in_byt ' 288mb contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.159274 25435 slave.cpp:2056 ] send queu task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 executor ( 1 ) @ 172.30.2.56:34658 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.159399 10129 exec.cpp:248 ] executor : :regist took 64598n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.159651 10128 exec.cpp:323 ] executor ask run task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.159704 10128 exec.cpp:332 ] executor : :launchtask took 30558n [ 00:48:30 ] : [ step 10/10 ] receiv subscrib event [ 00:48:30 ] : [ step 10/10 ] subscrib executor ip-172-30-2-56.mesosphere.io [ 00:48:30 ] : [ step 10/10 ] receiv launch event [ 00:48:30 ] : [ step 10/10 ] start task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb [ 00:48:30 ] : [ step 10/10 ] sh -c 'while true ; dd count=512 bs=1m if=/dev/zero of=./temp ; done' [ 00:48:30 ] : [ step 10/10 ] fork command 10134 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.163949 10126 exec.cpp:546 ] executor send statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.164324 25431 slave.cpp:3262 ] handl statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 executor ( 1 ) @ 172.30.2.56:34658 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.164824 25428 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.164849 25428 status_update_manager.cpp:497 ] creat statusupd stream task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165026 25428 status_update_manager.cpp:374 ] forward updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 agent [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165132 25433 slave.cpp:3660 ] forward updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 master @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165230 25433 slave.cpp:3554 ] statu updat manag success handl statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165251 25433 slave.cpp:3570 ] send acknowledg statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 executor ( 1 ) @ 172.30.2.56:34658 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165329 25430 master.cpp:5211 ] statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165349 25430 master.cpp:5259 ] forward statu updat task_run ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165410 25430 master.cpp:6871 ] updat state task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( latest state : task_run , statu updat state : task_run ) [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165560 10128 exec.cpp:369 ] executor receiv statu updat acknowledg 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165628 25432 sched.cpp:1005 ] schedul : :statusupd took 78385n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165765 25432 master.cpp:4365 ] process acknowledg call 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.165927 25428 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.166052 25428 slave.cpp:2648 ] statu updat manag success handl statu updat acknowledg ( uuid : 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 ) task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.583686 25428 master.cpp:4269 ] tell agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.583760 25428 slave.cpp:2086 ] ask kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.584074 10125 exec.cpp:343 ] executor ask kill task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.584121 10125 exec.cpp:352 ] executor : :killtask took 27333n [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.959868 25430 mem.cpp:625 ] oom notifi trigger contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.959900 25430 mem.cpp:644 ] oom detect contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.960912 25430 mem.cpp:685 ] memori limit exceed : request : 288mb maximum use : 288mb [ 00:48:30 ] w : [ step 10/10 ] [ 00:48:30 ] w : [ step 10/10 ] memori statist : [ 00:48:30 ] w : [ step 10/10 ] cach 297218048 [ 00:48:30 ] w : [ step 10/10 ] rss 4771840 [ 00:48:30 ] w : [ step 10/10 ] rss_huge 0 [ 00:48:30 ] w : [ step 10/10 ] mapped_fil 0 [ 00:48:30 ] w : [ step 10/10 ] pgpgin 75849 [ 00:48:30 ] w : [ step 10/10 ] pgpgout 2121 [ 00:48:30 ] w : [ step 10/10 ] pgfault 19539 [ 00:48:30 ] w : [ step 10/10 ] pgmajfault 0 [ 00:48:30 ] w : [ step 10/10 ] inactive_anon 0 [ 00:48:30 ] w : [ step 10/10 ] active_anon 4771840 [ 00:48:30 ] w : [ step 10/10 ] inactive_fil 296955904 [ 00:48:30 ] w : [ step 10/10 ] active_fil 253952 [ 00:48:30 ] w : [ step 10/10 ] unevict 0 [ 00:48:30 ] w : [ step 10/10 ] hierarchical_memory_limit 301989888 [ 00:48:30 ] w : [ step 10/10 ] total_cach 297218048 [ 00:48:30 ] w : [ step 10/10 ] total_rss 4771840 [ 00:48:30 ] w : [ step 10/10 ] total_rss_hug 0 [ 00:48:30 ] w : [ step 10/10 ] total_mapped_fil 0 [ 00:48:30 ] w : [ step 10/10 ] total_pgpgin 75849 [ 00:48:30 ] w : [ step 10/10 ] total_pgpgout 2121 [ 00:48:30 ] w : [ step 10/10 ] total_pgfault 19539 [ 00:48:30 ] w : [ step 10/10 ] total_pgmajfault 0 [ 00:48:30 ] w : [ step 10/10 ] total_inactive_anon 0 [ 00:48:30 ] w : [ step 10/10 ] total_active_anon 4771840 [ 00:48:30 ] w : [ step 10/10 ] total_inactive_fil 296873984 [ 00:48:30 ] w : [ step 10/10 ] total_active_fil 253952 [ 00:48:30 ] w : [ step 10/10 ] total_unevict 0 [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.961012 25430 containerizer.cpp:1833 ] contain 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 reach limit resourc mem ( * ) :288 termin [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.961050 25430 containerizer.cpp:1580 ] destroy contain '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' [ 00:48:30 ] w : [ step 10/10 ] i0617 00:48:30.962447 25431 cgroups.cpp:2676 ] freez cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:48:45 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/memory_pressure_tests.cpp:172 : failur [ 00:48:45 ] : [ step 10/10 ] fail wait 15sec kill [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585013 25429 master.cpp:1406 ] framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 disconnect [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585052 25429 master.cpp:2840 ] disconnect framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585072 25429 master.cpp:2864 ] deactiv framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585110 25429 master.cpp:1419 ] give framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 0n failov [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585193 25432 hierarchical.cpp:375 ] deactiv framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585247 25431 master.cpp:5624 ] framework failov timeout , remov framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585269 25431 master.cpp:6354 ] remov framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( default ) scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a @ 172.30.2.56:53790 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585325 25431 master.cpp:6871 ] updat state task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 ( latest state : task_kil , statu updat state : task_kil ) [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585352 25434 slave.cpp:2269 ] ask shut framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 master @ 172.30.2.56:53790 [ 00:48:45 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/memory_pressure_tests.cpp:128 : failur [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585373 25434 slave.cpp:2294 ] shut framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:45 ] : [ step 10/10 ] actual function call count n't match expect_cal ( sched , statusupd ( & driver , _ ) ) ... [ 00:48:45 ] : [ step 10/10 ] expect : call least twice [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585387 25434 slave.cpp:4465 ] shut executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb ' framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 executor ( 1 ) @ 172.30.2.56:34658 [ 00:48:45 ] : [ step 10/10 ] actual : call - unsatisfi activ [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585476 25429 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585492 25431 master.cpp:6937 ] remov task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) [ 00:48:45 ] w : [ step 10/10 ] i0617 00:48:45.585698 25431 hierarchical.cpp:326 ] remov framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 [ 00:49:00 ] : [ step 10/10 ] .. / .. /src/tests/cluster.cpp:551 : failur [ 00:49:00 ] : [ step 10/10 ] fail wait 15sec wait [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596581 25413 slave.cpp:834 ] agent termin [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596611 25413 slave.cpp:2269 ] ask shut framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 @ 0.0.0.0:0 [ 00:49:00 ] w : [ step 10/10 ] w0617 00:49:00.596624 25413 slave.cpp:2290 ] ignor shutdown framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 termin [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596742 25428 master.cpp:1367 ] agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) disconnect [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596761 25428 master.cpp:2899 ] disconnect agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596807 25428 master.cpp:2918 ] deactiv agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 slave ( 485 ) @ 172.30.2.56:53790 ( ip-172-30-2-56.mesosphere.io ) [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.596863 25428 hierarchical.cpp:560 ] agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 deactiv [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.598708 25413 master.cpp:1214 ] master termin [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.598848 25431 hierarchical.cpp:505 ] remov agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-s0 [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.601809 25433 cgroups.cpp:2676 ] freez cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.602758 25434 cgroups.cpp:1409 ] success froze cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 0n [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.603759 25431 cgroups.cpp:2694 ] thaw cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 [ 00:49:00 ] w : [ step 10/10 ] i0617 00:49:00.604717 25433 cgroups.cpp:1438 ] success thaw cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 0n [ 00:49:00 ] w : [ step 10/10 ] e0617 00:49:00.605662 25436 process.cpp:2050 ] fail shutdown socket fd 111 : transport endpoint connect [ 00:49:15 ] : [ step 10/10 ] .. / .. /src/tests/mesos.cpp:937 : failur [ 00:49:15 ] : [ step 10/10 ] fail wait 15sec cgroup : :destroy ( hierarchi , cgroup ) [ 00:49:15 ] : [ step 10/10 ] [ fail ] memorypressuremesostest.cgroups_root_statist ( 45618 ms ) { noformat }",MESOS-5671,2.0
"memorypressuremesostest.cgroups_root_slaverecoveri flaki . { noformat } [ 03:36:29 ] : [ step 10/10 ] [ run ] memorypressuremesostest.cgroups_root_slaverecoveri [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.461802 2797 cluster.cpp:155 ] creat default 'local ' author [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.469468 2797 leveldb.cpp:174 ] open db 7.527163m [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470188 2797 leveldb.cpp:181 ] compact db 699544n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470206 2797 leveldb.cpp:196 ] creat db iter 4293n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470211 2797 leveldb.cpp:202 ] seek begin db 535n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470216 2797 leveldb.cpp:271 ] iter 0 key db 321n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470230 2797 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470510 2815 recover.cpp:451 ] start replica recoveri [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.470592 2817 recover.cpp:477 ] replica empti statu [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471029 2813 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 19800 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471139 2816 recover.cpp:197 ] receiv recov respons replica empti statu [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471271 2818 recover.cpp:568 ] updat replica statu start [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471606 2811 master.cpp:382 ] master 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 ( ip-172-30-2-29.mesosphere.io ) start 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471619 2811 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/baxwq5/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/baxwq5/mast '' -- zk_session_timeout= '' 10sec '' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471745 2811 master.cpp:434 ] master allow authent framework regist [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471753 2811 master.cpp:448 ] master allow authent agent regist [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471757 2811 master.cpp:461 ] master allow authent http framework regist [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471761 2811 credentials.hpp:37 ] load credenti authent '/tmp/baxwq5/credentials' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471829 2811 master.cpp:506 ] use default 'crammd5 ' authent [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471868 2811 master.cpp:578 ] use default 'basic ' http authent [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471941 2811 master.cpp:658 ] use default 'basic ' http framework authent [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.471977 2811 master.cpp:705 ] author enabl [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472034 2817 hierarchical.cpp:142 ] initi hierarch alloc process [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472038 2814 whitelist_watcher.cpp:77 ] no whitelist given [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472506 2811 master.cpp:1969 ] the newli elect leader master @ 172.30.2.29:37328 id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472522 2811 master.cpp:1982 ] elect lead master ! [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472527 2811 master.cpp:1669 ] recov registrar [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.472573 2812 registrar.cpp:332 ] recov registrar [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.473511 2816 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.195002m [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.473527 2816 replica.cpp:320 ] persist replica statu start [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.473578 2816 recover.cpp:477 ] replica start statu [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.473877 2815 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 19803 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.473989 2814 recover.cpp:197 ] receiv recov respons replica start statu [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474126 2817 recover.cpp:568 ] updat replica statu vote [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474735 2811 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 547332n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474748 2811 replica.cpp:320 ] persist replica statu vote [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474783 2811 recover.cpp:582 ] success join paxo group [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474829 2811 recover.cpp:466 ] recov process termin [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.474969 2818 log.cpp:553 ] attempt start writer [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.475361 2811 replica.cpp:493 ] replica receiv implicit promis request ( 19804 ) @ 172.30.2.29:37328 propos 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.475944 2811 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 559444n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.475956 2811 replica.cpp:342 ] persist promis 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.476215 2815 coordinator.cpp:238 ] coordin attempt fill miss posit [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.476660 2816 replica.cpp:388 ] replica receiv explicit promis request ( 19805 ) @ 172.30.2.29:37328 posit 0 propos 2 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.477262 2816 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 584333n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.477273 2816 replica.cpp:712 ] persist action 0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.477699 2815 replica.cpp:537 ] replica receiv write request posit 0 ( 19806 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.477726 2815 leveldb.cpp:436 ] read posit leveldb took 8842n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.478277 2815 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 537361n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.478291 2815 replica.cpp:712 ] persist action 0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.478569 2811 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479132 2811 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 545208n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479146 2811 replica.cpp:712 ] persist action 0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479152 2811 replica.cpp:697 ] replica learn nop action posit 0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479317 2814 log.cpp:569 ] writer start end posit 0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479568 2811 leveldb.cpp:436 ] read posit leveldb took 8325n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479786 2814 registrar.cpp:365 ] success fetch registri ( 0b ) 7.192064m [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479822 2814 registrar.cpp:464 ] appli 1 oper 3018n ; attempt updat 'registry' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.479995 2818 log.cpp:577 ] attempt append 205 byte log [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.480044 2818 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.480309 2811 replica.cpp:537 ] replica receiv write request posit 1 ( 19807 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.480928 2811 leveldb.cpp:341 ] persist action ( 224 byte ) leveldb took 596433n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.480942 2811 replica.cpp:712 ] persist action 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.481148 2815 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.481710 2815 leveldb.cpp:341 ] persist action ( 226 byte ) leveldb took 545656n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.481722 2815 replica.cpp:712 ] persist action 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.481727 2815 replica.cpp:697 ] replica learn append action posit 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.481958 2816 registrar.cpp:509 ] success updat 'registri ' 2.119168m [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482014 2816 registrar.cpp:395 ] success recov registrar [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482045 2817 log.cpp:596 ] attempt truncat log 1 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482117 2817 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482166 2816 master.cpp:1777 ] recov 0 agent registri ( 166b ) ; allow 10min agent re-regist [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482177 2817 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482404 2817 replica.cpp:537 ] replica receiv write request posit 2 ( 19808 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482975 2817 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 552763n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.482986 2817 replica.cpp:712 ] persist action 2 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.483301 2813 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.483870 2813 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 547529n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.483896 2813 leveldb.cpp:399 ] delet ~1 key leveldb took 12161n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.483904 2813 replica.cpp:712 ] persist action 2 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.483911 2813 replica.cpp:697 ] replica learn truncat action posit 2 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.492995 2797 containerizer.cpp:201 ] use isol : cgroups/mem , filesystem/posix , network/cni [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.496548 2797 linux_launcher.cpp:101 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.503572 2797 cluster.cpp:432 ] creat default 'local ' author [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.503936 2817 slave.cpp:203 ] agent start 488 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.503952 2817 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' cgroups/mem '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl '' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504148 2817 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/credential' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504189 2817 slave.cpp:341 ] agent use credenti : test-princip [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504199 2817 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/http_credentials' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504245 2817 slave.cpp:393 ] use default 'basic ' http authent [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504410 2797 sched.cpp:224 ] version : 1.0.0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504416 2817 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 03:36:29 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504580 2818 sched.cpp:328 ] new master detect master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504613 2818 sched.cpp:394 ] authent master master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504622 2818 sched.cpp:401 ] use default cram-md5 authenticate [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504649 2817 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504673 2817 slave.cpp:600 ] agent attribut : [ ] [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504678 2817 slave.cpp:605 ] agent hostnam : ip-172-30-2-29.mesosphere.io [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504703 2816 authenticatee.cpp:121 ] creat new client sasl connect [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504830 2818 master.cpp:5943 ] authent scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504887 2816 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 991 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.504982 2811 authenticator.cpp:98 ] creat new server sasl connect [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505004 2816 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505105 2813 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505131 2813 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505138 2818 status_update_manager.cpp:200 ] recov statu updat manag [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505167 2813 authenticator.cpp:204 ] receiv sasl authent start [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505200 2813 authenticator.cpp:326 ] authent requir step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505200 2814 containerizer.cpp:514 ] recov container [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505241 2813 authenticatee.cpp:259 ] receiv sasl authent step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505300 2812 authenticator.cpp:232 ] receiv sasl authent step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505317 2812 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-29.mesosphere.io ' server fqdn : 'ip-172-30-2-29.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505323 2812 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505331 2812 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505337 2812 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-29.mesosphere.io ' server fqdn : 'ip-172-30-2-29.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505342 2812 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505347 2812 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505355 2812 authenticator.cpp:318 ] authent success [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505399 2813 authenticatee.cpp:299 ] authent success [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505421 2811 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 991 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505436 2812 master.cpp:5973 ] success authent princip 'test-princip ' scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505534 2816 sched.cpp:484 ] success authent master master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505553 2816 sched.cpp:800 ] send subscrib call master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505591 2816 sched.cpp:833 ] will retri registr 11.319315m necessari [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505672 2815 master.cpp:2539 ] receiv subscrib call framework 'default ' scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505702 2815 master.cpp:2008 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.505854 2818 master.cpp:2615 ] subscrib framework default checkpoint enabl capabl [ ] [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506031 2818 sched.cpp:723 ] framework regist 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506050 2816 hierarchical.cpp:264 ] ad framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506072 2816 hierarchical.cpp:1488 ] no alloc perform [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506073 2818 sched.cpp:737 ] schedul : :regist took 28711n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506093 2816 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506126 2816 hierarchical.cpp:1139 ] perform alloc 0 agent 59667n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506428 2818 provisioner.cpp:253 ] provision recoveri complet [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506570 2815 slave.cpp:4845 ] finish recoveri [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506747 2815 slave.cpp:5017 ] queri resourc estim oversubscrib resourc [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506878 2813 slave.cpp:967 ] new master detect master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506886 2814 status_update_manager.cpp:174 ] paus send statu updat [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506903 2813 slave.cpp:1029 ] authent master master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506924 2813 slave.cpp:1040 ] use default cram-md5 authenticate [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506976 2813 slave.cpp:1002 ] detect new master [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.506989 2816 authenticatee.cpp:121 ] creat new client sasl connect [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507069 2813 slave.cpp:5031 ] receiv oversubscrib resourc resourc estim [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507145 2815 master.cpp:5943 ] authent slave ( 488 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507202 2811 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 992 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507264 2817 authenticator.cpp:98 ] creat new server sasl connect [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507374 2817 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507387 2817 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507433 2813 authenticator.cpp:204 ] receiv sasl authent start [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507467 2813 authenticator.cpp:326 ] authent requir step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507511 2813 authenticatee.cpp:259 ] receiv sasl authent step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507578 2811 authenticator.cpp:232 ] receiv sasl authent step [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507597 2811 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-29.mesosphere.io ' server fqdn : 'ip-172-30-2-29.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507606 2811 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507617 2811 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507629 2811 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-29.mesosphere.io ' server fqdn : 'ip-172-30-2-29.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507640 2811 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507648 2811 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507686 2811 authenticator.cpp:318 ] authent success [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507750 2817 authenticatee.cpp:299 ] authent success [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507766 2811 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 992 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507786 2813 master.cpp:5973 ] success authent princip 'test-princip ' slave ( 488 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507863 2817 slave.cpp:1108 ] success authent master master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507910 2817 slave.cpp:1511 ] will retri registr 10.588836m necessari [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.507966 2812 master.cpp:4653 ] regist agent slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.508059 2817 registrar.cpp:464 ] appli 1 oper 13429n ; attempt updat 'registry' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.508244 2812 log.cpp:577 ] attempt append 390 byte log [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.508296 2817 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.508546 2815 replica.cpp:537 ] replica receiv write request posit 3 ( 19831 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509158 2815 leveldb.cpp:341 ] persist action ( 409 byte ) leveldb took 589901n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509171 2815 replica.cpp:712 ] persist action 3 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509403 2815 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509980 2815 leveldb.cpp:341 ] persist action ( 411 byte ) leveldb took 558737n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509992 2815 replica.cpp:712 ] persist action 3 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.509999 2815 replica.cpp:697 ] replica learn append action posit 3 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510262 2818 registrar.cpp:509 ] success updat 'registri ' 2.178048m [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510313 2811 log.cpp:596 ] attempt truncat log 3 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510375 2817 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510486 2818 slave.cpp:3747 ] receiv ping slave-observ ( 447 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510519 2816 master.cpp:4721 ] regist agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510540 2818 slave.cpp:1152 ] regist master master @ 172.30.2.29:37328 ; given agent id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510577 2818 fetcher.cpp:86 ] clear fetcher cach [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510577 2815 hierarchical.cpp:473 ] ad agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 ( ip-172-30-2-29.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510639 2811 replica.cpp:537 ] replica receiv write request posit 4 ( 19832 ) @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510658 2816 status_update_manager.cpp:181 ] resum send statu updat [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510730 2815 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510747 2815 hierarchical.cpp:1162 ] perform alloc agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 127305n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510766 2818 slave.cpp:1175 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/slave.info' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510848 2816 master.cpp:5772 ] send 1 offer framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510892 2818 slave.cpp:1212 ] forward total oversubscrib resourc [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510956 2818 master.cpp:5066 ] receiv updat agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) total oversubscrib resourc [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.510987 2817 sched.cpp:897 ] schedul : :resourceoff took 30391n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511080 2816 hierarchical.cpp:531 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 ( ip-172-30-2-29.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511124 2816 hierarchical.cpp:1488 ] no alloc perform [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511132 2797 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:256 ; disk:1024 [ 03:36:29 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511133 2816 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511167 2816 hierarchical.cpp:1162 ] perform alloc agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 57933n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511201 2811 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 542938n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511214 2811 replica.cpp:712 ] persist action 4 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511431 2818 master.cpp:3457 ] process accept call offer : [ 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-o0 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511461 2818 master.cpp:3095 ] author framework princip 'test-princip ' launch task e9fcbad2-73bf-409e-9f71-023b826b5286 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511560 2816 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511827 2811 master.hpp:177 ] ad task e9fcbad2-73bf-409e-9f71-023b826b5286 resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511859 2811 master.cpp:3946 ] launch task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511968 2814 slave.cpp:1551 ] got assign task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.511984 2815 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 ) agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512009 2815 hierarchical.cpp:928 ] framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 filter agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 5sec [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512022 2814 slave.cpp:5654 ] checkpoint frameworkinfo '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.info' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512127 2816 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 544409n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512138 2814 slave.cpp:5665 ] checkpoint framework pid 'scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 ' '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.pid' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512153 2816 leveldb.cpp:399 ] delet ~2 key leveldb took 13134n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512162 2816 replica.cpp:712 ] persist action 4 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512167 2816 replica.cpp:697 ] replica learn truncat action posit 4 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512245 2814 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 03:36:29 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512377 2814 slave.cpp:1670 ] launch task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512408 2814 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 03:36:29 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.512596 2814 paths.cpp:528 ] tri chown '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 ' user 'root' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.517411 2814 slave.cpp:6136 ] checkpoint executorinfo '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/executor.info' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.517659 2814 slave.cpp:5734 ] launch executor e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.517853 2814 slave.cpp:6159 ] checkpoint taskinfo '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/tasks/e9fcbad2-73bf-409e-9f71-023b826b5286/task.info' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.517861 2818 containerizer.cpp:773 ] start contain '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 ' executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework '6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.518013 2814 slave.cpp:1896 ] queu task 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.518056 2814 slave.cpp:920 ] success attach file '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.519455 2817 mem.cpp:602 ] start listen oom event contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.519815 2817 mem.cpp:722 ] start listen low memori pressur event contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.520133 2817 mem.cpp:722 ] start listen medium memori pressur event contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.520447 2817 mem.cpp:722 ] start listen critic memori pressur event contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.520769 2817 mem.cpp:353 ] updat 'memory.soft_limit_in_byt ' 288mb contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.521339 2817 mem.cpp:388 ] updat 'memory.limit_in_byt ' 288mb contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.521926 2816 containerizer.cpp:1267 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' \/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor '' } '' -- commands= '' { `` command '' : [ ] } '' -- help= '' fals '' -- pipe_read= '' 119 '' -- pipe_write= '' 120 '' -- sandbox= '' /mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 '' -- user= '' root '' ' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.521984 2816 linux_launcher.cpp:281 ] clone child process flag = [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.544052 2816 containerizer.cpp:1302 ] checkpoint executor 's fork pid 20673 '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/forked.pid' [ 03:36:29 ] w : [ step 10/10 ] warn : log initgooglelog ( ) written stderr [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.603862 20687 process.cpp:1060 ] libprocess initi 172.30.2.29:44617 8 worker thread [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.605692 20687 logging.cpp:199 ] log stderr [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.606240 20687 exec.cpp:161 ] version : 1.0.0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.606302 20704 exec.cpp:211 ] executor start : executor ( 1 ) @ 172.30.2.29:44617 pid 20687 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.606724 2814 slave.cpp:2884 ] got registr executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.606885 2814 slave.cpp:2970 ] checkpoint executor pid 'executor ( 1 ) @ 172.30.2.29:44617 ' '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/libprocess.pid' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.607306 20703 exec.cpp:236 ] executor regist agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.607925 2815 mem.cpp:353 ] updat 'memory.soft_limit_in_byt ' 288mb contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.608141 20703 exec.cpp:248 ] executor : :regist took 89576n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.608538 2816 slave.cpp:2061 ] send queu task 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.608767 20705 exec.cpp:323 ] executor ask run task 'e9fcbad2-73bf-409e-9f71-023b826b5286' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.608811 20705 exec.cpp:332 ] executor : :launchtask took 26475n [ 03:36:29 ] : [ step 10/10 ] receiv subscrib event [ 03:36:29 ] : [ step 10/10 ] subscrib executor ip-172-30-2-29.mesosphere.io [ 03:36:29 ] : [ step 10/10 ] receiv launch event [ 03:36:29 ] : [ step 10/10 ] start task e9fcbad2-73bf-409e-9f71-023b826b5286 [ 03:36:29 ] : [ step 10/10 ] fork command 20710 [ 03:36:29 ] : [ step 10/10 ] sh -c 'while true ; dd count=512 bs=1m if=/dev/zero of=./temp ; done' [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.611716 20705 exec.cpp:546 ] executor send statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.611974 2815 slave.cpp:3267 ] handl statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.612499 2818 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.612527 2818 status_update_manager.cpp:497 ] creat statusupd stream task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.612751 2818 status_update_manager.cpp:824 ] checkpoint updat statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.725725 2818 status_update_manager.cpp:374 ] forward updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 agent [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.725908 2817 slave.cpp:3665 ] forward updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 master @ 172.30.2.29:37328 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.725999 2817 slave.cpp:3559 ] statu updat manag success handl statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726016 2817 slave.cpp:3575 ] send acknowledg statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726124 2813 master.cpp:5211 ] statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726157 2813 master.cpp:5259 ] forward statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726238 2813 master.cpp:6871 ] updat state task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( latest state : task_run , statu updat state : task_run ) [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726300 20701 exec.cpp:369 ] executor receiv statu updat acknowledg bea75e2e-9827-4410-9864-288f29c0a618 task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726363 2818 sched.cpp:1005 ] schedul : :statusupd took 77055n [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726517 2814 master.cpp:4365 ] process acknowledg call bea75e2e-9827-4410-9864-288f29c0a618 task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726757 2816 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:29 ] w : [ step 10/10 ] i0618 03:36:29.726812 2816 status_update_manager.cpp:824 ] checkpoint ack statu updat task_run ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:30 ] w : [ step 10/10 ] i0618 03:36:30.472790 2817 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:30 ] w : [ step 10/10 ] i0618 03:36:30.472841 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:30 ] w : [ step 10/10 ] i0618 03:36:30.472847 2817 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:30 ] w : [ step 10/10 ] i0618 03:36:30.472864 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 181038n [ 03:36:31 ] w : [ step 10/10 ] i0618 03:36:31.474026 2814 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:31 ] w : [ step 10/10 ] i0618 03:36:31.474076 2814 hierarchical.cpp:1488 ] no alloc perform [ 03:36:31 ] w : [ step 10/10 ] i0618 03:36:31.474083 2814 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:31 ] w : [ step 10/10 ] i0618 03:36:31.474097 2814 hierarchical.cpp:1139 ] perform alloc 1 agent 180187n [ 03:36:32 ] w : [ step 10/10 ] i0618 03:36:32.475332 2817 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:32 ] w : [ step 10/10 ] i0618 03:36:32.475383 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:32 ] w : [ step 10/10 ] i0618 03:36:32.475389 2817 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:32 ] w : [ step 10/10 ] i0618 03:36:32.475402 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 176560n [ 03:36:33 ] w : [ step 10/10 ] i0618 03:36:33.476011 2814 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:33 ] w : [ step 10/10 ] i0618 03:36:33.476059 2814 hierarchical.cpp:1488 ] no alloc perform [ 03:36:33 ] w : [ step 10/10 ] i0618 03:36:33.476066 2814 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:33 ] w : [ step 10/10 ] i0618 03:36:33.476080 2814 hierarchical.cpp:1139 ] perform alloc 1 agent 194002n [ 03:36:33 ] w : [ step 10/10 ] 512+0 record [ 03:36:33 ] w : [ step 10/10 ] 512+0 record [ 03:36:33 ] w : [ step 10/10 ] 536870912 byte ( 537 mb , 512 mib ) copi , 4.23412 , 127 mb/ [ 03:36:34 ] w : [ step 10/10 ] i0618 03:36:34.477355 2814 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:34 ] w : [ step 10/10 ] i0618 03:36:34.477406 2814 hierarchical.cpp:1488 ] no alloc perform [ 03:36:34 ] w : [ step 10/10 ] i0618 03:36:34.477413 2814 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:34 ] w : [ step 10/10 ] i0618 03:36:34.477427 2814 hierarchical.cpp:1139 ] perform alloc 1 agent 184403n [ 03:36:35 ] w : [ step 10/10 ] i0618 03:36:35.477726 2811 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:35 ] w : [ step 10/10 ] i0618 03:36:35.477774 2811 hierarchical.cpp:1139 ] perform alloc 1 agent 202326n [ 03:36:35 ] w : [ step 10/10 ] i0618 03:36:35.477824 2818 master.cpp:5772 ] send 1 offer framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:35 ] w : [ step 10/10 ] i0618 03:36:35.477948 2818 sched.cpp:897 ] schedul : :resourceoff took 9712n [ 03:36:36 ] w : [ step 10/10 ] i0618 03:36:36.478219 2814 hierarchical.cpp:1488 ] no alloc perform [ 03:36:36 ] w : [ step 10/10 ] i0618 03:36:36.478235 2814 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:36 ] w : [ step 10/10 ] i0618 03:36:36.478245 2814 hierarchical.cpp:1139 ] perform alloc 1 agent 47187n [ 03:36:37 ] w : [ step 10/10 ] i0618 03:36:37.478663 2811 hierarchical.cpp:1488 ] no alloc perform [ 03:36:37 ] w : [ step 10/10 ] i0618 03:36:37.478678 2811 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:37 ] w : [ step 10/10 ] i0618 03:36:37.478693 2811 hierarchical.cpp:1139 ] perform alloc 1 agent 45629n [ 03:36:38 ] w : [ step 10/10 ] i0618 03:36:38.479481 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:38 ] w : [ step 10/10 ] i0618 03:36:38.479516 2817 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:38 ] w : [ step 10/10 ] i0618 03:36:38.479532 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 98966n [ 03:36:39 ] w : [ step 10/10 ] i0618 03:36:39.480494 2813 hierarchical.cpp:1488 ] no alloc perform [ 03:36:39 ] w : [ step 10/10 ] i0618 03:36:39.480526 2813 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:39 ] w : [ step 10/10 ] i0618 03:36:39.480543 2813 hierarchical.cpp:1139 ] perform alloc 1 agent 87017n [ 03:36:40 ] w : [ step 10/10 ] i0618 03:36:40.481472 2812 hierarchical.cpp:1488 ] no alloc perform [ 03:36:40 ] w : [ step 10/10 ] i0618 03:36:40.481504 2812 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:40 ] w : [ step 10/10 ] i0618 03:36:40.481519 2812 hierarchical.cpp:1139 ] perform alloc 1 agent 122806n [ 03:36:41 ] w : [ step 10/10 ] i0618 03:36:41.482342 2813 hierarchical.cpp:1488 ] no alloc perform [ 03:36:41 ] w : [ step 10/10 ] i0618 03:36:41.482378 2813 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:41 ] w : [ step 10/10 ] i0618 03:36:41.482393 2813 hierarchical.cpp:1139 ] perform alloc 1 agent 98739n [ 03:36:42 ] w : [ step 10/10 ] i0618 03:36:42.483055 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:42 ] w : [ step 10/10 ] i0618 03:36:42.483083 2817 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:42 ] w : [ step 10/10 ] i0618 03:36:42.483095 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 73620n [ 03:36:43 ] w : [ step 10/10 ] i0618 03:36:43.483800 2811 hierarchical.cpp:1488 ] no alloc perform [ 03:36:43 ] w : [ step 10/10 ] i0618 03:36:43.483837 2811 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:43 ] w : [ step 10/10 ] i0618 03:36:43.483853 2811 hierarchical.cpp:1139 ] perform alloc 1 agent 103486n [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.484480 2818 hierarchical.cpp:1488 ] no alloc perform [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.484508 2818 hierarchical.cpp:1583 ] no invers offer send ! [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.484522 2818 hierarchical.cpp:1139 ] perform alloc 1 agent 76447n [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.507843 2815 slave.cpp:5017 ] queri resourc estim oversubscrib resourc [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.507937 2815 slave.cpp:5031 ] receiv oversubscrib resourc resourc estim [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.511128 2812 slave.cpp:3747 ] receiv ping slave-observ ( 447 ) @ 172.30.2.29:37328 [ 03:36:44 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/memory_pressure_tests.cpp:263 : failur [ 03:36:44 ] : [ step 10/10 ] fail wait 15sec _statusupdateacknowledg [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727337 2815 master.cpp:1406 ] framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 disconnect [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727363 2815 master.cpp:2840 ] disconnect framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727396 2815 master.cpp:2864 ] deactiv framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727478 2814 hierarchical.cpp:375 ] deactiv framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:44 ] w : [ step 10/10 ] w0618 03:36:44.727489 2815 master.hpp:1967 ] master attempt send messag disconnect framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727519 2815 master.cpp:1419 ] give framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 0n failov [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727556 2814 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :768 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 ) agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.727741 2814 containerizer.cpp:1576 ] destroy contain '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728740 2813 master.cpp:5624 ] framework failov timeout , remov framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728765 2813 master.cpp:6354 ] remov framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( default ) scheduler-3e992438-052b-45f0-af6a-851091145739 @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728817 2813 master.cpp:6871 ] updat state task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ( latest state : task_kil , statu updat state : task_kil ) [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728827 2817 slave.cpp:2274 ] ask shut framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 master @ 172.30.2.29:37328 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728853 2817 slave.cpp:2299 ] shut framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728869 2817 slave.cpp:4470 ] shut executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728896 2811 cgroups.cpp:2676 ] freez cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728937 2815 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:44 ] : [ step 10/10 ] receiv shutdown event [ 03:36:44 ] : [ step 10/10 ] shut [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.728950 2813 master.cpp:6937 ] remov task e9fcbad2-73bf-409e-9f71-023b826b5286 resourc cpu ( * ) :1 ; mem ( * ) :256 ; disk ( * ) :1024 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:44 ] : [ step 10/10 ] send sigterm process tree pid 20710 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.729131 20707 exec.cpp:410 ] executor ask shutdown [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.729141 2815 hierarchical.cpp:326 ] remov framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.729179 20707 exec.cpp:425 ] executor : :shutdown took 6153n [ 03:36:44 ] w : [ step 10/10 ] i0618 03:36:44.729199 20707 exec.cpp:92 ] schedul shutdown executor 5sec [ 03:36:45 ] w : [ step 10/10 ] i0618 03:36:45.485015 2818 hierarchical.cpp:1488 ] no alloc perform [ 03:36:45 ] w : [ step 10/10 ] i0618 03:36:45.485038 2818 hierarchical.cpp:1139 ] perform alloc 1 agent 47043n [ 03:36:46 ] w : [ step 10/10 ] i0618 03:36:46.485332 2811 hierarchical.cpp:1488 ] no alloc perform [ 03:36:46 ] w : [ step 10/10 ] i0618 03:36:46.485350 2811 hierarchical.cpp:1139 ] perform alloc 1 agent 33542n [ 03:36:47 ] w : [ step 10/10 ] i0618 03:36:47.486548 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:47 ] w : [ step 10/10 ] i0618 03:36:47.486588 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 84621n [ 03:36:48 ] w : [ step 10/10 ] i0618 03:36:48.487707 2813 hierarchical.cpp:1488 ] no alloc perform [ 03:36:48 ] w : [ step 10/10 ] i0618 03:36:48.487751 2813 hierarchical.cpp:1139 ] perform alloc 1 agent 83039n [ 03:36:49 ] w : [ step 10/10 ] i0618 03:36:49.488706 2812 hierarchical.cpp:1488 ] no alloc perform [ 03:36:49 ] w : [ step 10/10 ] i0618 03:36:49.488745 2812 hierarchical.cpp:1139 ] perform alloc 1 agent 78192n [ 03:36:49 ] w : [ step 10/10 ] i0618 03:36:49.729018 2811 slave.cpp:4543 ] kill executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:50 ] w : [ step 10/10 ] i0618 03:36:50.489168 2817 hierarchical.cpp:1488 ] no alloc perform [ 03:36:50 ] w : [ step 10/10 ] i0618 03:36:50.489207 2817 hierarchical.cpp:1139 ] perform alloc 1 agent 87236n [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.369570 2818 slave.cpp:2653 ] statu updat manag success handl statu updat acknowledg ( uuid : bea75e2e-9827-4410-9864-288f29c0a618 ) task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.430644 2813 cgroups.cpp:1409 ] success froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 6.70171904sec [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.431812 2818 cgroups.cpp:2694 ] thaw cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.432981 2817 cgroups.cpp:1438 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 1.140992m [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.433709 2816 slave.cpp:3793 ] executor ( 1 ) @ 172.30.2.29:44617 exit [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.443989 2813 containerizer.cpp:1812 ] executor contain '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 ' exit [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.446597 2818 provisioner.cpp:411 ] ignor destroy request unknown contain 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.446734 2813 slave.cpp:4152 ] executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 termin signal kill [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.446758 2813 slave.cpp:4256 ] clean executor 'e9fcbad2-73bf-409e-9f71-023b826b5286 ' framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 executor ( 1 ) @ 172.30.2.29:44617 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.446943 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 ' gc 6.99999482767407day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447018 2813 slave.cpp:4344 ] clean framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447038 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286 ' gc 6.9999948270963day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447082 2816 status_update_manager.cpp:282 ] close statu updat stream framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447098 2816 status_update_manager.cpp:528 ] clean statu updat stream task e9fcbad2-73bf-409e-9f71-023b826b5286 framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447100 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 ' gc 6.99999482669037day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447103 2813 slave.cpp:839 ] agent termin [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447149 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286 ' gc 6.99999482630815day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447190 2816 master.cpp:1367 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) disconnect [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447209 2816 master.cpp:2899 ] disconnect agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447211 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ' gc 6.99999482555556day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447237 2816 master.cpp:2918 ] deactiv agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 slave ( 488 ) @ 172.30.2.29:37328 ( ip-172-30-2-29.mesosphere.io ) [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447254 2812 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/memorypressuremesostest_cgroups_root_slaverecovery_mbzwwl/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 ' gc 6.99999482534815day futur [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.447300 2816 hierarchical.cpp:560 ] agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 deactiv [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.448766 2797 master.cpp:1214 ] master termin [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.448875 2814 hierarchical.cpp:505 ] remov agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-s0 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.460062 2813 cgroups.cpp:2676 ] freez cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.562192 2816 cgroups.cpp:1409 ] success froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 102.104064m [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.563100 2816 cgroups.cpp:2694 ] thaw cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 [ 03:36:51 ] w : [ step 10/10 ] i0618 03:36:51.564021 2815 cgroups.cpp:1438 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 901888n [ 03:36:51 ] : [ step 10/10 ] [ fail ] memorypressuremesostest.cgroups_root_slaverecoveri ( 22119 ms ) { noformat }",MESOS-5670,2.0
"cni isol return failur /etc/hostnam exist host . /etc/hostnam may necessarili exist everi system ( e.g. , cento 6 ) . current cni isol return failur exist host , isol need mount contain . thi fine /etc/host /etc/resolv.conf , make except /etc/hostnam , hostnam may still access even /etc/hostnam n't exist . thi issu relat 3 failur test : { noformat } [ 22:45:21 ] : [ step 10/10 ] [ run ] cniisolatortest.root_internet_curl_launchcommandtask [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.647611 24647 cluster.cpp:155 ] creat default 'local ' author [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.655230 24647 leveldb.cpp:174 ] open db 7.510408m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657680 24647 leveldb.cpp:181 ] compact db 2.427309m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657702 24647 leveldb.cpp:196 ] creat db iter 6209n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657709 24647 leveldb.cpp:202 ] seek begin db 692n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657713 24647 leveldb.cpp:271 ] iter 0 key db 431n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657727 24647 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.657888 24662 recover.cpp:451 ] start replica recoveri [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.658051 24668 recover.cpp:477 ] replica empti statu [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.658495 24664 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 18401 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.658583 24662 recover.cpp:197 ] receiv recov respons replica empti statu [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.658687 24664 recover.cpp:568 ] updat replica statu start [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659111 24664 master.cpp:382 ] master 9a4a353b-91c5-43b9-8c37-19245c37758c ( ip-172-30-2-247.mesosphere.io ) start 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659126 24664 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/l8346z/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/l8346z/master '' -- zk_session_timeout= '' 10sec '' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659267 24664 master.cpp:434 ] master allow authent framework regist [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659276 24664 master.cpp:448 ] master allow authent agent regist [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659278 24664 master.cpp:461 ] master allow authent http framework regist [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659282 24664 credentials.hpp:37 ] load credenti authent '/tmp/l8346z/credentials' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659375 24664 master.cpp:506 ] use default 'crammd5 ' authent [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659415 24664 master.cpp:578 ] use default 'basic ' http authent [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659495 24664 master.cpp:658 ] use default 'basic ' http framework authent [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659569 24664 master.cpp:705 ] author enabl [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659684 24666 hierarchical.cpp:142 ] initi hierarch alloc process [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.659696 24665 whitelist_watcher.cpp:77 ] no whitelist given [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.660269 24666 master.cpp:1969 ] the newli elect leader master @ 172.30.2.247:42024 id 9a4a353b-91c5-43b9-8c37-19245c37758c [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.660281 24666 master.cpp:1982 ] elect lead master ! [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.660290 24666 master.cpp:1669 ] recov registrar [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.660342 24662 registrar.cpp:332 ] recov registrar [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661232 24669 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.48585m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661254 24669 replica.cpp:320 ] persist replica statu start [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661326 24669 recover.cpp:477 ] replica start statu [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661667 24668 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 18404 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661758 24665 recover.cpp:197 ] receiv recov respons replica start statu [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.661893 24664 recover.cpp:568 ] updat replica statu vote [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.663851 24664 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.915617m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.663866 24664 replica.cpp:320 ] persist replica statu vote [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.663899 24664 recover.cpp:582 ] success join paxo group [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.663944 24664 recover.cpp:466 ] recov process termin [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.664088 24668 log.cpp:553 ] attempt start writer [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.664556 24668 replica.cpp:493 ] replica receiv implicit promis request ( 18405 ) @ 172.30.2.247:42024 propos 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.666551 24668 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.971938m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.666566 24668 replica.cpp:342 ] persist promis 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.666767 24667 coordinator.cpp:238 ] coordin attempt fill miss posit [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.667230 24668 replica.cpp:388 ] replica receiv explicit promis request ( 18406 ) @ 172.30.2.247:42024 posit 0 propos 2 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.669271 24668 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 2.02399m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.669287 24668 replica.cpp:712 ] persist action 0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.669656 24669 replica.cpp:537 ] replica receiv write request posit 0 ( 18407 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.669680 24669 leveldb.cpp:436 ] read posit leveldb took 10808n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.671674 24669 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.977316m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.671689 24669 replica.cpp:712 ] persist action 0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.671907 24665 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.673920 24665 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.991274m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.673935 24665 replica.cpp:712 ] persist action 0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.673941 24665 replica.cpp:697 ] replica learn nop action posit 0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674190 24665 log.cpp:569 ] writer start end posit 0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674489 24663 leveldb.cpp:436 ] read posit leveldb took 9059n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674718 24663 registrar.cpp:365 ] success fetch registri ( 0b ) 14.355968m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674747 24663 registrar.cpp:464 ] appli 1 oper 3070n ; attempt updat 'registry' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674935 24665 log.cpp:577 ] attempt append 209 byte log [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.674978 24665 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.675242 24666 replica.cpp:537 ] replica receiv write request posit 1 ( 18408 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.677088 24666 leveldb.cpp:341 ] persist action ( 228 byte ) leveldb took 1.823904m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.677103 24666 replica.cpp:712 ] persist action 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.677299 24667 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679270 24667 leveldb.cpp:341 ] persist action ( 230 byte ) leveldb took 1.952303m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679286 24667 replica.cpp:712 ] persist action 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679291 24667 replica.cpp:697 ] replica learn append action posit 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679481 24663 registrar.cpp:509 ] success updat 'registri ' 4.715264m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679503 24666 log.cpp:596 ] attempt truncat log 1 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679560 24667 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679581 24663 registrar.cpp:395 ] success recov registrar [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679745 24664 master.cpp:1777 ] recov 0 agent registri ( 170b ) ; allow 10min agent re-regist [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679774 24662 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.679986 24662 replica.cpp:537 ] replica receiv write request posit 2 ( 18409 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.681895 24662 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.891877m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.681910 24662 replica.cpp:712 ] persist action 2 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.682160 24666 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.684331 24666 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.153217m [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.684375 24666 leveldb.cpp:399 ] delet ~1 key leveldb took 26973n [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.684383 24666 replica.cpp:712 ] persist action 2 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.684389 24666 replica.cpp:697 ] replica learn truncat action posit 2 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.691529 24647 containerizer.cpp:201 ] use isol : docker/runtim , filesystem/linux , network/cni [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.694491 24647 linux_launcher.cpp:101 ] use /cgroup/freez freezer hierarchi linux launcher [ 22:45:21 ] w : [ step 10/10 ] e0619 22:45:21.699741 24647 shell.hpp:106 ] command 'hadoop version 2 > & 1 ' fail ; output : [ 22:45:21 ] w : [ step 10/10 ] sh : hadoop : command found [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.699769 24647 fetcher.cpp:62 ] skip uri fetcher plugin 'hadoop ' could creat : fail creat hdf client : fail execut 'hadoop version 2 > & 1 ' ; command either found exit non-zero exit statu : 127 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.699823 24647 registry_puller.cpp:111 ] creat registri puller docker registri 'http : //registry-1.docker.io' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.700865 24647 linux.cpp:146 ] bind mount '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg ' make share mount [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.707801 24647 cni.cpp:286 ] bind mount '/var/run/mesos/isolators/network/cni ' make share mount [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.714337 24647 cluster.cpp:432 ] creat default 'local ' author [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.714825 24668 slave.cpp:203 ] agent start 468 ) @ 172.30.2.247:42024 [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.714839 24668 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/l8346z/store '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg/http_credenti '' -- image_providers= '' docker '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' docker/runtim , filesystem/linux , network/cni '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- network_cni_config_dir= '' /tmp/l8346z/config '' -- network_cni_plugins_dir= '' /tmp/l8346z/plugin '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg '' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.715116 24668 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg/credential' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.715195 24668 slave.cpp:341 ] agent use credenti : test-princip [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.715214 24668 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_cvawpg/http_credentials' [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.715296 24668 slave.cpp:393 ] use default 'basic ' http authent [ 22:45:21 ] w : [ step 10/10 ] i0619 22:45:21.715400 24668 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] { noformat } { noformat } [ 22:45:38 ] : [ step 10/10 ] [ run ] cniisolatortest.root_verifycheckpointedinfo [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.459836 24647 cluster.cpp:155 ] creat default 'local ' author [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.470319 24647 leveldb.cpp:174 ] open db 10.34226m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.472771 24647 leveldb.cpp:181 ] compact db 2.403554m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.472795 24647 leveldb.cpp:196 ] creat db iter 4446n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.472801 24647 leveldb.cpp:202 ] seek begin db 810n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.472806 24647 leveldb.cpp:271 ] iter 0 key db 393n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.472822 24647 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.473093 24665 recover.cpp:451 ] start replica recoveri [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.473260 24663 recover.cpp:477 ] replica empti statu [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.473647 24663 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 18464 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.473752 24665 recover.cpp:197 ] receiv recov respons replica empti statu [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.473896 24667 recover.cpp:568 ] updat replica statu start [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474319 24663 master.cpp:382 ] master 64f1f7ac-e810-4fb1-b549-6e29fc62622b ( ip-172-30-2-247.mesosphere.io ) start 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474329 24663 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/qjwqsy/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/qjwqsy/mast '' -- zk_session_timeout= '' 10sec '' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474452 24663 master.cpp:434 ] master allow authent framework regist [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474457 24663 master.cpp:448 ] master allow authent agent regist [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474459 24663 master.cpp:461 ] master allow authent http framework regist [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474463 24663 credentials.hpp:37 ] load credenti authent '/tmp/qjwqsy/credentials' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474551 24663 master.cpp:506 ] use default 'crammd5 ' authent [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474598 24663 master.cpp:578 ] use default 'basic ' http authent [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474643 24663 master.cpp:658 ] use default 'basic ' http framework authent [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474674 24663 master.cpp:705 ] author enabl [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474771 24668 whitelist_watcher.cpp:77 ] no whitelist given [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.474798 24664 hierarchical.cpp:142 ] initi hierarch alloc process [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.475177 24663 master.cpp:1969 ] the newli elect leader master @ 172.30.2.247:42024 id 64f1f7ac-e810-4fb1-b549-6e29fc62622b [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.475188 24663 master.cpp:1982 ] elect lead master ! [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.475191 24663 master.cpp:1669 ] recov registrar [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.475244 24662 registrar.cpp:332 ] recov registrar [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476292 24669 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.312046m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476308 24669 replica.cpp:320 ] persist replica statu start [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476368 24669 recover.cpp:477 ] replica start statu [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476687 24668 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 18467 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476824 24666 recover.cpp:197 ] receiv recov respons replica start statu [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.476953 24668 recover.cpp:568 ] updat replica statu vote [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.478798 24668 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.793996m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.478813 24668 replica.cpp:320 ] persist replica statu vote [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.478844 24668 recover.cpp:582 ] success join paxo group [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.478889 24668 recover.cpp:466 ] recov process termin [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.479060 24665 log.cpp:553 ] attempt start writer [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.479547 24667 replica.cpp:493 ] replica receiv implicit promis request ( 18468 ) @ 172.30.2.247:42024 propos 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.481433 24667 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.8684m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.481449 24667 replica.cpp:342 ] persist promis 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.481667 24662 coordinator.cpp:238 ] coordin attempt fill miss posit [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.482067 24668 replica.cpp:388 ] replica receiv explicit promis request ( 18469 ) @ 172.30.2.247:42024 posit 0 propos 2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.483842 24668 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.754044m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.483858 24668 replica.cpp:712 ] persist action 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.484235 24665 replica.cpp:537 ] replica receiv write request posit 0 ( 18470 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.484261 24665 leveldb.cpp:436 ] read posit leveldb took 10298n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.486331 24665 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 2.057217m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.486346 24665 replica.cpp:712 ] persist action 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.486574 24669 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.488533 24669 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.941228m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.488548 24669 replica.cpp:712 ] persist action 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.488553 24669 replica.cpp:697 ] replica learn nop action posit 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.488690 24666 log.cpp:569 ] writer start end posit 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489006 24662 leveldb.cpp:436 ] read posit leveldb took 11082n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489244 24667 registrar.cpp:365 ] success fetch registri ( 0b ) 13.976832m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489276 24667 registrar.cpp:464 ] appli 1 oper 3438n ; attempt updat 'registry' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489450 24662 log.cpp:577 ] attempt append 209 byte log [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489514 24665 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.489785 24662 replica.cpp:537 ] replica receiv write request posit 1 ( 18471 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.491642 24662 leveldb.cpp:341 ] persist action ( 228 byte ) leveldb took 1.838371m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.491657 24662 replica.cpp:712 ] persist action 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.491885 24665 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493649 24665 leveldb.cpp:341 ] persist action ( 230 byte ) leveldb took 1.743495m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493665 24665 replica.cpp:712 ] persist action 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493670 24665 replica.cpp:697 ] replica learn append action posit 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493930 24669 registrar.cpp:509 ] success updat 'registri ' 4.638976m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493983 24667 log.cpp:596 ] attempt truncat log 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.493994 24669 registrar.cpp:395 ] success recov registrar [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.494034 24668 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.494197 24662 master.cpp:1777 ] recov 0 agent registri ( 170b ) ; allow 10min agent re-regist [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.494210 24666 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.494396 24662 replica.cpp:537 ] replica receiv write request posit 2 ( 18472 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.496301 24662 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.884992m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.496315 24662 replica.cpp:712 ] persist action 2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.496574 24666 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.498500 24666 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.906093m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.498529 24666 leveldb.cpp:399 ] delet ~1 key leveldb took 13787n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.498538 24666 replica.cpp:712 ] persist action 2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.498543 24666 replica.cpp:697 ] replica learn truncat action posit 2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.505269 24647 containerizer.cpp:201 ] use isol : network/cni , filesystem/posix [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.508313 24647 linux_launcher.cpp:101 ] use /cgroup/freez freezer hierarchi linux launcher [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.509832 24647 cluster.cpp:432 ] creat default 'local ' author [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510205 24666 slave.cpp:203 ] agent start 469 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510213 24666 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' network/cni '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- network_cni_config_dir= '' /tmp/qjwqsy/config '' -- network_cni_plugins_dir= '' /tmp/qjwqsy/plugin '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu '' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510442 24666 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/credential' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510510 24666 slave.cpp:341 ] agent use credenti : test-princip [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510521 24666 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/http_credentials' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510604 24666 slave.cpp:393 ] use default 'basic ' http authent [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510696 24666 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 22:45:38 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510915 24647 sched.cpp:224 ] version : 1.0.0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510962 24666 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510984 24666 slave.cpp:600 ] agent attribut : [ ] [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.510989 24666 slave.cpp:605 ] agent hostnam : ip-172-30-2-247.mesosphere.io [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511077 24669 sched.cpp:328 ] new master detect master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511162 24669 sched.cpp:394 ] authent master master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511173 24669 sched.cpp:401 ] use default cram-md5 authenticate [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511294 24662 authenticatee.cpp:121 ] creat new client sasl connect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511371 24667 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/meta' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511494 24665 master.cpp:5943 ] authent scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511523 24668 status_update_manager.cpp:200 ] recov statu updat manag [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511566 24662 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 957 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511612 24664 containerizer.cpp:514 ] recov container [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511706 24667 authenticator.cpp:98 ] creat new server sasl connect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511800 24667 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511816 24667 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511865 24667 authenticator.cpp:204 ] receiv sasl authent start [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511934 24667 authenticator.cpp:326 ] authent requir step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.511977 24667 authenticatee.cpp:259 ] receiv sasl authent step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512080 24668 authenticator.cpp:232 ] receiv sasl authent step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512102 24668 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512112 24668 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512125 24668 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512136 24668 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512145 24668 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512152 24668 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512166 24668 authenticator.cpp:318 ] authent success [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512228 24665 authenticatee.cpp:299 ] authent success [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512233 24668 master.cpp:5973 ] success authent princip 'test-princip ' scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512253 24667 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 957 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512434 24664 sched.cpp:484 ] success authent master master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512445 24664 sched.cpp:800 ] send subscrib call master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512490 24667 provisioner.cpp:253 ] provision recoveri complet [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512609 24664 sched.cpp:833 ] will retri registr 550.501359m necessari [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512648 24663 master.cpp:2539 ] receiv subscrib call framework 'default ' scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512665 24663 master.cpp:2008 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512678 24667 slave.cpp:4845 ] finish recoveri [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512763 24663 master.cpp:2615 ] subscrib framework default checkpoint disabl capabl [ ] [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512876 24664 hierarchical.cpp:264 ] ad framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512893 24664 hierarchical.cpp:1488 ] no alloc perform [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512905 24664 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512922 24664 hierarchical.cpp:1139 ] perform alloc 0 agent 33065n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.512940 24667 slave.cpp:5017 ] queri resourc estim oversubscrib resourc [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513025 24666 sched.cpp:723 ] framework regist 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513056 24666 sched.cpp:737 ] schedul : :regist took 18725n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513074 24669 status_update_manager.cpp:174 ] paus send statu updat [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513089 24667 slave.cpp:967 ] new master detect master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513105 24667 slave.cpp:1029 ] authent master master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513120 24667 slave.cpp:1040 ] use default cram-md5 authenticate [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513169 24667 slave.cpp:1002 ] detect new master [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513192 24663 authenticatee.cpp:121 ] creat new client sasl connect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513260 24667 slave.cpp:5031 ] receiv oversubscrib resourc resourc estim [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513324 24666 master.cpp:5943 ] authent slave ( 469 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513365 24666 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 958 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513423 24665 authenticator.cpp:98 ] creat new server sasl connect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513484 24665 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513494 24665 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513525 24665 authenticator.cpp:204 ] receiv sasl authent start [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513563 24665 authenticator.cpp:326 ] authent requir step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513594 24665 authenticatee.cpp:259 ] receiv sasl authent step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513635 24665 authenticator.cpp:232 ] receiv sasl authent step [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513653 24665 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513661 24665 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513667 24665 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513674 24665 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513677 24665 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513680 24665 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513689 24665 authenticator.cpp:318 ] authent success [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513727 24665 authenticatee.cpp:299 ] authent success [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513737 24664 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 958 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513754 24666 master.cpp:5973 ] success authent princip 'test-princip ' slave ( 469 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513859 24669 slave.cpp:1108 ] success authent master master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513921 24669 slave.cpp:1511 ] will retri registr 834760n necessari [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.513974 24666 master.cpp:4653 ] regist agent slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) id 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.514077 24668 registrar.cpp:464 ] appli 1 oper 12262n ; attempt updat 'registry' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.514245 24666 log.cpp:577 ] attempt append 395 byte log [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.514282 24666 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.514566 24662 replica.cpp:537 ] replica receiv write request posit 3 ( 18487 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.515151 24665 slave.cpp:1511 ] will retri registr 1.465145m necessari [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.515202 24667 master.cpp:4641 ] ignor regist agent messag slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) admiss alreadi progress [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.517513 24663 slave.cpp:1511 ] will retri registr 70.844019m necessari [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.517555 24664 master.cpp:4641 ] ignor regist agent messag slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) admiss alreadi progress [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.518628 24662 leveldb.cpp:341 ] persist action ( 414 byte ) leveldb took 4.043654m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.518643 24662 replica.cpp:712 ] persist action 3 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.518877 24665 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.520764 24665 leveldb.cpp:341 ] persist action ( 416 byte ) leveldb took 1.869511m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.520779 24665 replica.cpp:712 ] persist action 3 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.520784 24665 replica.cpp:697 ] replica learn append action posit 3 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521023 24663 registrar.cpp:509 ] success updat 'registri ' 6.930176m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521083 24668 log.cpp:596 ] attempt truncat log 3 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521152 24665 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521239 24667 master.cpp:4721 ] regist agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521272 24665 slave.cpp:3747 ] receiv ping slave-observ ( 424 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521280 24664 hierarchical.cpp:473 ] ad agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 ( ip-172-30-2-247.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521340 24665 slave.cpp:1152 ] regist master master @ 172.30.2.247:42024 ; given agent id 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521354 24665 fetcher.cpp:86 ] clear fetcher cach [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521428 24664 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521455 24664 hierarchical.cpp:1162 ] perform alloc agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 131318n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521443 24669 replica.cpp:537 ] replica receiv write request posit 4 ( 18488 ) @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521466 24662 status_update_manager.cpp:181 ] resum send statu updat [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521502 24668 master.cpp:5772 ] send 1 offer framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521553 24665 slave.cpp:1175 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/meta/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/slave.info' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521667 24665 slave.cpp:1212 ] forward total oversubscrib resourc [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521709 24665 master.cpp:5066 ] receiv updat agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) total oversubscrib resourc [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521725 24668 sched.cpp:897 ] schedul : :resourceoff took 35814n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521827 24665 hierarchical.cpp:531 ] agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 ( ip-172-30-2-247.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521860 24665 hierarchical.cpp:1488 ] no alloc perform [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521870 24665 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521884 24665 hierarchical.cpp:1162 ] perform alloc agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 36469n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.521885 24647 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:128 [ 22:45:38 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522244 24666 master.cpp:3457 ] process accept call offer : [ 64f1f7ac-e810-4fb1-b549-6e29fc62622b-o0 ] agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522267 24666 master.cpp:3095 ] author framework princip 'test-princip ' launch task 123bdde2-b542-4206-9554-249c053f63d2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522642 24666 master.hpp:177 ] ad task 123bdde2-b542-4206-9554-249c053f63d2 resourc cpu ( * ) :1 ; mem ( * ) :128 agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522666 24666 master.cpp:3946 ] launch task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 resourc cpu ( * ) :1 ; mem ( * ) :128 agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522780 24662 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :896 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :128 ) agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522799 24667 slave.cpp:1551 ] got assign task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522804 24662 hierarchical.cpp:928 ] framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 filter agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 5sec [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.522893 24667 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:45:38 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.523059 24667 slave.cpp:1670 ] launch task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.523108 24667 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:45:38 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.523439 24669 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.965378m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.523454 24669 replica.cpp:712 ] persist action 4 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.523521 24667 paths.cpp:528 ] tri chown '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466 ' user 'root' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.526239 24665 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.528328 24665 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.028744m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.528360 24665 leveldb.cpp:399 ] delet ~2 key leveldb took 16691n [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.528368 24665 replica.cpp:712 ] persist action 4 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.528374 24665 replica.cpp:697 ] replica learn truncat action posit 4 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.528923 24667 slave.cpp:5734 ] launch executor 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.529093 24667 slave.cpp:1896 ] queu task '123bdde2-b542-4206-9554-249c053f63d2 ' executor '123bdde2-b542-4206-9554-249c053f63d2 ' framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.529100 24669 containerizer.cpp:773 ] start contain 'e533d091-9fc2-4161-b6b3-4c99a88be466 ' executor '123bdde2-b542-4206-9554-249c053f63d2 ' framework '64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.529126 24667 slave.cpp:920 ] success attach file '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.529799 24663 containerizer.cpp:1120 ] overwrit environ variabl 'libprocess_ip ' , origin : '172.30.2.247 ' , new : ' 0.0.0.0 ' , contain e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.530079 24666 containerizer.cpp:1267 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' \/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor '' } '' -- commands= '' { `` command '' : [ ] } '' -- help= '' fals '' -- pipe_read= '' 96 '' -- pipe_write= '' 106 '' -- sandbox= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466 '' -- user= '' root '' ' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.530154 24666 linux_launcher.cpp:281 ] clone child process flag = clone_newut | clone_newn [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.533272 24662 cni.cpp:683 ] bind mount '/proc/7922/ns/net ' '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/n ' contain e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.533452 24662 cni.cpp:977 ] invok cni plugin 'mockplugin ' network configur ' { `` arg '' : { `` org.apache.meso '' : { `` network_info '' : { `` name '' : '' __mesos_test__ '' } } } , '' name '' : '' __mesos_test__ '' , '' type '' : '' mockplugin '' } ' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.606812 24663 cni.cpp:1066 ] got assign ipv4 address '172.17.42.1/16 ' cni network '__mesos_test__ ' contain e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.607293 24666 cni.cpp:808 ] dn nameserv contain e533d091-9fc2-4161-b6b3-4c99a88be466 : [ 22:45:38 ] w : [ step 10/10 ] nameserv 172.30.0.2 [ 22:45:38 ] w : [ step 10/10 ] fail synchron agent ( 's probabl exit ) [ 22:45:38 ] w : [ step 10/10 ] e0619 22:45:38.707609 24662 slave.cpp:4039 ] contain 'e533d091-9fc2-4161-b6b3-4c99a88be466 ' executor '123bdde2-b542-4206-9554-249c053f63d2 ' framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 fail start : collect fail : fail setup hostnam network file : warn : log initgooglelog ( ) written stderr [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.645313 7938 cni.cpp:1449 ] set hostnam 'e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] mount point '/etc/hostnam ' exist host filesystem [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.707772 24669 containerizer.cpp:1576 ] destroy contain 'e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.707787 24669 containerizer.cpp:1624 ] wait isol complet contain 'e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.708878 24667 cgroups.cpp:2676 ] freez cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.807951 24664 containerizer.cpp:1812 ] executor contain 'e533d091-9fc2-4161-b6b3-4c99a88be466 ' exit [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.810672 24666 cgroups.cpp:1409 ] success froze cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 101.766144m [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.811637 24668 cgroups.cpp:2694 ] thaw cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.812523 24664 cgroups.cpp:1438 ] success thaw cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 864u [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.908664 24668 cni.cpp:1217 ] unmount network namespac handl '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/n ' contain e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.908843 24668 cni.cpp:1228 ] remov contain directori '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466' [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909222 24669 provisioner.cpp:411 ] ignor destroy request unknown contain e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909346 24664 slave.cpp:4152 ] executor '123bdde2-b542-4206-9554-249c053f63d2 ' framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 exit statu 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909437 24664 slave.cpp:3267 ] handl statu updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 @ 0.0.0.0:0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909620 24664 slave.cpp:6074 ] termin task 123bdde2-b542-4206-9554-249c053f63d2 [ 22:45:38 ] w : [ step 10/10 ] w0619 22:45:38.909713 24665 containerizer.cpp:1418 ] ignor updat unknown contain : e533d091-9fc2-4161-b6b3-4c99a88be466 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909871 24666 status_update_manager.cpp:320 ] receiv statu updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.909888 24666 status_update_manager.cpp:497 ] creat statusupd stream task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910080 24666 status_update_manager.cpp:374 ] forward updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 agent [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910163 24665 slave.cpp:3665 ] forward updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 master @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910253 24665 slave.cpp:3559 ] statu updat manag success handl statu updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910490 24667 master.cpp:5211 ] statu updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910512 24667 master.cpp:5259 ] forward statu updat task_fail ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910560 24667 master.cpp:6871 ] updat state task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( latest state : task_fail , statu updat state : task_fail ) [ 22:45:38 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:292 : failur [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910698 24668 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] : [ step 10/10 ] valu : statusrunning- > state ( ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910755 24666 sched.cpp:1005 ] schedul : :statusupd took 50939n [ 22:45:38 ] : [ step 10/10 ] actual : task_fail [ 22:45:38 ] : [ step 10/10 ] expect : task_run [ 22:45:38 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:296 : failur [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.910995 24662 master.cpp:4365 ] process acknowledg call 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 [ 22:45:38 ] : [ step 10/10 ] valu : containers.get ( ) .size ( ) [ 22:45:38 ] : [ step 10/10 ] actual : 0 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911026 24662 master.cpp:6937 ] remov task 123bdde2-b542-4206-9554-249c053f63d2 resourc cpu ( * ) :1 ; mem ( * ) :128 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] : [ step 10/10 ] expect : 1u [ 22:45:38 ] : [ step 10/10 ] which : 1 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911234 24665 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911278 24665 status_update_manager.cpp:528 ] clean statu updat stream task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911402 24669 master.cpp:1406 ] framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 disconnect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911418 24669 master.cpp:2840 ] disconnect framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911414 24665 slave.cpp:2653 ] statu updat manag success handl statu updat acknowledg ( uuid : 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 ) task 123bdde2-b542-4206-9554-249c053f63d2 framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911433 24669 master.cpp:2864 ] deactiv framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911453 24665 slave.cpp:6115 ] complet task 123bdde2-b542-4206-9554-249c053f63d2 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911470 24665 slave.cpp:4256 ] clean executor '123bdde2-b542-4206-9554-249c053f63d2 ' framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911548 24669 master.cpp:1419 ] give framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 0n failov [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911583 24662 hierarchical.cpp:375 ] deactiv framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911640 24669 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466 ' gc 6.99998944950222day futur [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911680 24665 slave.cpp:4344 ] clean framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911689 24669 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2 ' gc 6.99998944832296day futur [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911738 24664 status_update_manager.cpp:282 ] close statu updat stream framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911805 24662 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_verifycheckpointedinfo_iiworu/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ' gc 6.99998944754667day futur [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911918 24647 slave.cpp:839 ] agent termin [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.911991 24662 master.cpp:1367 ] agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) disconnect [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.912009 24662 master.cpp:2899 ] disconnect agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.912029 24662 master.cpp:2918 ] deactiv agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 slave ( 469 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.912135 24665 hierarchical.cpp:560 ] agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 deactiv [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.912824 24669 master.cpp:5624 ] framework failov timeout , remov framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.912842 24669 master.cpp:6354 ] remov framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 ( default ) scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897 @ 172.30.2.247:42024 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.913030 24669 hierarchical.cpp:326 ] remov framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.913905 24647 master.cpp:1214 ] master termin [ 22:45:38 ] w : [ step 10/10 ] i0619 22:45:38.914031 24664 hierarchical.cpp:505 ] remov agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-s0 [ 22:45:38 ] : [ step 10/10 ] [ fail ] cniisolatortest.root_verifycheckpointedinfo ( 457 ms ) { noformat } { noformat } [ 22:45:39 ] : [ step 10/10 ] [ run ] cniisolatortest.root_slaverecoveri [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.224643 24647 cluster.cpp:155 ] creat default 'local ' author [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.232614 24647 leveldb.cpp:174 ] open db 7.839626m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235198 24647 leveldb.cpp:181 ] compact db 2.563679m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235219 24647 leveldb.cpp:196 ] creat db iter 4353n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235224 24647 leveldb.cpp:202 ] seek begin db 668n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235231 24647 leveldb.cpp:271 ] iter 0 key db 399n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235246 24647 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235555 24662 recover.cpp:451 ] start replica recoveri [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.235777 24663 recover.cpp:477 ] replica empti statu [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236134 24669 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 18550 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236197 24663 recover.cpp:197 ] receiv recov respons replica empti statu [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236351 24667 recover.cpp:568 ] updat replica statu start [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236580 24668 master.cpp:382 ] master 032cd99a-1cdc-42d4-b94a-f7b00f37fb52 ( ip-172-30-2-247.mesosphere.io ) start 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236594 24668 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/ghfuib/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/ghfuib/mast '' -- zk_session_timeout= '' 10sec '' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236723 24668 master.cpp:434 ] master allow authent framework regist [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236729 24668 master.cpp:448 ] master allow authent agent regist [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236732 24668 master.cpp:461 ] master allow authent http framework regist [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236737 24668 credentials.hpp:37 ] load credenti authent '/tmp/ghfuib/credentials' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236829 24668 master.cpp:506 ] use default 'crammd5 ' authent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236871 24668 master.cpp:578 ] use default 'basic ' http authent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236946 24668 master.cpp:658 ] use default 'basic ' http framework authent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.236991 24668 master.cpp:705 ] author enabl [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237077 24663 whitelist_watcher.cpp:77 ] no whitelist given [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237159 24665 hierarchical.cpp:142 ] initi hierarch alloc process [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237638 24667 master.cpp:1969 ] the newli elect leader master @ 172.30.2.247:42024 id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237650 24667 master.cpp:1982 ] elect lead master ! [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237655 24667 master.cpp:1669 ] recov registrar [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.237700 24669 registrar.cpp:332 ] recov registrar [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239017 24662 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.616259m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239032 24662 replica.cpp:320 ] persist replica statu start [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239084 24662 recover.cpp:477 ] replica start statu [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239437 24669 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 18553 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239538 24662 recover.cpp:197 ] receiv recov respons replica start statu [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.239672 24663 recover.cpp:568 ] updat replica statu vote [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.241654 24662 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.871972m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.241670 24662 replica.cpp:320 ] persist replica statu vote [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.241703 24662 recover.cpp:582 ] success join paxo group [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.241745 24662 recover.cpp:466 ] recov process termin [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.241880 24662 log.cpp:553 ] attempt start writer [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.242295 24668 replica.cpp:493 ] replica receiv implicit promis request ( 18554 ) @ 172.30.2.247:42024 propos 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.244303 24668 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.98443m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.244318 24668 replica.cpp:342 ] persist promis 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.244529 24663 coordinator.cpp:238 ] coordin attempt fill miss posit [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.245007 24664 replica.cpp:388 ] replica receiv explicit promis request ( 18555 ) @ 172.30.2.247:42024 posit 0 propos 2 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.246898 24664 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.869865m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.246915 24664 replica.cpp:712 ] persist action 0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.247295 24666 replica.cpp:537 ] replica receiv write request posit 0 ( 18556 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.247320 24666 leveldb.cpp:436 ] read posit leveldb took 10783n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.249264 24666 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.93015m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.249279 24666 replica.cpp:712 ] persist action 0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.249492 24663 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.251349 24663 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.840655m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.251364 24663 replica.cpp:712 ] persist action 0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.251369 24663 replica.cpp:697 ] replica learn nop action posit 0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.251634 24668 log.cpp:569 ] writer start end posit 0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.251905 24666 leveldb.cpp:436 ] read posit leveldb took 11014n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.252132 24664 registrar.cpp:365 ] success fetch registri ( 0b ) 14.413312m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.252176 24664 registrar.cpp:464 ] appli 1 oper 5009n ; attempt updat 'registry' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.252378 24663 log.cpp:577 ] attempt append 209 byte log [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.252437 24669 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.252768 24666 replica.cpp:537 ] replica receiv write request posit 1 ( 18557 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.254878 24666 leveldb.cpp:341 ] persist action ( 228 byte ) leveldb took 2.087874m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.254894 24666 replica.cpp:712 ] persist action 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.255100 24664 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.256983 24664 leveldb.cpp:341 ] persist action ( 230 byte ) leveldb took 1.863178m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.256999 24664 replica.cpp:712 ] persist action 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257004 24664 replica.cpp:697 ] replica learn append action posit 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257231 24663 registrar.cpp:509 ] success updat 'registri ' 5.034752m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257283 24663 registrar.cpp:395 ] success recov registrar [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257304 24665 log.cpp:596 ] attempt truncat log 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257431 24666 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257462 24668 master.cpp:1777 ] recov 0 agent registri ( 170b ) ; allow 10min agent re-regist [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257484 24662 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.257690 24668 replica.cpp:537 ] replica receiv write request posit 2 ( 18558 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.259577 24668 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.867119m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.259593 24668 replica.cpp:712 ] persist action 2 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.259788 24667 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.261890 24667 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.084656m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.261920 24667 leveldb.cpp:399 ] delet ~1 key leveldb took 13997n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.261929 24667 replica.cpp:712 ] persist action 2 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.261934 24667 replica.cpp:697 ] replica learn truncat action posit 2 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.269104 24647 containerizer.cpp:201 ] use isol : network/cni , filesystem/posix [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.272172 24647 linux_launcher.cpp:101 ] use /cgroup/freez freezer hierarchi linux launcher [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273219 24647 cluster.cpp:432 ] creat default 'local ' author [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273654 24662 slave.cpp:203 ] agent start 471 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273664 24662 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' network/cni '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- network_cni_config_dir= '' /tmp/ghfuib/config '' -- network_cni_plugins_dir= '' /tmp/ghfuib/plugin '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj '' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273874 24662 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/credential' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273952 24662 slave.cpp:341 ] agent use credenti : test-princip [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.273967 24662 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/http_credentials' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274041 24662 slave.cpp:393 ] use default 'basic ' http authent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274193 24662 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 22:45:39 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274448 24647 sched.cpp:224 ] version : 1.0.0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274459 24662 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274492 24662 slave.cpp:600 ] agent attribut : [ ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274500 24662 slave.cpp:605 ] agent hostnam : ip-172-30-2-247.mesosphere.io [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274618 24669 sched.cpp:328 ] new master detect master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274714 24669 sched.cpp:394 ] authent master master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274724 24669 sched.cpp:401 ] use default cram-md5 authenticate [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274826 24667 authenticatee.cpp:121 ] creat new client sasl connect [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274855 24662 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.274950 24667 master.cpp:5943 ] authent scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275002 24669 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 961 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275041 24667 status_update_manager.cpp:200 ] recov statu updat manag [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275116 24668 authenticator.cpp:98 ] creat new server sasl connect [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275132 24662 containerizer.cpp:514 ] recov container [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275185 24668 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275197 24668 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275246 24666 authenticator.cpp:204 ] receiv sasl authent start [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275322 24666 authenticator.cpp:326 ] authent requir step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275370 24666 authenticatee.cpp:259 ] receiv sasl authent step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275445 24667 authenticator.cpp:232 ] receiv sasl authent step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275462 24667 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275468 24667 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275485 24667 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275492 24667 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275497 24667 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275501 24667 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275511 24667 authenticator.cpp:318 ] authent success [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275563 24667 authenticatee.cpp:299 ] authent success [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275574 24664 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 961 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275586 24665 master.cpp:5973 ] success authent princip 'test-princip ' scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275640 24666 sched.cpp:484 ] success authent master master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275653 24666 sched.cpp:800 ] send subscrib call master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275758 24666 sched.cpp:833 ] will retri registr 1.75141928sec necessari [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275781 24664 master.cpp:2539 ] receiv subscrib call framework 'default ' scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275804 24664 master.cpp:2008 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 22:45:39 ] w : [ step 10/10 ] w0619 22:45:39.275826 24665 cni.cpp:503 ] the checkpoint cni plugin output '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/__mesos_test__/eth0/network.info ' contain 422a6a27-4327-4dc1-9a4c-7de578226eab exist [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275856 24665 cni.cpp:407 ] remov unknown orphan contain 422a6a27-4327-4dc1-9a4c-7de578226eab [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.275918 24662 master.cpp:2615 ] subscrib framework default checkpoint enabl capabl [ ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.278825 24667 hierarchical.cpp:264 ] ad framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.278898 24667 hierarchical.cpp:1488 ] no alloc perform [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.278906 24667 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.278916 24667 hierarchical.cpp:1139 ] perform alloc 0 agent 28334n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.278957 24662 sched.cpp:723 ] framework regist 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279021 24662 sched.cpp:737 ] schedul : :regist took 46037n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279216 24669 provisioner.cpp:253 ] provision recoveri complet [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279381 24663 slave.cpp:4845 ] finish recoveri [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279583 24663 slave.cpp:5017 ] queri resourc estim oversubscrib resourc [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279724 24667 status_update_manager.cpp:174 ] paus send statu updat [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279791 24668 slave.cpp:967 ] new master detect master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279808 24668 slave.cpp:1029 ] authent master master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279826 24668 slave.cpp:1040 ] use default cram-md5 authenticate [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279878 24668 slave.cpp:1002 ] detect new master [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279916 24666 authenticatee.cpp:121 ] creat new client sasl connect [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.279953 24668 slave.cpp:5031 ] receiv oversubscrib resourc resourc estim [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280045 24666 master.cpp:5943 ] authent slave ( 471 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280129 24665 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 962 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280186 24665 authenticator.cpp:98 ] creat new server sasl connect [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280266 24665 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280279 24665 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280308 24665 authenticator.cpp:204 ] receiv sasl authent start [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280345 24665 authenticator.cpp:326 ] authent requir step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280383 24665 authenticatee.cpp:259 ] receiv sasl authent step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280447 24669 authenticator.cpp:232 ] receiv sasl authent step [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280468 24669 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280474 24669 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280483 24669 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280488 24669 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-247 ' server fqdn : 'ip-172-30-2-247 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280493 24669 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280496 24669 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280504 24669 authenticator.cpp:318 ] authent success [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280544 24669 authenticatee.cpp:299 ] authent success [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280568 24668 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 962 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280596 24665 master.cpp:5973 ] success authent princip 'test-princip ' slave ( 471 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280673 24669 slave.cpp:1108 ] success authent master master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280725 24669 slave.cpp:1511 ] will retri registr 8.06966m necessari [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280796 24667 master.cpp:4653 ] regist agent slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.280905 24663 registrar.cpp:464 ] appli 1 oper 11081n ; attempt updat 'registry' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.281116 24669 log.cpp:577 ] attempt append 395 byte log [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.281182 24664 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.281452 24663 replica.cpp:537 ] replica receiv write request posit 3 ( 18575 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.283769 24663 leveldb.cpp:341 ] persist action ( 414 byte ) leveldb took 2.297945m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.283785 24663 replica.cpp:712 ] persist action 3 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.283993 24663 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.285805 24663 leveldb.cpp:341 ] persist action ( 416 byte ) leveldb took 1.793213m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.285820 24663 replica.cpp:712 ] persist action 3 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.285826 24663 replica.cpp:697 ] replica learn append action posit 3 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286088 24668 registrar.cpp:509 ] success updat 'registri ' 5.161984m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286118 24666 log.cpp:596 ] attempt truncat log 3 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286172 24666 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286332 24667 slave.cpp:3747 ] receiv ping slave-observ ( 426 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286325 24665 master.cpp:4721 ] regist agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286382 24668 hierarchical.cpp:473 ] ad agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 ( ip-172-30-2-247.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286411 24667 slave.cpp:1152 ] regist master master @ 172.30.2.247:42024 ; given agent id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286424 24667 fetcher.cpp:86 ] clear fetcher cach [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286480 24665 status_update_manager.cpp:181 ] resum send statu updat [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286545 24669 replica.cpp:537 ] replica receiv write request posit 4 ( 18576 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286556 24668 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286579 24668 hierarchical.cpp:1162 ] perform alloc agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 176726n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286625 24667 slave.cpp:1175 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/slave.info' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286660 24663 master.cpp:5772 ] send 1 offer framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286782 24667 slave.cpp:1212 ] forward total oversubscrib resourc [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286842 24667 master.cpp:5066 ] receiv updat agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) total oversubscrib resourc [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286912 24663 sched.cpp:897 ] schedul : :resourceoff took 41729n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.286981 24667 hierarchical.cpp:531 ] agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 ( ip-172-30-2-247.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287026 24667 hierarchical.cpp:1488 ] no alloc perform [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287036 24667 hierarchical.cpp:1583 ] no invers offer send ! [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287051 24667 hierarchical.cpp:1162 ] perform alloc agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 40073n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287082 24647 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:128 [ 22:45:39 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287443 24668 master.cpp:3457 ] process accept call offer : [ 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-o0 ] agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287467 24668 master.cpp:3095 ] author framework princip 'test-princip ' launch task 44eba68b-0f7c-437f-935f-26a52ac3f64b [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287804 24667 master.hpp:177 ] ad task 44eba68b-0f7c-437f-935f-26a52ac3f64b resourc cpu ( * ) :1 ; mem ( * ) :128 agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287829 24667 master.cpp:3946 ] launch task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 resourc cpu ( * ) :1 ; mem ( * ) :128 agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287947 24664 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :896 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :128 ) agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287981 24665 slave.cpp:1551 ] got assign task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.287981 24664 hierarchical.cpp:928 ] framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 filter agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 5sec [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288043 24665 slave.cpp:5654 ] checkpoint frameworkinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.info' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288200 24665 slave.cpp:5665 ] checkpoint framework pid 'scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 ' '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.pid' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288331 24665 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:45:39 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288467 24669 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.901433m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288480 24669 replica.cpp:712 ] persist action 4 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288480 24665 slave.cpp:1670 ] launch task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288516 24665 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:45:39 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288709 24669 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.288784 24665 paths.cpp:528 ] tri chown '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' user 'root' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.290657 24669 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.915037m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.290714 24669 leveldb.cpp:399 ] delet ~2 key leveldb took 27510n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.290726 24669 replica.cpp:712 ] persist action 4 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.290736 24669 replica.cpp:697 ] replica learn truncat action posit 4 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.292919 24665 slave.cpp:6136 ] checkpoint executorinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/executor.info' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.293200 24665 slave.cpp:5734 ] launch executor 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.293373 24669 containerizer.cpp:773 ] start contain 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.293403 24665 slave.cpp:6159 ] checkpoint taskinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/tasks/44eba68b-0f7c-437f-935f-26a52ac3f64b/task.info' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.293581 24665 slave.cpp:1896 ] queu task '44eba68b-0f7c-437f-935f-26a52ac3f64b ' executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.293622 24665 slave.cpp:920 ] success attach file '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.294059 24662 containerizer.cpp:1120 ] overwrit environ variabl 'libprocess_ip ' , origin : '172.30.2.247 ' , new : ' 0.0.0.0 ' , contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.294361 24664 containerizer.cpp:1267 ] launch 'mesos-container ' flag ' -- command= '' { `` shell '' : true , '' valu '' : '' \/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor '' } '' -- commands= '' { `` command '' : [ ] } '' -- help= '' fals '' -- pipe_read= '' 96 '' -- pipe_write= '' 107 '' -- sandbox= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d '' -- user= '' root '' ' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.294427 24664 linux_launcher.cpp:281 ] clone child process flag = clone_newut | clone_newn [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.296911 24664 containerizer.cpp:1302 ] checkpoint executor 's fork pid 7982 '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/pids/forked.pid' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.297456 24663 cni.cpp:683 ] bind mount '/proc/7982/ns/net ' '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/n ' contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.297610 24663 cni.cpp:977 ] invok cni plugin 'mockplugin ' network configur ' { `` arg '' : { `` org.apache.meso '' : { `` network_info '' : { `` name '' : '' __mesos_test__ '' } } } , '' name '' : '' __mesos_test__ '' , '' type '' : '' mockplugin '' } ' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.311471 24667 cni.cpp:1066 ] got assign ipv4 address '172.17.42.1/16 ' cni network '__mesos_test__ ' contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.311671 24667 cni.cpp:1217 ] unmount network namespac handl '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/n ' contain 422a6a27-4327-4dc1-9a4c-7de578226eab [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.311774 24667 cni.cpp:1228 ] remov contain directori '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.311911 24667 cni.cpp:808 ] dn nameserv contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d : [ 22:45:39 ] w : [ step 10/10 ] nameserv 172.30.0.2 [ 22:45:39 ] w : [ step 10/10 ] efail synchron agent ( 's probabl exit ) 0619 22:45:39.412294 24666 slave.cpp:4039 ] contain 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 fail start : collect fail : fail setup hostnam network file : warn : log initgooglelog ( ) written stderr [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.352311 7998 cni.cpp:1449 ] set hostnam 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] mount point '/etc/hostnam ' exist host filesystem [ 22:45:39 ] w : [ step 10/10 ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.412427 24664 containerizer.cpp:1576 ] destroy contain 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.412444 24664 containerizer.cpp:1624 ] wait isol complet contain 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.413815 24662 cgroups.cpp:2676 ] freez cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.512704 24665 containerizer.cpp:1812 ] executor contain 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' exit [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.516521 24662 cgroups.cpp:1409 ] success froze cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d 102.685184m [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.517462 24664 cgroups.cpp:2694 ] thaw cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.518301 24662 cgroups.cpp:1438 ] success thaw cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d 813824n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614039 24664 cni.cpp:1217 ] unmount network namespac handl '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/n ' contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614209 24664 cni.cpp:1228 ] remov contain directori '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614532 24662 provisioner.cpp:411 ] ignor destroy request unknown contain ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614665 24668 slave.cpp:4152 ] executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 exit statu 1 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614742 24668 slave.cpp:3267 ] handl statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 @ 0.0.0.0:0 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.614964 24669 slave.cpp:6074 ] termin task 44eba68b-0f7c-437f-935f-26a52ac3f64b [ 22:45:39 ] w : [ step 10/10 ] w0619 22:45:39.615054 24669 containerizer.cpp:1418 ] ignor updat unknown contain : ef0b6221-1073-42f0-adfc-cfe75ddb3a5d [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.615197 24668 status_update_manager.cpp:320 ] receiv statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.615211 24668 status_update_manager.cpp:497 ] creat statusupd stream task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.615552 24668 status_update_manager.cpp:824 ] checkpoint updat statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.623800 24668 status_update_manager.cpp:374 ] forward updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 agent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.623888 24662 slave.cpp:3665 ] forward updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 master @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.623988 24662 slave.cpp:3559 ] statu updat manag success handl statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624162 24668 master.cpp:5211 ] statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624181 24668 master.cpp:5259 ] forward statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624243 24668 master.cpp:6871 ] updat state task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( latest state : task_fail , statu updat state : task_fail ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624356 24666 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624442 24669 sched.cpp:1005 ] schedul : :statusupd took 38999n [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624614 24664 master.cpp:4365 ] process acknowledg call addda641-2dda-4d21-bc97-f2d8f9e28fe7 task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 [ 22:45:39 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:487 : failur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624634 24664 master.cpp:6937 ] remov task 44eba68b-0f7c-437f-935f-26a52ac3f64b resourc cpu ( * ) :1 ; mem ( * ) :128 framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] : [ step 10/10 ] valu : statusrunning- > state ( ) [ 22:45:39 ] : [ step 10/10 ] actual : task_fail [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624804 24665 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] : [ step 10/10 ] expect : task_run [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.624847 24665 status_update_manager.cpp:824 ] checkpoint ack statu updat task_fail ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628197 24665 status_update_manager.cpp:528 ] clean statu updat stream task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628276 24665 slave.cpp:2653 ] statu updat manag success handl statu updat acknowledg ( uuid : addda641-2dda-4d21-bc97-f2d8f9e28fe7 ) task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628291 24665 slave.cpp:6115 ] complet task 44eba68b-0f7c-437f-935f-26a52ac3f64b [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628304 24665 slave.cpp:4256 ] clean executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628487 24669 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' gc 6.99999272627556day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628530 24669 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b ' gc 6.99999272575111day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628592 24663 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' gc 6.99999272505778day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628623 24665 slave.cpp:4344 ] clean framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628635 24663 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b ' gc 6.9999927244563day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628679 24663 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ' gc 6.99999272379556day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628686 24667 status_update_manager.cpp:282 ] close statu updat stream framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628693 24665 slave.cpp:839 ] agent termin [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628718 24663 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ' gc 6.9999927235763day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628746 24665 master.cpp:1367 ] agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) disconnect [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628760 24665 master.cpp:2899 ] disconnect agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628783 24665 master.cpp:2918 ] deactiv agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 slave ( 471 ) @ 172.30.2.247:42024 ( ip-172-30-2-247.mesosphere.io ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.628832 24665 hierarchical.cpp:560 ] agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 deactiv [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.629724 24647 containerizer.cpp:201 ] use isol : network/cni , filesystem/posix [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.633214 24647 linux_launcher.cpp:101 ] use /cgroup/freez freezer hierarchi linux launcher [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634099 24647 cluster.cpp:432 ] creat default 'local ' author [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634585 24664 slave.cpp:203 ] agent start 472 ) @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634599 24664 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' network/cni '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- network_cni_config_dir= '' /tmp/ghfuib/config '' -- network_cni_plugins_dir= '' /tmp/ghfuib/plugin '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj '' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634822 24664 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/credential' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634886 24664 slave.cpp:341 ] agent use credenti : test-princip [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634897 24664 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/http_credentials' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.634953 24664 slave.cpp:393 ] use default 'basic ' http authent [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635114 24664 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 22:45:39 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635422 24664 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635444 24664 slave.cpp:600 ] agent attribut : [ ] [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635449 24664 slave.cpp:605 ] agent hostnam : ip-172-30-2-247.mesosphere.io [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635941 24668 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.635964 24668 state.cpp:697 ] no checkpoint resourc found '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/resources/resources.info' [ 22:45:39 ] w : [ step 10/10 ] w0619 22:45:39.636117 24669 master.cpp:4232 ] can kill task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 unknown ; perform reconcili [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636142 24669 master.cpp:5510 ] perform explicit task state reconcili 1 task framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:504 : failur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636162 24669 master.cpp:5600 ] send explicit reconcili state task_lost task 44eba68b-0f7c-437f-935f-26a52ac3f64b framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] : [ step 10/10 ] valu : statuskilled- > state ( ) [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636317 24669 sched.cpp:1005 ] schedul : :statusupd took 28596n [ 22:45:39 ] : [ step 10/10 ] actual : task_lost [ 22:45:39 ] : [ step 10/10 ] expect : task_kil [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636548 24647 sched.cpp:1964 ] ask stop driver [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636605 24669 sched.cpp:1167 ] stop framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636728 24662 master.cpp:6342 ] process teardown call framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] w0619 22:45:39.636739 24668 state.cpp:544 ] fail find executor libprocess pid/http marker file [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636747 24662 master.cpp:6354 ] remov framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ( default ) scheduler-fb3e96c6-1106-4910-80d7-e83d75960307 @ 172.30.2.247:42024 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636823 24664 hierarchical.cpp:375 ] deactiv framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.636973 24667 hierarchical.cpp:326 ] remov framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637154 24663 fetcher.cpp:86 ] clear fetcher cach [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637192 24663 slave.cpp:4933 ] recov framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637208 24663 slave.cpp:5858 ] recov executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637316 24663 slave.cpp:6074 ] termin task 44eba68b-0f7c-437f-935f-26a52ac3f64b [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637331 24663 slave.cpp:6115 ] complet task 44eba68b-0f7c-437f-935f-26a52ac3f64b [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637421 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' gc 6.99999262296296day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637454 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d ' gc 6.99999262252444day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637460 24663 slave.cpp:4344 ] clean framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637477 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b ' gc 6.99999262231407day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637514 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b ' gc 6.99999262212741day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637534 24667 status_update_manager.cpp:282 ] close statu updat stream framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637547 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ' gc 6.99999262155259day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637574 24666 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_slaverecovery_ubk9bj/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 ' gc 6.99999262134222day futur [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637648 24668 status_update_manager.cpp:200 ] recov statu updat manag [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637660 24668 status_update_manager.cpp:208 ] recov executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637676 24668 status_update_manager.cpp:233 ] skip recov updat executor '44eba68b-0f7c-437f-935f-26a52ac3f64b ' framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 latest run ef0b6221-1073-42f0-adfc-cfe75ddb3a5d complet [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.637729 24663 slave.cpp:839 ] agent termin [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.639163 24647 master.cpp:1214 ] master termin [ 22:45:39 ] w : [ step 10/10 ] i0619 22:45:39.639279 24667 hierarchical.cpp:505 ] remov agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-s0 [ 22:45:39 ] : [ step 10/10 ] [ fail ] cniisolatortest.root_slaverecoveri ( 418 ms ) { noformat }",MESOS-5669,3.0
"add cgroup namespac linux ns helper . sinc linux kernel 4.6 , cgroup namespac ad . need support handl cgroup namespac process . thi also relat two test failur ubuntu 16 : { noformat } [ 22:41:26 ] : [ step 10/10 ] [ run ] nstest.root_setn [ 22:41:26 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/ns_tests.cpp:75 : failur [ 22:41:26 ] : [ step 10/10 ] nstype : unknown namespac 'cgroup' [ 22:41:26 ] : [ step 10/10 ] [ fail ] nstest.root_setn ( 1 ms ) { noformat } { noformat } [ 22:41:26 ] : [ step 10/10 ] [ run ] nstest.root_getn [ 22:41:26 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/ns_tests.cpp:160 : failur [ 22:41:26 ] : [ step 10/10 ] nstype : unknown namespac 'cgroup' [ 22:41:26 ] : [ step 10/10 ] [ fail ] nstest.root_getn ( 0 ms ) { noformat }",MESOS-5668,3.0
"cniisolatortest.root_internet_curl_launchcommandtask fail cento 7 . { noformat } [ 22:41:54 ] : [ step 10/10 ] [ run ] cniisolatortest.root_internet_curl_launchcommandtask [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.348641 30896 cluster.cpp:155 ] creat default 'local ' author [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.353384 30896 leveldb.cpp:174 ] open db 4.634552m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354763 30896 leveldb.cpp:181 ] compact db 1.360201m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354784 30896 leveldb.cpp:196 ] creat db iter 3421n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354790 30896 leveldb.cpp:202 ] seek begin db 633n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354797 30896 leveldb.cpp:271 ] iter 0 key db 401n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354811 30896 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.354990 30913 recover.cpp:451 ] start replica recoveri [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.355123 30915 recover.cpp:477 ] replica empti statu [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.355391 30915 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 18695 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.355479 30912 recover.cpp:197 ] receiv recov respons replica empti statu [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.355581 30914 recover.cpp:568 ] updat replica statu start [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356091 30910 master.cpp:382 ] master 27c796db-6f98-4d61-96c0-f583f22787ff ( ip-172-30-2-105.mesosphere.io ) start 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356104 30910 master.cpp:384 ] flag startup : -- acls= '' '' -- agent_ping_timeout= '' 15sec '' -- agent_reregister_timeout= '' 10min '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate_agents= '' true '' -- authenticate_frameworks= '' true '' -- authenticate_http= '' true '' -- authenticate_http_frameworks= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/khgyrq/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_framework_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_agent_ping_timeouts= '' 5 '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- quiet= '' fals '' -- recovery_agent_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/khgyrq/mast '' -- zk_session_timeout= '' 10sec '' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356237 30910 master.cpp:434 ] master allow authent framework regist [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356245 30910 master.cpp:448 ] master allow authent agent regist [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356247 30910 master.cpp:461 ] master allow authent http framework regist [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356251 30910 credentials.hpp:37 ] load credenti authent '/tmp/khgyrq/credentials' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356351 30910 master.cpp:506 ] use default 'crammd5 ' authent [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356389 30910 master.cpp:578 ] use default 'basic ' http authent [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356439 30910 master.cpp:658 ] use default 'basic ' http framework authent [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356467 30910 master.cpp:705 ] author enabl [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356531 30913 whitelist_watcher.cpp:77 ] no whitelist given [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356549 30912 hierarchical.cpp:142 ] initi hierarch alloc process [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356868 30916 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.232816m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356884 30916 replica.cpp:320 ] persist replica statu start [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.356945 30916 recover.cpp:477 ] replica start statu [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357100 30917 master.cpp:1969 ] the newli elect leader master @ 172.30.2.105:40724 id 27c796db-6f98-4d61-96c0-f583f22787ff [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357115 30917 master.cpp:1982 ] elect lead master ! [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357122 30917 master.cpp:1669 ] recov registrar [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357213 30910 registrar.cpp:332 ] recov registrar [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357429 30913 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 18698 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357549 30914 recover.cpp:197 ] receiv recov respons replica start statu [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.357728 30913 recover.cpp:568 ] updat replica statu vote [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.358937 30913 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.14792m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.358952 30913 replica.cpp:320 ] persist replica statu vote [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.358986 30913 recover.cpp:582 ] success join paxo group [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.359041 30913 recover.cpp:466 ] recov process termin [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.359180 30916 log.cpp:553 ] attempt start writer [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.359578 30917 replica.cpp:493 ] replica receiv implicit promis request ( 18699 ) @ 172.30.2.105:40724 propos 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.360752 30917 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.157449m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.360767 30917 replica.cpp:342 ] persist promis 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.360982 30914 coordinator.cpp:238 ] coordin attempt fill miss posit [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.361426 30910 replica.cpp:388 ] replica receiv explicit promis request ( 18700 ) @ 172.30.2.105:40724 posit 0 propos 2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.362571 30910 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.124969m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.362587 30910 replica.cpp:712 ] persist action 0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.362999 30911 replica.cpp:537 ] replica receiv write request posit 0 ( 18701 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.363030 30911 leveldb.cpp:436 ] read posit leveldb took 14967n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.364264 30911 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.214497m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.364279 30911 replica.cpp:712 ] persist action 0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.364470 30910 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.365622 30910 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.131398m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.365636 30910 replica.cpp:712 ] persist action 0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.365643 30910 replica.cpp:697 ] replica learn nop action posit 0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.365769 30915 log.cpp:569 ] writer start end posit 0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366080 30913 leveldb.cpp:436 ] read posit leveldb took 8794n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366284 30915 registrar.cpp:365 ] success fetch registri ( 0b ) 9.053952m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366315 30915 registrar.cpp:464 ] appli 1 oper 3436n ; attempt updat 'registry' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366487 30911 log.cpp:577 ] attempt append 209 byte log [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366539 30917 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.366839 30917 replica.cpp:537 ] replica receiv write request posit 1 ( 18702 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.367966 30917 leveldb.cpp:341 ] persist action ( 228 byte ) leveldb took 1.106053m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.367982 30917 replica.cpp:712 ] persist action 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.368201 30915 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.371786 30915 leveldb.cpp:341 ] persist action ( 230 byte ) leveldb took 3.566076m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.371803 30915 replica.cpp:712 ] persist action 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.371809 30915 replica.cpp:697 ] replica learn append action posit 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372032 30910 registrar.cpp:509 ] success updat 'registri ' 5.693952m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372097 30910 registrar.cpp:395 ] success recov registrar [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372107 30911 log.cpp:596 ] attempt truncat log 1 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372151 30910 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372218 30911 master.cpp:1777 ] recov 0 agent registri ( 170b ) ; allow 10min agent re-regist [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372242 30915 hierarchical.cpp:169 ] skip recoveri hierarch alloc : noth recov [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.372467 30914 replica.cpp:537 ] replica receiv write request posit 2 ( 18703 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.373693 30914 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.207676m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.373708 30914 replica.cpp:712 ] persist action 2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.373920 30913 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.375115 30913 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.17978m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.375145 30913 leveldb.cpp:399 ] delet ~1 key leveldb took 14216n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.375154 30913 replica.cpp:712 ] persist action 2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.375159 30913 replica.cpp:697 ] replica learn truncat action posit 2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.383839 30896 containerizer.cpp:201 ] use isol : docker/runtim , filesystem/linux , network/cni [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.388789 30896 linux_launcher.cpp:101 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 22:41:54 ] w : [ step 10/10 ] e0619 22:41:54.393234 30896 shell.hpp:106 ] command 'hadoop version 2 > & 1 ' fail ; output : [ 22:41:54 ] w : [ step 10/10 ] sh : hadoop : command found [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.393265 30896 fetcher.cpp:62 ] skip uri fetcher plugin 'hadoop ' could creat : fail creat hdf client : fail execut 'hadoop version 2 > & 1 ' ; command either found exit non-zero exit statu : 127 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.393316 30896 registry_puller.cpp:111 ] creat registri puller docker registri 'http : //registry-1.docker.io' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.395668 30896 cluster.cpp:432 ] creat default 'local ' author [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396100 30914 slave.cpp:203 ] agent start 469 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396116 30914 slave.cpp:204 ] flag startup : -- acls= '' '' -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- authorizer= '' local '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/khgyrq/stor '' -- docker_volume_checkpoint_dir= '' /var/run/mesos/isolators/docker/volum '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_command_executor= '' fals '' -- http_credentials= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/http_credenti '' -- image_providers= '' docker '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' docker/runtim , filesystem/linux , network/cni '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- network_cni_config_dir= '' /tmp/khgyrq/config '' -- network_cni_plugins_dir= '' /tmp/khgyrq/plugin '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi '' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396380 30914 credentials.hpp:86 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/credential' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396495 30914 slave.cpp:341 ] agent use credenti : test-princip [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396509 30914 credentials.hpp:37 ] load credenti authent '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/http_credentials' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396586 30914 slave.cpp:393 ] use default 'basic ' http authent [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396698 30914 resources.cpp:572 ] pars resourc json fail : cpus:2 ; gpus:0 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 22:41:54 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396780 30896 sched.cpp:224 ] version : 1.0.0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.396991 30914 slave.cpp:592 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397020 30914 slave.cpp:600 ] agent attribut : [ ] [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397029 30914 slave.cpp:605 ] agent hostnam : ip-172-30-2-105.mesosphere.io [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397040 30916 sched.cpp:328 ] new master detect master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397068 30916 sched.cpp:394 ] authent master master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397078 30916 sched.cpp:401 ] use default cram-md5 authenticate [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397188 30916 authenticatee.cpp:121 ] creat new client sasl connect [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397467 30914 state.cpp:57 ] recov state '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/meta' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397476 30912 master.cpp:5943 ] authent scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397544 30913 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 953 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397614 30915 status_update_manager.cpp:200 ] recov statu updat manag [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397668 30912 authenticator.cpp:98 ] creat new server sasl connect [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397709 30915 containerizer.cpp:514 ] recov container [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397869 30912 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397886 30912 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397927 30912 authenticator.cpp:204 ] receiv sasl authent start [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.397964 30912 authenticator.cpp:326 ] authent requir step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398000 30912 authenticatee.cpp:259 ] receiv sasl authent step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398052 30912 authenticator.cpp:232 ] receiv sasl authent step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398066 30912 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-105.mesosphere.io ' server fqdn : 'ip-172-30-2-105.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398073 30912 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398087 30912 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398098 30912 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-105.mesosphere.io ' server fqdn : 'ip-172-30-2-105.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398103 30912 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398108 30912 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398116 30912 authenticator.cpp:318 ] authent success [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398162 30914 authenticatee.cpp:299 ] authent success [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398181 30913 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 953 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398200 30912 master.cpp:5973 ] success authent princip 'test-princip ' scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398270 30914 sched.cpp:484 ] success authent master master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398280 30914 sched.cpp:800 ] send subscrib call master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398342 30914 sched.cpp:833 ] will retri registr 869.123866m necessari [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398381 30916 master.cpp:2539 ] receiv subscrib call framework 'default ' scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398398 30916 master.cpp:2008 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398483 30916 master.cpp:2615 ] subscrib framework default checkpoint disabl capabl [ ] [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398679 30916 sched.cpp:723 ] framework regist 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398701 30916 sched.cpp:737 ] schedul : :regist took 10291n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398784 30910 hierarchical.cpp:264 ] ad framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398802 30910 hierarchical.cpp:1488 ] no alloc perform [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398808 30910 hierarchical.cpp:1583 ] no invers offer send ! [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.398818 30910 hierarchical.cpp:1139 ] perform alloc 0 agent 22451n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399222 30916 metadata_manager.cpp:205 ] no imag load disk . docker provision imag storag path '/tmp/khgyrq/store/storedimag ' exist [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399318 30910 provisioner.cpp:253 ] provision recoveri complet [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399453 30913 slave.cpp:4845 ] finish recoveri [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399690 30913 slave.cpp:5017 ] queri resourc estim oversubscrib resourc [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399796 30911 slave.cpp:967 ] new master detect master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399811 30911 slave.cpp:1029 ] authent master master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399801 30914 status_update_manager.cpp:174 ] paus send statu updat [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399821 30911 slave.cpp:1040 ] use default cram-md5 authenticate [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399855 30911 slave.cpp:1002 ] detect new master [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399879 30915 authenticatee.cpp:121 ] creat new client sasl connect [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.399910 30911 slave.cpp:5031 ] receiv oversubscrib resourc resourc estim [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400044 30915 master.cpp:5943 ] authent slave ( 469 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400099 30910 authenticator.cpp:414 ] start authent session crammd5_authenticate ( 954 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400151 30910 authenticator.cpp:98 ] creat new server sasl connect [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400316 30910 authenticatee.cpp:213 ] receiv sasl authent mechan : cram-md5 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400329 30910 authenticatee.cpp:239 ] attempt authent mechan 'cram-md5' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400367 30910 authenticator.cpp:204 ] receiv sasl authent start [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400398 30910 authenticator.cpp:326 ] authent requir step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400431 30910 authenticatee.cpp:259 ] receiv sasl authent step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400516 30917 authenticator.cpp:232 ] receiv sasl authent step [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400530 30917 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-105.mesosphere.io ' server fqdn : 'ip-172-30-2-105.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400537 30917 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400544 30917 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400550 30917 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-105.mesosphere.io ' server fqdn : 'ip-172-30-2-105.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400554 30917 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400558 30917 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400566 30917 authenticator.cpp:318 ] authent success [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400609 30914 authenticatee.cpp:299 ] authent success [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400640 30912 authenticator.cpp:432 ] authent session cleanup crammd5_authenticate ( 954 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400682 30917 master.cpp:5973 ] success authent princip 'test-princip ' slave ( 469 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400738 30911 slave.cpp:1108 ] success authent master master @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400790 30911 slave.cpp:1511 ] will retri registr 13.364855m necessari [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400848 30913 master.cpp:4653 ] regist agent slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) id 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.400950 30914 registrar.cpp:464 ] appli 1 oper 16921n ; attempt updat 'registry' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.401154 30915 log.cpp:577 ] attempt append 395 byte log [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.401213 30914 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.401515 30914 replica.cpp:537 ] replica receiv write request posit 3 ( 18725 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.402851 30914 leveldb.cpp:341 ] persist action ( 414 byte ) leveldb took 1.317458m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.402866 30914 replica.cpp:712 ] persist action 3 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.403101 30917 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404217 30917 leveldb.cpp:341 ] persist action ( 416 byte ) leveldb took 1.100393m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404233 30917 replica.cpp:712 ] persist action 3 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404239 30917 replica.cpp:697 ] replica learn append action posit 3 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404495 30915 registrar.cpp:509 ] success updat 'registri ' 3.521792m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404561 30913 log.cpp:596 ] attempt truncat log 3 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404621 30915 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404690 30910 master.cpp:4721 ] regist agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404726 30915 slave.cpp:3747 ] receiv ping slave-observ ( 429 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404747 30916 hierarchical.cpp:473 ] ad agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 ( ip-172-30-2-105.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404825 30915 slave.cpp:1152 ] regist master master @ 172.30.2.105:40724 ; given agent id 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404840 30915 fetcher.cpp:86 ] clear fetcher cach [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404880 30910 replica.cpp:537 ] replica receiv write request posit 4 ( 18726 ) @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404911 30916 hierarchical.cpp:1583 ] no invers offer send ! [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404932 30913 status_update_manager.cpp:181 ] resum send statu updat [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.404942 30916 hierarchical.cpp:1162 ] perform alloc agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 168147n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405025 30911 master.cpp:5772 ] send 1 offer framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405082 30915 slave.cpp:1175 ] checkpoint slaveinfo '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/meta/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/slave.info' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405177 30911 sched.cpp:897 ] schedul : :resourceoff took 55063n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405239 30915 slave.cpp:1212 ] forward total oversubscrib resourc [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405299 30911 master.cpp:5066 ] receiv updat agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) total oversubscrib resourc [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405318 30896 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:128 [ 22:41:54 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405387 30911 hierarchical.cpp:531 ] agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 ( ip-172-30-2-105.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405421 30911 hierarchical.cpp:1488 ] no alloc perform [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405431 30911 hierarchical.cpp:1583 ] no invers offer send ! [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405447 30911 hierarchical.cpp:1162 ] perform alloc agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 40224n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405643 30914 master.cpp:3457 ] process accept call offer : [ 27c796db-6f98-4d61-96c0-f583f22787ff-o0 ] agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.405668 30914 master.cpp:3095 ] author framework princip 'test-princip ' launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406030 30912 master.hpp:177 ] ad task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 resourc cpu ( * ) :1 ; mem ( * ) :128 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406056 30912 master.cpp:3946 ] launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 resourc cpu ( * ) :1 ; mem ( * ) :128 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406158 30916 slave.cpp:1551 ] got assign task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406193 30912 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :896 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :1 ; mem ( * ) :128 ) agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406214 30912 hierarchical.cpp:928 ] framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 filter agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 5sec [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406250 30916 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:41:54 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406347 30910 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.44747m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406359 30910 replica.cpp:712 ] persist action 4 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406381 30916 slave.cpp:1670 ] launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406420 30916 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 22:41:54 ] w : [ step 10/10 ] tri semicolon-delimit string format instead [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406555 30914 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.406793 30916 paths.cpp:528 ] tri chown '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 ' user 'root' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.408360 30914 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.635458m [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.408453 30914 leveldb.cpp:399 ] delet ~2 key leveldb took 53370n [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.408469 30914 replica.cpp:712 ] persist action 4 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.408480 30914 replica.cpp:697 ] replica learn truncat action posit 4 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.411355 30916 slave.cpp:5734 ] launch executor d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.411485 30916 slave.cpp:1896 ] queu task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.411516 30915 containerizer.cpp:773 ] start contain '548370b5-05f2-4e33-8f6f-015aa3fd1af4 ' executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.411521 30916 slave.cpp:920 ] success attach file '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.411733 30914 metadata_manager.cpp:167 ] look imag 'alpine' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.412009 30911 registry_puller.cpp:235 ] pull imag 'library/alpin ' 'docker-manifest : //registry-1.docker.io:443library/alpin ? latest # http ' '/tmp/khgyrq/store/staging/0cvljm' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.870712 30915 registry_puller.cpp:258 ] the manifest imag 'library/alpin ' ' { [ 22:41:54 ] w : [ step 10/10 ] `` schemavers '' : 1 , [ 22:41:54 ] w : [ step 10/10 ] `` name '' : `` library/alpin '' , [ 22:41:54 ] w : [ step 10/10 ] `` tag '' : `` latest '' , [ 22:41:54 ] w : [ step 10/10 ] `` architectur '' : `` amd64 '' , [ 22:41:54 ] w : [ step 10/10 ] `` fslayer '' : [ [ 22:41:54 ] w : [ step 10/10 ] { [ 22:41:54 ] w : [ step 10/10 ] `` blobsum '' : `` sha256 : fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 '' [ 22:41:54 ] w : [ step 10/10 ] } [ 22:41:54 ] w : [ step 10/10 ] ] , [ 22:41:54 ] w : [ step 10/10 ] `` histori '' : [ [ 22:41:54 ] w : [ step 10/10 ] { [ 22:41:54 ] w : [ step 10/10 ] `` v1compat '' : `` { \ '' architecture\ '' : \ '' amd64\ '' , \ '' config\ '' : { \ '' hostname\ '' : \ '' 571cde9b03ce\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : null , \ '' cmd\ '' : null , \ '' image\ '' : \ '' \ '' , \ '' volumes\ '' : null , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' onbuild\ '' : null , \ '' labels\ '' : null } , \ '' container\ '' : \ '' 571cde9b03ce6f46b78b8e9c5089d03034863a4ab9f05d3e4997d0e5e80a2a6e\ '' , \ '' container_config\ '' : { \ '' hostname\ '' : \ '' 571cde9b03ce\ '' , \ '' domainname\ '' : \ '' \ '' , \ '' user\ '' : \ '' \ '' , \ '' attachstdin\ '' : fals , \ '' attachstdout\ '' : fals , \ '' attachstderr\ '' : fals , \ '' tty\ '' : fals , \ '' openstdin\ '' : fals , \ '' stdinonce\ '' : fals , \ '' env\ '' : null , \ '' cmd\ '' : [ \ '' /bin/sh\ '' , \ '' -c\ '' , \ '' # ( nop ) add file:701fd33a2f463fd4bd459779276897ef01dcf998dd47f6c8eae34fa5e0886046 /\ '' ] , \ '' image\ '' : \ '' \ '' , \ '' volumes\ '' : null , \ '' workingdir\ '' : \ '' \ '' , \ '' entrypoint\ '' : null , \ '' onbuild\ '' : null , \ '' labels\ '' : null } , \ '' created\ '' : \ '' 2016-06-02t21:43:31.291506236z\ '' , \ '' docker_version\ '' : \ '' 1.9.1\ '' , \ '' id\ '' : \ '' e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b\ '' , \ '' os\ '' : \ '' linux\ '' } '' [ 22:41:54 ] w : [ step 10/10 ] } [ 22:41:54 ] w : [ step 10/10 ] ] , [ 22:41:54 ] w : [ step 10/10 ] `` signatur '' : [ [ 22:41:54 ] w : [ step 10/10 ] { [ 22:41:54 ] w : [ step 10/10 ] `` header '' : { [ 22:41:54 ] w : [ step 10/10 ] `` jwk '' : { [ 22:41:54 ] w : [ step 10/10 ] `` crv '' : `` p-256 '' , [ 22:41:54 ] w : [ step 10/10 ] `` kid '' : `` iz4c : akg6 : llbk:4y62:6ywu : oi2g : k2en : zojh : ghry:5pka : pfee : wzwd '' , [ 22:41:54 ] w : [ step 10/10 ] `` kti '' : `` ec '' , [ 22:41:54 ] w : [ step 10/10 ] `` x '' : `` hu3h5pmha0tgt3mf41bh5ebsly9tv3o-bla53s8-25g '' , [ 22:41:54 ] w : [ step 10/10 ] `` '' : `` y9sm4txh_3kkkeehikweggtulqlyjxpwcxcs_bvp4pc '' [ 22:41:54 ] w : [ step 10/10 ] } , [ 22:41:54 ] w : [ step 10/10 ] `` alg '' : `` es256 '' [ 22:41:54 ] w : [ step 10/10 ] } , [ 22:41:54 ] w : [ step 10/10 ] `` signatur '' : `` 8szvgfkd_ovz9ftfnmolrwkwayoy9zatq4bgpnkpufpk-48nhdtmlkmz52nqm2shck2xtyykhzlte6wuctrjqa '' , [ 22:41:54 ] w : [ step 10/10 ] `` protect '' : `` eyjmb3jtyxrmzw5ndggiojezntgsimzvcm1hdfrhawwioijdbjailcj0aw1lijoimjaxni0wni0xovqymjo0mto1nfoifq '' [ 22:41:54 ] w : [ step 10/10 ] } [ 22:41:54 ] w : [ step 10/10 ] ] [ 22:41:54 ] w : [ step 10/10 ] } ' [ 22:41:54 ] w : [ step 10/10 ] i0619 22:41:54.870767 30915 registry_puller.cpp:368 ] fetch blob 'sha256 : fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 ' layer 'e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b ' imag 'library/alpine' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.357898 30910 hierarchical.cpp:1674 ] filter offer cpu ( * ) :1 ; mem ( * ) :896 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.357965 30910 hierarchical.cpp:1488 ] no alloc perform [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.357980 30910 hierarchical.cpp:1583 ] no invers offer send ! [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.358002 30910 hierarchical.cpp:1139 ] perform alloc 1 agent 238814n [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.474309 30911 registry_puller.cpp:305 ] extract layer tar ball '/tmp/khgyrq/store/staging/0cvljm/sha256 : fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 rootf '/tmp/khgyrq/store/staging/0cvljm/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.575764 30910 metadata_manager.cpp:155 ] success cach imag 'alpine' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.576198 30911 provisioner.cpp:294 ] provis imag rootf '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f ' contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.576556 30910 copy.cpp:128 ] copi layer path '/tmp/khgyrq/store/layers/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootf ' rootf '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.676825 30916 containerizer.cpp:1267 ] launch 'mesos-container ' flag ' -- command= '' { `` argument '' : [ `` mesos-executor '' , '' -- sandbox_directory=\/mnt\/mesos\/sandbox '' , '' -- user=root '' , '' -- rootfs=\/mnt\/teamcity\/temp\/buildtmp\/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f '' ] , '' shell '' : fals , '' user '' : '' root '' , '' valu '' : '' \/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor '' } '' -- commands= '' { `` command '' : [ { `` shell '' : true , '' valu '' : '' # ! \/bin\/sh\nset -x -e\n\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-container mount -- help=\ '' false\ '' -- operation=\ '' make-rslave\ '' -- path=\ '' \/\ '' \nmount -n -- rbind '\/mnt\/teamcity\/temp\/buildtmp\/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi\/slaves\/27c796db-6f98-4d61-96c0-f583f22787ff-s0\/frameworks\/27c796db-6f98-4d61-96c0-f583f22787ff-0000\/executors\/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2\/runs\/548370b5-05f2-4e33-8f6f-015aa3fd1af4 ' '\/mnt\/teamcity\/temp\/buildtmp\/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f\/mnt\/mesos\/sandbox'\n '' } ] } '' -- help= '' fals '' -- pipe_read= '' 17 '' -- pipe_write= '' 20 '' -- sandbox= '' /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 '' -- user= '' root '' ' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.676923 30916 linux_launcher.cpp:281 ] clone child process flag = clone_newut | clone_newn [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.681491 30913 cni.cpp:683 ] bind mount '/proc/13484/ns/net ' '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/n ' contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.681712 30913 cni.cpp:977 ] invok cni plugin 'mockplugin ' network configur ' { `` arg '' : { `` org.apache.meso '' : { `` network_info '' : { `` name '' : '' __mesos_test__ '' } } } , '' name '' : '' __mesos_test__ '' , '' type '' : '' mockplugin '' } ' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.776078 30916 cni.cpp:1066 ] got assign ipv4 address '172.17.0.1/16 ' cni network '__mesos_test__ ' contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.776463 30913 cni.cpp:808 ] dn nameserv contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 : [ 22:41:55 ] w : [ step 10/10 ] nameserv 172.30.0.2 [ 22:41:55 ] w : [ step 10/10 ] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-container mount -- help=fals -- operation=make-rslav -- path=/ [ 22:41:55 ] w : [ step 10/10 ] + mount -n -- rbind /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 /mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f/mnt/mesos/sandbox [ 22:41:55 ] w : [ step 10/10 ] warn : log initgooglelog ( ) written stderr [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.944355 13484 process.cpp:1060 ] libprocess initi 172.17.0.1:60396 8 worker thread [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.946605 13484 logging.cpp:199 ] log stderr [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.947335 13484 exec.cpp:161 ] version : 1.0.0 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.947404 13541 exec.cpp:211 ] executor start : executor ( 1 ) @ 172.17.0.1:60396 pid 13484 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.947883 30917 slave.cpp:2884 ] got registr executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.948427 13543 exec.cpp:236 ] executor regist agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.948524 30914 slave.cpp:2061 ] send queu task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.949061 13543 exec.cpp:248 ] executor : :regist took 75489n [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.949213 13543 exec.cpp:323 ] executor ask run task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.949246 13543 exec.cpp:332 ] executor : :launchtask took 21245n [ 22:41:55 ] : [ step 10/10 ] receiv subscrib event [ 22:41:55 ] : [ step 10/10 ] subscrib executor ip-172-30-2-105.mesosphere.io [ 22:41:55 ] : [ step 10/10 ] receiv launch event [ 22:41:55 ] : [ step 10/10 ] start task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 [ 22:41:55 ] : [ step 10/10 ] fork command 13550 [ 22:41:55 ] : [ step 10/10 ] sh -c 'ifconfig' [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.953589 13547 exec.cpp:546 ] executor send statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] fail exec : no file directori [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.953891 30917 slave.cpp:3267 ] handl statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954368 30910 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954385 30910 status_update_manager.cpp:497 ] creat statusupd stream task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954545 30910 status_update_manager.cpp:374 ] forward updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 agent [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954637 30911 slave.cpp:3665 ] forward updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 master @ 172.30.2.105:40724 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954711 30911 slave.cpp:3559 ] statu updat manag success handl statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954732 30911 slave.cpp:3575 ] send acknowledg statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954761 30914 master.cpp:5211 ] statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954788 30914 master.cpp:5259 ] forward statu updat task_run ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954843 30914 master.cpp:6871 ] updat state task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( latest state : task_run , statu updat state : task_run ) [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954934 13548 exec.cpp:369 ] executor receiv statu updat acknowledg 5caccf6c-9e1e-44cc-93d4-6851987802cd task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.954967 30910 sched.cpp:1005 ] schedul : :statusupd took 57021n [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.955070 30914 master.cpp:4365 ] process acknowledg call 5caccf6c-9e1e-44cc-93d4-6851987802cd task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.955150 30911 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:55 ] w : [ step 10/10 ] i0619 22:41:55.955219 30911 slave.cpp:2653 ] statu updat manag success handl statu updat acknowledg ( uuid : 5caccf6c-9e1e-44cc-93d4-6851987802cd ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] : [ step 10/10 ] command termin signal abort ( pid : 13550 ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.054153 13541 exec.cpp:546 ] executor send statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.054498 30913 slave.cpp:3267 ] handl statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.054955 30917 slave.cpp:6074 ] termin task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055366 30912 status_update_manager.cpp:320 ] receiv statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055409 30912 status_update_manager.cpp:374 ] forward updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 agent [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055485 30916 slave.cpp:3665 ] forward updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 master @ 172.30.2.105:40724 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055558 30916 slave.cpp:3559 ] statu updat manag success handl statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] : [ step 10/10 ] .. / .. /src/tests/containerizer/cni_isolator_tests.cpp:216 : failur [ 22:41:56 ] : [ step 10/10 ] valu : statusfinished- > state ( ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055572 30916 slave.cpp:3575 ] send acknowledg statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:56 ] : [ step 10/10 ] actual : task_fail [ 22:41:56 ] : [ step 10/10 ] expect : task_finish [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055613 30914 master.cpp:5211 ] statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055640 30914 master.cpp:5259 ] forward statu updat task_fail ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055696 30914 master.cpp:6871 ] updat state task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( latest state : task_fail , statu updat state : task_fail ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055773 30912 sched.cpp:1005 ] schedul : :statusupd took 29145n [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055780 13546 exec.cpp:369 ] executor receiv statu updat acknowledg 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055816 30916 hierarchical.cpp:891 ] recov cpu ( * ) :1 ; mem ( * ) :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055887 30911 master.cpp:4365 ] process acknowledg call 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055907 30911 master.cpp:6937 ] remov task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 resourc cpu ( * ) :1 ; mem ( * ) :128 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.055971 30896 sched.cpp:1964 ] ask stop driver [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056030 30913 sched.cpp:1167 ] stop framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000' [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056040 30916 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056073 30916 status_update_manager.cpp:528 ] clean statu updat stream task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056151 30915 master.cpp:6342 ] process teardown call framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056172 30915 master.cpp:6354 ] remov framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 ( default ) scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8 @ 172.30.2.105:40724 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056197 30916 slave.cpp:2653 ] statu updat manag success handl statu updat acknowledg ( uuid : 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 ) task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056216 30916 slave.cpp:6115 ] complet task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056218 30913 hierarchical.cpp:375 ] deactiv framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056248 30916 slave.cpp:2274 ] ask shut framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 master @ 172.30.2.105:40724 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056265 30916 slave.cpp:2299 ] shut framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056277 30916 slave.cpp:4470 ] shut executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056468 30914 hierarchical.cpp:326 ] remov framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.056634 30911 containerizer.cpp:1576 ] destroy contain '548370b5-05f2-4e33-8f6f-015aa3fd1af4' [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.057258 13543 exec.cpp:410 ] executor ask shutdown [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.057303 13543 exec.cpp:425 ] executor : :shutdown took 6363n [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.057324 13547 exec.cpp:92 ] schedul shutdown executor 5sec [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.058279 30910 cgroups.cpp:2676 ] freez cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.059762 30912 cgroups.cpp:1409 ] success froze cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 1.460736m [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.061364 30910 cgroups.cpp:2694 ] thaw cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.062861 30915 cgroups.cpp:1438 ] success thaw cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 1.478912m [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.064016 30910 slave.cpp:3793 ] executor ( 1 ) @ 172.17.0.1:60396 exit [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.078352 30915 containerizer.cpp:1812 ] executor contain '548370b5-05f2-4e33-8f6f-015aa3fd1af4 ' exit [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.179833 30916 cni.cpp:1217 ] unmount network namespac handl '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/n ' contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.179924 30916 cni.cpp:1228 ] remov contain directori '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4' [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.180981 30913 provisioner.cpp:434 ] destroy contain rootf '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f ' contain 548370b5-05f2-4e33-8f6f-015aa3fd1af4 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280364 30912 slave.cpp:4152 ] executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 termin signal kill [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280406 30912 slave.cpp:4256 ] clean executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 executor ( 1 ) @ 172.17.0.1:60396 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280545 30915 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 ' gc 6.99999675365926day futur [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280575 30912 slave.cpp:4344 ] clean framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280647 30915 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 ' gc 6.99999675293037day futur [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280654 30914 status_update_manager.cpp:282 ] close statu updat stream framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280710 30915 gc.cpp:55 ] schedul '/mnt/teamcity/temp/buildtmp/cniisolatortest_root_internet_curl_launchcommandtask_gcx6xi/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-s0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000 ' gc 6.99999675200296day futur [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280745 30915 slave.cpp:839 ] agent termin [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280810 30912 master.cpp:1367 ] agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) disconnect [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280827 30912 master.cpp:2899 ] disconnect agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280844 30912 master.cpp:2918 ] deactiv agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 slave ( 469 ) @ 172.30.2.105:40724 ( ip-172-30-2-105.mesosphere.io ) [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.280912 30912 hierarchical.cpp:560 ] agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 deactiv [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.283011 30896 master.cpp:1214 ] master termin [ 22:41:56 ] w : [ step 10/10 ] i0619 22:41:56.283140 30916 hierarchical.cpp:505 ] remov agent 27c796db-6f98-4d61-96c0-f583f22787ff-s0 [ 22:41:56 ] : [ step 10/10 ] [ fail ] cniisolatortest.root_internet_curl_launchcommandtask ( 1945 ms ) { noformat }",MESOS-5667,2.0
"executor inherit environ variabl agent . current executor inherit environ variabl form slave meso container . thi problemat , two reason : 1 . when use docker imag ( ` mongo ` ) unifi container , duplic environ variabl inherit slave lead initi failur , lang and/or lc_ * environ variabl set correctli . 2 . when look environ variabl executor task , page environ variabl list , redund danger . depend reason , propos longer allow executor inherit environ variabl slave . instead , user specifi environ variabl need set slave flag ` -- executor_environment_vari ` json format .",MESOS-5657,3.0
"put initi scaffold place implement subscrib call v1 master api . as discuss mesos-5498 , ticket track work put initi scaffold place stream task statu updat event client subscrib { { api/v1 } } oper api endpoint . other events/support snapshot would done part mesos-5498 .",MESOS-5609,5.0
modul use replic log state api requir zookeep header the state api use zookeep client header henc bundl zookeep header need instal meso instal .,MESOS-5577,1.0
need remov refer `` messages/messages.hpp '' ` state ` api in order expos ` state ` api use replic log meso modul necessari ` state ` api refer header expos part meso instal . current include/mesos/state/protobuf.hpp refer src/messages/messages.hpp make ` state ` api unus modul . we need move protobuf ` serial ` / ` deseri ` function messages.hpp move ` stout/protobuf.hpp ` . thi help us remov refer messages.hpp ` state ` api .,MESOS-5561,2.0
"fix method popul devic entri ` /dev/nvidia-uvm ` , etc . current , major/minor number ` /dev/nvidiactl ` ` /dev/nvidia-uvm ` hard-cod . thi caus problem ` /dev/nvidia-uvm ` major number part `` experiment '' devic rang linux . becaus rang experiment , guarante devic number assign given machin . we use ` os : stat : :rdev ( ) ` extract major/minor number programat .",MESOS-5556,2.0
"chang major/minor devic type nvidia gpu ` unsign int ` current , gpu struct specifi type ` major ` ` minor ` field ` dev_t ` , actual concaten major minor devic number access ` major ( ) ` ` minor ( ) ` macro . these macro return ` unsign int ` hand ` dev_t ` , make sens field type instead .",MESOS-5554,1.0
"http v1 subscrib schedul event alway nil http_interval_second i 'm write control go monitor heartbeat . i 'd like use interv commun master , specifi subscrib event . but 's . { code } 2016/06/03 18:34:04 { type : subscrib subscrib : & event_subscrib { frameworkid : & mesos.frameworkid { valu : ffdb6d6e-0167-4fa2-98f9-2c3f8157fc25-0004 , } , heartbeatintervalsecond : nil , } offer : nil rescind : nil updat : nil messag : nil failur : nil error : nil } { code } { code } $ dpkg -l |grep -e meso ii meso 0.28.0-2.0.16.ubuntu1404 amd64 cluster resourc manag effici resourc isol { code } i * * see heartbeat event . just see interv specifi subscrib event .",MESOS-5537,1.0
"re-en style-check stout . after 3rdparti reorg , mesos-styl checker stop check stout .",MESOS-5531,1.0
confirm error author persist volum test the test { { persistentvolumetest.badacldropcreateanddestroy } } { { persistentvolumetest.badaclnoprincip } } check fail destroy oper confirm persist volum still contain offer receiv attempt oper . we also explicitli check oper succeed due fail author .,MESOS-5470,1.0
"cni store subnet address networkinfo when cni isol execut cni plugin , cni plugin return ip address subnet ( 192.168.0.1/32 ) . meso strip subnet store address task.networkinfo.ipaddress . reason - current meso compon expect subnet task 's networkinfo.ipaddress , instead expect ip address . thi caus error compon , mesos-dn fail return networkinfo address ( instead default next configur ipsourc ) , marathon gener invalid link task ( includ /32 link )",MESOS-5453,2.0
"make sasl depend option . right hard depend sasl , probabl wo n't work well window ( least ) near futur use case . in futur , would nice pluggabl authent layer .",MESOS-5450,2.0
"allow libprocess/stout build without first ` make ` 3rdparti . after 3rdparti reorg , libprocess/stout enabl build depend one ` make ` 3rdpart/ build libprocess/stout .",MESOS-5445,2.0
"gpu resourc broke framework data tabl webui in agent_framework.html master/static/agent.html , add { { gpu ( use / alloc ) } } tabl header . but n't add correspond column tabl bodi well . on hand , n't provid statist gpu monitor endpoint . to provid data webui , requir implement gpu statist monitor endpoint firstli .",MESOS-5436,1.0
"` network/cni ` isol skip bind mount cni network inform root directori possibl current creat ( ) method ` network/cni ` isol , cni network inform root directori ( i.e. , { { /var/run/mesos/isolators/network/cni } } ) , self bind mount make sure share mount peer group . howev , self bind mount mount contain cni network inform root directori alreadi share mount share peer group , like ` filesystem/linux ` isol [ mesos-5239 | http : //issues.apache.org/jira/browse/mesos-5239 ] .",MESOS-5413,3.0
delet /observ http endpoint the `` /observ '' endpoint introduc long time ago support function never implement . we kill endpoint associ code avoid tech debt .,MESOS-5408,2.0
"make field author : :request protobuf option . current { { author : :request } } protobuf declar { { subject } } { { object } } requir field . howev , codebas alway set , render messag uniniti state , exampl : * http : //github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp # l603 * http : //github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp # l2057 i believ reason n't see issu relat never send authz request wire , i.e. , never serialize/deseri . howev , still invalid protobuf messag . moreov , extern author may serial messag . we either ensur requir field set make { { subject } } { { object } } field option . thi also requir updat local author , properli handl situat field absent . we may also want notifi author extern author updat code accordingli . it look like deprec necessari , mainli already—erron ! —treat field option .",MESOS-5405,3.0
introduc objectapprov interfac author . as outlin ( http : //docs.google.com/document/d/1fus79p8uj5pibycrblkjsbkotmeo8ezauinxxwia3qa ) plan add option retriev filterobject author goal allow effici author larg number ( potenti larg ) object .,MESOS-5403,5.0
slave/ag renam phase 1 : updat term websit the follow file need updat site/source/index.html.md,MESOS-5397,1.0
v1 executor proto includ maven jar accord mesos-4793 executor v1 http api releas meso 0.28.0 howev correspond proto includ maven jar version 0.28.0 0.28.1 . script verifi { code } wget http : //repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos-0.28.1.jar & & unzip -lf mesos-0.28.1.jar | grep `` v1\/executor '' | wc -l { code },MESOS-5390,1.0
docker container prefix rel volume.container_path valu path sandbox docker container current requir absolut path valu volume.container_path . inconsist meso container requir rel container_path . make confus api . meso level well marathon level . ideal docker container would allow framework specifi rel path volume.container_path case automat convert absolut path prepend sandbox directori . /cc [ ~jieyu ],MESOS-5389,3.0
"mesoscontainerizerlaunch flag execut arbitrari command via shell . for exampl , docker volum isol 's containerpath append ( without sanit ) command 's execut manner . as , 's possibl inject arbitrari shell command execut meso . http : //github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp # l206 perhap instead string command could/should sent string array could pass argv argument w/o shell interpret ?",MESOS-5388,5.0
add ` handl ` overload function take file descriptor,MESOS-5386,3.0
implement os : :sethostnam,MESOS-5383,1.0
implement os : :fsync,MESOS-5382,1.0
"the schedul librari delay initi connect master . current , schedul librari { { src/scheduler/scheduler.cpp } } artifici induc delay tri initi establish connect master . in event master failov zk disconnect , larg number framework get disconnect therebi overwhelm master tcp syn request . on larg cluster mani agent , master alreadi overwhelm handl connect request agent . thi compound issu master .",MESOS-5359,3.0
"get /master/maintenance/schedule/ produc 404 . attempt make get request /master/maintenance/schedule/ result 404 . howev , i make get request /master/maintenance/schedul ( without trail / ) , work . my current ( untest ) theori might relat fact also /master/maintenance/schedule/statu endpoint ( endpoint built top function endpoint ) , request /help /help/ ( without trail slash ) produc function result .",MESOS-5333,3.0
make ` os : :close ` alway catch structur except window,MESOS-5318,2.0
"env ` mesos_sandbox ` set properli command task chang rootf . thi context meso container ( a.k.a. , unifi container ) . i simpl test : { noformat } sudo sbin/mesos-mast -- work_dir=/tmp/mesos/mast sudo glog_v=1 sbin/mesos-slav -- master=10.0.2.15:5050 -- isolation=docker/runtim , filesystem/linux -- work_dir=/tmp/mesos/slave/ -- image_providers=dock -- executor_environment_variables= '' { } '' sudo bin/mesos-execut -- master=10.0.2.15:5050 -- name=test -- docker_image=alpin -- command= '' env '' mesos_executor_id=test shlvl=1 mesos_checkpoint=0 mesos_executor_shutdown_grace_period=5sec libprocess_port=0 mesos_agent_endpoint=10.0.2.15:5051 mesos_sandbox=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-s0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6 mesos_native_java_library=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so mesos_framework_id=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000 mesos_slave_id=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-s0 mesos_native_library=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so mesos_directory=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-s0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6 pwd=/mnt/mesos/sandbox mesos_slave_pid=slav ( 1 ) @ 10.0.2.15:5051 { noformat } ` mesos_sandbox ` ` /mnt/mesos/sandbox ` .",MESOS-5312,2.0
add capabl support meso execut cli . add support ` user ` ` capabl ` execut cli . thi help test ` capabl ` featur unifi container .,MESOS-5303,3.0
"need add remov semant copi backend some dockerfil run ` rm ` command remov file base imag use `` run '' direct dockerfil . an exampl found : http : //github.com/ngineered/nginx-php-fpm.git in final rootf remov file present . presenc file final imag make contain misbehav . for exampl , nginx-php-fpm docker imag referenc tri remov default nginx config replac config point differ html root . if default nginx config still present build imag , nginx start point differ html root one set dockerfil . current copi backend handl remov file intermedi layer . thi caus issu docker imag built use dockerfil similar one list . henc , need add remov semant copi backend .",MESOS-5277,5.0
add test case docker volum driver,MESOS-5266,5.0
updat mesos-execut support docker volum isol . the mesos-execut need updat support docker volum isol .,MESOS-5265,3.0
"isol cleanup invok prepar yet . if meso container destroy contain provis state , isol cleanup still call , incorrect isol prepar yet . in case , need clean isol , call provision destroy directli .",MESOS-5253,2.0
"persist volum dockercontainer support assum proper mount propag setup host . we recent ad persist volum support dockercontainer ( mesos-3413 ) . to understand problem , first need understand persist volum support dockercontainer . to support persist volum dockercontainer , bind mount persist volum contain 's sandbox ( 'container_path ' rel persist volum ) . when docker contain launch , sinc alway add volum ( -v ) sandbox , persist volum bind mount contain well ( sinc docker 'rbind ' ) . the assumpt work docker daemon see persist volum mount meso mount host mount tabl . it 's problem docker daemon use host mount namespac . howev , systemd enabl system , docker daemon run separ mount namespac mount mount namespac mark slave mount due [ patch|http : //github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a ] . so mean : order work , parent mount agent 's work_dir share mount docker daemon start . thi typic true centos7 , coreo mount share mount default . howev , caus issu 'filesystem/linux ' isol . to understand , first i need show typic problem deal share mount . let explain use follow command centos7 machin : { noformat } [ root @ core-dev run ] # cat /proc/self/mountinfo 24 60 0:19 / /run rw , nosuid , nodev shared:22 - tmpf tmpf rw , seclabel , mode=755 [ root @ core-dev run ] # mkdir /run/netn [ root @ core-dev run ] # mount -- bind /run/netn /run/netn [ root @ core-dev run ] # cat /proc/self/mountinfo 24 60 0:19 / /run rw , nosuid , nodev shared:22 - tmpf tmpf rw , seclabel , mode=755 121 24 0:19 /netn /run/netn rw , nosuid , nodev shared:22 - tmpf tmpf rw , seclabel , mode=755 [ root @ core-dev run ] # ip netn add test [ root @ core-dev run ] # cat /proc/self/mountinfo 24 60 0:19 / /run rw , nosuid , nodev shared:22 - tmpf tmpf rw , seclabel , mode=755 121 24 0:19 /netn /run/netn rw , nosuid , nodev shared:22 - tmpf tmpf rw , seclabel , mode=755 162 121 0:3 / /run/netns/test rw , nosuid , nodev , noexec , relatim shared:5 - proc proc rw 163 24 0:3 / /run/netns/test rw , nosuid , nodev , noexec , relatim shared:5 - proc proc rw { noformat } as see , 're two entri ( /run/netns/test ) mount tabl ( unexpect ) . thi confus system sometim . the reason creat self bind mount ( /run/netn - > /run/netn ) , mount put share mount peer group ( shared:22 ) parent ( /run ) . then , creat anoth mount underneath ( /run/netns/test ) , mount oper propag mount peer group ( shared:22 ) , result unexpect addit mount creat . the reason need self bind mount meso sometim , need make sure mount share get copi new mount namespac creat . howev , system , mount privat default ( e.g. , ubuntu 14.04 ) . in case , sinc chang system mount , self bind mount set mount propag share . for instanc , filesytem/linux isol , self bind mount agent 's work_dir . to avoid self bind mount pitfal mention , filesystem/linux isol , creat mount , make-slav + make-shar mount share mount peer group . in way , mount underneath propag back . howev , oper break assumpt persist volum dockercontainer support make . as result , 're see problem persist volum dockercontainer filesystem/linux isol turn .",MESOS-5239,3.0
check failur appcprovisionerintegrationtest.root_simplelinuximagetest observ mesospher intern ci : { noformat } [ 22:56:28 ] w : [ step 10/10 ] f0420 22:56:28.056788 629 containerizer.cpp:1634 ] check fail : containers_.contain ( containerid ) { noformat } complet test log attach file .,MESOS-5238,2.0
implement http docker executor use executor librari similar http command executor mesos-3558 http docker executor speak v1 executor api .,MESOS-5227,5.0
"document docker volum driver isol . should includ follow : 1 . what featur ( driver option ) support docker volum driver isol . 2 . how use docker volum driver isol . * relat agent flag introduct usag . * isol depend clarif ( e.g. , filesystem/linux ) . * relat driver daemon preprocess . * volum pre-specifi user volum cleanup .",MESOS-5216,5.0
"the filesystem/linux isol set permiss host_path . the { { filesystem/linux } } isol drop replac { { filesystem/shar } } isol . thi consid latter deprec . we current use { { filesystem/shar } } isol togeth follow slave option . thi provid us privat { { /tmp } } { { /var/tmp } } folder task . { code } -- default_container_info= ' { `` type '' : `` meso '' , `` volum '' : [ { `` host_path '' : `` system/tmp '' , `` container_path '' : `` /tmp '' , `` mode '' : `` rw '' } , { `` host_path '' : `` system/vartmp '' , `` container_path '' : `` /var/tmp '' , `` mode '' : `` rw '' } ] } ' { code } when brows meso sandbox , one see follow permiss : { code } mode nlink uid gid size mtime drwxrwxrwx 3 root root 4 kb apr 11 18:16 tmp drwxrwxrwx 2 root root 4 kb apr 11 18:15 vartmp { code } howev , run new { { filesystem/linux } } isol , permiss differ : { code } mode nlink uid gid size mtime drwxr-xr-x 2 root root 4 kb apr 12 10:34 tmp drwxr-xr-x 2 root root 4 kb apr 12 10:34 vartmp { code } thi prevent user code ( run non-root user ) write folder , i.e . everi write attempt fail permiss deni . * context * : * we use apach aurora . aurora run custom executor root switch non-privileg user run actual user code . * the follow code seem enabl usecas exist { { filesystem/shar } } isol : http : //github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp # l175-l198",MESOS-5187,3.0
"registri puller fetch blob correctli http redirect 3xx url . when registri puller pull privat repositori privat registri ( e.g. , quay.io ) , error may occur fetch blob , point fetch manifest repo finish correctli . the error messag ` unexpect http respons '400 bad request ' tri download blob ` . thi may aris logic fetch blob , incorrect format uri request blob .",MESOS-5172,3.0
run meso build powerpc platform asf ci thi last step declar offici support powerpc . thi current block asf infra ad powerpc base jenkin machin asf ci .,MESOS-5156,1.0
"sandbox content protect unauthor user mesos-4956 introduc authent support sandbox . howev , authent go far tell whether user known meso . an extra addit step necessari verifi whether known user allow execut request oper sandbox ( brows , read , download , debug ) .",MESOS-5153,8.0
"masterallocatortest/1.rebalancedforupdatedweight flaki . observ asf ci : { code } [ run ] masterallocatortest/1.rebalancedforupdatedweight i0407 22:34:10.330394 29278 cluster.cpp:149 ] creat default 'local ' author i0407 22:34:10.466182 29278 leveldb.cpp:174 ] open db 135.608207m i0407 22:34:10.516398 29278 leveldb.cpp:181 ] compact db 50.159558m i0407 22:34:10.516464 29278 leveldb.cpp:196 ] creat db iter 34959n i0407 22:34:10.516484 29278 leveldb.cpp:202 ] seek begin db 10195n i0407 22:34:10.516496 29278 leveldb.cpp:271 ] iter 0 key db 7324n i0407 22:34:10.516547 29278 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0407 22:34:10.517277 29298 recover.cpp:447 ] start replica recoveri i0407 22:34:10.517693 29300 recover.cpp:473 ] replica empti statu i0407 22:34:10.520251 29310 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 4775 ) @ 172.17.0.3:35855 i0407 22:34:10.520611 29311 recover.cpp:193 ] receiv recov respons replica empti statu i0407 22:34:10.521164 29299 recover.cpp:564 ] updat replica statu start i0407 22:34:10.523435 29298 master.cpp:382 ] master f59f9057-a5c7-43e1-b129-96862e640a12 ( 129e11060069 ) start 172.17.0.3:35855 i0407 22:34:10.523473 29298 master.cpp:384 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/3rzy8c/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.29.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/3rzy8c/mast '' -- zk_session_timeout= '' 10sec '' i0407 22:34:10.523885 29298 master.cpp:433 ] master allow authent framework regist i0407 22:34:10.523901 29298 master.cpp:438 ] master allow authent agent regist i0407 22:34:10.523913 29298 credentials.hpp:37 ] load credenti authent '/tmp/3rzy8c/credentials' i0407 22:34:10.524298 29298 master.cpp:480 ] use default 'crammd5 ' authent i0407 22:34:10.524441 29298 master.cpp:551 ] use default 'basic ' http authent i0407 22:34:10.524564 29298 master.cpp:589 ] author enabl i0407 22:34:10.525269 29305 hierarchical.cpp:145 ] initi hierarch alloc process i0407 22:34:10.525333 29305 whitelist_watcher.cpp:77 ] no whitelist given i0407 22:34:10.527331 29298 master.cpp:1832 ] the newli elect leader master @ 172.17.0.3:35855 id f59f9057-a5c7-43e1-b129-96862e640a12 i0407 22:34:10.527441 29298 master.cpp:1845 ] elect lead master ! i0407 22:34:10.527545 29298 master.cpp:1532 ] recov registrar i0407 22:34:10.527889 29298 registrar.cpp:331 ] recov registrar i0407 22:34:10.549734 29299 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 28.25177m i0407 22:34:10.549782 29299 replica.cpp:320 ] persist replica statu start i0407 22:34:10.550010 29299 recover.cpp:473 ] replica start statu i0407 22:34:10.551352 29299 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 4777 ) @ 172.17.0.3:35855 i0407 22:34:10.551676 29299 recover.cpp:193 ] receiv recov respons replica start statu i0407 22:34:10.552315 29308 recover.cpp:564 ] updat replica statu vote i0407 22:34:10.574865 29308 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 22.413614m i0407 22:34:10.574928 29308 replica.cpp:320 ] persist replica statu vote i0407 22:34:10.575103 29308 recover.cpp:578 ] success join paxo group i0407 22:34:10.575346 29308 recover.cpp:462 ] recov process termin i0407 22:34:10.575913 29308 log.cpp:659 ] attempt start writer i0407 22:34:10.577512 29308 replica.cpp:493 ] replica receiv implicit promis request ( 4778 ) @ 172.17.0.3:35855 propos 1 i0407 22:34:10.599984 29308 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 22.453613m i0407 22:34:10.600026 29308 replica.cpp:342 ] persist promis 1 i0407 22:34:10.601773 29304 coordinator.cpp:238 ] coordin attempt fill miss posit i0407 22:34:10.603757 29307 replica.cpp:388 ] replica receiv explicit promis request ( 4779 ) @ 172.17.0.3:35855 posit 0 propos 2 i0407 22:34:10.634392 29307 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 30.269987m i0407 22:34:10.634829 29307 replica.cpp:712 ] persist action 0 i0407 22:34:10.637017 29297 replica.cpp:537 ] replica receiv write request posit 0 ( 4780 ) @ 172.17.0.3:35855 i0407 22:34:10.637099 29297 leveldb.cpp:436 ] read posit leveldb took 52948n i0407 22:34:10.676170 29297 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 38.917487m i0407 22:34:10.676352 29297 replica.cpp:712 ] persist action 0 i0407 22:34:10.677564 29306 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0407 22:34:10.717959 29306 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 40.306229m i0407 22:34:10.718202 29306 replica.cpp:712 ] persist action 0 i0407 22:34:10.718399 29306 replica.cpp:697 ] replica learn nop action posit 0 i0407 22:34:10.719883 29306 log.cpp:675 ] writer start end posit 0 i0407 22:34:10.721688 29305 leveldb.cpp:436 ] read posit leveldb took 75934n i0407 22:34:10.723640 29306 registrar.cpp:364 ] success fetch registri ( 0b ) 195648u i0407 22:34:10.723999 29306 registrar.cpp:463 ] appli 1 oper 108099n ; attempt updat 'registry' i0407 22:34:10.725077 29311 log.cpp:683 ] attempt append 170 byte log i0407 22:34:10.725328 29308 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0407 22:34:10.726552 29299 replica.cpp:537 ] replica receiv write request posit 1 ( 4781 ) @ 172.17.0.3:35855 i0407 22:34:10.759747 29299 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 33.089719m i0407 22:34:10.759976 29299 replica.cpp:712 ] persist action 1 i0407 22:34:10.761739 29299 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0407 22:34:10.801522 29299 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 39.694064m i0407 22:34:10.801602 29299 replica.cpp:712 ] persist action 1 i0407 22:34:10.801638 29299 replica.cpp:697 ] replica learn append action posit 1 i0407 22:34:10.803371 29311 registrar.cpp:508 ] success updat 'registri ' 79.163904m i0407 22:34:10.803829 29311 registrar.cpp:394 ] success recov registrar i0407 22:34:10.804585 29311 master.cpp:1640 ] recov 0 agent registri ( 131b ) ; allow 10min agent re-regist i0407 22:34:10.805269 29308 log.cpp:702 ] attempt truncat log 1 i0407 22:34:10.805721 29310 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0407 22:34:10.805276 29296 hierarchical.cpp:172 ] skip recoveri hierarch alloc : noth recov i0407 22:34:10.806529 29307 replica.cpp:537 ] replica receiv write request posit 2 ( 4782 ) @ 172.17.0.3:35855 i0407 22:34:10.843320 29307 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 36.77593m i0407 22:34:10.843531 29307 replica.cpp:712 ] persist action 2 i0407 22:34:10.845369 29311 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0407 22:34:10.885098 29311 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 39.641102m i0407 22:34:10.885401 29311 leveldb.cpp:399 ] delet ~1 key leveldb took 88701n i0407 22:34:10.885745 29311 replica.cpp:712 ] persist action 2 i0407 22:34:10.885862 29311 replica.cpp:697 ] replica learn truncat action posit 2 i0407 22:34:10.900660 29278 containerizer.cpp:155 ] use isol : posix/cpu , posix/mem , filesystem/posix w0407 22:34:10.901793 29278 backend.cpp:66 ] fail creat 'bind ' backend : bindbackend requir root privileg i0407 22:34:10.905488 29302 slave.cpp:201 ] agent start 111 ) @ 172.17.0.3:35855 i0407 22:34:10.905553 29302 slave.cpp:202 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_credentials= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.29.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya '' i0407 22:34:10.906365 29302 credentials.hpp:86 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/credential' i0407 22:34:10.906787 29302 slave.cpp:339 ] agent use credenti : test-princip i0407 22:34:10.907202 29302 credentials.hpp:37 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/http_credentials' i0407 22:34:10.907713 29302 slave.cpp:391 ] use default 'basic ' http authent i0407 22:34:10.908499 29302 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:10.910189 29302 slave.cpp:590 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:10.910362 29302 slave.cpp:598 ] agent attribut : [ ] i0407 22:34:10.910465 29302 slave.cpp:603 ] agent hostnam : 129e11060069 i0407 22:34:10.913280 29303 state.cpp:57 ] recov state '/tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/meta' i0407 22:34:10.914621 29303 status_update_manager.cpp:200 ] recov statu updat manag i0407 22:34:10.915226 29303 containerizer.cpp:416 ] recov container i0407 22:34:10.917246 29301 provisioner.cpp:245 ] provision recoveri complet i0407 22:34:10.917733 29301 slave.cpp:4784 ] finish recoveri i0407 22:34:10.918226 29301 slave.cpp:4956 ] queri resourc estim oversubscrib resourc i0407 22:34:10.918529 29301 slave.cpp:4970 ] receiv oversubscrib resourc resourc estim i0407 22:34:10.918908 29304 slave.cpp:939 ] new master detect master @ 172.17.0.3:35855 i0407 22:34:10.918988 29304 slave.cpp:1002 ] authent master master @ 172.17.0.3:35855 i0407 22:34:10.919098 29301 status_update_manager.cpp:174 ] paus send statu updat i0407 22:34:10.919309 29304 slave.cpp:1007 ] use default cram-md5 authenticate i0407 22:34:10.919535 29304 slave.cpp:975 ] detect new master i0407 22:34:10.919747 29308 authenticatee.cpp:121 ] creat new client sasl connect i0407 22:34:10.920413 29308 master.cpp:5695 ] authent slave ( 111 ) @ 172.17.0.3:35855 i0407 22:34:10.920650 29308 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 278 ) @ 172.17.0.3:35855 i0407 22:34:10.921020 29308 authenticator.cpp:98 ] creat new server sasl connect i0407 22:34:10.921308 29308 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0407 22:34:10.921424 29308 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0407 22:34:10.921596 29308 authenticator.cpp:203 ] receiv sasl authent start i0407 22:34:10.921752 29308 authenticator.cpp:325 ] authent requir step i0407 22:34:10.921957 29307 authenticatee.cpp:258 ] receiv sasl authent step i0407 22:34:10.922178 29308 authenticator.cpp:231 ] receiv sasl authent step i0407 22:34:10.922214 29308 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0407 22:34:10.922229 29308 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0407 22:34:10.922281 29308 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0407 22:34:10.922309 29308 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0407 22:34:10.922322 29308 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0407 22:34:10.922332 29308 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0407 22:34:10.922353 29308 authenticator.cpp:317 ] authent success i0407 22:34:10.922436 29307 authenticatee.cpp:298 ] authent success i0407 22:34:10.922587 29308 master.cpp:5725 ] success authent princip 'test-princip ' slave ( 111 ) @ 172.17.0.3:35855 i0407 22:34:10.922668 29299 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 278 ) @ 172.17.0.3:35855 i0407 22:34:10.923256 29307 slave.cpp:1072 ] success authent master master @ 172.17.0.3:35855 i0407 22:34:10.923429 29307 slave.cpp:1468 ] will retri registr 3.220345m necessari i0407 22:34:10.923707 29302 master.cpp:4406 ] regist agent slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) id f59f9057-a5c7-43e1-b129-96862e640a12-s0 i0407 22:34:10.924239 29309 registrar.cpp:463 ] appli 1 oper 105794n ; attempt updat 'registry' i0407 22:34:10.925787 29309 log.cpp:683 ] attempt append 339 byte log i0407 22:34:10.926028 29309 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0407 22:34:10.927139 29309 replica.cpp:537 ] replica receiv write request posit 3 ( 4797 ) @ 172.17.0.3:35855 i0407 22:34:10.929083 29305 slave.cpp:1468 ] will retri registr 39.293556m necessari i0407 22:34:10.929363 29305 master.cpp:4394 ] ignor regist agent messag slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:10.968843 29309 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 41.68025m i0407 22:34:10.969005 29309 replica.cpp:712 ] persist action 3 i0407 22:34:10.969741 29309 slave.cpp:1468 ] will retri registr 54.852242m necessari i0407 22:34:10.970118 29309 master.cpp:4394 ] ignor regist agent messag slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:10.970852 29306 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0407 22:34:11.010634 29306 leveldb.cpp:341 ] persist action ( 360 byte ) leveldb took 39.680272m i0407 22:34:11.010840 29306 replica.cpp:712 ] persist action 3 i0407 22:34:11.011014 29306 replica.cpp:697 ] replica learn append action posit 3 i0407 22:34:11.014020 29306 registrar.cpp:508 ] success updat 'registri ' 89.684224m i0407 22:34:11.014181 29296 log.cpp:702 ] attempt truncat log 3 i0407 22:34:11.014606 29296 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0407 22:34:11.015836 29298 replica.cpp:537 ] replica receiv write request posit 4 ( 4798 ) @ 172.17.0.3:35855 i0407 22:34:11.016973 29296 master.cpp:4474 ] regist agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:11.017518 29304 hierarchical.cpp:476 ] ad agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0407 22:34:11.017763 29311 slave.cpp:1116 ] regist master master @ 172.17.0.3:35855 ; given agent id f59f9057-a5c7-43e1-b129-96862e640a12-s0 i0407 22:34:11.018362 29311 fetcher.cpp:81 ] clear fetcher cach i0407 22:34:11.018870 29311 slave.cpp:1139 ] checkpoint slaveinfo '/tmp/masterallocatortest_1_rebalancedforupdatedweights_9acaya/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-s0/slave.info' i0407 22:34:11.018890 29307 status_update_manager.cpp:181 ] resum send statu updat i0407 22:34:11.019182 29304 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.019304 29304 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 1.077349m i0407 22:34:11.019493 29311 slave.cpp:1176 ] forward total oversubscrib resourc i0407 22:34:11.019726 29311 slave.cpp:3675 ] receiv ping slave-observ ( 112 ) @ 172.17.0.3:35855 i0407 22:34:11.019878 29299 master.cpp:4818 ] receiv updat agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) total oversubscrib resourc i0407 22:34:11.020845 29305 hierarchical.cpp:534 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 ( 129e11060069 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0407 22:34:11.021005 29305 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.021065 29305 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 173907n i0407 22:34:11.022289 29278 containerizer.cpp:155 ] use isol : posix/cpu , posix/mem , filesystem/posix w0407 22:34:11.023422 29278 backend.cpp:66 ] fail creat 'bind ' backend : bindbackend requir root privileg i0407 22:34:11.026309 29309 slave.cpp:201 ] agent start 112 ) @ 172.17.0.3:35855 i0407 22:34:11.026410 29309 slave.cpp:202 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_credentials= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.29.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o '' i0407 22:34:11.027070 29309 credentials.hpp:86 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/credential' i0407 22:34:11.027308 29309 slave.cpp:339 ] agent use credenti : test-princip i0407 22:34:11.027354 29309 credentials.hpp:37 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/http_credentials' i0407 22:34:11.027698 29309 slave.cpp:391 ] use default 'basic ' http authent i0407 22:34:11.028147 29309 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:11.028854 29309 slave.cpp:590 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:11.028998 29309 slave.cpp:598 ] agent attribut : [ ] i0407 22:34:11.029064 29309 slave.cpp:603 ] agent hostnam : 129e11060069 i0407 22:34:11.031188 29309 state.cpp:57 ] recov state '/tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/meta' i0407 22:34:11.031844 29300 status_update_manager.cpp:200 ] recov statu updat manag i0407 22:34:11.032091 29300 containerizer.cpp:416 ] recov container i0407 22:34:11.033805 29300 provisioner.cpp:245 ] provision recoveri complet i0407 22:34:11.034364 29300 slave.cpp:4784 ] finish recoveri i0407 22:34:11.061807 29300 slave.cpp:4956 ] queri resourc estim oversubscrib resourc i0407 22:34:11.062371 29300 slave.cpp:939 ] new master detect master @ 172.17.0.3:35855 i0407 22:34:11.062450 29300 slave.cpp:1002 ] authent master master @ 172.17.0.3:35855 i0407 22:34:11.062469 29300 slave.cpp:1007 ] use default cram-md5 authenticate i0407 22:34:11.062630 29300 slave.cpp:975 ] detect new master i0407 22:34:11.062737 29300 slave.cpp:4970 ] receiv oversubscrib resourc resourc estim i0407 22:34:11.062820 29300 status_update_manager.cpp:174 ] paus send statu updat i0407 22:34:11.062952 29300 authenticatee.cpp:121 ] creat new client sasl connect i0407 22:34:11.063413 29300 master.cpp:5695 ] authent slave ( 112 ) @ 172.17.0.3:35855 i0407 22:34:11.063591 29300 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 279 ) @ 172.17.0.3:35855 i0407 22:34:11.063907 29300 authenticator.cpp:98 ] creat new server sasl connect i0407 22:34:11.064159 29300 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0407 22:34:11.064201 29300 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0407 22:34:11.064296 29300 authenticator.cpp:203 ] receiv sasl authent start i0407 22:34:11.064363 29300 authenticator.cpp:325 ] authent requir step i0407 22:34:11.064443 29300 authenticatee.cpp:258 ] receiv sasl authent step i0407 22:34:11.064537 29300 authenticator.cpp:231 ] receiv sasl authent step i0407 22:34:11.064569 29300 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0407 22:34:11.064584 29300 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0407 22:34:11.064640 29300 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0407 22:34:11.064668 29300 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0407 22:34:11.064680 29300 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0407 22:34:11.064689 29300 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0407 22:34:11.064708 29300 authenticator.cpp:317 ] authent success i0407 22:34:11.064856 29300 authenticatee.cpp:298 ] authent success i0407 22:34:11.064941 29300 master.cpp:5725 ] success authent princip 'test-princip ' slave ( 112 ) @ 172.17.0.3:35855 i0407 22:34:11.065019 29300 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 279 ) @ 172.17.0.3:35855 i0407 22:34:11.065431 29305 slave.cpp:1072 ] success authent master master @ 172.17.0.3:35855 i0407 22:34:11.065580 29305 slave.cpp:1468 ] will retri registr 14.268351m necessari i0407 22:34:11.065948 29305 master.cpp:4406 ] regist agent slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) id f59f9057-a5c7-43e1-b129-96862e640a12-s1 i0407 22:34:11.066653 29296 registrar.cpp:463 ] appli 1 oper 190813n ; attempt updat 'registry' i0407 22:34:11.075197 29298 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 59.338116m i0407 22:34:11.075359 29298 replica.cpp:712 ] persist action 4 i0407 22:34:11.076177 29301 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0407 22:34:11.080481 29309 slave.cpp:1468 ] will retri registr 23.018984m necessari i0407 22:34:11.080770 29309 master.cpp:4394 ] ignor regist agent messag slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:11.100519 29301 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 24.288152m i0407 22:34:11.100792 29301 leveldb.cpp:399 ] delet ~2 key leveldb took 98264n i0407 22:34:11.100883 29301 replica.cpp:712 ] persist action 4 i0407 22:34:11.101002 29301 replica.cpp:697 ] replica learn truncat action posit 4 i0407 22:34:11.102180 29309 log.cpp:683 ] attempt append 505 byte log i0407 22:34:11.102334 29301 coordinator.cpp:348 ] coordin attempt write append action posit 5 i0407 22:34:11.103551 29309 replica.cpp:537 ] replica receiv write request posit 5 ( 4813 ) @ 172.17.0.3:35855 i0407 22:34:11.105705 29305 slave.cpp:1468 ] will retri registr 49.972787m necessari i0407 22:34:11.106020 29305 master.cpp:4394 ] ignor regist agent messag slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:11.126212 29309 leveldb.cpp:341 ] persist action ( 524 byte ) leveldb took 22.638848m i0407 22:34:11.126296 29309 replica.cpp:712 ] persist action 5 i0407 22:34:11.127374 29305 replica.cpp:691 ] replica receiv learn notic posit 5 @ 0.0.0.0:0 i0407 22:34:11.150754 29305 leveldb.cpp:341 ] persist action ( 526 byte ) leveldb took 23.376079m i0407 22:34:11.150952 29305 replica.cpp:712 ] persist action 5 i0407 22:34:11.150992 29305 replica.cpp:697 ] replica learn append action posit 5 i0407 22:34:11.154031 29305 registrar.cpp:508 ] success updat 'registri ' 87.26784m i0407 22:34:11.154491 29305 log.cpp:702 ] attempt truncat log 5 i0407 22:34:11.154824 29305 coordinator.cpp:348 ] coordin attempt write truncat action posit 6 i0407 22:34:11.155413 29308 slave.cpp:3675 ] receiv ping slave-observ ( 113 ) @ 172.17.0.3:35855 i0407 22:34:11.155467 29303 master.cpp:4474 ] regist agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:11.155580 29308 slave.cpp:1116 ] regist master master @ 172.17.0.3:35855 ; given agent id f59f9057-a5c7-43e1-b129-96862e640a12-s1 i0407 22:34:11.155606 29308 fetcher.cpp:81 ] clear fetcher cach i0407 22:34:11.155856 29304 status_update_manager.cpp:181 ] resum send statu updat i0407 22:34:11.156281 29308 slave.cpp:1139 ] checkpoint slaveinfo '/tmp/masterallocatortest_1_rebalancedforupdatedweights_h8kw9o/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-s1/slave.info' i0407 22:34:11.156661 29304 replica.cpp:537 ] replica receiv write request posit 6 ( 4814 ) @ 172.17.0.3:35855 i0407 22:34:11.156949 29305 hierarchical.cpp:476 ] ad agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0407 22:34:11.157217 29305 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.157346 29305 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 304432n i0407 22:34:11.157224 29308 slave.cpp:1176 ] forward total oversubscrib resourc i0407 22:34:11.157788 29303 master.cpp:4818 ] receiv updat agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) total oversubscrib resourc i0407 22:34:11.158424 29303 hierarchical.cpp:534 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 ( 129e11060069 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0407 22:34:11.158633 29303 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.158699 29303 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 178482n i0407 22:34:11.162139 29278 containerizer.cpp:155 ] use isol : posix/cpu , posix/mem , filesystem/posix w0407 22:34:11.192978 29278 backend.cpp:66 ] fail creat 'bind ' backend : bindbackend requir root privileg i0407 22:34:11.197527 29307 slave.cpp:201 ] agent start 113 ) @ 172.17.0.3:35855 i0407 22:34:11.197581 29307 slave.cpp:202 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_credentials= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.29.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru '' i0407 22:34:11.198328 29307 credentials.hpp:86 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/credential' i0407 22:34:11.198562 29307 slave.cpp:339 ] agent use credenti : test-princip i0407 22:34:11.198598 29307 credentials.hpp:37 ] load credenti authent '/tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/http_credentials' i0407 22:34:11.198884 29307 slave.cpp:391 ] use default 'basic ' http authent i0407 22:34:11.199286 29307 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:11.199820 29307 slave.cpp:590 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:11.199905 29307 slave.cpp:598 ] agent attribut : [ ] i0407 22:34:11.199920 29307 slave.cpp:603 ] agent hostnam : 129e11060069 i0407 22:34:11.201535 29297 state.cpp:57 ] recov state '/tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/meta' i0407 22:34:11.201773 29309 status_update_manager.cpp:200 ] recov statu updat manag i0407 22:34:11.202081 29307 containerizer.cpp:416 ] recov container i0407 22:34:11.202180 29304 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 45.487899m i0407 22:34:11.202221 29304 replica.cpp:712 ] persist action 6 i0407 22:34:11.203219 29302 replica.cpp:691 ] replica receiv learn notic posit 6 @ 0.0.0.0:0 i0407 22:34:11.205412 29301 provisioner.cpp:245 ] provision recoveri complet i0407 22:34:11.205984 29301 slave.cpp:4784 ] finish recoveri i0407 22:34:11.206735 29301 slave.cpp:4956 ] queri resourc estim oversubscrib resourc i0407 22:34:11.207351 29301 slave.cpp:4970 ] receiv oversubscrib resourc resourc estim i0407 22:34:11.207679 29301 slave.cpp:939 ] new master detect master @ 172.17.0.3:35855 i0407 22:34:11.207804 29309 status_update_manager.cpp:174 ] paus send statu updat i0407 22:34:11.208039 29301 slave.cpp:1002 ] authent master master @ 172.17.0.3:35855 i0407 22:34:11.208072 29301 slave.cpp:1007 ] use default cram-md5 authenticate i0407 22:34:11.208431 29301 slave.cpp:975 ] detect new master i0407 22:34:11.208650 29309 authenticatee.cpp:121 ] creat new client sasl connect i0407 22:34:11.208976 29309 master.cpp:5695 ] authent slave ( 113 ) @ 172.17.0.3:35855 i0407 22:34:11.209081 29307 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 280 ) @ 172.17.0.3:35855 i0407 22:34:11.209432 29304 authenticator.cpp:98 ] creat new server sasl connect i0407 22:34:11.209971 29304 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0407 22:34:11.210103 29304 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0407 22:34:11.210382 29304 authenticator.cpp:203 ] receiv sasl authent start i0407 22:34:11.210515 29304 authenticator.cpp:325 ] authent requir step i0407 22:34:11.210726 29304 authenticatee.cpp:258 ] receiv sasl authent step i0407 22:34:11.210940 29305 authenticator.cpp:231 ] receiv sasl authent step i0407 22:34:11.210980 29305 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0407 22:34:11.210997 29305 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0407 22:34:11.211060 29305 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0407 22:34:11.211100 29305 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0407 22:34:11.211175 29305 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0407 22:34:11.211244 29305 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0407 22:34:11.211272 29305 authenticator.cpp:317 ] authent success i0407 22:34:11.211462 29305 authenticatee.cpp:298 ] authent success i0407 22:34:11.211575 29305 master.cpp:5725 ] success authent princip 'test-princip ' slave ( 113 ) @ 172.17.0.3:35855 i0407 22:34:11.211673 29305 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 280 ) @ 172.17.0.3:35855 i0407 22:34:11.212026 29305 slave.cpp:1072 ] success authent master master @ 172.17.0.3:35855 i0407 22:34:11.212280 29305 slave.cpp:1468 ] will retri registr 6.415977m necessari i0407 22:34:11.212704 29304 master.cpp:4406 ] regist agent slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) id f59f9057-a5c7-43e1-b129-96862e640a12-s2 i0407 22:34:11.213373 29311 registrar.cpp:463 ] appli 1 oper 154555n ; attempt updat 'registry' i0407 22:34:11.223568 29303 master.cpp:4394 ] ignor regist agent messag slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:11.224171 29300 slave.cpp:1468 ] will retri registr 22.418267m necessari i0407 22:34:11.243433 29302 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 40.20863m i0407 22:34:11.243851 29302 leveldb.cpp:399 ] delet ~2 key leveldb took 204965n i0407 22:34:11.243980 29302 replica.cpp:712 ] persist action 6 i0407 22:34:11.244148 29302 replica.cpp:697 ] replica learn truncat action posit 6 i0407 22:34:11.245827 29302 log.cpp:683 ] attempt append 671 byte log i0407 22:34:11.246206 29310 coordinator.cpp:348 ] coordin attempt write append action posit 7 i0407 22:34:11.247114 29296 replica.cpp:537 ] replica receiv write request posit 7 ( 4829 ) @ 172.17.0.3:35855 i0407 22:34:11.248457 29304 slave.cpp:1468 ] will retri registr 14.981599m necessari i0407 22:34:11.248837 29302 master.cpp:4394 ] ignor regist agent messag slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:11.265728 29301 slave.cpp:1468 ] will retri registr 117.285894m necessari i0407 22:34:11.266026 29301 master.cpp:4394 ] ignor regist agent messag slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) admiss alreadi progress i0407 22:34:11.278012 29296 leveldb.cpp:341 ] persist action ( 690 byte ) leveldb took 30.789344m i0407 22:34:11.278064 29296 replica.cpp:712 ] persist action 7 i0407 22:34:11.278990 29303 replica.cpp:691 ] replica receiv learn notic posit 7 @ 0.0.0.0:0 i0407 22:34:11.337220 29303 leveldb.cpp:341 ] persist action ( 692 byte ) leveldb took 58.231676m i0407 22:34:11.337312 29303 replica.cpp:712 ] persist action 7 i0407 22:34:11.337347 29303 replica.cpp:697 ] replica learn append action posit 7 i0407 22:34:11.340283 29305 registrar.cpp:508 ] success updat 'registri ' 126.71616m i0407 22:34:11.340703 29309 log.cpp:702 ] attempt truncat log 7 i0407 22:34:11.341044 29309 coordinator.cpp:348 ] coordin attempt write truncat action posit 8 i0407 22:34:11.341847 29309 slave.cpp:3675 ] receiv ping slave-observ ( 114 ) @ 172.17.0.3:35855 i0407 22:34:11.342489 29309 slave.cpp:1116 ] regist master master @ 172.17.0.3:35855 ; given agent id f59f9057-a5c7-43e1-b129-96862e640a12-s2 i0407 22:34:11.342532 29309 fetcher.cpp:81 ] clear fetcher cach i0407 22:34:11.341804 29303 master.cpp:4474 ] regist agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] i0407 22:34:11.342871 29297 status_update_manager.cpp:181 ] resum send statu updat i0407 22:34:11.342267 29300 hierarchical.cpp:476 ] ad agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 ( 129e11060069 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0407 22:34:11.342963 29299 replica.cpp:537 ] replica receiv write request posit 8 ( 4830 ) @ 172.17.0.3:35855 i0407 22:34:11.343101 29300 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.343178 29300 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 242921n i0407 22:34:11.342921 29309 slave.cpp:1139 ] checkpoint slaveinfo '/tmp/masterallocatortest_1_rebalancedforupdatedweights_eg5sru/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-s2/slave.info' i0407 22:34:11.343636 29309 slave.cpp:1176 ] forward total oversubscrib resourc i0407 22:34:11.343863 29309 master.cpp:4818 ] receiv updat agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) total oversubscrib resourc i0407 22:34:11.344173 29278 sched.cpp:224 ] version : 0.29.0 i0407 22:34:11.344425 29309 hierarchical.cpp:534 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 ( 129e11060069 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0407 22:34:11.344568 29309 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.344621 29309 hierarchical.cpp:1165 ] perform alloc agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 155620n i0407 22:34:11.345155 29303 sched.cpp:328 ] new master detect master @ 172.17.0.3:35855 i0407 22:34:11.345387 29303 sched.cpp:384 ] authent master master @ 172.17.0.3:35855 i0407 22:34:11.345479 29303 sched.cpp:391 ] use default cram-md5 authenticate i0407 22:34:11.346035 29303 authenticatee.cpp:121 ] creat new client sasl connect i0407 22:34:11.346884 29303 master.cpp:5695 ] authent scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:11.347530 29303 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 281 ) @ 172.17.0.3:35855 i0407 22:34:11.349140 29303 authenticator.cpp:98 ] creat new server sasl connect i0407 22:34:11.349580 29303 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0407 22:34:11.349707 29303 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0407 22:34:11.349957 29309 authenticator.cpp:203 ] receiv sasl authent start i0407 22:34:11.350040 29309 authenticator.cpp:325 ] authent requir step i0407 22:34:11.350168 29309 authenticatee.cpp:258 ] receiv sasl authent step i0407 22:34:11.350275 29309 authenticator.cpp:231 ] receiv sasl authent step i0407 22:34:11.350309 29309 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0407 22:34:11.350323 29309 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0407 22:34:11.350375 29309 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0407 22:34:11.350407 29309 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0407 22:34:11.350420 29309 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0407 22:34:11.350430 29309 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0407 22:34:11.350450 29309 authenticator.cpp:317 ] authent success i0407 22:34:11.350550 29303 authenticatee.cpp:298 ] authent success i0407 22:34:11.350647 29309 master.cpp:5725 ] success authent princip 'test-princip ' scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:11.350803 29303 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 281 ) @ 172.17.0.3:35855 i0407 22:34:11.350986 29309 sched.cpp:474 ] success authent master master @ 172.17.0.3:35855 i0407 22:34:11.351011 29309 sched.cpp:779 ] send subscrib call master @ 172.17.0.3:35855 i0407 22:34:11.351109 29309 sched.cpp:812 ] will retri registr 82.651114m necessari i0407 22:34:11.351313 29296 master.cpp:2362 ] receiv subscrib call framework 'default ' scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:11.351343 29296 master.cpp:1871 ] author framework princip 'test-princip ' receiv offer role 'role1' i0407 22:34:11.351662 29310 master.cpp:2433 ] subscrib framework default checkpoint disabl capabl [ ] i0407 22:34:11.352442 29311 hierarchical.cpp:267 ] ad framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:11.353435 29309 sched.cpp:706 ] framework regist f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:11.353519 29309 sched.cpp:720 ] schedul : :regist took 66350n i0407 22:34:11.355201 29311 hierarchical.cpp:1586 ] no invers offer send ! i0407 22:34:11.355293 29311 hierarchical.cpp:1142 ] perform alloc 3 agent 2.836617m i0407 22:34:11.356238 29301 master.cpp:5524 ] send 3 offer framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:11.357260 29311 sched.cpp:876 ] schedul : :resourceoff took 327028n i0407 22:34:11.357628 29278 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:11.358330 29278 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:11.358959 29278 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:11.360607 29278 sched.cpp:224 ] version : 0.29.0 i0407 22:34:11.361264 29307 sched.cpp:328 ] new master detect master @ 172.17.0.3:35855 i0407 22:34:11.361342 29307 sched.cpp:384 ] authent master master @ 172.17.0.3:35855 i0407 22:34:11.361366 29307 sched.cpp:391 ] use default cram-md5 authenticate i0407 22:34:11.361670 29307 authenticatee.cpp:121 ] creat new client sasl connect i0407 22:34:11.361959 29307 master.cpp:5695 ] authent scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:11.362195 29307 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 282 ) @ 172.17.0.3:35855 i0407 22:34:11.362535 29311 authenticator.cpp:98 ] creat new server sasl connect i0407 22:34:11.362890 29307 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0407 22:34:11.362926 29307 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0407 22:34:11.363021 29307 authenticator.cpp:203 ] receiv sasl authent start i0407 22:34:11.363082 29307 authenticator.cpp:325 ] authent requir step i0407 22:34:11.363199 29311 authenticatee.cpp:258 ] receiv sasl authent step i0407 22:34:11.363313 29311 authenticator.cpp:231 ] receiv sasl authent step i0407 22:34:11.363406 29311 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0407 22:34:11.363512 29311 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0407 22:34:11.363605 29311 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0407 22:34:11.363651 29311 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '129e11060069 ' server fqdn : '129e11060069 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0407 22:34:11.363673 29311 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0407 22:34:11.363685 29311 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0407 22:34:11.363706 29311 authenticator.cpp:317 ] authent success i0407 22:34:11.363785 29307 authenticatee.cpp:298 ] authent success i0407 22:34:11.363858 29297 master.cpp:5725 ] success authent princip 'test-princip ' scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:11.363903 29311 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 282 ) @ 172.17.0.3:35855 i0407 22:34:11.365274 29297 sched.cpp:474 ] success authent master master @ 172.17.0.3:35855 i0407 22:34:11.365301 29297 sched.cpp:779 ] send subscrib call master @ 172.17.0.3:35855 i0407 22:34:11.365396 29297 sched.cpp:812 ] will retri registr 1.739883809sec necessari i0407 22:34:11.365500 29311 master.cpp:2362 ] receiv subscrib call framework 'default ' scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:11.365528 29311 master.cpp:1871 ] author framework princip 'test-princip ' receiv offer role 'role2' i0407 22:34:11.365952 29297 master.cpp:2433 ] subscrib framework default checkpoint disabl capabl [ ] i0407 22:34:11.366518 29297 sched.cpp:706 ] framework regist f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:11.366564 29311 hierarchical.cpp:267 ] ad framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:11.366590 29297 sched.cpp:720 ] schedul : :regist took 57363n i0407 22:34:11.366768 29311 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.366837 29311 hierarchical.cpp:1586 ] no invers offer send ! i0407 22:34:11.366914 29311 hierarchical.cpp:1142 ] perform alloc 3 agent 340908n i0407 22:34:11.369886 29309 process.cpp:3165 ] handl http event process 'master ' path : '/master/weights' i0407 22:34:11.370643 29309 http.cpp:313 ] http put /master/weight 172.17.0.3:59397 i0407 22:34:11.370762 29309 weights_handler.cpp:58 ] updat weight request : ' [ { `` role '' : '' role2 '' , '' weight '' :2.0 } ] ' i0407 22:34:11.370908 29309 weights_handler.cpp:198 ] author princip 'test-princip ' updat weight role ' [ role2 ] ' i0407 22:34:11.372067 29306 registrar.cpp:463 ] appli 1 oper 136060n ; attempt updat 'registry' i0407 22:34:11.388222 29299 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 45.245469m i0407 22:34:11.388381 29299 replica.cpp:712 ] persist action 8 i0407 22:34:11.389389 29305 replica.cpp:691 ] replica receiv learn notic posit 8 @ 0.0.0.0:0 i0407 22:34:11.435415 29305 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 45.918275m i0407 22:34:11.435688 29305 leveldb.cpp:399 ] delet ~2 key leveldb took 98518n i0407 22:34:11.435835 29305 replica.cpp:712 ] persist action 8 i0407 22:34:11.435956 29305 replica.cpp:697 ] replica learn truncat action posit 8 i0407 22:34:11.437063 29310 log.cpp:683 ] attempt append 691 byte log i0407 22:34:11.437297 29300 coordinator.cpp:348 ] coordin attempt write append action posit 9 i0407 22:34:11.437979 29300 replica.cpp:537 ] replica receiv write request posit 9 ( 4834 ) @ 172.17.0.3:35855 i0407 22:34:11.479363 29300 leveldb.cpp:341 ] persist action ( 710 byte ) leveldb took 41.36295m i0407 22:34:11.479432 29300 replica.cpp:712 ] persist action 9 i0407 22:34:11.480434 29296 replica.cpp:691 ] replica receiv learn notic posit 9 @ 0.0.0.0:0 i0407 22:34:11.521299 29296 leveldb.cpp:341 ] persist action ( 712 byte ) leveldb took 40.855981m i0407 22:34:11.521378 29296 replica.cpp:712 ] persist action 9 i0407 22:34:11.521412 29296 replica.cpp:697 ] replica learn append action posit 9 i0407 22:34:11.524554 29304 registrar.cpp:508 ] success updat 'registri ' 152.402176m i0407 22:34:11.524790 29298 log.cpp:702 ] attempt truncat log 9 i0407 22:34:11.524960 29304 coordinator.cpp:348 ] coordin attempt write truncat action posit 10 i0407 22:34:11.525243 29298 hierarchical.cpp:1491 ] no resourc avail alloc ! i0407 22:34:11.525387 29298 hierarchical.cpp:1586 ] no invers offer send ! i0407 22:34:11.525538 29298 hierarchical.cpp:1142 ] perform alloc 3 agent 540681n i0407 22:34:11.525856 29296 replica.cpp:537 ] replica receiv write request posit 10 ( 4835 ) @ 172.17.0.3:35855 i0407 22:34:11.526267 29308 sched.cpp:902 ] rescind offer f59f9057-a5c7-43e1-b129-96862e640a12-o1 i0407 22:34:11.526398 29308 sched.cpp:913 ] schedul : :offerrescind took 54437n i0407 22:34:11.526425 29298 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:11.527235 29299 sched.cpp:902 ] rescind offer f59f9057-a5c7-43e1-b129-96862e640a12-o2 i0407 22:34:11.527299 29299 sched.cpp:913 ] schedul : :offerrescind took 29764n i0407 22:34:11.527825 29300 sched.cpp:902 ] rescind offer f59f9057-a5c7-43e1-b129-96862e640a12-o0 i0407 22:34:11.527920 29298 hierarchical.cpp:1586 ] no invers offer send ! i0407 22:34:11.527990 29298 hierarchical.cpp:1142 ] perform alloc 3 agent 1.481251m i0407 22:34:11.528009 29300 sched.cpp:913 ] schedul : :offerrescind took 333035n i0407 22:34:11.528591 29298 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:11.529536 29311 master.cpp:5524 ] send 1 offer framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:11.529846 29298 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:11.530747 29304 sched.cpp:876 ] schedul : :resourceoff took 128400n i0407 22:34:11.560456 29296 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 34.585376m i0407 22:34:11.560539 29296 replica.cpp:712 ] persist action 10 i0407 22:34:11.564628 29303 replica.cpp:691 ] replica receiv learn notic posit 10 @ 0.0.0.0:0 i0407 22:34:11.601330 29303 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 36.57815m i0407 22:34:11.601774 29303 leveldb.cpp:399 ] delet ~2 key leveldb took 221499n i0407 22:34:11.601899 29303 replica.cpp:712 ] persist action 10 i0407 22:34:11.602052 29303 replica.cpp:697 ] replica learn truncat action posit 10 i0407 22:34:12.531602 29308 hierarchical.cpp:1586 ] no invers offer send ! i0407 22:34:12.532578 29308 hierarchical.cpp:1142 ] perform alloc 3 agent 3.892929m i0407 22:34:12.532403 29306 master.cpp:5524 ] send 1 offer framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 .. / .. /src/tests/master_allocator_tests.cpp:1587 : failur mock function call time expect - return directli . function call : resourceoff ( 0x7fffe87e3370 , @ 0x2adef432e6f0 { 144-byte object < e0-9c 76-ea de-2a 00-00 00-00 00-00 00-00 00-00 1f-00 00-00 00-00 00-00 90-4b 00-2c df-2a 00-00 30-6a 00-2c df-2a 00-00 80-6a 00-2c df-2a 00-00 20-62 00-2c df-2a 00-00 60-38 00-2c df-2a 00-00 ... 04-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 > } ) expect : call actual : call twice - over-satur activ i0407 22:34:12.533665 29301 sched.cpp:876 ] schedul : :resourceoff took 250853n i0407 22:34:12.533915 29306 master.cpp:5524 ] send 1 offer framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:12.534454 29306 sched.cpp:876 ] schedul : :resourceoff took 157733n .. / .. /src/tests/master_allocator_tests.cpp:1629 : failur valu : framework2offers.get ( ) .size ( ) actual : 1 expect : 2u which : 2 i0407 22:34:12.534997 29278 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:4096 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0407 22:34:12.537264 29301 master.cpp:1275 ] framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 disconnect i0407 22:34:12.537297 29301 master.cpp:2658 ] disconnect framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:12.537330 29301 master.cpp:2682 ] deactiv framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 w0407 22:34:12.537849 29301 master.hpp:1822 ] master attempt send messag disconnect framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 w0407 22:34:12.538306 29301 master.hpp:1822 ] master attempt send messag disconnect framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:12.538394 29301 master.cpp:1299 ] give framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 0n failov i0407 22:34:12.539371 29302 hierarchical.cpp:378 ] deactiv framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.540053 29302 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.540732 29302 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.540974 29301 master.cpp:1275 ] framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 disconnect i0407 22:34:12.541178 29301 master.cpp:2658 ] disconnect framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:12.541292 29301 master.cpp:2682 ] deactiv framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:12.541553 29300 hierarchical.cpp:378 ] deactiv framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:12.542654 29300 hierarchical.cpp:894 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : ) agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 w0407 22:34:12.543051 29301 master.hpp:1822 ] master attempt send messag disconnect framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:12.543525 29301 master.cpp:1299 ] give framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 0n failov i0407 22:34:12.543861 29301 master.cpp:5376 ] framework failov timeout , remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:12.543959 29301 master.cpp:6109 ] remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 ( default ) scheduler-37080386-2aa8-4592-bf09-8288bd04727a @ 172.17.0.3:35855 i0407 22:34:12.544445 29301 slave.cpp:2226 ] ask shut framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 master @ 172.17.0.3:35855 w0407 22:34:12.545446 29301 slave.cpp:2241 ] can shut unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.544556 29300 slave.cpp:2226 ] ask shut framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 master @ 172.17.0.3:35855 w0407 22:34:12.545661 29300 slave.cpp:2241 ] can shut unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.545774 29300 slave.cpp:811 ] agent termin i0407 22:34:12.544791 29305 hierarchical.cpp:329 ] remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.545241 29296 master.cpp:5376 ] framework failov timeout , remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 i0407 22:34:12.544518 29302 slave.cpp:2226 ] ask shut framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 master @ 172.17.0.3:35855 i0407 22:34:12.546140 29296 master.cpp:6109 ] remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 ( default ) scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721 @ 172.17.0.3:35855 w0407 22:34:12.546159 29302 slave.cpp:2241 ] can shut unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 i0407 22:34:12.546496 29296 master.cpp:1236 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) disconnect i0407 22:34:12.546527 29296 master.cpp:2717 ] disconnect agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.546581 29296 master.cpp:2736 ] deactiv agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 slave ( 111 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.546752 29296 slave.cpp:2226 ] ask shut framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 master @ 172.17.0.3:35855 w0407 22:34:12.546782 29296 slave.cpp:2241 ] can shut unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:12.546844 29296 slave.cpp:2226 ] ask shut framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 master @ 172.17.0.3:35855 w0407 22:34:12.546869 29296 slave.cpp:2241 ] can shut unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:12.547111 29296 hierarchical.cpp:329 ] remov framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 i0407 22:34:12.547302 29296 hierarchical.cpp:563 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 deactiv i0407 22:34:12.553478 29278 slave.cpp:811 ] agent termin i0407 22:34:12.553766 29306 master.cpp:1236 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) disconnect i0407 22:34:12.555483 29306 master.cpp:2717 ] disconnect agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.555858 29306 master.cpp:2736 ] deactiv agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 slave ( 112 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.556190 29307 hierarchical.cpp:563 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 deactiv i0407 22:34:12.559095 29299 slave.cpp:811 ] agent termin i0407 22:34:12.559301 29300 master.cpp:1236 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) disconnect i0407 22:34:12.559327 29300 master.cpp:2717 ] disconnect agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.559370 29300 master.cpp:2736 ] deactiv agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 slave ( 113 ) @ 172.17.0.3:35855 ( 129e11060069 ) i0407 22:34:12.559516 29309 hierarchical.cpp:563 ] agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 deactiv i0407 22:34:12.561872 29278 master.cpp:1089 ] master termin i0407 22:34:12.562566 29304 hierarchical.cpp:508 ] remov agent f59f9057-a5c7-43e1-b129-96862e640a12-s2 i0407 22:34:12.562890 29304 hierarchical.cpp:508 ] remov agent f59f9057-a5c7-43e1-b129-96862e640a12-s1 i0407 22:34:12.565459 29304 hierarchical.cpp:508 ] remov agent f59f9057-a5c7-43e1-b129-96862e640a12-s0 [ fail ] masterallocatortest/1.rebalancedforupdatedweight , typeparam = meso : :intern : :test : :modul < meso : :master : :alloc : :alloc , ( meso : :intern : :test : :moduleid ) 6 > ( 2240 ms ) { code }",MESOS-5146,1.0
add agent flag http author . flag ad agent : 1 . enabl author ( { { -- author } } ) 2 . provid acl ( { { -- acl } } ),MESOS-5142,2.0
"some provisionerdockerlocalstoretest . * flaki due tar issu . these test still occasion fail meso 1.5.0-wip : { code } provisionerdockerlocalstoretest.localstoretestwithtar provisionerdockerlocalstoretest.metadatamanageriniti provisionerdockerlocalstoretest.missinglay { code } found asf ci test 0.28.1-rc2 { code } [ run ] provisionerdockerlocalstoretest.localstoretestwithtar e0406 18:29:30.870481 520 shell.hpp:93 ] command 'hadoop version 2 > & 1 ' fail ; output : sh : 1 : hadoop : found e0406 18:29:30.870576 520 fetcher.cpp:59 ] fail creat uri fetcher plugin 'hadoop ' : fail creat hdf client : fail execut 'hadoop version 2 > & 1 ' ; command either found exit non-zero exit statu : 127 i0406 18:29:30.871052 520 local_puller.cpp:90 ] creat local puller docker registri '/tmp/3l8zbv/images' i0406 18:29:30.873325 539 metadata_manager.cpp:159 ] look imag 'abc' i0406 18:29:30.874438 539 local_puller.cpp:142 ] untar imag 'abc ' '/tmp/3l8zbv/images/abc.tar ' '/tmp/3l8zbv/store/staging/5tw8bd' i0406 18:29:30.901916 547 local_puller.cpp:162 ] the repositori json file imag 'abc ' ' { `` abc '' : { `` latest '' : '' 456 '' } } ' i0406 18:29:30.902304 547 local_puller.cpp:290 ] extract layer tar ball '/tmp/3l8zbv/store/staging/5tw8bd/123/layer.tar rootf '/tmp/3l8zbv/store/staging/5tw8bd/123/rootfs' i0406 18:29:30.909144 547 local_puller.cpp:290 ] extract layer tar ball '/tmp/3l8zbv/store/staging/5tw8bd/456/layer.tar rootf '/tmp/3l8zbv/store/staging/5tw8bd/456/rootfs' .. / .. /src/tests/containerizer/provisioner_docker_tests.cpp:183 : failur ( imageinfo ) .failur ( ) : collect fail : subprocess 'tar , tar , -x , -f , /tmp/3l8zbv/store/staging/5tw8bd/456/layer.tar , -c , /tmp/3l8zbv/store/staging/5tw8bd/456/rootf ' fail : tar : thi look like tar archiv tar : exit failur statu due previou error [ fail ] provisionerdockerlocalstoretest.localstoretestwithtar ( 243 ms ) { code }",MESOS-5139,2.0
"persistentvolumetest.accesspersistentvolum flaki observ asf ci : { code } [ run ] diskresource/persistentvolumetest.accesspersistentvolume/0 i0405 17:29:19.134435 31837 cluster.cpp:139 ] creat default 'local ' author i0405 17:29:19.251143 31837 leveldb.cpp:174 ] open db 116.386403m i0405 17:29:19.310050 31837 leveldb.cpp:181 ] compact db 58.80688m i0405 17:29:19.310180 31837 leveldb.cpp:196 ] creat db iter 37145n i0405 17:29:19.310199 31837 leveldb.cpp:202 ] seek begin db 4212n i0405 17:29:19.310210 31837 leveldb.cpp:271 ] iter 0 key db 410n i0405 17:29:19.310279 31837 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0405 17:29:19.311069 31861 recover.cpp:447 ] start replica recoveri i0405 17:29:19.311362 31861 recover.cpp:473 ] replica empti statu i0405 17:29:19.312641 31861 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 14359 ) @ 172.17.0.4:43972 i0405 17:29:19.313045 31860 recover.cpp:193 ] receiv recov respons replica empti statu i0405 17:29:19.313608 31860 recover.cpp:564 ] updat replica statu start i0405 17:29:19.316416 31867 master.cpp:376 ] master 9565ff6f-f1b6-4259-8430-690e635c391f ( 4090d10eba90 ) start 172.17.0.4:43972 i0405 17:29:19.316470 31867 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/0a9elu/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.29.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/0a9elu/mast '' -- zk_session_timeout= '' 10sec '' i0405 17:29:19.316938 31867 master.cpp:427 ] master allow authent framework regist i0405 17:29:19.316951 31867 master.cpp:432 ] master allow authent agent regist i0405 17:29:19.316961 31867 credentials.hpp:37 ] load credenti authent '/tmp/0a9elu/credentials' i0405 17:29:19.317402 31867 master.cpp:474 ] use default 'crammd5 ' authent i0405 17:29:19.317643 31867 master.cpp:545 ] use default 'basic ' http authent i0405 17:29:19.317854 31867 master.cpp:583 ] author enabl i0405 17:29:19.318081 31864 whitelist_watcher.cpp:77 ] no whitelist given i0405 17:29:19.318079 31861 hierarchical.cpp:144 ] initi hierarch alloc process i0405 17:29:19.320838 31864 master.cpp:1826 ] the newli elect leader master @ 172.17.0.4:43972 id 9565ff6f-f1b6-4259-8430-690e635c391f i0405 17:29:19.320888 31864 master.cpp:1839 ] elect lead master ! i0405 17:29:19.320909 31864 master.cpp:1526 ] recov registrar i0405 17:29:19.321218 31871 registrar.cpp:331 ] recov registrar i0405 17:29:19.347045 31860 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 33.164133m i0405 17:29:19.347126 31860 replica.cpp:320 ] persist replica statu start i0405 17:29:19.347611 31869 recover.cpp:473 ] replica start statu i0405 17:29:19.349215 31871 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 14361 ) @ 172.17.0.4:43972 i0405 17:29:19.349653 31870 recover.cpp:193 ] receiv recov respons replica start statu i0405 17:29:19.350236 31866 recover.cpp:564 ] updat replica statu vote i0405 17:29:19.388882 31864 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 38.38299m i0405 17:29:19.388993 31864 replica.cpp:320 ] persist replica statu vote i0405 17:29:19.389369 31856 recover.cpp:578 ] success join paxo group i0405 17:29:19.389735 31856 recover.cpp:462 ] recov process termin i0405 17:29:19.390476 31868 log.cpp:659 ] attempt start writer i0405 17:29:19.392125 31862 replica.cpp:493 ] replica receiv implicit promis request ( 14362 ) @ 172.17.0.4:43972 propos 1 i0405 17:29:19.430706 31862 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 38.505062m i0405 17:29:19.430816 31862 replica.cpp:342 ] persist promis 1 i0405 17:29:19.431918 31856 coordinator.cpp:238 ] coordin attempt fill miss posit i0405 17:29:19.433725 31861 replica.cpp:388 ] replica receiv explicit promis request ( 14363 ) @ 172.17.0.4:43972 posit 0 propos 2 i0405 17:29:19.472491 31861 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 38.659492m i0405 17:29:19.472595 31861 replica.cpp:712 ] persist action 0 i0405 17:29:19.474556 31864 replica.cpp:537 ] replica receiv write request posit 0 ( 14364 ) @ 172.17.0.4:43972 i0405 17:29:19.474652 31864 leveldb.cpp:436 ] read posit leveldb took 49423n i0405 17:29:19.528175 31864 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 53.443616m i0405 17:29:19.528300 31864 replica.cpp:712 ] persist action 0 i0405 17:29:19.529389 31865 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0405 17:29:19.571137 31865 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 41.676495m i0405 17:29:19.571254 31865 replica.cpp:712 ] persist action 0 i0405 17:29:19.571302 31865 replica.cpp:697 ] replica learn nop action posit 0 i0405 17:29:19.572322 31856 log.cpp:675 ] writer start end posit 0 i0405 17:29:19.574060 31861 leveldb.cpp:436 ] read posit leveldb took 83200n i0405 17:29:19.575417 31864 registrar.cpp:364 ] success fetch registri ( 0b ) 0n i0405 17:29:19.575565 31864 registrar.cpp:463 ] appli 1 oper 46419n ; attempt updat 'registry' i0405 17:29:19.576517 31857 log.cpp:683 ] attempt append 170 byte log i0405 17:29:19.576849 31857 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0405 17:29:19.578390 31857 replica.cpp:537 ] replica receiv write request posit 1 ( 14365 ) @ 172.17.0.4:43972 i0405 17:29:19.780277 31857 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 201.808617m i0405 17:29:19.780366 31857 replica.cpp:712 ] persist action 1 i0405 17:29:19.782024 31857 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0405 17:29:19.823770 31857 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 41.667662m i0405 17:29:19.823851 31857 replica.cpp:712 ] persist action 1 i0405 17:29:19.823889 31857 replica.cpp:697 ] replica learn append action posit 1 i0405 17:29:19.825701 31867 registrar.cpp:508 ] success updat 'registri ' 0n i0405 17:29:19.825929 31867 registrar.cpp:394 ] success recov registrar i0405 17:29:19.826015 31857 log.cpp:702 ] attempt truncat log 1 i0405 17:29:19.826262 31867 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0405 17:29:19.827647 31867 replica.cpp:537 ] replica receiv write request posit 2 ( 14366 ) @ 172.17.0.4:43972 i0405 17:29:19.828018 31857 master.cpp:1634 ] recov 0 agent registri ( 131b ) ; allow 10min agent re-regist i0405 17:29:19.828065 31861 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0405 17:29:19.865555 31867 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 37.822178m i0405 17:29:19.865661 31867 replica.cpp:712 ] persist action 2 i0405 17:29:19.866921 31867 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0405 17:29:19.907341 31867 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 40.356649m i0405 17:29:19.907531 31867 leveldb.cpp:399 ] delet ~1 key leveldb took 91109n i0405 17:29:19.907560 31867 replica.cpp:712 ] persist action 2 i0405 17:29:19.907599 31867 replica.cpp:697 ] replica learn truncat action posit 2 i0405 17:29:19.923305 31837 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:2048 tri semicolon-delimit string format instead i0405 17:29:19.926491 31837 containerizer.cpp:155 ] use isol : posix/cpu , posix/mem , filesystem/posix w0405 17:29:19.927836 31837 backend.cpp:66 ] fail creat 'bind ' backend : bindbackend requir root privileg i0405 17:29:19.932029 31862 slave.cpp:200 ] agent start 441 ) @ 172.17.0.4:43972 i0405 17:29:19.932086 31862 slave.cpp:201 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticate_http= '' true '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- http_credentials= '' /tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/http_credenti '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.29.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' [ { `` name '' : '' cpu '' , '' role '' : '' * '' , '' scalar '' : { `` valu '' :2.0 } , '' type '' : '' scalar '' } , { `` name '' : '' mem '' , '' role '' : '' * '' , '' scalar '' : { `` valu '' :2048.0 } , '' type '' : '' scalar '' } , { `` name '' : '' disk '' , '' role '' : '' role1 '' , '' scalar '' : { `` valu '' :4096.0 } , '' type '' : '' scalar '' } ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac '' i0405 17:29:19.932665 31862 credentials.hpp:86 ] load credenti authent '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/credential' i0405 17:29:19.932934 31862 slave.cpp:338 ] agent use credenti : test-princip i0405 17:29:19.932968 31862 credentials.hpp:37 ] load credenti authent '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/http_credentials' i0405 17:29:19.933284 31862 slave.cpp:390 ] use default 'basic ' http authent i0405 17:29:19.934916 31837 sched.cpp:222 ] version : 0.29.0 i0405 17:29:19.935566 31862 slave.cpp:589 ] agent resourc : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :4096 ; port ( * ) : [ 31000-32000 ] i0405 17:29:19.935664 31862 slave.cpp:597 ] agent attribut : [ ] i0405 17:29:19.935679 31862 slave.cpp:602 ] agent hostnam : 4090d10eba90 i0405 17:29:19.938390 31864 state.cpp:57 ] recov state '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/meta' i0405 17:29:19.940608 31869 sched.cpp:326 ] new master detect master @ 172.17.0.4:43972 i0405 17:29:19.940749 31869 sched.cpp:382 ] authent master master @ 172.17.0.4:43972 i0405 17:29:19.940773 31869 sched.cpp:389 ] use default cram-md5 authenticate i0405 17:29:19.942371 31869 authenticatee.cpp:121 ] creat new client sasl connect i0405 17:29:19.942873 31859 master.cpp:5679 ] authent scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:19.943156 31859 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 896 ) @ 172.17.0.4:43972 i0405 17:29:19.943507 31863 authenticator.cpp:98 ] creat new server sasl connect i0405 17:29:19.943740 31859 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0405 17:29:19.943783 31859 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0405 17:29:19.943892 31859 authenticator.cpp:203 ] receiv sasl authent start i0405 17:29:19.943977 31859 authenticator.cpp:325 ] authent requir step i0405 17:29:19.944066 31859 authenticatee.cpp:258 ] receiv sasl authent step i0405 17:29:19.944164 31859 authenticator.cpp:231 ] receiv sasl authent step i0405 17:29:19.944193 31859 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4090d10eba90 ' server fqdn : '4090d10eba90 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0405 17:29:19.944206 31859 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0405 17:29:19.944268 31859 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0405 17:29:19.944300 31859 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4090d10eba90 ' server fqdn : '4090d10eba90 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0405 17:29:19.944313 31859 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0405 17:29:19.944321 31859 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0405 17:29:19.944339 31859 authenticator.cpp:317 ] authent success i0405 17:29:19.944541 31859 authenticatee.cpp:298 ] authent success i0405 17:29:19.944655 31859 master.cpp:5709 ] success authent princip 'test-princip ' scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:19.944737 31859 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 896 ) @ 172.17.0.4:43972 i0405 17:29:19.945111 31859 sched.cpp:472 ] success authent master master @ 172.17.0.4:43972 i0405 17:29:19.945132 31859 sched.cpp:777 ] send subscrib call master @ 172.17.0.4:43972 i0405 17:29:19.945591 31859 sched.cpp:810 ] will retri registr 372.80738m necessari i0405 17:29:19.945744 31865 master.cpp:2346 ] receiv subscrib call framework 'default ' scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:19.945838 31865 master.cpp:1865 ] author framework princip 'test-princip ' receiv offer role 'role1' i0405 17:29:19.946194 31865 master.cpp:2417 ] subscrib framework default checkpoint disabl capabl [ ] i0405 17:29:19.946866 31866 hierarchical.cpp:266 ] ad framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:19.946974 31866 hierarchical.cpp:1490 ] no resourc avail alloc ! i0405 17:29:19.947010 31866 hierarchical.cpp:1585 ] no invers offer send ! i0405 17:29:19.947054 31865 sched.cpp:704 ] framework regist 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:19.947074 31866 hierarchical.cpp:1141 ] perform alloc 0 agent 178242n i0405 17:29:19.947124 31865 sched.cpp:718 ] schedul : :regist took 38907n i0405 17:29:19.948712 31866 status_update_manager.cpp:200 ] recov statu updat manag i0405 17:29:19.948901 31866 containerizer.cpp:416 ] recov container i0405 17:29:19.951021 31866 provisioner.cpp:245 ] provision recoveri complet i0405 17:29:19.951802 31866 slave.cpp:4773 ] finish recoveri i0405 17:29:19.952518 31866 slave.cpp:4945 ] queri resourc estim oversubscrib resourc i0405 17:29:19.953248 31866 slave.cpp:928 ] new master detect master @ 172.17.0.4:43972 i0405 17:29:19.953305 31865 status_update_manager.cpp:174 ] paus send statu updat i0405 17:29:19.953626 31866 slave.cpp:991 ] authent master master @ 172.17.0.4:43972 i0405 17:29:19.953716 31866 slave.cpp:996 ] use default cram-md5 authenticate i0405 17:29:19.954074 31866 slave.cpp:964 ] detect new master i0405 17:29:19.954167 31861 authenticatee.cpp:121 ] creat new client sasl connect i0405 17:29:19.954372 31866 slave.cpp:4959 ] receiv oversubscrib resourc resourc estim i0405 17:29:19.954756 31866 master.cpp:5679 ] authent slave ( 441 ) @ 172.17.0.4:43972 i0405 17:29:19.954944 31861 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 897 ) @ 172.17.0.4:43972 i0405 17:29:19.955368 31863 authenticator.cpp:98 ] creat new server sasl connect i0405 17:29:19.955687 31861 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0405 17:29:19.955801 31861 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0405 17:29:19.956075 31861 authenticator.cpp:203 ] receiv sasl authent start i0405 17:29:19.956279 31861 authenticator.cpp:325 ] authent requir step i0405 17:29:19.956455 31861 authenticatee.cpp:258 ] receiv sasl authent step i0405 17:29:19.956676 31861 authenticator.cpp:231 ] receiv sasl authent step i0405 17:29:19.956815 31861 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4090d10eba90 ' server fqdn : '4090d10eba90 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0405 17:29:19.956907 31861 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0405 17:29:19.957044 31861 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0405 17:29:19.957166 31861 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4090d10eba90 ' server fqdn : '4090d10eba90 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0405 17:29:19.957264 31861 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0405 17:29:19.957353 31861 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0405 17:29:19.957449 31861 authenticator.cpp:317 ] authent success i0405 17:29:19.957664 31857 authenticatee.cpp:298 ] authent success i0405 17:29:19.957813 31857 master.cpp:5709 ] success authent princip 'test-princip ' slave ( 441 ) @ 172.17.0.4:43972 i0405 17:29:19.958008 31861 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 897 ) @ 172.17.0.4:43972 i0405 17:29:19.958732 31857 slave.cpp:1061 ] success authent master master @ 172.17.0.4:43972 i0405 17:29:19.958930 31857 slave.cpp:1457 ] will retri registr 18.568334m necessari i0405 17:29:19.959262 31857 master.cpp:4390 ] regist agent slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) id 9565ff6f-f1b6-4259-8430-690e635c391f-s0 i0405 17:29:19.959934 31857 registrar.cpp:463 ] appli 1 oper 99197n ; attempt updat 'registry' i0405 17:29:19.961587 31857 log.cpp:683 ] attempt append 343 byte log i0405 17:29:19.961879 31857 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0405 17:29:19.963135 31857 replica.cpp:537 ] replica receiv write request posit 3 ( 14381 ) @ 172.17.0.4:43972 i0405 17:29:19.999408 31857 leveldb.cpp:341 ] persist action ( 362 byte ) leveldb took 36.200109m i0405 17:29:19.999512 31857 replica.cpp:712 ] persist action 3 i0405 17:29:20.001049 31869 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0405 17:29:20.038849 31869 leveldb.cpp:341 ] persist action ( 364 byte ) leveldb took 37.709507m i0405 17:29:20.038930 31869 replica.cpp:712 ] persist action 3 i0405 17:29:20.038965 31869 replica.cpp:697 ] replica learn append action posit 3 i0405 17:29:20.041484 31869 registrar.cpp:508 ] success updat 'registri ' 0n i0405 17:29:20.041785 31869 log.cpp:702 ] attempt truncat log 3 i0405 17:29:20.042364 31859 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0405 17:29:20.043767 31859 replica.cpp:537 ] replica receiv write request posit 4 ( 14382 ) @ 172.17.0.4:43972 i0405 17:29:20.044585 31869 master.cpp:4458 ] regist agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :4096 ; port ( * ) : [ 31000-32000 ] i0405 17:29:20.044910 31864 slave.cpp:1105 ] regist master master @ 172.17.0.4:43972 ; given agent id 9565ff6f-f1b6-4259-8430-690e635c391f-s0 i0405 17:29:20.045075 31864 fetcher.cpp:81 ] clear fetcher cach i0405 17:29:20.045140 31870 hierarchical.cpp:476 ] ad agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 ( 4090d10eba90 ) cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :4096 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0405 17:29:20.045581 31864 slave.cpp:1128 ] checkpoint slaveinfo '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/slave.info' i0405 17:29:20.045974 31864 slave.cpp:1165 ] forward total oversubscrib resourc i0405 17:29:20.046077 31864 slave.cpp:3664 ] receiv ping slave-observ ( 399 ) @ 172.17.0.4:43972 i0405 17:29:20.046193 31864 status_update_manager.cpp:181 ] resum send statu updat i0405 17:29:20.046289 31870 hierarchical.cpp:1585 ] no invers offer send ! i0405 17:29:20.046370 31870 hierarchical.cpp:1164 ] perform alloc agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 1.153377m i0405 17:29:20.046499 31864 master.cpp:4802 ] receiv updat agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) total oversubscrib resourc i0405 17:29:20.047142 31868 hierarchical.cpp:534 ] agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 ( 4090d10eba90 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :4096 ; port ( * ) : [ 31000-32000 ] , alloc : disk ( role1 ) :4096 ; cpu ( * ) :2 ; mem ( * ) :2048 ; port ( * ) : [ 31000-32000 ] ) i0405 17:29:20.047960 31868 hierarchical.cpp:1490 ] no resourc avail alloc ! i0405 17:29:20.048009 31868 hierarchical.cpp:1585 ] no invers offer send ! i0405 17:29:20.048065 31868 hierarchical.cpp:1164 ] perform alloc agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 866803n i0405 17:29:20.048591 31864 master.cpp:5508 ] send 1 offer framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:20.049188 31860 sched.cpp:874 ] schedul : :resourceoff took 114867n i0405 17:29:20.080921 31859 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 37.025538m i0405 17:29:20.081001 31859 replica.cpp:712 ] persist action 4 i0405 17:29:20.082425 31859 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0405 17:29:20.106056 31859 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 23.583037m i0405 17:29:20.106205 31859 leveldb.cpp:399 ] delet ~2 key leveldb took 76995n i0405 17:29:20.106240 31859 replica.cpp:712 ] persist action 4 i0405 17:29:20.106278 31859 replica.cpp:697 ] replica learn truncat action posit 4 i0405 17:29:20.119488 31837 resources.cpp:572 ] pars resourc json fail : cpus:1 ; mem:128 tri semicolon-delimit string format instead i0405 17:29:20.121356 31859 master.cpp:3288 ] process accept call offer : [ 9565ff6f-f1b6-4259-8430-690e635c391f-o0 ] agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:20.121485 31859 master.cpp:3046 ] author princip 'test-princip ' creat volum i0405 17:29:20.121692 31859 master.cpp:2891 ] author framework princip 'test-princip ' launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 user 'mesos' i0405 17:29:20.123877 31871 master.cpp:3617 ] appli creat oper volum disk ( role1 ) [ id1 : path1 ] :2048 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.125424 31871 master.cpp:6747 ] send checkpoint resourc disk ( role1 ) [ id1 : path1 ] :2048 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.126397 31856 hierarchical.cpp:656 ] updat alloc framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 disk ( role1 ) :4096 ; cpu ( * ) :2 ; mem ( * ) :2048 ; port ( * ) : [ 31000-32000 ] disk ( role1 ) :2048 ; cpu ( * ) :2 ; mem ( * ) :2048 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :2048 i0405 17:29:20.126667 31871 master.hpp:177 ] ad task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 resourc cpu ( * ) :1 ; mem ( * ) :128 ; disk ( role1 ) [ id1 : path1 ] :2048 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 ( 4090d10eba90 ) i0405 17:29:20.126875 31871 master.cpp:3773 ] launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 resourc cpu ( * ) :1 ; mem ( * ) :128 ; disk ( role1 ) [ id1 : path1 ] :2048 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.127390 31856 slave.cpp:2523 ] updat checkpoint resourc disk ( role1 ) [ id1 : path1 ] :2048 i0405 17:29:20.127615 31856 slave.cpp:1497 ] got assign task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.127876 31856 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0405 17:29:20.127841 31871 hierarchical.cpp:893 ] recov disk ( role1 ) :2048 ; cpu ( * ) :1 ; mem ( * ) :1920 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :2048 , alloc : disk ( role1 ) [ id1 : path1 ] :2048 ; cpu ( * ) :1 ; mem ( * ) :128 ) agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.127913 31871 hierarchical.cpp:930 ] framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filter agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 5sec i0405 17:29:20.128667 31856 slave.cpp:1616 ] launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.128937 31856 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0405 17:29:20.129776 31856 paths.cpp:528 ] tri chown '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09 ' user 'mesos' i0405 17:29:20.145324 31856 slave.cpp:5575 ] launch executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' i0405 17:29:20.146057 31858 containerizer.cpp:675 ] start contain 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09 ' executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000' i0405 17:29:20.146078 31856 slave.cpp:1834 ] queu task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.146203 31856 slave.cpp:881 ] success attach file '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' i0405 17:29:20.147619 31859 posix.cpp:206 ] chang ownership persist volum '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/volumes/roles/role1/id1 ' uid 1000 gid 1000 i0405 17:29:20.162421 31859 posix.cpp:250 ] ad symlink '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/volumes/roles/role1/id1 ' '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1 ' persist volum disk ( role1 ) [ id1 : path1 ] :2048 contain bc8b48e5-dd32-4283-a1a6-e1988c82ae09 i0405 17:29:20.172133 31861 launcher.cpp:123 ] fork child pid '7927 ' contain 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' warn : log initgooglelog ( ) written stderr i0405 17:29:20.376197 7941 process.cpp:986 ] libprocess initi 172.17.0.4:50952 16 worker thread i0405 17:29:20.378132 7941 logging.cpp:195 ] log stderr i0405 17:29:20.380861 7941 exec.cpp:150 ] version : 0.29.0 i0405 17:29:20.396257 7966 exec.cpp:200 ] executor start : executor ( 1 ) @ 172.17.0.4:50952 pid 7941 i0405 17:29:20.399426 31860 slave.cpp:2825 ] got registr executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.402995 7966 exec.cpp:225 ] executor regist agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 i0405 17:29:20.403014 31860 slave.cpp:1999 ] send queu task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.405624 7966 exec.cpp:237 ] executor : :regist took 393272n i0405 17:29:20.406108 7966 exec.cpp:312 ] executor ask run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' regist executor 4090d10eba90 i0405 17:29:20.406708 7966 exec.cpp:321 ] executor : :launchtask took 568039n start task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 fork command 7972 sh -c 'echo abc > path1/file' i0405 17:29:20.411375 7966 exec.cpp:535 ] executor send statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.413156 31857 slave.cpp:3184 ] handl statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.415714 31857 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.415788 31857 status_update_manager.cpp:497 ] creat statusupd stream task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.416345 31857 status_update_manager.cpp:374 ] forward updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent i0405 17:29:20.416720 31870 slave.cpp:3582 ] forward updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 master @ 172.17.0.4:43972 i0405 17:29:20.416954 31870 slave.cpp:3476 ] statu updat manag success handl statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.416997 31870 slave.cpp:3492 ] send acknowledg statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.417505 31870 master.cpp:4947 ] statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.417549 31870 master.cpp:4995 ] forward statu updat task_run ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.417724 31870 master.cpp:6608 ] updat state task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( latest state : task_run , statu updat state : task_run ) i0405 17:29:20.417943 7960 exec.cpp:358 ] executor receiv statu updat acknowledg cf4f8fe9-44f2-43ce-8868-b3a09b7298cf task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.418002 31870 sched.cpp:982 ] schedul : :statusupd took 105225n i0405 17:29:20.418623 31870 master.cpp:4102 ] process acknowledg call cf4f8fe9-44f2-43ce-8868-b3a09b7298cf task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 i0405 17:29:20.419181 31860 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.419816 31860 slave.cpp:2594 ] statu updat manag success handl statu updat acknowledg ( uuid : cf4f8fe9-44f2-43ce-8868-b3a09b7298cf ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.513465 7969 exec.cpp:535 ] executor send statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 command exit statu 0 ( pid : 7972 ) i0405 17:29:20.515449 31870 slave.cpp:3184 ] handl statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.516875 31860 slave.cpp:5885 ] termin task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 i0405 17:29:20.517496 31867 posix.cpp:156 ] remov symlink '/tmp/diskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-s0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1 ' persist volum disk ( role1 ) [ id1 : path1 ] :2048 contain bc8b48e5-dd32-4283-a1a6-e1988c82ae09 i0405 17:29:20.519361 31864 status_update_manager.cpp:320 ] receiv statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.519850 31864 status_update_manager.cpp:374 ] forward updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent i0405 17:29:20.520678 31870 slave.cpp:3582 ] forward updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 master @ 172.17.0.4:43972 i0405 17:29:20.520901 31870 slave.cpp:3476 ] statu updat manag success handl statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.520949 31870 slave.cpp:3492 ] send acknowledg statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:20.521550 31864 master.cpp:4947 ] statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.521610 31864 master.cpp:4995 ] forward statu updat task_finish ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.522099 31871 sched.cpp:982 ] schedul : :statusupd took 102502n i0405 17:29:20.522367 31864 master.cpp:6608 ] updat state task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( latest state : task_finish , statu updat state : task_finish ) i0405 17:29:20.524288 31871 hierarchical.cpp:1676 ] filter offer disk ( role1 ) :2048 ; cpu ( * ) :1 ; mem ( * ) :1920 ; port ( * ) : [ 31000-32000 ] agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.524379 31871 hierarchical.cpp:1490 ] no resourc avail alloc ! i0405 17:29:20.524451 31871 hierarchical.cpp:1585 ] no invers offer send ! i0405 17:29:20.524551 31871 hierarchical.cpp:1141 ] perform alloc 1 agent 961746n i0405 17:29:20.525182 31858 hierarchical.cpp:893 ] recov cpu ( * ) :1 ; mem ( * ) :128 ; disk ( role1 ) [ id1 : path1 ] :2048 ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :2048 , alloc : ) agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.525197 31864 master.cpp:4102 ] process acknowledg call 128eb7af-a662-4cbb-9401-125dca38f719 task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 i0405 17:29:20.525380 31864 master.cpp:6674 ] remov task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 resourc cpu ( * ) :1 ; mem ( * ) :128 ; disk ( role1 ) [ id1 : path1 ] :2048 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:20.526067 31864 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.526425 31864 status_update_manager.cpp:528 ] clean statu updat stream task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.526917 31864 slave.cpp:2594 ] statu updat manag success handl statu updat acknowledg ( uuid : 128eb7af-a662-4cbb-9401-125dca38f719 ) task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:20.527048 31864 slave.cpp:5926 ] complet task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 i0405 17:29:20.527732 7964 exec.cpp:358 ] executor receiv statu updat acknowledg 128eb7af-a662-4cbb-9401-125dca38f719 task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:21.527920 31859 slave.cpp:3710 ] executor ( 1 ) @ 172.17.0.4:50952 exit .. / .. /src/tests/persistent_volume_tests.cpp:825 : failur fail wait 15sec offer i0405 17:29:35.542609 31856 master.cpp:1269 ] framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 disconnect i0405 17:29:35.542811 31856 master.cpp:2642 ] disconnect framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:35.542994 31856 master.cpp:2666 ] deactiv framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:35.543349 31860 hierarchical.cpp:378 ] deactiv framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:35.543501 31856 master.cpp:1293 ] give framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 0n failov i0405 17:29:35.543903 31868 master.cpp:5360 ] framework failov timeout , remov framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:35.543936 31868 master.cpp:6093 ] remov framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ( default ) scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf @ 172.17.0.4:43972 i0405 17:29:35.544337 31861 slave.cpp:2215 ] ask shut framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 master @ 172.17.0.4:43972 i0405 17:29:35.544381 31861 slave.cpp:2240 ] shut framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 i0405 17:29:35.544456 31861 slave.cpp:4398 ] shut executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542 ' framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 executor ( 1 ) @ 172.17.0.4:50952 i0405 17:29:35.544960 31872 poll_socket.cpp:110 ] socket error connect i0405 17:29:35.545013 31872 process.cpp:1650 ] fail send 'mesos.internal.shutdownexecutormessag ' '172.17.0.4:50952 ' , connect : socket error connect e0405 17:29:35.545106 31872 process.cpp:1958 ] fail shutdown socket fd 27 : transport endpoint connect i0405 17:29:35.545474 31864 hierarchical.cpp:329 ] remov framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 .. / .. /src/tests/persistent_volume_tests.cpp:819 : failur actual function call count n't match expect_cal ( sched , resourceoff ( & driver , _ ) ) ... expect : call least actual : never call - unsatisfi activ i0405 17:29:35.558538 31858 containerizer.cpp:1432 ] destroy contain 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' .. / .. /src/tests/cluster.cpp:453 : failur fail wait 15sec wait i0405 17:29:50.565403 31870 slave.cpp:800 ] agent termin i0405 17:29:50.565512 31870 slave.cpp:2215 ] ask shut framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 @ 0.0.0.0:0 w0405 17:29:50.565544 31870 slave.cpp:2236 ] ignor shutdown framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 termin i0405 17:29:50.574620 31866 master.cpp:1230 ] agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) disconnect i0405 17:29:50.574766 31866 master.cpp:2701 ] disconnect agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:50.575003 31866 master.cpp:2720 ] deactiv agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 slave ( 441 ) @ 172.17.0.4:43972 ( 4090d10eba90 ) i0405 17:29:50.575294 31865 hierarchical.cpp:563 ] agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 deactiv i0405 17:29:50.605787 31837 master.cpp:1083 ] master termin i0405 17:29:50.606533 31866 hierarchical.cpp:508 ] remov agent 9565ff6f-f1b6-4259-8430-690e635c391f-s0 [ fail ] diskresource/persistentvolumetest.accesspersistentvolume/0 , getparam ( ) = 0 ( 31491 ms ) { code }",MESOS-5128,3.0
"reset ` libprocess_ip ` ` network\cni ` isol . current ` libprocess_ip ` environ variabl set agent ip environ variabl defin ` framework ` . for contain ip address ( contain cni network ) becom problem sinc command executor tri bind ` libprocess_ip ` exist network namespac , fail . thu , contain launch cni network ` libprocess_ip ` set , rather set `` 0.0.0.0 '' , allow contain bind ip address provid cni network .",MESOS-5127,1.0
"pivot_root avail powerpc when compil ppc64le , error messag : src/linux/fs.cpp:443:2 : error : # error `` pivot_root avail '' the current code logic src/linux/fs.cpp : { code } # ifdef __nr_pivot_root int ret = : :syscal ( __nr_pivot_root , newroot.c_str ( ) , putold.c_str ( ) ) ; # elif __x86_64__ // a workaround system old glib new // kernel . the magic number '155 ' syscal number // 'pivot_root ' x86_64 architectur , see // arch/x86/syscalls/syscall_64.tbl int ret = : :syscal ( 155 , newroot.c_str ( ) , putold.c_str ( ) ) ; # els # error `` pivot_root avail '' # endif { code } there old glib version new kernel version , never run code * # ifdef __nr_pivot_root * condit , i build ubuntu 16.04 ( it latest linux kernel glibc ) , still ca n't step * # ifdef __nr_pivot_root * condit . for powerpc case , i ad anoth condit : { code } # elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__ // a workaround powerpc . the magic number '203 ' syscal // number 'pivot_root ' powerpc architectur , see // http : //w3challs.com/syscalls/ ? arch=powerpc_64 int ret = : :syscal ( 203 , newroot.c_str ( ) , putold.c_str ( ) ) ; { code }",MESOS-5121,1.0
"grant access /dev/nvidiactl /dev/nvidia-uvm nvidia gpu isol . call 'nvidia-smi ' fail insid contain even access gpu grant . moreov , access /dev/nvidiactl actual requir contain anyth use gpu even access . we grant/revok access /dev/nvidiactl /dev/nvidia-uvm gpu ad remov contain nvidia gpu isol .",MESOS-5115,2.0
"` network/cni ` isol crash launch without -- network_cni_plugins_dir flag if start agent -- isolation='network/cni ' specifi -- network_cni_plugins_dir flag , agent crash follow stack dump : 0x00007ffff2324cc9 __gi_rais ( sig=sig @ entry=6 ) .. /nptl/sysdeps/unix/sysv/linux/raise.c:56 56 .. /nptl/sysdeps/unix/sysv/linux/raise.c : no file directori . ( gdb ) bt # 0 0x00007ffff2324cc9 __gi_rais ( sig=sig @ entry=6 ) .. /nptl/sysdeps/unix/sysv/linux/raise.c:56 # 1 0x00007ffff23280d8 __gi_abort ( ) abort.c:89 # 2 0x00007ffff231db86 __assert_fail_bas ( fmt=0x7ffff246e830 `` % % % : % u : % % sassert ` % ' failed.\n % n '' , assertion=assert @ entry=0x451f5c `` issom ( ) '' , file=fil @ entry=0x451f65 `` .. / .. /3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp '' , line=lin @ entry=111 , function=funct @ entry=0x45294a `` const t & option < std : :basic_str < char > > : :get ( ) const & [ t = std : :basic_str < char > ] '' ) assert.c:92 # 3 0x00007ffff231dc32 __gi___assert_fail ( assertion=0x451f5c `` issom ( ) '' , file=0x451f65 `` .. / .. /3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp '' , line=111 , function=0x45294a `` const t & option < std : :basic_str < char > > : :get ( ) const & [ t = std : :basic_str < char > ] '' ) assert.c:101 # 4 0x0000000000432c0d option < std : :string > : :get ( ) const & ( this=0x6c1ea8 ) .. / .. /3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111 python except < class 'indexerror ' > list index rang : # 5 0x00007ffff63ef7cc meso : :intern : :slave : :networkcniisolatorprocess : :recov ( this=0x6c1e70 , states=empti std : :list , orphans= ... ) .. / .. /src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331 # 6 0x00007ffff60cddd8 oper ( ) ( this=0x7fffc0001e00 , process=0x6c1ef8 ) .. / .. /3rdparty/libprocess/include/process/dispatch.hpp:239 # 7 0x00007ffff60cd972 std : :_function_handl < void ( process : :processbas * ) , process : :futur < noth > process : :dispatch < noth , meso : :intern : :slave : :mesosisolatorprocess , std : :list < meso : :slave : :containerst , std : :alloc < meso : :slave : :containerst > > const & , hashset < meso : :containerid , std : :hash < meso : :containerid > , std : :equal_to < meso : :containerid > > const & , std : :list < meso : :slave : :containerst , std : :alloc < meso : :slave : :containerst > > , hashset < meso : :containerid , std : :hash < meso : :containerid > , std : :equal_to < meso : :containerid > > > ( process : :pid < meso : :intern : :slave : :mesosisolatorprocess > const & , process : :futur < noth > ( meso : :intern : :slave : :mesosisolatorprocess : : * ) ( std : :list < meso : :slave : :containerst , std : :alloc < meso : :slave : :containerst > > const & , hashset < meso : :containerid , std : :hash < meso : :containerid > , std : :equal_to < meso : :containerid > > const & ) , std : :list < meso : :slave : :containerst , std : :alloc < meso : :slave : :containerst > > , hashset < meso : :containerid , std : :hash < meso : :containerid > , std : :equal_to < meso : :containerid > > ) : : { lambda ( process : :processbas * ) # 1 } > : :_m_invok ( std : :_any_data const & , process : :processbas * ) ( __functor= ... , __args=0x6c1ef8 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:2071 # 8 0x00007ffff6a6bf38 std : :function < void ( process : :processbas * ) > : :oper ( ) ( process : :processbas * ) const ( this=0x7fffc0001d70 , __args=0x6c1ef8 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:2471 # 9 0x00007ffff6a561b4 process : :processbas : :visit ( this=0x6c1ef8 , event= ... ) .. / .. / .. /3rdparty/libprocess/src/process.cpp:3130 # 10 0x00007ffff6aac5f process : :dispatchev : :visit ( this=0x7fffc0001570 , visitor=0x6c1ef8 ) .. / .. / .. /3rdparty/libprocess/include/process/event.hpp:161 # 11 0x00007ffff55e9c91 process : :processbas : :serv ( this=0x6c1ef8 , event= ... ) .. / .. /3rdparty/libprocess/include/process/process.hpp:82 # 12 0x00007ffff6a53ed4 process : :processmanag : :resum ( this=0x67cca0 , process=0x6c1ef8 ) .. / .. / .. /3rdparty/libprocess/src/process.cpp:2570 # 13 0x00007ffff6a5bff5 oper ( ) ( this=0x697d70 , joining= ... ) .. / .. / .. /3rdparty/libprocess/src/process.cpp:2218 # 14 0x00007ffff6a5bf33 std : :_bind < process : :processmanag : :init_thread ( ) : : $ _1 ( std : :reference_wrapp < std : :atomic_bool const > ) > : :__call < void , , 0ul > ( std : :tupl < > & & , std : :_index_tupl < 0ul > ) ( this=0x697d70 , __args= < unknown type /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so , cu 0x45bb552 , die 0x469efe5 > ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:1295 # 15 0x00007ffff6a5bee6 std : :_bind < process : :processmanag : :init_thread ( ) : : $ _1 ( std : :reference_wrapp < std : :atomic_bool const > ) > : :oper ( ) < , void > ( ) ( this=0x697d70 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:1353 # 16 0x00007ffff6a5be95 std : :_bind_simpl < std : :_bind < process : :processmanag : :init_thread ( ) : : $ _1 ( std : :reference_wrapp < std : :atomic_bool const > ) > ( ) > : :_m_invok < > ( std : :_index_tupl < > ) ( this=0x697d70 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:1731 # 17 0x00007ffff6a5be65 std : :_bind_simpl < std : :_bind < process : :processmanag : :init_thread ( ) : : $ _1 ( std : :reference_wrapp < std : :atomic_bool const > ) > ( ) > : :oper ( ) ( ) ( this=0x697d70 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/functional:1720 # 18 0x00007ffff6a5be3c std : :thread : :_impl < std : :_bind_simpl < std : :_bind < process : :processmanag : :init_thread ( ) : : $ _1 ( std : :reference_wrapp < std : :atomic_bool const > ) > ( ) > > : :_m_run ( ) ( this=0x697d58 ) /usr/bin/ .. /lib/gcc/x86_64-linux-gnu/4.8/ .. / .. / .. / .. /include/c++/4.8/thread:115 # 19 0x00007ffff2b98a60 ? ? ( ) /usr/lib/x86_64-linux-gnu/libstdc++.so.6 # 20 0x00007ffff26bb182 start_thread ( arg=0x7fffeb92d700 ) pthread_create.c:312 # 21 0x00007ffff23e847d clone ( ) .. /sysdeps/unix/sysv/linux/x86_64/clone.s:111 ( gdb ) frame 4 # 4 0x0000000000432c0d option < std : :string > : :get ( ) const & ( this=0x6c1ea8 ) .. / .. /3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",MESOS-5113,1.0
"fix bug nvidia gpu devic isol expos discrep clang gcc 'use ' declar there appear discrep clang gcc , allow clang accept ` use ` declar form ` use ns_name : :name ; ` contain nest class , struct , enum ` name ` field declar ( e.g . ` use ns_name : :name : :enum ; ` ) . the languag describ function ambigu c++11 specif referenc : http : //en.cppreference.com/w/cpp/language/namespac # using-declar",MESOS-5082,1.0
expos per-rol domin share a client 's domin share crucial measur like receiv offer futur . we expos dedic alloc metric . as current { { hierarchicalallocatorprocess } } work gener { { sorter } } notion drf share need decid whether would need limit gener order expos innard current use { { drfsorter } } .,MESOS-5058,2.0
"author action enum support upgrad . we need make action enum option author : :request , add ` unknown = 0 ; ` enum valu . see mesos-4997 detail .",MESOS-5031,2.0
"copi provision replac directori symlink i 'm tri play new imag provision custom docker imag , one layer fail get copi , possibl due dangl symlink . error log glog_v=1 : { quot } i0324 05:42:48.926678 15067 copy.cpp:127 ] copi layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootf ' rootf '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6' e0324 05:42:49.028506 15062 slave.cpp:3773 ] contain '5f05be6c-c970-4539-aa64-fd0eef2ec7a ' executor 'test ' framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 fail start : collect fail : collect fail : fail copi layer : cp : overwrit directori ‘ /var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt ’ non-directori { quot } content _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ point non-exist absolut path ( provid exact path 's result us tri mount apt key docker contain build time ) . i believ happen execut script build time , contain equival : { quot } rm -rf /etc/apt/ * & & ln -sf /build-mount-point/ /etc/apt { quot }",MESOS-5028,3.0
mesoscontainerizerprovisionertest.destroywhileprovis flaki . observ apach jenkin . { noformat } [ run ] mesoscontainerizerprovisionertest.provisionfail i0324 13:38:56.284261 2948 containerizer.cpp:666 ] start contain 'test_contain ' executor 'executor ' framework `` i0324 13:38:56.285825 2939 containerizer.cpp:1421 ] destroy contain 'test_container' i0324 13:38:56.285854 2939 containerizer.cpp:1424 ] wait provision complet contain 'test_container' [ ok ] mesoscontainerizerprovisionertest.provisionfail ( 7 ms ) [ run ] mesoscontainerizerprovisionertest.destroywhileprovis i0324 13:38:56.291187 2944 containerizer.cpp:666 ] start contain 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2 ' executor 'executor ' framework `` i0324 13:38:56.292157 2944 containerizer.cpp:1421 ] destroy contain 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' i0324 13:38:56.292179 2944 containerizer.cpp:1424 ] wait provision complet contain 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' f0324 13:38:56.292899 2944 containerizer.cpp:752 ] check fail : containers_.contain ( containerid ) * * * check failur stack trace : * * * @ 0x2ac9973d0ae4 googl : :logmessag : :fail ( ) @ 0x2ac9973d0a30 googl : :logmessag : :sendtolog ( ) @ 0x2ac9973d0432 googl : :logmessag : :flush ( ) @ 0x2ac9973d3346 googl : :logmessagefat : :~logmessagefat ( ) @ 0x2ac996af897c meso : :intern : :slave : :mesoscontainerizerprocess : :_launch ( ) @ 0x2ac996b1f18a _zzn7process8dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns1_11containeriderk6optionins1_8taskinfoeerkns1_12executorinfoerkssrks8_isserkns1_7slaveiderkns_3pidins3_5slaveeeebrks8_ins3_13provisioninfoees5_sa_sd_sssi_sl_sq_bsu_eens_6futureit_eerknso_it0_eems10_fsz_t1_t2_t3_t4_t5_t6_t7_t8_t9_et10_t11_t12_t13_t14_t15_t16_t17_t18_enkulpns_11processbaseee_cles1p_ @ 0x2ac996b479d9 _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns5_11containeriderk6optionins5_8taskinfoeerkns5_12executorinfoerkssrksc_isserkns5_7slaveiderkns0_3pidins7_5slaveeeebrksc_ins7_13provisioninfoees9_se_sh_sssm_sp_su_bsy_eens0_6futureit_eerknss_it0_eems14_fs13_t1_t2_t3_t4_t5_t6_t7_t8_t9_et10_t11_t12_t13_t14_t15_t16_t17_t18_euls2_e_e9_m_invokeerkst9_any_datas2_ @ 0x2ac997334fef std : :function < > : :oper ( ) ( ) @ 0x2ac99731b1c7 process : :processbas : :visit ( ) @ 0x2ac997321154 process : :dispatchev : :visit ( ) @ 0x9a699c process : :processbas : :serv ( ) @ 0x2ac9973173c0 process : :processmanag : :resum ( ) @ 0x2ac99731445a _zzn7process14processmanager12init_threadsevenkulrkst11atomic_boole_cles3_ @ 0x2ac997320916 _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eee6__callivieilm0eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_ee @ 0x2ac9973208c6 _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eeecliieveet0_dpot_ @ 0x2ac997320858 _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeevee9_m_invokeiieeevst12_index_tupleiixspt_ee @ 0x2ac9973207af _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeeveeclev @ 0x2ac997320748 _znst6thread5_implist12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis6_eeeveee6_m_runev @ 0x2ac9989aea60 ( unknown ) @ 0x2ac999125182 start_thread @ 0x2ac99943547d ( unknown ) make [ 4 ] : leav directori ` /mesos/mesos-0.29.0/_build/src' make [ 4 ] : * * * [ check-loc ] abort make [ 3 ] : * * * [ check-am ] error 2 make [ 3 ] : leav directori ` /mesos/mesos-0.29.0/_build/src' make [ 2 ] : * * * [ check ] error 2 make [ 2 ] : leav directori ` /mesos/mesos-0.29.0/_build/src' make [ 1 ] : * * * [ check-recurs ] error 1 make [ 1 ] : leav directori ` /mesos/mesos-0.29.0/_build' make : * * * [ distcheck ] error 1 build step 'execut shell ' mark build failur { noformat },MESOS-5023,2.0
add docker volum driver isol meso container . the isol interact docker volum driver plugin mount unmount extern volum contain .,MESOS-5013,8.0
"mastertest.masterlost flaki the test { { mastertest.masterlost } } { { exceptiontest.disallowscheduleractionsonabort } } fail least half time os x ( clang , optim , { { 30efac7 } } ) , e.g. , { code } [ ========== ] run 1 test 1 test case . [ -- -- -- -- -- ] global test environ set-up . [ -- -- -- -- -- ] 1 test mastertest [ run ] mastertest.masterlost * * * abort 1458650698 ( unix time ) tri `` date -d @ 1458650698 '' use gnu date * * * pc : @ 0x109685fcc meso : :intern : :state : :state : :store ( ) * * * sigsegv ( @ 0x0 ) receiv pid 18620 ( tid 0x111259000 ) stack trace : * * * @ 0x7fff850e1f1a _sigtramp @ 0x108c74eaf boost : :uuid : :detail : :sha1 : :process_byte_impl ( ) @ 0x1095fd723 meso : :intern : :state : :protobuf : :state : :store < > ( ) @ 0x1095fbd3e meso : :intern : :master : :registrarprocess : :updat ( ) @ 0x1095fcf6c meso : :intern : :master : :registrarprocess : :_appli ( ) @ 0x1096797a0 _zzn7process8dispatchibn5mesos8internal6master16registrarprocessens_5ownedins3_9operationeees7_eens_6futureit_eerkns_3pidit0_eemsc_fsa_t1_et2_enkulpns_11processbaseee_clesl_ @ 0x1096795f0 _znst3__128__invoke_void_return_wrapperive6__callijrzn7process8dispatchibn5mesos8internal6master16registrarprocessens3_5ownedins7_9operationeeesb_eens3_6futureit_eerkns3_3pidit0_eemsg_fse_t1_et2_eulpns3_11processbaseee_sp_eeevdpot_ @ 0x1096792d9 _znst3__110__function6__funcizn7process8dispatchibn5mesos8internal6master16registrarprocessens2_5ownedins6_9operationeeesa_eens2_6futureit_eerkns2_3pidit0_eemsf_fsd_t1_et2_eulpns2_11processbaseee_ns_9allocatorisp_eefvso_eecleoso_ @ 0x10b2e9e4c std : :__1 : :function < > : :oper ( ) ( ) @ 0x10b2e9d9c process : :processbas : :visit ( ) @ 0x10b31d26e process : :dispatchev : :visit ( ) @ 0x108ad7d81 process : :processbas : :serv ( ) @ 0x10b2e3cb4 process : :processmanag : :resum ( ) @ 0x10b36c479 process : :processmanag : :init_thread ( ) : : $ _1 : :oper ( ) ( ) @ 0x10b36c0a2 _znst3__114__thread_proxyins_5tupleijns_6__bindizn7process14processmanager12init_threadseve3 $ _1jns_17reference_wrapperikns_6atomicibeeeeeeeeeeeepvsd_ @ 0x7fff90eca05a _pthread_bodi @ 0x7fff90ec9fd7 _pthread_start @ 0x7fff90ec73 thread_start { code } sometim also { { faulttolerancetest.schedulerfailov } } fail stack trace . i could trace recent refactor test helper ( mesos-4633 , mesos-4634 ) , { code } there 'skip'p commit left test . the first bad commit could : 75ca1e6c9fde655c41fdf835aa20c47570d21f10 56e9406763e8514a7557ab3862d2f352a61425d5 b377557c2bfc35c894e87becb47122955540f133 7bf6e4f70131175edd4d6d77ea0dc7692b3e72a c7df1d7bcb1604c95800871cc0473c946e5b5d16 951539317525f3afe9490ed098617e5d4563a80a we bisect ! { code } it appear lifetim object still order correctli .",MESOS-5000,3.0
"destroy contain 's provis lead leak provis directori . here possibl sequenc event : 1 ) containerizer- > launch 2 ) provisioner- > provis call . fetch imag 3 ) executor registr time 4 ) containerizer- > destroy call 5 ) container- > state still prepar 6 ) provisioner- > destroy call so call provisioner- > destori provisioner- > provis n't finish yet . provisioner- > destroy might skip sinc 's inform contain yet , later , provision prepar root filesystem . thi root filesystem destroy destroy alreadi finish .",MESOS-4985,3.0
"mastertest.slavesendpointtwoslav flaki observ arch linux gcc 6 , run virtualbox vm : [ run ] mastertest.slavesendpointtwoslav /mesos-2/src/tests/master_tests.cpp:1710 : failur valu : array.get ( ) .values.s ( ) actual : 1 expect : 2u which : 2 [ fail ] mastertest.slavesendpointtwoslav ( 86 ms ) seem fail non-determinist , perhap often concurr cpu load machin .",MESOS-4984,2.0
updat mesos-execut appc chang . mesos-execut cli applic current support appc imag . ad support would make integr test easier .,MESOS-4978,3.0
"containerloggertest.logrotate_rotateinsandbox flaki the logger subprocess may exit reach { { waitpid } } test . if happen , { { waitpid } } return { { -1 } } process longer exist . verbos log : { code } [ run ] containerloggertest.logrotate_rotateinsandbox i0316 14:28:51.329337 1242 cluster.cpp:139 ] creat default 'local ' author i0316 14:28:51.332823 1242 leveldb.cpp:174 ] open db 3.079559m i0316 14:28:51.333916 1242 leveldb.cpp:181 ] compact db 1.054247m i0316 14:28:51.333979 1242 leveldb.cpp:196 ] creat db iter 21450n i0316 14:28:51.334005 1242 leveldb.cpp:202 ] seek begin db 2205n i0316 14:28:51.334025 1242 leveldb.cpp:271 ] iter 0 key db 410n i0316 14:28:51.334089 1242 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0316 14:28:51.334661 1275 recover.cpp:447 ] start replica recoveri i0316 14:28:51.335044 1275 recover.cpp:473 ] replica empti statu i0316 14:28:51.336207 1262 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 484 ) @ 172.17.0.3:45919 i0316 14:28:51.336730 1270 recover.cpp:193 ] receiv recov respons replica empti statu i0316 14:28:51.337257 1275 recover.cpp:564 ] updat replica statu start i0316 14:28:51.338001 1267 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 537200n i0316 14:28:51.338032 1267 replica.cpp:320 ] persist replica statu start i0316 14:28:51.338183 1261 master.cpp:376 ] master c7653f60-33e9-4406-9f62-dc74c906bf83 ( 2cbb23302fe5 ) start 172.17.0.3:45919 i0316 14:28:51.338295 1263 recover.cpp:473 ] replica start statu i0316 14:28:51.338213 1261 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/xtqwks/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.29.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/xtqwks/master '' -- zk_session_timeout= '' 10sec '' i0316 14:28:51.338562 1261 master.cpp:423 ] master allow authent framework regist i0316 14:28:51.338572 1261 master.cpp:428 ] master allow authent slave regist i0316 14:28:51.338580 1261 credentials.hpp:35 ] load credenti authent '/tmp/xtqwks/credentials' i0316 14:28:51.338877 1261 master.cpp:468 ] use default 'crammd5 ' authent i0316 14:28:51.339030 1262 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 485 ) @ 172.17.0.3:45919 i0316 14:28:51.339246 1261 master.cpp:537 ] use default 'basic ' http authent i0316 14:28:51.339393 1261 master.cpp:571 ] author enabl i0316 14:28:51.339390 1266 recover.cpp:193 ] receiv recov respons replica start statu i0316 14:28:51.339606 1271 whitelist_watcher.cpp:77 ] no whitelist given i0316 14:28:51.339607 1275 hierarchical.cpp:144 ] initi hierarch alloc process i0316 14:28:51.340077 1268 recover.cpp:564 ] updat replica statu vote i0316 14:28:51.340533 1270 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 331558n i0316 14:28:51.340558 1270 replica.cpp:320 ] persist replica statu vote i0316 14:28:51.340672 1270 recover.cpp:578 ] success join paxo group i0316 14:28:51.340827 1270 recover.cpp:462 ] recov process termin i0316 14:28:51.341684 1270 master.cpp:1806 ] the newli elect leader master @ 172.17.0.3:45919 id c7653f60-33e9-4406-9f62-dc74c906bf83 i0316 14:28:51.341717 1270 master.cpp:1819 ] elect lead master ! i0316 14:28:51.341740 1270 master.cpp:1508 ] recov registrar i0316 14:28:51.341954 1263 registrar.cpp:307 ] recov registrar i0316 14:28:51.342499 1273 log.cpp:659 ] attempt start writer i0316 14:28:51.343616 1266 replica.cpp:493 ] replica receiv implicit promis request ( 487 ) @ 172.17.0.3:45919 propos 1 i0316 14:28:51.344183 1266 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 536941n i0316 14:28:51.344208 1266 replica.cpp:342 ] persist promis 1 i0316 14:28:51.344825 1267 coordinator.cpp:238 ] coordin attempt fill miss posit i0316 14:28:51.346009 1276 replica.cpp:388 ] replica receiv explicit promis request ( 488 ) @ 172.17.0.3:45919 posit 0 propos 2 i0316 14:28:51.346371 1276 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 327890n i0316 14:28:51.346393 1276 replica.cpp:712 ] persist action 0 i0316 14:28:51.347363 1267 replica.cpp:537 ] replica receiv write request posit 0 ( 489 ) @ 172.17.0.3:45919 i0316 14:28:51.347414 1267 leveldb.cpp:436 ] read posit leveldb took 24861n i0316 14:28:51.347774 1267 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 323654n i0316 14:28:51.347796 1267 replica.cpp:712 ] persist action 0 i0316 14:28:51.348323 1276 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0316 14:28:51.348714 1276 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 361981n i0316 14:28:51.348738 1276 replica.cpp:712 ] persist action 0 i0316 14:28:51.348760 1276 replica.cpp:697 ] replica learn nop action posit 0 i0316 14:28:51.349318 1274 log.cpp:675 ] writer start end posit 0 i0316 14:28:51.350275 1267 leveldb.cpp:436 ] read posit leveldb took 23849n i0316 14:28:51.351171 1271 registrar.cpp:340 ] success fetch registri ( 0b ) 9.173248m i0316 14:28:51.351300 1271 registrar.cpp:439 ] appli 1 oper 32119n ; attempt updat 'registry' i0316 14:28:51.351989 1272 log.cpp:683 ] attempt append 170 byte log i0316 14:28:51.352108 1266 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0316 14:28:51.352802 1263 replica.cpp:537 ] replica receiv write request posit 1 ( 490 ) @ 172.17.0.3:45919 i0316 14:28:51.353313 1263 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 474854n i0316 14:28:51.353338 1263 replica.cpp:712 ] persist action 1 i0316 14:28:51.354101 1273 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0316 14:28:51.354483 1273 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 338210n i0316 14:28:51.354507 1273 replica.cpp:712 ] persist action 1 i0316 14:28:51.354529 1273 replica.cpp:697 ] replica learn append action posit 1 i0316 14:28:51.355444 1275 registrar.cpp:484 ] success updat 'registri ' 4.084224m i0316 14:28:51.355569 1275 registrar.cpp:370 ] success recov registrar i0316 14:28:51.355697 1268 log.cpp:702 ] attempt truncat log 1 i0316 14:28:51.355870 1269 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0316 14:28:51.356016 1274 master.cpp:1616 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i0316 14:28:51.356032 1272 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0316 14:28:51.356761 1273 replica.cpp:537 ] replica receiv write request posit 2 ( 491 ) @ 172.17.0.3:45919 i0316 14:28:51.357203 1273 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 406053n i0316 14:28:51.357226 1273 replica.cpp:712 ] persist action 2 i0316 14:28:51.357718 1270 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0316 14:28:51.358093 1270 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 345370n i0316 14:28:51.358175 1270 leveldb.cpp:399 ] delet ~1 key leveldb took 57u i0316 14:28:51.358201 1270 replica.cpp:712 ] persist action 2 i0316 14:28:51.358220 1270 replica.cpp:697 ] replica learn truncat action posit 2 i0316 14:28:51.368399 1242 containerizer.cpp:149 ] use isol : posix/cpu , posix/mem , filesystem/posix w0316 14:28:51.406371 1242 backend.cpp:66 ] fail creat 'bind ' backend : bindbackend requir root privileg i0316 14:28:51.410480 1266 slave.cpp:193 ] slave start 12 ) @ 172.17.0.3:45919 i0316 14:28:51.410518 1266 slave.cpp:194 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- container_logger= '' org_apache_mesos_logrotatecontainerlogg '' -- containerizers= '' meso '' -- credential= '' /tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.29.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gi '' i0316 14:28:51.411118 1266 credentials.hpp:83 ] load credenti authent '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/credential' i0316 14:28:51.411381 1266 slave.cpp:324 ] slave use credenti : test-princip i0316 14:28:51.411696 1266 resources.cpp:572 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0316 14:28:51.412075 1266 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0316 14:28:51.412148 1266 slave.cpp:472 ] slave attribut : [ ] i0316 14:28:51.412160 1266 slave.cpp:477 ] slave hostnam : 2cbb23302fe5 i0316 14:28:51.413516 1263 state.cpp:58 ] recov state '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/meta' i0316 14:28:51.413774 1266 status_update_manager.cpp:200 ] recov statu updat manag i0316 14:28:51.414029 1276 containerizer.cpp:407 ] recov container i0316 14:28:51.415222 1269 provisioner.cpp:245 ] provision recoveri complet i0316 14:28:51.415650 1268 slave.cpp:4565 ] finish recoveri i0316 14:28:51.416115 1268 slave.cpp:4737 ] queri resourc estim oversubscrib resourc i0316 14:28:51.416365 1268 slave.cpp:796 ] new master detect master @ 172.17.0.3:45919 i0316 14:28:51.416448 1276 status_update_manager.cpp:174 ] paus send statu updat i0316 14:28:51.416445 1268 slave.cpp:859 ] authent master master @ 172.17.0.3:45919 i0316 14:28:51.416522 1268 slave.cpp:864 ] use default cram-md5 authenticate i0316 14:28:51.416671 1268 slave.cpp:832 ] detect new master i0316 14:28:51.416731 1275 authenticatee.cpp:121 ] creat new client sasl connect i0316 14:28:51.416807 1268 slave.cpp:4751 ] receiv oversubscrib resourc resourc estim i0316 14:28:51.417006 1263 master.cpp:5659 ] authent slave ( 12 ) @ 172.17.0.3:45919 i0316 14:28:51.417103 1262 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 38 ) @ 172.17.0.3:45919 i0316 14:28:51.417348 1273 authenticator.cpp:98 ] creat new server sasl connect i0316 14:28:51.417548 1266 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0316 14:28:51.417582 1266 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0316 14:28:51.417696 1264 authenticator.cpp:203 ] receiv sasl authent start i0316 14:28:51.417753 1264 authenticator.cpp:325 ] authent requir step i0316 14:28:51.417948 1265 authenticatee.cpp:258 ] receiv sasl authent step i0316 14:28:51.418107 1267 authenticator.cpp:231 ] receiv sasl authent step i0316 14:28:51.418159 1267 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '2cbb23302fe5 ' server fqdn : '2cbb23302fe5 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0316 14:28:51.418180 1267 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0316 14:28:51.418233 1267 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0316 14:28:51.418270 1267 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '2cbb23302fe5 ' server fqdn : '2cbb23302fe5 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0316 14:28:51.418289 1267 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0316 14:28:51.418300 1267 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0316 14:28:51.418323 1267 authenticator.cpp:317 ] authent success i0316 14:28:51.418414 1264 authenticatee.cpp:298 ] authent success i0316 14:28:51.418473 1269 master.cpp:5689 ] success authent princip 'test-princip ' slave ( 12 ) @ 172.17.0.3:45919 i0316 14:28:51.418514 1275 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 38 ) @ 172.17.0.3:45919 i0316 14:28:51.418781 1276 slave.cpp:927 ] success authent master master @ 172.17.0.3:45919 i0316 14:28:51.418937 1276 slave.cpp:1321 ] will retri registr 1.983001m necessari i0316 14:28:51.419108 1262 master.cpp:4370 ] regist slave slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) id c7653f60-33e9-4406-9f62-dc74c906bf83-s0 i0316 14:28:51.419643 1266 registrar.cpp:439 ] appli 1 oper 75642n ; attempt updat 'registry' i0316 14:28:51.420670 1272 log.cpp:683 ] attempt append 339 byte log i0316 14:28:51.420820 1269 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0316 14:28:51.421495 1270 slave.cpp:1321 ] will retri registr 1.437257m necessari i0316 14:28:51.421716 1275 master.cpp:4358 ] ignor regist slave messag slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) admiss alreadi progress i0316 14:28:51.422107 1267 replica.cpp:537 ] replica receiv write request posit 3 ( 505 ) @ 172.17.0.3:45919 i0316 14:28:51.423033 1267 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 762815n i0316 14:28:51.423066 1267 replica.cpp:712 ] persist action 3 i0316 14:28:51.424069 1267 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0316 14:28:51.424232 1264 slave.cpp:1321 ] will retri registr 66.01292m necessari i0316 14:28:51.424342 1269 master.cpp:4358 ] ignor regist slave messag slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) admiss alreadi progress i0316 14:28:51.424686 1267 leveldb.cpp:341 ] persist action ( 360 byte ) leveldb took 574743n i0316 14:28:51.424757 1267 replica.cpp:712 ] persist action 3 i0316 14:28:51.424792 1267 replica.cpp:697 ] replica learn append action posit 3 i0316 14:28:51.426441 1272 registrar.cpp:484 ] success updat 'registri ' 6.721024m i0316 14:28:51.426677 1262 log.cpp:702 ] attempt truncat log 3 i0316 14:28:51.426808 1264 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0316 14:28:51.427584 1261 slave.cpp:3482 ] receiv ping slave-observ ( 11 ) @ 172.17.0.3:45919 i0316 14:28:51.428213 1262 hierarchical.cpp:473 ] ad slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 ( 2cbb23302fe5 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0316 14:28:51.427865 1266 master.cpp:4438 ] regist slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0316 14:28:51.428270 1267 slave.cpp:971 ] regist master master @ 172.17.0.3:45919 ; given slave id c7653f60-33e9-4406-9f62-dc74c906bf83-s0 i0316 14:28:51.428412 1265 replica.cpp:537 ] replica receiv write request posit 4 ( 506 ) @ 172.17.0.3:45919 i0316 14:28:51.428443 1267 fetcher.cpp:81 ] clear fetcher cach i0316 14:28:51.428503 1262 hierarchical.cpp:1453 ] no resourc avail alloc ! i0316 14:28:51.428535 1262 hierarchical.cpp:1150 ] perform alloc slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 205421n i0316 14:28:51.428750 1273 status_update_manager.cpp:181 ] resum send statu updat i0316 14:28:51.429157 1265 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 695258n i0316 14:28:51.429225 1267 slave.cpp:994 ] checkpoint slaveinfo '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/slave.info' i0316 14:28:51.429275 1265 replica.cpp:712 ] persist action 4 i0316 14:28:51.429759 1267 slave.cpp:1030 ] forward total oversubscrib resourc i0316 14:28:51.430055 1265 master.cpp:4782 ] receiv updat slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) total oversubscrib resourc i0316 14:28:51.430614 1271 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0316 14:28:51.430891 1242 sched.cpp:222 ] version : 0.29.0 i0316 14:28:51.431043 1265 hierarchical.cpp:531 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 ( 2cbb23302fe5 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0316 14:28:51.431236 1271 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 536892n i0316 14:28:51.431267 1265 hierarchical.cpp:1453 ] no resourc avail alloc ! i0316 14:28:51.431584 1271 leveldb.cpp:399 ] delet ~2 key leveldb took 66904n i0316 14:28:51.431538 1273 sched.cpp:326 ] new master detect master @ 172.17.0.3:45919 i0316 14:28:51.431622 1271 replica.cpp:712 ] persist action 4 i0316 14:28:51.431623 1265 hierarchical.cpp:1150 ] perform alloc slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 518588n i0316 14:28:51.431660 1271 replica.cpp:697 ] replica learn truncat action posit 4 i0316 14:28:51.431711 1273 sched.cpp:382 ] authent master master @ 172.17.0.3:45919 i0316 14:28:51.431737 1273 sched.cpp:389 ] use default cram-md5 authenticate i0316 14:28:51.431982 1266 authenticatee.cpp:121 ] creat new client sasl connect i0316 14:28:51.432369 1261 master.cpp:5659 ] authent scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:51.432509 1263 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 39 ) @ 172.17.0.3:45919 i0316 14:28:51.432868 1267 authenticator.cpp:98 ] creat new server sasl connect i0316 14:28:51.433135 1276 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0316 14:28:51.433233 1276 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0316 14:28:51.433423 1276 authenticator.cpp:203 ] receiv sasl authent start i0316 14:28:51.433502 1276 authenticator.cpp:325 ] authent requir step i0316 14:28:51.433606 1274 authenticatee.cpp:258 ] receiv sasl authent step i0316 14:28:51.433744 1273 authenticator.cpp:231 ] receiv sasl authent step i0316 14:28:51.433785 1273 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '2cbb23302fe5 ' server fqdn : '2cbb23302fe5 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0316 14:28:51.433801 1273 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0316 14:28:51.433861 1273 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0316 14:28:51.433897 1273 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '2cbb23302fe5 ' server fqdn : '2cbb23302fe5 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0316 14:28:51.433912 1273 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0316 14:28:51.433924 1273 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0316 14:28:51.433944 1273 authenticator.cpp:317 ] authent success i0316 14:28:51.434037 1274 authenticatee.cpp:298 ] authent success i0316 14:28:51.434108 1268 master.cpp:5689 ] success authent princip 'test-princip ' scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:51.434211 1272 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 39 ) @ 172.17.0.3:45919 i0316 14:28:51.434512 1274 sched.cpp:471 ] success authent master master @ 172.17.0.3:45919 i0316 14:28:51.434535 1274 sched.cpp:776 ] send subscrib call master @ 172.17.0.3:45919 i0316 14:28:51.434648 1274 sched.cpp:809 ] will retri registr 356.547014m necessari i0316 14:28:51.434819 1266 master.cpp:2326 ] receiv subscrib call framework 'default ' scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:51.434905 1266 master.cpp:1845 ] author framework princip 'test-princip ' receiv offer role ' * ' i0316 14:28:51.435464 1265 master.cpp:2397 ] subscrib framework default checkpoint disabl capabl [ ] i0316 14:28:51.435979 1269 hierarchical.cpp:265 ] ad framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.436213 1272 sched.cpp:703 ] framework regist c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.436316 1272 sched.cpp:717 ] schedul : :regist took 73782n i0316 14:28:51.436928 1269 hierarchical.cpp:1548 ] no invers offer send ! i0316 14:28:51.436978 1269 hierarchical.cpp:1130 ] perform alloc 1 slave 970638n i0316 14:28:51.437278 1272 master.cpp:5488 ] send 1 offer framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:51.437782 1262 sched.cpp:873 ] schedul : :resourceoff took 129952n i0316 14:28:51.440006 1274 master.cpp:3268 ] process accept call offer : [ c7653f60-33e9-4406-9f62-dc74c906bf83-o0 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:51.440094 1274 master.cpp:2871 ] author framework princip 'test-princip ' launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 user 'mesos' i0316 14:28:51.442152 1274 master.hpp:177 ] ad task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 ( 2cbb23302fe5 ) i0316 14:28:51.442348 1274 master.cpp:3753 ] launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:51.442749 1265 slave.cpp:1361 ] got assign task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.443006 1265 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0316 14:28:51.443624 1265 slave.cpp:1480 ] launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.443730 1265 resources.cpp:572 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0316 14:28:51.444629 1265 paths.cpp:528 ] tri chown '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621 ' user 'mesos' i0316 14:28:51.449493 1265 slave.cpp:5367 ] launch executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' i0316 14:28:51.450256 1261 containerizer.cpp:666 ] start contain '6e2770ca-32d3-47ad-b4fe-7d9f26489621 ' executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000' i0316 14:28:51.450299 1265 slave.cpp:1698 ] queu task '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.450428 1265 slave.cpp:749 ] success attach file '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' i0316 14:28:51.459421 1268 launcher.cpp:147 ] fork child pid '1453 ' contain '6e2770ca-32d3-47ad-b4fe-7d9f26489621' i0316 14:28:51.613296 1274 slave.cpp:2643 ] got registr executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:51.615416 1271 slave.cpp:1863 ] send queu task '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:51.622187 1272 slave.cpp:3002 ] handl statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:51.623610 1275 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.623646 1275 status_update_manager.cpp:497 ] creat statusupd stream task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.624053 1275 status_update_manager.cpp:374 ] forward updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 slave i0316 14:28:51.624423 1274 slave.cpp:3400 ] forward updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 master @ 172.17.0.3:45919 i0316 14:28:51.624621 1274 slave.cpp:3294 ] statu updat manag success handl statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.624677 1274 slave.cpp:3310 ] send acknowledg statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:51.624836 1270 master.cpp:4927 ] statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:51.624881 1270 master.cpp:4975 ] forward statu updat task_run ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.625077 1270 master.cpp:6588 ] updat state task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( latest state : task_run , statu updat state : task_run ) i0316 14:28:51.625355 1269 sched.cpp:981 ] schedul : :statusupd took 141149n i0316 14:28:51.625671 1266 master.cpp:4082 ] process acknowledg call aee0de1c-8acd-46eb-8723-d26cd203228f task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 i0316 14:28:51.625977 1267 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:51.626369 1265 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : aee0de1c-8acd-46eb-8723-d26cd203228f ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:52.340801 1266 hierarchical.cpp:1453 ] no resourc avail alloc ! i0316 14:28:52.340884 1266 hierarchical.cpp:1548 ] no invers offer send ! i0316 14:28:52.340922 1266 hierarchical.cpp:1130 ] perform alloc 1 slave 350313n i0316 14:28:53.342003 1263 hierarchical.cpp:1453 ] no resourc avail alloc ! i0316 14:28:53.342077 1263 hierarchical.cpp:1548 ] no invers offer send ! i0316 14:28:53.342110 1263 hierarchical.cpp:1130 ] perform alloc 1 slave 332715n warn : log initgooglelog ( ) written stderr i0316 14:28:53.619144 1451 process.cpp:986 ] libprocess initi 172.17.0.3:40885 16 cpu warn : log initgooglelog ( ) written stderr i0316 14:28:53.790701 1452 process.cpp:986 ] libprocess initi 172.17.0.3:50144 16 cpu i0316 14:28:53.939643 1268 slave.cpp:3002 ] handl statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:53.940950 1267 slave.cpp:5677 ] termin task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 i0316 14:28:53.942181 1275 status_update_manager.cpp:320 ] receiv statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.942358 1275 status_update_manager.cpp:374 ] forward updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 slave i0316 14:28:53.942715 1265 slave.cpp:3400 ] forward updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 master @ 172.17.0.3:45919 i0316 14:28:53.942919 1265 slave.cpp:3294 ] statu updat manag success handl statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.942961 1265 slave.cpp:3310 ] send acknowledg statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:53.943159 1273 master.cpp:4927 ] statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:53.943218 1273 master.cpp:4975 ] forward statu updat task_finish ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.943392 1273 master.cpp:6588 ] updat state task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( latest state : task_finish , statu updat state : task_finish ) i0316 14:28:53.944248 1275 sched.cpp:981 ] schedul : :statusupd took 172957n i0316 14:28:53.944351 1262 hierarchical.cpp:890 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.944548 1242 sched.cpp:1903 ] ask stop driver i0316 14:28:53.944672 1275 sched.cpp:1143 ] stop framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000' i0316 14:28:53.944736 1263 master.cpp:4082 ] process acknowledg call a873c6e2-442e-439e-a13f-54bb19df1881 task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 i0316 14:28:53.944795 1263 master.cpp:6654 ] remov task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:53.945226 1263 master.cpp:6061 ] process teardown call framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:53.945253 1263 master.cpp:6073 ] remov framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ( default ) scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79 @ 172.17.0.3:45919 i0316 14:28:53.945324 1275 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.945412 1274 hierarchical.cpp:375 ] deactiv framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.945462 1276 slave.cpp:2079 ] ask shut framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 master @ 172.17.0.3:45919 i0316 14:28:53.945579 1276 slave.cpp:2104 ] shut framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.945669 1276 slave.cpp:4198 ] shut executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:53.945714 1275 status_update_manager.cpp:528 ] clean statu updat stream task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.945818 1274 hierarchical.cpp:326 ] remov framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.946151 1265 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : a873c6e2-442e-439e-a13f-54bb19df1881 ) task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:53.946213 1265 slave.cpp:5718 ] complet task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 i0316 14:28:54.343000 1263 hierarchical.cpp:1453 ] no resourc avail alloc ! i0316 14:28:54.343056 1263 hierarchical.cpp:1130 ] perform alloc 1 slave 213036n i0316 14:28:54.943627 1261 slave.cpp:3528 ] executor ( 1 ) @ 172.17.0.3:56062 exit i0316 14:28:54.944002 1274 containerizer.cpp:1608 ] executor contain '6e2770ca-32d3-47ad-b4fe-7d9f26489621 ' exit i0316 14:28:54.944205 1274 containerizer.cpp:1392 ] destroy contain '6e2770ca-32d3-47ad-b4fe-7d9f26489621' i0316 14:28:54.949076 1276 provisioner.cpp:306 ] ignor destroy request unknown contain 6e2770ca-32d3-47ad-b4fe-7d9f26489621 i0316 14:28:54.949502 1276 slave.cpp:3886 ] executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 exit statu 0 i0316 14:28:54.949556 1276 slave.cpp:3990 ] clean executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 executor ( 1 ) @ 172.17.0.3:56062 i0316 14:28:54.949807 1270 gc.cpp:55 ] schedul '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621 ' gc 6.99998900785778day futur i0316 14:28:54.949931 1276 slave.cpp:4078 ] clean framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:54.950188 1276 status_update_manager.cpp:282 ] close statu updat stream framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 i0316 14:28:54.950196 1270 gc.cpp:55 ] schedul '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917 ' gc 6.99998900606519day futur i0316 14:28:54.950458 1270 gc.cpp:55 ] schedul '/tmp/containerloggertest_logrotate_rotateinsandbox_jhp0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-s0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000 ' gc 6.99998900418963day futur .. / .. /src/tests/container_logger_tests.cpp:461 : failur valu : waitpid ( pstree.process.pid , __null , 0 ) actual : -1 expect : pstree.process.pid which : 1453 i0316 14:28:54.952739 1264 slave.cpp:668 ] slave termin i0316 14:28:54.952980 1275 master.cpp:1212 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) disconnect i0316 14:28:54.953069 1275 master.cpp:2681 ] disconnect slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:54.953172 1275 master.cpp:2700 ] deactiv slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 slave ( 12 ) @ 172.17.0.3:45919 ( 2cbb23302fe5 ) i0316 14:28:54.953404 1269 hierarchical.cpp:560 ] slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 deactiv i0316 14:28:54.957495 1274 master.cpp:1065 ] master termin i0316 14:28:54.958026 1276 hierarchical.cpp:505 ] remov slave c7653f60-33e9-4406-9f62-dc74c906bf83-s0 [ fail ] containerloggertest.logrotate_rotateinsandbox ( 3635 ms ) { code }",MESOS-4961,1.0
"implement reconnect funtion schedul librari . current , way schedul forc reconnect attempt master use schedul librari { { src/scheduler/scheduler.cpp } } . it specif use scenario one way network partit master . due , schedul receiv { { heartbeat } } event master . in case , schedul might want forc reconnect attempt master instead reli { { disconnect } } callback .",MESOS-4950,3.0
"docker runtim isol test may caus disk issu . current slave work directori use docker store dir archiv dir , problemat . becaus slave work dir exactli ` environment- > mkdtemp ( ) ` , get clean end whole test . but runtim isol local puller test cp host 's rootf , size rel big . cleanup done test tear .",MESOS-4942,2.0
"setup proper /etc/hostnam , /etc/host /etc/resolv.conf contain network/cni isol . the network/cni isol need properli setup /etc/hostnam /etc/host contain hostnam ( e.g. , randomli gener ) assign ip return cni plugin . we consid follow case : 1 ) contain use host filesystem 2 ) contain use differ filesystem 3 ) custom executor command executor",MESOS-4922,5.0
linuxfilesystemisolatortest.root_multiplecontain fail . observ ci : { noformat } [ 09:34:15 ] : [ step 11/11 ] [ run ] linuxfilesystemisolatortest.root_multiplecontain [ 09:34:19 ] w : [ step 11/11 ] i0309 09:34:19.906719 2357 linux.cpp:81 ] make '/tmp/mlvlnv ' share mount [ 09:34:19 ] w : [ step 11/11 ] i0309 09:34:19.923548 2357 linux_launcher.cpp:101 ] use /sys/fs/cgroup/freez freezer hierarchi linux launcher [ 09:34:19 ] w : [ step 11/11 ] i0309 09:34:19.924705 2376 containerizer.cpp:666 ] start contain 'da610f7f-a709-4de8-94d3-74f4a520619b ' executor 'test_executor1 ' framework `` [ 09:34:19 ] w : [ step 11/11 ] i0309 09:34:19.925355 2371 provisioner.cpp:285 ] provis imag rootf '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0 ' contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:19 ] w : [ step 11/11 ] i0309 09:34:19.925881 2377 copy.cpp:127 ] copi layer path '/tmp/mlvlnv/test_image1 ' rootf '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.835127 2376 linux.cpp:355 ] bind mount work directori '/tmp/mlvlnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b ' '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox ' contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.835392 2376 linux.cpp:683 ] chang ownership persist volum '/tmp/mlvlnv/volumes/roles/test_role/persistent_volume_id ' uid 0 gid 0 [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.840425 2376 linux.cpp:723 ] mount '/tmp/mlvlnv/volumes/roles/test_role/persistent_volume_id ' '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volum ' persist volum disk ( test_rol ) [ persistent_volume_id : volum ] :32 contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.843878 2374 linux_launcher.cpp:304 ] clone child process flag = clone_newn [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.848302 2371 containerizer.cpp:666 ] start contain 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087 ' executor 'test_executor2 ' framework `` [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.848758 2371 containerizer.cpp:1392 ] destroy contain 'da610f7f-a709-4de8-94d3-74f4a520619b' [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.848865 2373 provisioner.cpp:285 ] provis imag rootf '/tmp/mlvlnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917 ' contain fe4729c5-1e63-4cc6-a2e3-fe5006ffe087 [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.849449 2375 copy.cpp:127 ] copi layer path '/tmp/mlvlnv/test_image2 ' rootf '/tmp/mlvlnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.854038 2374 cgroups.cpp:2427 ] freez cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.856693 2372 cgroups.cpp:1409 ] success froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b 2.608128m [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.859237 2377 cgroups.cpp:2445 ] thaw cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.861454 2377 cgroups.cpp:1438 ] successfullli thaw cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b 2176u [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.934608 2378 containerizer.cpp:1608 ] executor contain 'da610f7f-a709-4de8-94d3-74f4a520619b ' exit [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.937692 2372 linux.cpp:798 ] unmount volum '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volum ' contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.937742 2372 linux.cpp:817 ] unmount sandbox/work directori '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox ' contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:30 ] w : [ step 11/11 ] i0309 09:34:30.938129 2375 provisioner.cpp:330 ] destroy contain rootf '/tmp/mlvlnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0 ' contain da610f7f-a709-4de8-94d3-74f4a520619b [ 09:34:45 ] : [ step 11/11 ] .. / .. /src/tests/containerizer/filesystem_isolator_tests.cpp:1318 : failur [ 09:34:45 ] : [ step 11/11 ] fail wait 15sec wait1 [ 09:34:48 ] : [ step 11/11 ] [ fail ] linuxfilesystemisolatortest.root_multiplecontain ( 32341 ms ) { noformat },MESOS-4912,3.0
"executor driver respect executor shutdown grace period . executor shutdown grace period , configur agent , propag executor via ` mesos_executor_shutdown_grace_period ` environ variabl . the executor driver must use timeout delay hard shutdown relat executor .",MESOS-4911,1.0
"allow multipl load modul manifest the modulemanag : :load ( ) design call exactli process lifetim . thi work well master/ag environ . howev , fail schedul environ . for exampl , singl schedul binari might implement multipl schedul driver caus multipl call modulemanag : :load ( ) lead failur .",MESOS-4903,3.0
"default cmd execut incorrect command . when meso container launch contain use docker imag , contain default cmd . the execut command incorrect sequenc . for exampl : if imag default entrypoint null , cmd `` sh '' , user defin shell=fals , valu none , argument [ -c , echo 'hello world ' ] . the execut command ` [ sh , -c , echo 'hello world ' , sh ] ` , incorrect . it ` [ sh , sh , -c , echo 'hello world ' ] ` instead . thi problem expos case : sh=0 , value=0 , argv=1 , entrypoint=0 , cmd=1 .",MESOS-4888,2.0
meso container ca n't handl top level docker imag like `` alpin '' ( must use `` library/alpin '' ) thi demonstr { { mesos-execut } } command : # docker container imag { { alpin } } : success { code } sudo ./build/src/mesos-execut -- docker_image=alpin -- containerizer=dock -- name=just-a-test -- command= '' sleep 1000 '' -- master=localhost:5050 { code } # meso container imag { { alpin } } : failur { code } sudo ./build/src/mesos-execut -- docker_image=alpin -- containerizer=meso -- name=just-a-test -- command= '' sleep 1000 '' -- master=localhost:5050 { code } # meso container imag { { library/alpin } } : success { code } sudo ./build/src/mesos-execut -- docker_image=library/alpin -- containerizer=meso -- name=just-a-test -- command= '' sleep 1000 '' -- master=localhost:5050 { code } in slave log : { code } ea-4460-83 9c-838da86af34c-0007' i0306 16:32:41.418269 3403 metadata_manager.cpp:159 ] look imag 'alpin : latest' i0306 16:32:41.418699 3403 registry_puller.cpp:194 ] pull imag 'alpin : latest ' 'docker-manifest : //registry-1.docker.io:443alpin ? latest # http ' '/tmp/mesos-test /store/docker/staging/ka7mlq' e0306 16:32:43.098131 3400 slave.cpp:3773 ] contain '4bf9132d-9a57-4baa-a78c-e7164e93ace6 ' executor 'just-a-test ' framework 4f055c6f-1bea-4460-839c-838da86af34c-0 007 fail start : collect fail : unexpect http respons '401 unauthor { code } curl command execut : { code } $ sudo sysdig -a -p `` * % evt.tim % proc.cmdlin '' evt.type=execv proc.name=curl 16:42:53.198998042 curl -s -s -l -d - http : //registry-1.docker.io:443/v2/alpine/manifests/latest 16:42:53.784958541 curl -s -s -l -d - http : //auth.docker.io/token ? service=registry.docker.io & scope=repositori : alpin : pull 16:42:54.294192024 curl -s -s -l -d - -h author : bearer eyjhbgcioijfuzi1niisinr5cci6ikpxvcising1yyi6wyjnsuldthpdq0fku2dbd0lcqwdjqkfequtcz2dxagtqt1bruurbakjhtvvrd1fnwurwuvferxp0uk5gb3ppa2rytjbrnldgulfsrhbjvfrsuk9rovvwrmc2tmtgrlf6cfnuve5et2tgu01rttzumfkztnpwq1zrvkjpa2xhulvrnlexazftekflrncwee5uqtjnalv4t1rvmu5ewmfgdzb4tmpbmk1quxhpvfuxtkrayu1fwxhsrejdqmdovkjbtvrpmghhu1uwnldgzfzwam8yuvzksu9swlpuvek2ttfnmvrecfnwrekxt2s5vfnrbzztmvexumpwwvrssklpbfjmtmtnnlmxukxoanbcuvv0vu1ga3dfd1lis29asxpqmenbuvljs29asxpqmerbuwneuwdbrxl2uzivdei3t3jlmkvxcgrdefdts1nqv1n2vmj2twurwgvftunvmdbyqji0akniuvhrefdmoss0muxqmlznq29bk0rmrkiwvjbgzgdwajlowu5rl2pxt0jzaknccnpbt0jntlziuthcqwy4rujbtunbsuf3rhdzrfzsmgxcqwd3qmdzrvzsmgxbrejfqmdovkhrnevquve3u0vaslrucflwmvzxt2paqlywzzzwbgxotwpveldevk1pbepvtwpvnlqxtkttanbmvkrwr09sae9va2c2vkvzmlnecexwrxmyt2tgqlmxuxdsz1levliwakjeohdqwue3vvrsyu16cehwemrkt2xovvvfutztrtawvvrwufzgullpalpculvnnlvrmhprenbcvwpkre9roudoemm2uwxarlfucepsa1zkt2towk5vc3ddz1ljs29asxpqmevbd0leu1fbd1jnswhbtxzit2h4chhrtktqsdrhmfbns0lfdxrmtjztrdfvmws4zejovgxuwvfudkfpruf0yvjgsgjsr2o4zlvsszz4uvjhrurvqm1zz3dzelr3z3bmagjbzznoumfvpsjdfq.eyjhy2nlc3mioltdlcjhdwqioijyzwdpc3ryes5kb2nrzxiuaw8ilcjlehaioje0ntcyodi4nzqsimlhdci6mtq1nzi4mju3ncwiaxnzijoiyxv0ac5kb2nrzxiuaw8ilcjqdgkioijaogtynxzxnejmwknirs1icvjiacisim5izii6mtq1nzi4mju3ncwic3viijoiin0.c2wtjq_p-m0buparhmqjdfh6ztiahcvgn3tfwizeclsgxlvq_saqxaalnzkwaql2chj7nphx -- 0gw-ael_28aw http : //registry-1.docker.io:443/v2/alpine/manifests/latest { code } also got result { { ubuntu } } docker imag .,MESOS-4877,3.0
fix rmdir window thi due bug mesos-4415 land 0.27.0 .,MESOS-4836,1.0
"cgroupsanyhierarchywithfreezertest.root_cgroups_destroytracedprocess flaki verbos log : { code } [ run ] cgroupsanyhierarchywithfreezertest.root_cgroups_destroytracedprocess i0302 00:43:14.127846 11755 cgroups.cpp:2427 ] freez cgroup /sys/fs/cgroup/freezer/mesos_test i0302 00:43:14.267411 11758 cgroups.cpp:1409 ] success froze cgroup /sys/fs/cgroup/freezer/mesos_test 139.46496m i0302 00:43:14.409395 11751 cgroups.cpp:2445 ] thaw cgroup /sys/fs/cgroup/freezer/mesos_test i0302 00:43:14.551304 11751 cgroups.cpp:1438 ] successfullli thaw cgroup /sys/fs/cgroup/freezer/mesos_test 141.811968m .. / .. /src/tests/containerizer/cgroups_tests.cpp:949 : failur valu : : :waitpid ( pid , & statu , 0 ) actual : 23809 expect : -1 .. / .. /src/tests/containerizer/cgroups_tests.cpp:950 : failur valu : ( * __errno_loc ( ) ) actual : 0 expect : 10 [ fail ] cgroupsanyhierarchywithfreezertest.root_cgroups_destroytracedprocess ( 1055 ms ) { code }",MESOS-4835,2.0
"bind docker runtim isol docker imag provid . if imag provid specifi ` docker ` docker/runtim set , would meaning , execut . a check ad make sure docker runtim isol use docker imag provid .",MESOS-4830,1.0
"`` filesystem/linux '' isol unmount orphan persist volum a persist volum orphan : # a framework regist checkpoint enabl . # the framework start task + persist volum . # the agent exit . the task continu run . # someth wipe agent 's { { meta } } directori . thi remov checkpoint framework info agent . # the agent come back recov . the framework task found , task consid orphan . the agent current unmount persist volum , say ( { { glog_v=1 } } ) { code } i0229 23:55:42.078940 5635 linux.cpp:711 ] ignor cleanup request unknown contain : a35189d3-85d5-4d02-b568-67f675b6dc97 { code } test implement : http : //reviews.apache.org/r/44122/",MESOS-4824,2.0
"meso fail escap command health check as describ http : //github.com/mesosphere/marathon/issues/3333 i would like run command health check { noformat } /bin/bash -c `` < /dev/tcp/ $ host/ $ port0 '' { noformat } the health check fail meso , run command insid doubl quot sh -c `` '' n't escap doubl quot command . if i escap doubl quot command health check succe . but would mean user need intim knowledg meso execut command ca n't right . i told marathon meso issu open jira . i n't know affect command health check .",MESOS-4812,5.0
"iotest.bufferedread write current directori libprocess 's { { iotest.bufferedread } } write current directori . thi bad number reason , e.g. , * test fail data might leak random locat , * test execut write-onli directori , * execut test parallel would race exist creat file , show bogu behavior . the test probabl execut temporari directori , e.g. , via stout 's { { temporarydirectorytest } } fixtur .",MESOS-4807,1.0
"leveldbstatetest write current directori all { { leveldbstatetest } } test write current directori . thi bad number reason , e.g. , * test fail data might leak random locat , * test execut write-onli directori , * execut test suit parallel ( e.g. , { { gtest-parallel } } would race exist creat file , show bogu behavior . the test probabl execut temporari directori , e.g. , via stout 's { { temporarydirectorytest } } fixtur .",MESOS-4806,2.0
"executor env variabl leak command task . current , command task inherit env variabl command executor . thi less ideal command executor environ variabl includ meso intern env variabl like mesos_xxx libprocess_xxx . also , behavior match docker container . we construct env variabl scratch command task , rather reli inherit env variabl command executor .",MESOS-4781,3.0
"mastermaintenancetest.inverseoff flaki [ mesos-4169 ] significantli sped test , also surfac flaki . thi fix way [ mesos-4059 ] . verbos log asf centos7 build : { code } [ run ] mastermaintenancetest.inverseoff i0224 22:35:53.714018 1948 leveldb.cpp:174 ] open db 2.034387m i0224 22:35:53.714663 1948 leveldb.cpp:181 ] compact db 608839n i0224 22:35:53.714709 1948 leveldb.cpp:196 ] creat db iter 19043n i0224 22:35:53.714844 1948 leveldb.cpp:202 ] seek begin db 2330n i0224 22:35:53.714956 1948 leveldb.cpp:271 ] iter 0 key db 518n i0224 22:35:53.715092 1948 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0224 22:35:53.715646 1968 recover.cpp:447 ] start replica recoveri i0224 22:35:53.715915 1981 recover.cpp:473 ] replica empti statu i0224 22:35:53.717067 1972 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 4533 ) @ 172.17.0.1:36678 i0224 22:35:53.717445 1981 recover.cpp:193 ] receiv recov respons replica empti statu i0224 22:35:53.717888 1978 recover.cpp:564 ] updat replica statu start i0224 22:35:53.718585 1979 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 525061n i0224 22:35:53.718618 1979 replica.cpp:320 ] persist replica statu start i0224 22:35:53.718827 1982 recover.cpp:473 ] replica start statu i0224 22:35:53.719728 1969 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 4534 ) @ 172.17.0.1:36678 i0224 22:35:53.719974 1971 recover.cpp:193 ] receiv recov respons replica start statu i0224 22:35:53.720369 1970 recover.cpp:564 ] updat replica statu vote i0224 22:35:53.720789 1982 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 322308n i0224 22:35:53.720823 1982 replica.cpp:320 ] persist replica statu vote i0224 22:35:53.720968 1982 recover.cpp:578 ] success join paxo group i0224 22:35:53.721101 1982 recover.cpp:462 ] recov process termin i0224 22:35:53.721698 1982 master.cpp:376 ] master aab18b61-7811-4c43-a672-d1a63818c880 ( 4db5fa128d2d ) start 172.17.0.1:36678 i0224 22:35:53.721719 1982 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' fals '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/mjbcwp/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.28.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/mjbcwp/master '' -- zk_session_timeout= '' 10sec '' i0224 22:35:53.722039 1982 master.cpp:425 ] master allow unauthent framework regist i0224 22:35:53.722053 1982 master.cpp:428 ] master allow authent slave regist i0224 22:35:53.722061 1982 credentials.hpp:35 ] load credenti authent '/tmp/mjbcwp/credentials' i0224 22:35:53.722394 1982 master.cpp:468 ] use default 'crammd5 ' authent i0224 22:35:53.722525 1982 master.cpp:537 ] use default 'basic ' http authent i0224 22:35:53.722661 1982 master.cpp:571 ] author enabl i0224 22:35:53.722813 1968 hierarchical.cpp:144 ] initi hierarch alloc process i0224 22:35:53.722846 1980 whitelist_watcher.cpp:77 ] no whitelist given i0224 22:35:53.724957 1977 master.cpp:1712 ] the newli elect leader master @ 172.17.0.1:36678 id aab18b61-7811-4c43-a672-d1a63818c880 i0224 22:35:53.725000 1977 master.cpp:1725 ] elect lead master ! i0224 22:35:53.725023 1977 master.cpp:1470 ] recov registrar i0224 22:35:53.725306 1967 registrar.cpp:307 ] recov registrar i0224 22:35:53.725808 1977 log.cpp:659 ] attempt start writer i0224 22:35:53.727145 1973 replica.cpp:493 ] replica receiv implicit promis request ( 4536 ) @ 172.17.0.1:36678 propos 1 i0224 22:35:53.727728 1973 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 424560n i0224 22:35:53.727828 1973 replica.cpp:342 ] persist promis 1 i0224 22:35:53.729080 1973 coordinator.cpp:238 ] coordin attempt fill miss posit i0224 22:35:53.731009 1979 replica.cpp:388 ] replica receiv explicit promis request ( 4537 ) @ 172.17.0.1:36678 posit 0 propos 2 i0224 22:35:53.731580 1979 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 478479n i0224 22:35:53.731613 1979 replica.cpp:712 ] persist action 0 i0224 22:35:53.734354 1979 replica.cpp:537 ] replica receiv write request posit 0 ( 4538 ) @ 172.17.0.1:36678 i0224 22:35:53.734485 1979 leveldb.cpp:436 ] read posit leveldb took 60879n i0224 22:35:53.735877 1979 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.324061m i0224 22:35:53.735930 1979 replica.cpp:712 ] persist action 0 i0224 22:35:53.737061 1970 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0224 22:35:53.738881 1970 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.772814m i0224 22:35:53.738939 1970 replica.cpp:712 ] persist action 0 i0224 22:35:53.738975 1970 replica.cpp:697 ] replica learn nop action posit 0 i0224 22:35:53.740136 1976 log.cpp:675 ] writer start end posit 0 i0224 22:35:53.741750 1976 leveldb.cpp:436 ] read posit leveldb took 74863n i0224 22:35:53.743479 1976 registrar.cpp:340 ] success fetch registri ( 0b ) 18.11968m i0224 22:35:53.743755 1976 registrar.cpp:439 ] appli 1 oper 56670n ; attempt updat 'registry' i0224 22:35:53.745604 1978 log.cpp:683 ] attempt append 170 byte log i0224 22:35:53.745905 1977 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0224 22:35:53.746968 1981 replica.cpp:537 ] replica receiv write request posit 1 ( 4539 ) @ 172.17.0.1:36678 i0224 22:35:53.747480 1981 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 456947n i0224 22:35:53.747609 1981 replica.cpp:712 ] persist action 1 i0224 22:35:53.750448 1981 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0224 22:35:53.751158 1981 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 535163n i0224 22:35:53.751258 1981 replica.cpp:712 ] persist action 1 i0224 22:35:53.751389 1981 replica.cpp:697 ] replica learn append action posit 1 i0224 22:35:53.753149 1979 registrar.cpp:484 ] success updat 'registri ' 9.228032m i0224 22:35:53.753324 1979 registrar.cpp:370 ] success recov registrar i0224 22:35:53.753593 1979 log.cpp:702 ] attempt truncat log 1 i0224 22:35:53.753805 1979 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0224 22:35:53.754055 1981 master.cpp:1522 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i0224 22:35:53.754349 1979 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0224 22:35:53.755764 1977 replica.cpp:537 ] replica receiv write request posit 2 ( 4540 ) @ 172.17.0.1:36678 i0224 22:35:53.756459 1977 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 488559n i0224 22:35:53.756561 1977 replica.cpp:712 ] persist action 2 i0224 22:35:53.757932 1972 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0224 22:35:53.758400 1972 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 343827n i0224 22:35:53.758539 1972 leveldb.cpp:399 ] delet ~1 key leveldb took 34231n i0224 22:35:53.758658 1972 replica.cpp:712 ] persist action 2 i0224 22:35:53.758782 1972 replica.cpp:697 ] replica learn truncat action posit 2 i0224 22:35:53.778059 1978 slave.cpp:193 ] slave start 115 ) @ 172.17.0.1:36678 i0224 22:35:53.778105 1978 slave.cpp:194 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/mastermaintenancetest_inverseoffers_ywqvff/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/mastermaintenancetest_inverseoffers_ywqvff/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname= '' maintenance-host '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.28.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/mastermaintenancetest_inverseoffers_ywqvff '' i0224 22:35:53.778609 1978 credentials.hpp:83 ] load credenti authent '/tmp/mastermaintenancetest_inverseoffers_ywqvff/credential' i0224 22:35:53.779175 1978 slave.cpp:324 ] slave use credenti : test-princip i0224 22:35:53.779520 1978 resources.cpp:576 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0224 22:35:53.780192 1978 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0224 22:35:53.780362 1978 slave.cpp:472 ] slave attribut : [ ] i0224 22:35:53.780483 1978 slave.cpp:477 ] slave hostnam : maintenance-host i0224 22:35:53.782126 1967 state.cpp:58 ] recov state '/tmp/mastermaintenancetest_inverseoffers_ywqvff/meta' i0224 22:35:53.782892 1969 status_update_manager.cpp:200 ] recov statu updat manag i0224 22:35:53.783242 1969 slave.cpp:4565 ] finish recoveri i0224 22:35:53.784001 1969 slave.cpp:4737 ] queri resourc estim oversubscrib resourc i0224 22:35:53.784678 1969 slave.cpp:796 ] new master detect master @ 172.17.0.1:36678 i0224 22:35:53.784874 1967 status_update_manager.cpp:174 ] paus send statu updat i0224 22:35:53.784808 1969 slave.cpp:859 ] authent master master @ 172.17.0.1:36678 i0224 22:35:53.784945 1969 slave.cpp:864 ] use default cram-md5 authenticate i0224 22:35:53.785181 1969 slave.cpp:832 ] detect new master i0224 22:35:53.785326 1969 slave.cpp:4751 ] receiv oversubscrib resourc resourc estim i0224 22:35:53.785557 1969 authenticatee.cpp:121 ] creat new client sasl connect i0224 22:35:53.786227 1969 master.cpp:5526 ] authent slave ( 115 ) @ 172.17.0.1:36678 i0224 22:35:53.786492 1969 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 298 ) @ 172.17.0.1:36678 i0224 22:35:53.786962 1969 authenticator.cpp:98 ] creat new server sasl connect i0224 22:35:53.787274 1969 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0224 22:35:53.787308 1969 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0224 22:35:53.787400 1969 authenticator.cpp:203 ] receiv sasl authent start i0224 22:35:53.787470 1969 authenticator.cpp:325 ] authent requir step i0224 22:35:53.787884 1972 authenticatee.cpp:258 ] receiv sasl authent step i0224 22:35:53.787992 1972 authenticator.cpp:231 ] receiv sasl authent step i0224 22:35:53.788027 1972 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4db5fa128d2d ' server fqdn : '4db5fa128d2d ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0224 22:35:53.788040 1972 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0224 22:35:53.788090 1972 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0224 22:35:53.788122 1972 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '4db5fa128d2d ' server fqdn : '4db5fa128d2d ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0224 22:35:53.788136 1972 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0224 22:35:53.788146 1972 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0224 22:35:53.788164 1972 authenticator.cpp:317 ] authent success i0224 22:35:53.788331 1972 authenticatee.cpp:298 ] authent success i0224 22:35:53.788439 1972 master.cpp:5556 ] success authent princip 'test-princip ' slave ( 115 ) @ 172.17.0.1:36678 i0224 22:35:53.788529 1972 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 298 ) @ 172.17.0.1:36678 i0224 22:35:53.788988 1972 slave.cpp:927 ] success authent master master @ 172.17.0.1:36678 i0224 22:35:53.789139 1972 slave.cpp:1321 ] will retri registr 1.535786m necessari i0224 22:35:53.789515 1972 master.cpp:4240 ] regist slave slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) id aab18b61-7811-4c43-a672-d1a63818c880-s0 i0224 22:35:53.790577 1972 registrar.cpp:439 ] appli 1 oper 78745n ; attempt updat 'registry' i0224 22:35:53.791128 1971 process.cpp:3141 ] handl http event process 'master ' path : '/master/maintenance/schedule' i0224 22:35:53.791877 1971 http.cpp:501 ] http post /master/maintenance/schedul 172.17.0.1:45095 i0224 22:35:53.793313 1972 log.cpp:683 ] attempt append 343 byte log i0224 22:35:53.793586 1972 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0224 22:35:53.794533 1971 replica.cpp:537 ] replica receiv write request posit 3 ( 4547 ) @ 172.17.0.1:36678 i0224 22:35:53.794862 1971 leveldb.cpp:341 ] persist action ( 362 byte ) leveldb took 283614n i0224 22:35:53.794893 1971 replica.cpp:712 ] persist action 3 i0224 22:35:53.796646 1979 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0224 22:35:53.797102 1972 slave.cpp:1321 ] will retri registr 17.198963m necessari i0224 22:35:53.797186 1979 leveldb.cpp:341 ] persist action ( 364 byte ) leveldb took 498502n i0224 22:35:53.797230 1979 replica.cpp:712 ] persist action 3 i0224 22:35:53.797260 1979 replica.cpp:697 ] replica learn append action posit 3 i0224 22:35:53.797417 1972 master.cpp:4228 ] ignor regist slave messag slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) admiss alreadi progress i0224 22:35:53.799119 1978 registrar.cpp:484 ] success updat 'registri ' 8.45824m i0224 22:35:53.799613 1978 registrar.cpp:439 ] appli 1 oper 176193n ; attempt updat 'registry' i0224 22:35:53.800472 1972 master.cpp:4308 ] regist slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0224 22:35:53.800623 1978 log.cpp:702 ] attempt truncat log 3 i0224 22:35:53.801255 1969 hierarchical.cpp:473 ] ad slave aab18b61-7811-4c43-a672-d1a63818c880-s0 ( maintenance-host ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0224 22:35:53.801301 1978 slave.cpp:971 ] regist master master @ 172.17.0.1:36678 ; given slave id aab18b61-7811-4c43-a672-d1a63818c880-s0 i0224 22:35:53.801331 1978 fetcher.cpp:81 ] clear fetcher cach i0224 22:35:53.801431 1969 hierarchical.cpp:1434 ] no resourc avail alloc ! i0224 22:35:53.801466 1969 hierarchical.cpp:1147 ] perform alloc slave aab18b61-7811-4c43-a672-d1a63818c880-s0 162751n i0224 22:35:53.801532 1969 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0224 22:35:53.801867 1978 slave.cpp:994 ] checkpoint slaveinfo '/tmp/mastermaintenancetest_inverseoffers_ywqvff/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/slave.info' i0224 22:35:53.801877 1969 status_update_manager.cpp:181 ] resum send statu updat i0224 22:35:53.802898 1977 replica.cpp:537 ] replica receiv write request posit 4 ( 4548 ) @ 172.17.0.1:36678 i0224 22:35:53.803252 1978 slave.cpp:1030 ] forward total oversubscrib resourc i0224 22:35:53.803640 1970 master.cpp:4649 ] receiv updat slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) total oversubscrib resourc i0224 22:35:53.803858 1977 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 912626n i0224 22:35:53.803889 1977 replica.cpp:712 ] persist action 4 i0224 22:35:53.804144 1978 slave.cpp:3482 ] receiv ping slave-observ ( 117 ) @ 172.17.0.1:36678 i0224 22:35:53.804535 1971 hierarchical.cpp:531 ] slave aab18b61-7811-4c43-a672-d1a63818c880-s0 ( maintenance-host ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0224 22:35:53.804684 1971 hierarchical.cpp:1434 ] no resourc avail alloc ! i0224 22:35:53.804714 1971 hierarchical.cpp:1147 ] perform alloc slave aab18b61-7811-4c43-a672-d1a63818c880-s0 131453n i0224 22:35:53.805541 1967 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0224 22:35:53.805941 1967 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 366444n i0224 22:35:53.806015 1967 leveldb.cpp:399 ] delet ~2 key leveldb took 42808n i0224 22:35:53.806041 1967 replica.cpp:712 ] persist action 4 i0224 22:35:53.806066 1967 replica.cpp:697 ] replica learn truncat action posit 4 i0224 22:35:53.807355 1978 log.cpp:683 ] attempt append 465 byte log i0224 22:35:53.807551 1978 coordinator.cpp:348 ] coordin attempt write append action posit 5 i0224 22:35:53.809638 1979 replica.cpp:537 ] replica receiv write request posit 5 ( 4549 ) @ 172.17.0.1:36678 i0224 22:35:53.810858 1979 leveldb.cpp:341 ] persist action ( 484 byte ) leveldb took 1.167663m i0224 22:35:53.810904 1979 replica.cpp:712 ] persist action 5 i0224 22:35:53.811997 1979 replica.cpp:691 ] replica receiv learn notic posit 5 @ 0.0.0.0:0 i0224 22:35:53.812348 1979 leveldb.cpp:341 ] persist action ( 486 byte ) leveldb took 318928n i0224 22:35:53.812376 1979 replica.cpp:712 ] persist action 5 i0224 22:35:53.812397 1979 replica.cpp:697 ] replica learn append action posit 5 i0224 22:35:53.815132 1973 registrar.cpp:484 ] success updat 'registri ' 15.437312m i0224 22:35:53.815491 1976 log.cpp:702 ] attempt truncat log 5 i0224 22:35:53.815610 1973 coordinator.cpp:348 ] coordin attempt write truncat action posit 6 i0224 22:35:53.815661 1968 master.cpp:4705 ] updat unavail slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) , start 2410.99235909694week i0224 22:35:53.815845 1968 master.cpp:4705 ] updat unavail slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) , start 2410.99235909694week i0224 22:35:53.816069 1975 hierarchical.cpp:1434 ] no resourc avail alloc ! i0224 22:35:53.816103 1975 hierarchical.cpp:1147 ] perform alloc slave aab18b61-7811-4c43-a672-d1a63818c880-s0 175822n i0224 22:35:53.816272 1975 hierarchical.cpp:1434 ] no resourc avail alloc ! i0224 22:35:53.816303 1975 hierarchical.cpp:1147 ] perform alloc slave aab18b61-7811-4c43-a672-d1a63818c880-s0 110913n i0224 22:35:53.817291 1972 replica.cpp:537 ] replica receiv write request posit 6 ( 4550 ) @ 172.17.0.1:36678 i0224 22:35:53.817908 1972 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 576032n i0224 22:35:53.817932 1972 replica.cpp:712 ] persist action 6 i0224 22:35:53.818686 1980 replica.cpp:691 ] replica receiv learn notic posit 6 @ 0.0.0.0:0 i0224 22:35:53.819021 1980 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 305298n i0224 22:35:53.819095 1980 leveldb.cpp:399 ] delet ~2 key leveldb took 44332n i0224 22:35:53.819120 1980 replica.cpp:712 ] persist action 6 i0224 22:35:53.819162 1980 replica.cpp:697 ] replica learn truncat action posit 6 i0224 22:35:53.820662 1967 process.cpp:3141 ] handl http event process 'master ' path : '/master/maintenance/status' i0224 22:35:53.821190 1976 http.cpp:501 ] http get /master/maintenance/statu 172.17.0.1:45096 i0224 22:35:53.823709 1948 scheduler.cpp:154 ] version : 0.28.0 i0224 22:35:53.824424 1972 scheduler.cpp:236 ] new master detect master @ 172.17.0.1:36678 i0224 22:35:53.825402 1982 scheduler.cpp:298 ] send subscrib call master @ 172.17.0.1:36678 i0224 22:35:53.827201 1978 process.cpp:3141 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0224 22:35:53.827636 1978 http.cpp:501 ] http post /master/api/v1/schedul 172.17.0.1:45097 i0224 22:35:53.827922 1978 master.cpp:1974 ] receiv subscript request http framework 'default' i0224 22:35:53.827991 1978 master.cpp:1751 ] author framework princip 'test-princip ' receiv offer role ' * ' i0224 22:35:53.828418 1982 master.cpp:2065 ] subscrib framework 'default ' checkpoint disabl capabl [ ] i0224 22:35:53.828943 1968 hierarchical.cpp:265 ] ad framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.829124 1982 master.hpp:1657 ] send heartbeat aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.829987 1968 hierarchical.cpp:1127 ] perform alloc 1 slave 1.011356m i0224 22:35:53.830204 1982 master.cpp:5355 ] send 1 offer framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) i0224 22:35:53.830801 1982 master.cpp:5445 ] send 1 invers offer framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) i0224 22:35:53.831132 1969 scheduler.cpp:457 ] enqueu event subscrib receiv master @ 172.17.0.1:36678 i0224 22:35:53.832396 1968 scheduler.cpp:457 ] enqueu event heartbeat receiv master @ 172.17.0.1:36678 i0224 22:35:53.833050 1976 master_maintenance_tests.cpp:177 ] ignor heartbeat event i0224 22:35:53.833256 1979 scheduler.cpp:457 ] enqueu event offer receiv master @ 172.17.0.1:36678 i0224 22:35:53.833775 1979 scheduler.cpp:457 ] enqueu event offer receiv master @ 172.17.0.1:36678 i0224 22:35:53.835662 1980 scheduler.cpp:298 ] send accept call master @ 172.17.0.1:36678 i0224 22:35:53.837591 1967 process.cpp:3141 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0224 22:35:53.838021 1967 http.cpp:501 ] http post /master/api/v1/schedul 172.17.0.1:45098 i0224 22:35:53.838851 1967 master.cpp:3138 ] process accept call offer : [ aab18b61-7811-4c43-a672-d1a63818c880-o0 ] slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) i0224 22:35:53.838946 1967 master.cpp:2825 ] author framework princip 'test-princip ' launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 user 'mesos' w0224 22:35:53.841048 1967 validation.cpp:404 ] executor default task 90bcae0c-9d40-40b7-9537-dae7e83479f6 use less cpu ( none ) minimum requir ( 0.01 ) . pleas updat executor , mandatori futur releas . w0224 22:35:53.841101 1967 validation.cpp:416 ] executor default task 90bcae0c-9d40-40b7-9537-dae7e83479f6 use less memori ( none ) minimum requir ( 32mb ) . pleas updat executor , mandatori futur releas . i0224 22:35:53.841624 1967 master.hpp:176 ] ad task 90bcae0c-9d40-40b7-9537-dae7e83479f6 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave aab18b61-7811-4c43-a672-d1a63818c880-s0 ( maintenance-host ) i0224 22:35:53.842157 1967 master.cpp:3623 ] launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) i0224 22:35:53.842571 1980 slave.cpp:1361 ] got assign task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.843122 1980 slave.cpp:1480 ] launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.843718 1980 paths.cpp:474 ] tri chown '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159 ' user 'mesos' i0224 22:35:53.852052 1980 slave.cpp:5367 ] launch executor default framework aab18b61-7811-4c43-a672-d1a63818c880-0000 resourc work directori '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' i0224 22:35:53.854452 1980 exec.cpp:143 ] version : 0.28.0 i0224 22:35:53.854812 1967 exec.cpp:193 ] executor start : executor ( 47 ) @ 172.17.0.1:36678 pid 1948 i0224 22:35:53.855108 1980 slave.cpp:1698 ] queu task '90bcae0c-9d40-40b7-9537-dae7e83479f6 ' executor 'default ' framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.855264 1980 slave.cpp:749 ] success attach file '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' i0224 22:35:53.855362 1980 slave.cpp:2643 ] got registr executor 'default ' framework aab18b61-7811-4c43-a672-d1a63818c880-0000 executor ( 47 ) @ 172.17.0.1:36678 i0224 22:35:53.855785 1974 exec.cpp:217 ] executor regist slave aab18b61-7811-4c43-a672-d1a63818c880-s0 i0224 22:35:53.855857 1974 exec.cpp:229 ] executor : :regist took 42512n i0224 22:35:53.856391 1980 slave.cpp:1863 ] send queu task '90bcae0c-9d40-40b7-9537-dae7e83479f6 ' executor 'default ' framework aab18b61-7811-4c43-a672-d1a63818c880-0000 executor ( 47 ) @ 172.17.0.1:36678 i0224 22:35:53.856720 1974 exec.cpp:304 ] executor ask run task '90bcae0c-9d40-40b7-9537-dae7e83479f6' i0224 22:35:53.856812 1974 exec.cpp:313 ] executor : :launchtask took 65703n i0224 22:35:53.856922 1974 exec.cpp:526 ] executor send statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.857378 1980 slave.cpp:3002 ] handl statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 executor ( 47 ) @ 172.17.0.1:36678 i0224 22:35:53.858175 1980 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.858222 1980 status_update_manager.cpp:497 ] creat statusupd stream task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.858687 1980 status_update_manager.cpp:374 ] forward updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 slave i0224 22:35:53.859210 1980 slave.cpp:3400 ] forward updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 master @ 172.17.0.1:36678 i0224 22:35:53.859390 1980 slave.cpp:3294 ] statu updat manag success handl statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.859436 1980 slave.cpp:3310 ] send acknowledg statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 executor ( 47 ) @ 172.17.0.1:36678 i0224 22:35:53.859663 1980 exec.cpp:350 ] executor receiv statu updat acknowledg 249b169a-6b5f-4776-95c8-c897ba6b3f0b task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.859657 1967 master.cpp:4794 ] statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) i0224 22:35:53.859851 1967 master.cpp:4842 ] forward statu updat task_run ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.860587 1967 master.cpp:6450 ] updat state task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( latest state : task_run , statu updat state : task_run ) i0224 22:35:53.862711 1967 scheduler.cpp:457 ] enqueu event updat receiv master @ 172.17.0.1:36678 i0224 22:35:53.866711 1976 scheduler.cpp:298 ] send acknowledg call master @ 172.17.0.1:36678 i0224 22:35:53.870667 1972 process.cpp:3141 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0224 22:35:53.871269 1972 http.cpp:501 ] http post /master/api/v1/schedul 172.17.0.1:45099 i0224 22:35:53.871459 1972 master.cpp:3952 ] process acknowledg call 249b169a-6b5f-4776-95c8-c897ba6b3f0b task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) slave aab18b61-7811-4c43-a672-d1a63818c880-s0 i0224 22:35:53.872184 1972 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.872537 1972 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : 249b169a-6b5f-4776-95c8-c897ba6b3f0b ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:35:53.874407 1975 scheduler.cpp:298 ] send declin call master @ 172.17.0.1:36678 i0224 22:35:53.877537 1979 hierarchical.cpp:1434 ] no resourc avail alloc ! i0224 22:35:53.877795 1979 hierarchical.cpp:1127 ] perform alloc 1 slave 482441n i0224 22:35:53.878082 1981 process.cpp:3141 ] handl http event process 'master ' path : '/master/api/v1/scheduler' i0224 22:35:53.878675 1978 http.cpp:501 ] http post /master/api/v1/schedul 172.17.0.1:45100 i0224 22:35:53.878931 1978 master.cpp:3675 ] process declin call offer : [ aab18b61-7811-4c43-a672-d1a63818c880-o1 ] framework aab18b61-7811-4c43-a672-d1a63818c880-0000 ( default ) .. / .. /src/tests/master_maintenance_tests.cpp:1222 : failur fail wait 15sec event i0224 22:36:08.881649 1948 master.cpp:1027 ] master termin w0224 22:36:08.881925 1948 master.cpp:6502 ] remov task 90bcae0c-9d40-40b7-9537-dae7e83479f6 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework aab18b61-7811-4c43-a672-d1a63818c880-0000 slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) non-termin state task_run i0224 22:36:08.882961 1948 master.cpp:6545 ] remov executor 'default ' resourc framework aab18b61-7811-4c43-a672-d1a63818c880-0000 slave aab18b61-7811-4c43-a672-d1a63818c880-s0 slave ( 115 ) @ 172.17.0.1:36678 ( maintenance-host ) i0224 22:36:08.884789 1969 hierarchical.cpp:505 ] remov slave aab18b61-7811-4c43-a672-d1a63818c880-s0 i0224 22:36:08.887261 1969 hierarchical.cpp:326 ] remov framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.916983 1976 slave.cpp:3528 ] master @ 172.17.0.1:36678 exit w0224 22:36:08.917191 1976 slave.cpp:3531 ] master disconnect ! wait new master elect i0224 22:36:08.934546 1975 slave.cpp:3528 ] executor ( 47 ) @ 172.17.0.1:36678 exit i0224 22:36:08.934806 1974 slave.cpp:3886 ] executor 'default ' framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exit statu 0 i0224 22:36:08.935024 1974 slave.cpp:3002 ] handl statu updat task_fail ( uuid : 77d415df-58bd-4cf5-9c49-6106691d9599 ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 @ 0.0.0.0:0 i0224 22:36:08.935505 1974 slave.cpp:5677 ] termin task 90bcae0c-9d40-40b7-9537-dae7e83479f6 i0224 22:36:08.936190 1967 status_update_manager.cpp:320 ] receiv statu updat task_fail ( uuid : 77d415df-58bd-4cf5-9c49-6106691d9599 ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.936368 1967 status_update_manager.cpp:374 ] forward updat task_fail ( uuid : 77d415df-58bd-4cf5-9c49-6106691d9599 ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 slave i0224 22:36:08.936606 1974 slave.cpp:3400 ] forward updat task_fail ( uuid : 77d415df-58bd-4cf5-9c49-6106691d9599 ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 master @ 172.17.0.1:36678 i0224 22:36:08.936779 1974 slave.cpp:3294 ] statu updat manag success handl statu updat task_fail ( uuid : 77d415df-58bd-4cf5-9c49-6106691d9599 ) task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.955370 1967 slave.cpp:668 ] slave termin i0224 22:36:08.955499 1967 slave.cpp:2079 ] ask shut framework aab18b61-7811-4c43-a672-d1a63818c880-0000 @ 0.0.0.0:0 i0224 22:36:08.955538 1967 slave.cpp:2104 ] shut framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.955606 1967 slave.cpp:3990 ] clean executor 'default ' framework aab18b61-7811-4c43-a672-d1a63818c880-0000 executor ( 47 ) @ 172.17.0.1:36678 i0224 22:36:08.956053 1967 slave.cpp:4078 ] clean framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.956327 1967 gc.cpp:54 ] schedul '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159 ' gc 1.00002336880296week futur i0224 22:36:08.956495 1973 status_update_manager.cpp:282 ] close statu updat stream framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.956524 1967 gc.cpp:54 ] schedul '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default ' gc 1.00002336880296week futur i0224 22:36:08.956549 1973 status_update_manager.cpp:528 ] clean statu updat stream task 90bcae0c-9d40-40b7-9537-dae7e83479f6 framework aab18b61-7811-4c43-a672-d1a63818c880-0000 i0224 22:36:08.956619 1967 gc.cpp:54 ] schedul '/tmp/mastermaintenancetest_inverseoffers_ywqvff/slaves/aab18b61-7811-4c43-a672-d1a63818c880-s0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000 ' gc 1.00002336880296week futur [ fail ] mastermaintenancetest.inverseoff ( 15258 ms ) { code }",MESOS-4768,1.0
"meso container get uid/gid pivot_root . current , call os : :su ( user ) pivot_root . thi problemat /etc/passwd /etc/group might miss contain 's root filesystem . we instead , get uid/gid pivot_root , call setuid/setgroup pivot_root .",MESOS-4757,3.0
"the `` executor '' field expos backward incompat schema . in 0.26.0 , master 's { { /state } } endpoint gener follow : { code } { / * ... * / `` framework '' : [ { / * ... * / `` executor '' : [ { `` command '' : { `` argv '' : [ ] , `` uri '' : [ ] , `` valu '' : `` /users/mpark/projects/mesos/build/opt/src/long-lived-executor '' } , `` executor_id '' : `` default '' , `` framework_id '' : `` 0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000 '' , `` name '' : `` long live executor ( c++ ) '' , `` resourc '' : { `` cpu '' : 0 , `` disk '' : 0 , `` mem '' : 0 } , `` slave_id '' : `` 8a513678-03a1-4cb5-9279-c3c0c591f1d8-s0 '' } ] , / * ... * / } ] / * ... * / } { code } in 0.27.1 , { { executorinfo } } mistakenli expos raw protobuf schema : { code } { / * ... * / `` framework '' : [ { / * ... * / `` executor '' : [ { `` command '' : { `` shell '' : true , `` valu '' : `` /users/mpark/projects/mesos/build/opt/src/long-lived-executor '' } , `` executor_id '' : { `` valu '' : `` default '' } , `` framework_id '' : { `` valu '' : `` 368a5a49-480b-41f6-a13b-24a69c92a72e-0000 '' } , `` name '' : `` long live executor ( c++ ) '' , `` slave_id '' : `` 8a513678-03a1-4cb5-9279-c3c0c591f1d8-s0 '' , `` sourc '' : `` cpp_long_lived_framework '' } ] , / * ... * / } ] / * ... * / } { code } thi backward incompat api chang .",MESOS-4754,2.0
"containerloggertest.mesoscontainerizerrecov execut isol some cleanup spawn process miss { { containerloggertest.mesoscontainerizerrecov } } test run isol global teardown might find linger process . { code } [ ========== ] run 1 test 1 test case . [ -- -- -- -- -- ] global test environ set-up . [ -- -- -- -- -- ] 1 test containerloggertest [ run ] containerloggertest.mesoscontainerizerrecov [ ok ] containerloggertest.mesoscontainerizerrecov ( 13 ms ) [ -- -- -- -- -- ] 1 test containerloggertest ( 13 ms total ) [ -- -- -- -- -- ] global test environ tear-down .. / .. /src/tests/environment.cpp:728 : failur fail test complet child process remain : -+- 7112 /some/path/src/mesos/build/src/.libs/mesos-test -- gtest_filter=containerloggertest.mesoscontainerizerrecov \ -- - 7130 ( sh ) [ ========== ] 1 test 1 test case ran . ( 23 ms total ) [ pass ] 1 test . [ fail ] 0 test , list : 0 fail test { code } observ os x clang-trunk unoptim build .",MESOS-4747,1.0
"dockercontainerizertest.root_docker_launchwithpersistentvolum fail cento 6 thi test pass consist os 's , fail consist cento 6 . verbos log test failur : { code } [ run ] dockercontainerizertest.root_docker_launchwithpersistentvolum i0222 18:16:12.327957 26681 leveldb.cpp:174 ] open db 7.466102m i0222 18:16:12.330528 26681 leveldb.cpp:181 ] compact db 2.540139m i0222 18:16:12.330580 26681 leveldb.cpp:196 ] creat db iter 16908n i0222 18:16:12.330592 26681 leveldb.cpp:202 ] seek begin db 1403n i0222 18:16:12.330600 26681 leveldb.cpp:271 ] iter 0 key db 315n i0222 18:16:12.330634 26681 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0222 18:16:12.331082 26698 recover.cpp:447 ] start replica recoveri i0222 18:16:12.331289 26698 recover.cpp:473 ] replica empti statu i0222 18:16:12.332162 26703 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 13761 ) @ 172.30.2.148:35274 i0222 18:16:12.332701 26701 recover.cpp:193 ] receiv recov respons replica empti statu i0222 18:16:12.333230 26699 recover.cpp:564 ] updat replica statu start i0222 18:16:12.334102 26698 master.cpp:376 ] master 652149b4-3932-4d8b-ba6f-8c9d9045be70 ( ip-172-30-2-148.mesosphere.io ) start 172.30.2.148:35274 i0222 18:16:12.334116 26698 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/qehlbs/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/qehlbs/mast '' -- zk_session_timeout= '' 10sec '' i0222 18:16:12.334354 26698 master.cpp:423 ] master allow authent framework regist i0222 18:16:12.334363 26698 master.cpp:428 ] master allow authent slave regist i0222 18:16:12.334369 26698 credentials.hpp:35 ] load credenti authent '/tmp/qehlbs/credentials' i0222 18:16:12.335366 26698 master.cpp:468 ] use default 'crammd5 ' authent i0222 18:16:12.335492 26698 master.cpp:537 ] use default 'basic ' http authent i0222 18:16:12.335623 26698 master.cpp:571 ] author enabl i0222 18:16:12.335752 26703 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.314693m i0222 18:16:12.335769 26700 whitelist_watcher.cpp:77 ] no whitelist given i0222 18:16:12.335778 26703 replica.cpp:320 ] persist replica statu start i0222 18:16:12.335821 26697 hierarchical.cpp:144 ] initi hierarch alloc process i0222 18:16:12.335965 26701 recover.cpp:473 ] replica start statu i0222 18:16:12.336771 26703 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 13763 ) @ 172.30.2.148:35274 i0222 18:16:12.337191 26696 recover.cpp:193 ] receiv recov respons replica start statu i0222 18:16:12.337635 26700 recover.cpp:564 ] updat replica statu vote i0222 18:16:12.337671 26703 master.cpp:1712 ] the newli elect leader master @ 172.30.2.148:35274 id 652149b4-3932-4d8b-ba6f-8c9d9045be70 i0222 18:16:12.337698 26703 master.cpp:1725 ] elect lead master ! i0222 18:16:12.337713 26703 master.cpp:1470 ] recov registrar i0222 18:16:12.337828 26696 registrar.cpp:307 ] recov registrar i0222 18:16:12.339972 26702 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 2.06039m i0222 18:16:12.339994 26702 replica.cpp:320 ] persist replica statu vote i0222 18:16:12.340082 26700 recover.cpp:578 ] success join paxo group i0222 18:16:12.340267 26700 recover.cpp:462 ] recov process termin i0222 18:16:12.340591 26699 log.cpp:659 ] attempt start writer i0222 18:16:12.341594 26698 replica.cpp:493 ] replica receiv implicit promis request ( 13764 ) @ 172.30.2.148:35274 propos 1 i0222 18:16:12.343598 26698 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.97941m i0222 18:16:12.343619 26698 replica.cpp:342 ] persist promis 1 i0222 18:16:12.344182 26698 coordinator.cpp:238 ] coordin attempt fill miss posit i0222 18:16:12.345285 26702 replica.cpp:388 ] replica receiv explicit promis request ( 13765 ) @ 172.30.2.148:35274 posit 0 propos 2 i0222 18:16:12.347275 26702 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.960198m i0222 18:16:12.347296 26702 replica.cpp:712 ] persist action 0 i0222 18:16:12.348201 26703 replica.cpp:537 ] replica receiv write request posit 0 ( 13766 ) @ 172.30.2.148:35274 i0222 18:16:12.348247 26703 leveldb.cpp:436 ] read posit leveldb took 21399n i0222 18:16:12.350667 26703 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 2.39166m i0222 18:16:12.350690 26703 replica.cpp:712 ] persist action 0 i0222 18:16:12.351191 26696 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0222 18:16:12.353152 26696 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.935798m i0222 18:16:12.353173 26696 replica.cpp:712 ] persist action 0 i0222 18:16:12.353188 26696 replica.cpp:697 ] replica learn nop action posit 0 i0222 18:16:12.353639 26696 log.cpp:675 ] writer start end posit 0 i0222 18:16:12.354508 26697 leveldb.cpp:436 ] read posit leveldb took 25625n i0222 18:16:12.355274 26696 registrar.cpp:340 ] success fetch registri ( 0b ) 17.406976m i0222 18:16:12.355357 26696 registrar.cpp:439 ] appli 1 oper 20977n ; attempt updat 'registry' i0222 18:16:12.355929 26697 log.cpp:683 ] attempt append 210 byte log i0222 18:16:12.356032 26703 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0222 18:16:12.356657 26698 replica.cpp:537 ] replica receiv write request posit 1 ( 13767 ) @ 172.30.2.148:35274 i0222 18:16:12.358566 26698 leveldb.cpp:341 ] persist action ( 229 byte ) leveldb took 1.881945m i0222 18:16:12.358588 26698 replica.cpp:712 ] persist action 1 i0222 18:16:12.359081 26697 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0222 18:16:12.361002 26697 leveldb.cpp:341 ] persist action ( 231 byte ) leveldb took 1.894331m i0222 18:16:12.361023 26697 replica.cpp:712 ] persist action 1 i0222 18:16:12.361038 26697 replica.cpp:697 ] replica learn append action posit 1 i0222 18:16:12.361883 26697 registrar.cpp:484 ] success updat 'registri ' 6.482944m i0222 18:16:12.361981 26697 registrar.cpp:370 ] success recov registrar i0222 18:16:12.362052 26701 log.cpp:702 ] attempt truncat log 1 i0222 18:16:12.362167 26703 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0222 18:16:12.362421 26696 master.cpp:1522 ] recov 0 slave registri ( 171b ) ; allow 10min slave re-regist i0222 18:16:12.362447 26698 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0222 18:16:12.362911 26701 replica.cpp:537 ] replica receiv write request posit 2 ( 13768 ) @ 172.30.2.148:35274 i0222 18:16:12.364760 26701 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.819954m i0222 18:16:12.364783 26701 replica.cpp:712 ] persist action 2 i0222 18:16:12.365384 26697 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0222 18:16:12.367961 26697 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.55143m i0222 18:16:12.368015 26697 leveldb.cpp:399 ] delet ~1 key leveldb took 28196n i0222 18:16:12.368028 26697 replica.cpp:712 ] persist action 2 i0222 18:16:12.368044 26697 replica.cpp:697 ] replica learn truncat action posit 2 i0222 18:16:12.376824 26703 slave.cpp:193 ] slave start 396 ) @ 172.30.2.148:35274 i0222 18:16:12.376838 26703 slave.cpp:194 ] flag startup : -- appc_simple_discovery_uri_prefix= '' http : // '' -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' http : //auth.docker.io '' -- docker_kill_orphans= '' true '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpu:2 ; mem:2048 ; disk ( role1 ) :2048 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_enable_support= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1 '' i0222 18:16:12.377109 26703 credentials.hpp:83 ] load credenti authent '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/credential' i0222 18:16:12.377300 26703 slave.cpp:324 ] slave use credenti : test-princip i0222 18:16:12.377439 26703 resources.cpp:576 ] pars resourc json fail : cpu:2 ; mem:2048 ; disk ( role1 ) :2048 tri semicolon-delimit string format instead i0222 18:16:12.377804 26703 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] i0222 18:16:12.377881 26703 slave.cpp:472 ] slave attribut : [ ] i0222 18:16:12.377889 26703 slave.cpp:477 ] slave hostnam : ip-172-30-2-148.mesosphere.io i0222 18:16:12.378779 26701 state.cpp:58 ] recov state '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/meta' i0222 18:16:12.379092 26697 status_update_manager.cpp:200 ] recov statu updat manag i0222 18:16:12.379156 26681 sched.cpp:222 ] version : 0.28.0 i0222 18:16:12.379250 26697 docker.cpp:722 ] recov docker contain i0222 18:16:12.379421 26703 slave.cpp:4565 ] finish recoveri i0222 18:16:12.379627 26700 sched.cpp:326 ] new master detect master @ 172.30.2.148:35274 i0222 18:16:12.379735 26703 slave.cpp:4737 ] queri resourc estim oversubscrib resourc i0222 18:16:12.379765 26700 sched.cpp:382 ] authent master master @ 172.30.2.148:35274 i0222 18:16:12.379781 26700 sched.cpp:389 ] use default cram-md5 authenticate i0222 18:16:12.379964 26696 status_update_manager.cpp:174 ] paus send statu updat i0222 18:16:12.379992 26702 authenticatee.cpp:121 ] creat new client sasl connect i0222 18:16:12.380030 26697 slave.cpp:796 ] new master detect master @ 172.30.2.148:35274 i0222 18:16:12.380106 26697 slave.cpp:859 ] authent master master @ 172.30.2.148:35274 i0222 18:16:12.380127 26697 slave.cpp:864 ] use default cram-md5 authenticate i0222 18:16:12.380188 26699 master.cpp:5526 ] authent scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:12.380269 26700 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 832 ) @ 172.30.2.148:35274 i0222 18:16:12.380280 26698 authenticatee.cpp:121 ] creat new client sasl connect i0222 18:16:12.380307 26697 slave.cpp:832 ] detect new master i0222 18:16:12.380450 26697 slave.cpp:4751 ] receiv oversubscrib resourc resourc estim i0222 18:16:12.380452 26699 master.cpp:5526 ] authent slave ( 396 ) @ 172.30.2.148:35274 i0222 18:16:12.380506 26698 authenticator.cpp:98 ] creat new server sasl connect i0222 18:16:12.380540 26697 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 833 ) @ 172.30.2.148:35274 i0222 18:16:12.380635 26700 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0222 18:16:12.380659 26700 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0222 18:16:12.380762 26700 authenticator.cpp:203 ] receiv sasl authent start i0222 18:16:12.380765 26701 authenticator.cpp:98 ] creat new server sasl connect i0222 18:16:12.380843 26700 authenticator.cpp:325 ] authent requir step i0222 18:16:12.380911 26698 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0222 18:16:12.380931 26702 authenticatee.cpp:258 ] receiv sasl authent step i0222 18:16:12.380936 26698 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0222 18:16:12.381036 26702 authenticator.cpp:231 ] receiv sasl authent step i0222 18:16:12.381052 26698 authenticator.cpp:203 ] receiv sasl authent start i0222 18:16:12.381062 26702 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-148 ' server fqdn : 'ip-172-30-2-148 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0222 18:16:12.381072 26702 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0222 18:16:12.381104 26702 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0222 18:16:12.381104 26698 authenticator.cpp:325 ] authent requir step i0222 18:16:12.381134 26702 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-148 ' server fqdn : 'ip-172-30-2-148 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0222 18:16:12.381142 26702 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0222 18:16:12.381147 26702 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0222 18:16:12.381162 26702 authenticator.cpp:317 ] authent success i0222 18:16:12.381184 26698 authenticatee.cpp:258 ] receiv sasl authent step i0222 18:16:12.381247 26699 authenticatee.cpp:298 ] authent success i0222 18:16:12.381283 26696 authenticator.cpp:231 ] receiv sasl authent step i0222 18:16:12.381311 26696 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-148 ' server fqdn : 'ip-172-30-2-148 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0222 18:16:12.381325 26696 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0222 18:16:12.381319 26701 master.cpp:5556 ] success authent princip 'test-princip ' scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:12.381345 26700 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 832 ) @ 172.30.2.148:35274 i0222 18:16:12.381361 26696 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0222 18:16:12.381397 26696 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-148 ' server fqdn : 'ip-172-30-2-148 ' sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0222 18:16:12.381413 26696 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0222 18:16:12.381422 26696 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0222 18:16:12.381441 26696 authenticator.cpp:317 ] authent success i0222 18:16:12.381548 26698 sched.cpp:471 ] success authent master master @ 172.30.2.148:35274 i0222 18:16:12.381563 26698 sched.cpp:776 ] send subscrib call master @ 172.30.2.148:35274 i0222 18:16:12.381634 26700 authenticatee.cpp:298 ] authent success i0222 18:16:12.381660 26698 sched.cpp:809 ] will retri registr 770.60771m necessari i0222 18:16:12.381675 26697 master.cpp:5556 ] success authent princip 'test-princip ' slave ( 396 ) @ 172.30.2.148:35274 i0222 18:16:12.381734 26702 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 833 ) @ 172.30.2.148:35274 i0222 18:16:12.381811 26697 master.cpp:2280 ] receiv subscrib call framework 'default ' scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:12.381882 26697 master.cpp:1751 ] author framework princip 'test-princip ' receiv offer role 'role1' i0222 18:16:12.382004 26698 slave.cpp:927 ] success authent master master @ 172.30.2.148:35274 i0222 18:16:12.382123 26698 slave.cpp:1321 ] will retri registr 8.1941m necessari i0222 18:16:12.382282 26701 master.cpp:4240 ] regist slave slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) id 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 i0222 18:16:12.382482 26701 master.cpp:2351 ] subscrib framework default checkpoint disabl capabl [ ] i0222 18:16:12.382612 26703 registrar.cpp:439 ] appli 1 oper 46327n ; attempt updat 'registry' i0222 18:16:12.382829 26699 hierarchical.cpp:265 ] ad framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.382910 26699 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:12.382915 26701 sched.cpp:703 ] framework regist 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.382936 26699 hierarchical.cpp:1529 ] no invers offer send ! i0222 18:16:12.382953 26699 hierarchical.cpp:1127 ] perform alloc 0 slave 89949n i0222 18:16:12.382982 26701 sched.cpp:717 ] schedul : :regist took 46498n i0222 18:16:12.383536 26698 log.cpp:683 ] attempt append 423 byte log i0222 18:16:12.383628 26699 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0222 18:16:12.384196 26700 replica.cpp:537 ] replica receiv write request posit 3 ( 13775 ) @ 172.30.2.148:35274 i0222 18:16:12.386602 26700 leveldb.cpp:341 ] persist action ( 442 byte ) leveldb took 2.377119m i0222 18:16:12.386625 26700 replica.cpp:712 ] persist action 3 i0222 18:16:12.387104 26698 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0222 18:16:12.389159 26698 leveldb.cpp:341 ] persist action ( 444 byte ) leveldb took 2.032301m i0222 18:16:12.389181 26698 replica.cpp:712 ] persist action 3 i0222 18:16:12.389196 26698 replica.cpp:697 ] replica learn append action posit 3 i0222 18:16:12.390281 26698 registrar.cpp:484 ] success updat 'registri ' 7.619072m i0222 18:16:12.390444 26702 log.cpp:702 ] attempt truncat log 3 i0222 18:16:12.390569 26701 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0222 18:16:12.390904 26701 slave.cpp:3482 ] receiv ping slave-observ ( 364 ) @ 172.30.2.148:35274 i0222 18:16:12.391054 26700 master.cpp:4308 ] regist slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] i0222 18:16:12.391144 26703 slave.cpp:971 ] regist master master @ 172.30.2.148:35274 ; given slave id 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 i0222 18:16:12.391168 26703 fetcher.cpp:81 ] clear fetcher cach i0222 18:16:12.391238 26700 replica.cpp:537 ] replica receiv write request posit 4 ( 13776 ) @ 172.30.2.148:35274 i0222 18:16:12.391263 26701 status_update_manager.cpp:181 ] resum send statu updat i0222 18:16:12.391304 26697 hierarchical.cpp:473 ] ad slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 ( ip-172-30-2-148.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0222 18:16:12.391388 26703 slave.cpp:994 ] checkpoint slaveinfo '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/slave.info' i0222 18:16:12.391636 26703 slave.cpp:1030 ] forward total oversubscrib resourc i0222 18:16:12.391772 26699 master.cpp:4649 ] receiv updat slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) total oversubscrib resourc i0222 18:16:12.392011 26697 hierarchical.cpp:1529 ] no invers offer send ! i0222 18:16:12.392053 26697 hierarchical.cpp:1147 ] perform alloc slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 708377n i0222 18:16:12.392307 26703 master.cpp:5355 ] send 1 offer framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:12.392374 26697 hierarchical.cpp:531 ] slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 ( ip-172-30-2-148.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] , alloc : disk ( role1 ) :2048 ; cpu ( * ) :2 ; mem ( * ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] ) i0222 18:16:12.392500 26697 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:12.392531 26697 hierarchical.cpp:1529 ] no invers offer send ! i0222 18:16:12.392556 26697 hierarchical.cpp:1147 ] perform alloc slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 136779n i0222 18:16:12.392704 26701 sched.cpp:873 ] schedul : :resourceoff took 94330n i0222 18:16:12.393086 26681 resources.cpp:576 ] pars resourc json fail : cpus:1 ; mem:64 ; tri semicolon-delimit string format instead i0222 18:16:12.393600 26700 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 2.326382m i0222 18:16:12.393625 26700 replica.cpp:712 ] persist action 4 i0222 18:16:12.394162 26696 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0222 18:16:12.394533 26701 master.cpp:3138 ] process accept call offer : [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-o0 ] slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:12.394567 26701 master.cpp:2926 ] author princip 'test-princip ' creat volum i0222 18:16:12.394628 26701 master.cpp:2825 ] author framework princip 'test-princip ' launch task 1 user 'root' i0222 18:16:12.395519 26701 master.cpp:3467 ] appli creat oper volum disk ( role1 ) [ id1 : path1 ] :64 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:12.395808 26701 master.cpp:6589 ] send checkpoint resourc disk ( role1 ) [ id1 : path1 ] :64 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:12.396316 26696 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.130659m i0222 18:16:12.396317 26703 slave.cpp:2341 ] updat checkpoint resourc disk ( role1 ) [ id1 : path1 ] :64 i0222 18:16:12.396368 26696 leveldb.cpp:399 ] delet ~2 key leveldb took 30004n i0222 18:16:12.396381 26696 replica.cpp:712 ] persist action 4 i0222 18:16:12.396397 26696 replica.cpp:697 ] replica learn truncat action posit 4 i0222 18:16:12.396533 26701 master.hpp:176 ] ad task 1 resourc cpu ( * ) :1 ; mem ( * ) :64 ; disk ( role1 ) [ id1 : path1 ] :64 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:12.396680 26701 master.cpp:3623 ] launch task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 resourc cpu ( * ) :1 ; mem ( * ) :64 ; disk ( role1 ) [ id1 : path1 ] :64 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:12.397009 26696 slave.cpp:1361 ] got assign task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.397143 26696 resources.cpp:576 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0222 18:16:12.397306 26699 hierarchical.cpp:653 ] updat alloc framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 disk ( role1 ) :2048 ; cpu ( * ) :2 ; mem ( * ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] disk ( role1 ) :1984 ; cpu ( * ) :2 ; mem ( * ) :2048 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :64 i0222 18:16:12.397625 26699 hierarchical.cpp:892 ] recov disk ( role1 ) :1984 ; cpu ( * ) :2 ; mem ( * ) :1984 ; cpu ( * ) :7 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :1984 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :64 , alloc : disk ( role1 ) [ id1 : path1 ] :64 ; cpu ( * ) :1 ; mem ( * ) :64 ) slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.397857 26696 slave.cpp:1480 ] launch task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.397943 26696 resources.cpp:576 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0222 18:16:12.398560 26696 paths.cpp:474 ] tri chown '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc ' user 'root' i0222 18:16:12.403491 26696 slave.cpp:5367 ] launch executor 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' i0222 18:16:12.404115 26696 slave.cpp:1698 ] queu task ' 1 ' executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:12.405709 26696 slave.cpp:749 ] success attach file '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' i0222 18:16:12.408308 26697 docker.cpp:1019 ] start contain '207172a3-0ebd-4faa-946b-75a829fc75fc ' task ' 1 ' ( executor ' 1 ' ) framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' i0222 18:16:12.408592 26697 docker.cpp:1053 ] run docker -h unix : ///var/run/docker.sock inspect alpin : latest i0222 18:16:12.520663 26702 docker.cpp:390 ] docker pull alpin complet i0222 18:16:12.520853 26702 docker.cpp:479 ] chang ownership persist volum '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/volumes/roles/role1/id1 ' uid 0 gid 0 i0222 18:16:12.524782 26702 docker.cpp:500 ] mount '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/volumes/roles/role1/id1 ' '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1 ' persist volum disk ( role1 ) [ id1 : path1 ] :64 contain 207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:12.580834 26700 slave.cpp:2643 ] got registr executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:12.581961 26699 docker.cpp:1299 ] ignor updat contain '207172a3-0ebd-4faa-946b-75a829fc75fc ' resourc pass updat ident exist resourc i0222 18:16:12.582307 26698 slave.cpp:1863 ] send queu task ' 1 ' executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.295573 26703 slave.cpp:3002 ] handl statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.295940 26703 slave.cpp:3002 ] handl statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.296381 26701 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.296422 26701 status_update_manager.cpp:497 ] creat statusupd stream task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.296422 26703 slave.cpp:5677 ] termin task 1 i0222 18:16:13.296839 26701 status_update_manager.cpp:374 ] forward updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave i0222 18:16:13.296902 26702 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:13.299427 26699 slave.cpp:3400 ] forward updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 master @ 172.30.2.148:35274 i0222 18:16:13.299921 26699 slave.cpp:3294 ] statu updat manag success handl statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.299969 26699 slave.cpp:3310 ] send acknowledg statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.300130 26696 master.cpp:4794 ] statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:13.300176 26696 master.cpp:4842 ] forward statu updat task_run ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.300375 26696 master.cpp:6450 ] updat state task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( latest state : task_finish , statu updat state : task_run ) i0222 18:16:13.300765 26703 sched.cpp:981 ] schedul : :statusupd took 164263n i0222 18:16:13.300962 26700 hierarchical.cpp:892 ] recov cpu ( * ) :1 ; mem ( * ) :64 ; disk ( role1 ) [ id1 : path1 ] :64 ( total : cpu ( * ) :2 ; mem ( * ) :2048 ; disk ( role1 ) :1984 ; cpu ( * ) :8 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :64 , alloc : ) slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.301178 26699 master.cpp:3952 ] process acknowledg call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 i0222 18:16:13.301450 26699 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.301697 26701 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.327133 26697 status_update_manager.cpp:320 ] receiv statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.327280 26697 status_update_manager.cpp:374 ] forward updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave i0222 18:16:13.327481 26696 slave.cpp:3400 ] forward updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 master @ 172.30.2.148:35274 i0222 18:16:13.327621 26696 slave.cpp:3294 ] statu updat manag success handl statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.327679 26696 slave.cpp:3310 ] send acknowledg statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.327800 26698 master.cpp:4794 ] statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:13.327850 26698 master.cpp:4842 ] forward statu updat task_finish ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.327977 26698 master.cpp:6450 ] updat state task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( latest state : task_finish , statu updat state : task_finish ) i0222 18:16:13.328248 26699 sched.cpp:981 ] schedul : :statusupd took 100279n i0222 18:16:13.328588 26700 master.cpp:3952 ] process acknowledg call ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 i0222 18:16:13.328662 26681 sched.cpp:1903 ] ask stop driver i0222 18:16:13.328630 26700 master.cpp:6516 ] remov task 1 resourc cpu ( * ) :1 ; mem ( * ) :64 ; disk ( role1 ) [ id1 : path1 ] :64 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 slave ( 396 ) @ 172.30.2.148:35274 ( ip-172-30-2-148.mesosphere.io ) i0222 18:16:13.328747 26697 sched.cpp:1143 ] stop framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' i0222 18:16:13.329064 26696 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329069 26700 master.cpp:5926 ] process teardown call framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:13.329100 26700 master.cpp:5938 ] remov framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ( default ) scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270 @ 172.30.2.148:35274 i0222 18:16:13.329200 26696 status_update_manager.cpp:528 ] clean statu updat stream task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329218 26703 hierarchical.cpp:375 ] deactiv framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329309 26697 slave.cpp:2079 ] ask shut framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 master @ 172.30.2.148:35274 i0222 18:16:13.329346 26697 slave.cpp:2104 ] shut framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329418 26697 slave.cpp:4198 ] shut executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:13.329578 26699 hierarchical.cpp:326 ] remov framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329684 26697 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b ) task 1 framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:13.329733 26697 slave.cpp:5718 ] complet task 1 i0222 18:16:13.337236 26703 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:13.337266 26703 hierarchical.cpp:1127 ] perform alloc 1 slave 153077n i0222 18:16:14.297827 26702 slave.cpp:3528 ] executor ( 1 ) @ 172.30.2.148:56026 exit i0222 18:16:14.332489 26697 docker.cpp:1915 ] executor contain '207172a3-0ebd-4faa-946b-75a829fc75fc ' exit i0222 18:16:14.332512 26697 docker.cpp:1679 ] destroy contain '207172a3-0ebd-4faa-946b-75a829fc75fc' i0222 18:16:14.332600 26697 docker.cpp:1807 ] run docker stop contain '207172a3-0ebd-4faa-946b-75a829fc75fc' i0222 18:16:14.333111 26697 docker.cpp:908 ] unmount volum contain '207172a3-0ebd-4faa-946b-75a829fc75fc' i0222 18:16:14.333288 26700 slave.cpp:3886 ] executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 exit statu 0 i0222 18:16:14.333340 26700 slave.cpp:3990 ] clean executor ' 1 ' framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 executor ( 1 ) @ 172.30.2.148:56026 i0222 18:16:14.333603 26703 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc ' gc 6.99999614056593day futur i0222 18:16:14.333669 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:14.333704 26700 slave.cpp:4078 ] clean framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:14.333726 26703 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1 ' gc 6.99999613825185day futur i0222 18:16:14.336545 26703 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-s0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 ' gc 6.9999961115763day futur i0222 18:16:14.336699 26701 status_update_manager.cpp:282 ] close statu updat stream framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 i0222 18:16:14.338240 26699 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:14.338270 26699 hierarchical.cpp:1127 ] perform alloc 1 slave 191822n i0222 18:16:14.635416 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:14.940042 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:15.245256 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:15.339015 26697 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:15.339053 26697 hierarchical.cpp:1127 ] perform alloc 1 slave 265093n i0222 18:16:15.549804 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:15.854646 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:16.159210 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:16.339910 26698 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:16.339951 26698 hierarchical.cpp:1127 ] perform alloc 1 slave 255857n i0222 18:16:16.463809 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:16.768708 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:17.073479 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:17.340798 26696 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:17.340864 26696 hierarchical.cpp:1127 ] perform alloc 1 slave 260467n i0222 18:16:17.377902 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:17.683398 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:17.988231 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:18.292505 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:18.330112 26700 slave.cpp:4231 ] framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 seem exit . ignor shutdown timeout executor ' 1' i0222 18:16:18.341600 26702 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:18.341634 26702 hierarchical.cpp:1127 ] perform alloc 1 slave 252012n i0222 18:16:18.596279 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:18.901157 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:19.204834 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:19.342326 26699 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:19.342358 26699 hierarchical.cpp:1127 ] perform alloc 1 slave 186829n i0222 18:16:19.508533 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:19.812255 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:20.116345 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:20.343556 26698 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:20.343588 26698 hierarchical.cpp:1127 ] perform alloc 1 slave 194704n i0222 18:16:20.420814 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:20.724819 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:21.029549 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:21.334319 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:21.344859 26702 hierarchical.cpp:1434 ] no resourc avail alloc ! i0222 18:16:21.344892 26702 hierarchical.cpp:1127 ] perform alloc 1 slave 241099n i0222 18:16:21.638164 26681 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc .. / .. /src/tests/containerizer/docker_containerizer_tests.cpp:1434 : failur os : :read ( path : :join ( volumepath , `` file '' ) ) : fail open file '/tmp/dockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1/volumes/roles/role1/id1/fil ' : no file directori i0222 18:16:21.943008 26703 master.cpp:1027 ] master termin i0222 18:16:21.943635 26696 hierarchical.cpp:505 ] remov slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-s0 i0222 18:16:21.943989 26702 slave.cpp:3528 ] master @ 172.30.2.148:35274 exit w0222 18:16:21.944016 26702 slave.cpp:3531 ] master disconnect ! wait new master elect i0222 18:16:21.948807 26699 slave.cpp:668 ] slave termin i0222 18:16:21.951902 26681 docker.cpp:885 ] run docker -h unix : ///var/run/docker.sock ps -a i0222 18:16:22.044273 26698 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-s0.207172a3-0ebd-4faa-946b-75a829fc75fc i0222 18:16:22.148877 26681 docker.cpp:727 ] run docker -h unix : ///var/run/docker.sock rm -f -v 422bfef31d51d2d3d2aafcf49b3e502654354bd98a98b076f4089b9a8e274d05 [ fail ] dockercontainerizertest.root_docker_launchwithpersistentvolum ( 10535 ms ) { code }",MESOS-4736,2.0
"make stout configur modular consum downstream ( e.g. , libprocess agent ) stout configur replic least 3 configur file -- stout , libprocess , agent . more follow futur . we make stoutconfigure.cmak includ packag downstream .",MESOS-4703,1.0
"slavetest.stateendpoint flaki { code } [ run ] slavetest.stateendpoint .. / .. /src/tests/slave_tests.cpp:1220 : failur valu : state.valu [ `` start_tim '' ] .a < json : :number > ( ) .a < int > ( ) actual : 1458159086 expect : static_cast < int > ( clock : :now ( ) .sec ( ) ) which : 1458159085 [ fail ] slavetest.stateendpoint ( 193 ms ) { code } even though test { { clock : :paus ( ) } } start agent , 's possibl numified-stringifi doubl equal , even round nearest int .",MESOS-4695,1.0
creat base docker imag test suit . thi wide use unifi container test . should basic includ : * least one layer . * repositori . for layer : * root file system layer tar ball . * docker imag json ( manifest ) . * docker version .,MESOS-4684,3.0
"document docker runtim isol . should includ follow inform : * what featur current support docker runtim isol . * how use docker runtim isol ( user manual ) . * compar differ semant v. . docker container , explain .",MESOS-4683,2.0
"root_docker_log flaki . { noformat } [ 18:06:25 ] [ step 8/8 ] [ run ] dockercontainerizertest.root_docker_log [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.256103 1740 leveldb.cpp:174 ] open db 6.548327m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258002 1740 leveldb.cpp:181 ] compact db 1.837816m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258059 1740 leveldb.cpp:196 ] creat db iter 22044n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258076 1740 leveldb.cpp:202 ] seek begin db 2347n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258091 1740 leveldb.cpp:271 ] iter 0 key db 571n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258152 1740 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.258936 1758 recover.cpp:447 ] start replica recoveri [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.259177 1758 recover.cpp:473 ] replica empti statu [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.260327 1757 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 13608 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.260545 1758 recover.cpp:193 ] receiv recov respons replica empti statu [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261065 1757 master.cpp:376 ] master 112363e2-c680-4946-8fee-d0626ed8b21 ( ip-172-30-2-239.mesosphere.io ) start 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261209 1761 recover.cpp:564 ] updat replica statu start [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261086 1757 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/hncllj/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /usr/local/share/mesos/webui '' -- work_dir= '' /tmp/hncllj/master '' -- zk_session_timeout= '' 10sec '' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261446 1757 master.cpp:423 ] master allow authent framework regist [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261456 1757 master.cpp:428 ] master allow authent slave regist [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261462 1757 credentials.hpp:35 ] load credenti authent '/tmp/hncllj/credentials' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261723 1757 master.cpp:468 ] use default 'crammd5 ' authent [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.261855 1757 master.cpp:537 ] use default 'basic ' http authent [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.262022 1757 master.cpp:571 ] author enabl [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.262177 1755 hierarchical.cpp:144 ] initi hierarch alloc process [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.262177 1758 whitelist_watcher.cpp:77 ] no whitelist given [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.262899 1760 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.517992m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.262924 1760 replica.cpp:320 ] persist replica statu start [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.263144 1754 recover.cpp:473 ] replica start statu [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264010 1757 master.cpp:1712 ] the newli elect leader master @ 172.30.2.239:39785 id 112363e2-c680-4946-8fee-d0626ed8b21 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264044 1757 master.cpp:1725 ] elect lead master ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264061 1757 master.cpp:1470 ] recov registrar [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264117 1760 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 13610 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264197 1758 registrar.cpp:307 ] recov registrar [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.264827 1756 recover.cpp:193 ] receiv recov respons replica start statu [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.265219 1757 recover.cpp:564 ] updat replica statu vote [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.267302 1754 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.887739m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.267326 1754 replica.cpp:320 ] persist replica statu vote [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.267453 1759 recover.cpp:578 ] success join paxo group [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.267632 1759 recover.cpp:462 ] recov process termin [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.268007 1757 log.cpp:659 ] attempt start writer [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.269055 1759 replica.cpp:493 ] replica receiv implicit promis request ( 13611 ) @ 172.30.2.239:39785 propos 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.270488 1759 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 1.406068m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.270511 1759 replica.cpp:342 ] persist promis 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.271078 1761 coordinator.cpp:238 ] coordin attempt fill miss posit [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.272146 1756 replica.cpp:388 ] replica receiv explicit promis request ( 13612 ) @ 172.30.2.239:39785 posit 0 propos 2 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.273478 1756 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 1.297217m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.273500 1756 replica.cpp:712 ] persist action 0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.274355 1757 replica.cpp:537 ] replica receiv write request posit 0 ( 13613 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.274405 1757 leveldb.cpp:436 ] read posit leveldb took 25294n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.275800 1757 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 1.362978m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.275823 1757 replica.cpp:712 ] persist action 0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.276348 1755 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.277765 1755 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.391531m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.277788 1755 replica.cpp:712 ] persist action 0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.277802 1755 replica.cpp:697 ] replica learn nop action posit 0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.278336 1754 log.cpp:675 ] writer start end posit 0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.279371 1755 leveldb.cpp:436 ] read posit leveldb took 29214n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.280272 1758 registrar.cpp:340 ] success fetch registri ( 0b ) 16.02688m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.280385 1758 registrar.cpp:439 ] appli 1 oper 31040n ; attempt updat 'registry' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.281054 1755 log.cpp:683 ] attempt append 210 byte log [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.281165 1757 coordinator.cpp:348 ] coordin attempt write append action posit 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.281780 1757 replica.cpp:537 ] replica receiv write request posit 1 ( 13614 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.283159 1757 leveldb.cpp:341 ] persist action ( 229 byte ) leveldb took 1.348041m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.283184 1757 replica.cpp:712 ] persist action 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.283695 1759 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.285059 1759 leveldb.cpp:341 ] persist action ( 231 byte ) leveldb took 1.334577m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.285084 1759 replica.cpp:712 ] persist action 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.285099 1759 replica.cpp:697 ] replica learn append action posit 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.285910 1758 registrar.cpp:484 ] success updat 'registri ' 5.46816m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.286043 1758 registrar.cpp:370 ] success recov registrar [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.286121 1755 log.cpp:702 ] attempt truncat log 1 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.286301 1756 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.286478 1759 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.286476 1754 master.cpp:1522 ] recov 0 slave registri ( 171b ) ; allow 10min slave re-regist [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.287137 1755 replica.cpp:537 ] replica receiv write request posit 2 ( 13615 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.289104 1755 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.938609m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.289127 1755 replica.cpp:712 ] persist action 2 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.289667 1759 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.290956 1759 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.256421m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.291007 1759 leveldb.cpp:399 ] delet ~1 key leveldb took 28064n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.291021 1759 replica.cpp:712 ] persist action 2 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.291038 1759 replica.cpp:697 ] replica learn truncat action posit 2 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.300550 1760 slave.cpp:193 ] slave start 393 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.300573 1760 slave.cpp:194 ] flag startup : -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/dockercontainerizertest_root_docker_logs_a4ns2n/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' http : //auth.docker.io '' -- docker_kill_orphans= '' true '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/dockercontainerizertest_root_docker_logs_a4ns2n/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mnt/teamcity/work/4240ba9ddd0997c3/build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/dockercontainerizertest_root_docker_logs_a4ns2n '' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.300868 1760 credentials.hpp:83 ] load credenti authent '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/credential' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.301030 1760 slave.cpp:324 ] slave use credenti : test-princip [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.301180 1760 resources.cpp:576 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] [ 18:06:25 ] [ step 8/8 ] tri semicolon-delimit string format instead [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.301553 1760 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.301609 1760 slave.cpp:472 ] slave attribut : [ ] [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.301620 1760 slave.cpp:477 ] slave hostnam : ip-172-30-2-239.mesosphere.io [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.302417 1757 state.cpp:58 ] recov state '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/meta' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.302515 1740 sched.cpp:222 ] version : 0.28.0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.302772 1755 status_update_manager.cpp:200 ] recov statu updat manag [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.302956 1758 docker.cpp:559 ] recov docker contain [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303050 1761 sched.cpp:326 ] new master detect master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303133 1754 slave.cpp:4565 ] finish recoveri [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303154 1761 sched.cpp:382 ] authent master master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303169 1761 sched.cpp:389 ] use default cram-md5 authenticate [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303364 1759 authenticatee.cpp:121 ] creat new client sasl connect [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303467 1754 slave.cpp:4737 ] queri resourc estim oversubscrib resourc [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303668 1756 master.cpp:5523 ] authent scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303707 1760 status_update_manager.cpp:174 ] paus send statu updat [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303707 1754 slave.cpp:796 ] new master detect master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303767 1755 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 829 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303791 1754 slave.cpp:859 ] authent master master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303805 1754 slave.cpp:864 ] use default cram-md5 authenticate [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303956 1754 slave.cpp:832 ] detect new master [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303971 1761 authenticatee.cpp:121 ] creat new client sasl connect [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.303984 1760 authenticator.cpp:98 ] creat new server sasl connect [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304131 1754 slave.cpp:4751 ] receiv oversubscrib resourc resourc estim [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304275 1757 master.cpp:5523 ] authent slave ( 393 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304344 1754 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304369 1754 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304373 1761 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 830 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304440 1757 authenticator.cpp:203 ] receiv sasl authent start [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304491 1757 authenticator.cpp:325 ] authent requir step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304548 1754 authenticator.cpp:98 ] creat new server sasl connect [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304582 1761 authenticatee.cpp:258 ] receiv sasl authent step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304688 1761 authenticator.cpp:231 ] receiv sasl authent step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304714 1761 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-239.mesosphere.io ' server fqdn : 'ip-172-30-2-239.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304723 1761 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304767 1761 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304805 1761 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-239.mesosphere.io ' server fqdn : 'ip-172-30-2-239.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304817 1761 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304824 1761 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304836 1761 authenticator.cpp:317 ] authent success [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304841 1758 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304870 1758 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304909 1757 authenticatee.cpp:298 ] authent success [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.304983 1756 authenticator.cpp:203 ] receiv sasl authent start [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305033 1756 authenticator.cpp:325 ] authent requir step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305042 1759 master.cpp:5553 ] success authent princip 'test-princip ' scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305071 1755 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 829 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305124 1756 authenticatee.cpp:258 ] receiv sasl authent step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305222 1758 sched.cpp:471 ] success authent master master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305246 1758 sched.cpp:776 ] send subscrib call master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305286 1760 authenticator.cpp:231 ] receiv sasl authent step [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305310 1760 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-239.mesosphere.io ' server fqdn : 'ip-172-30-2-239.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305318 1760 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305344 1760 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305363 1758 sched.cpp:809 ] will retri registr 1.888777185sec necessari [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305379 1760 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'ip-172-30-2-239.mesosphere.io ' server fqdn : 'ip-172-30-2-239.mesosphere.io ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305397 1760 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305408 1760 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305426 1760 authenticator.cpp:317 ] authent success [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305466 1761 master.cpp:2280 ] receiv subscrib call framework 'default ' scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305506 1756 authenticatee.cpp:298 ] authent success [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305534 1761 master.cpp:1751 ] author framework princip 'test-princip ' receiv offer role ' * ' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305625 1755 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 830 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305701 1761 master.cpp:5553 ] success authent princip 'test-princip ' slave ( 393 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305831 1758 slave.cpp:927 ] success authent master master @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305902 1757 master.cpp:2351 ] subscrib framework default checkpoint disabl capabl [ ] [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.305953 1758 slave.cpp:1321 ] will retri registr 1.941456m necessari [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306280 1761 hierarchical.cpp:265 ] ad framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306352 1759 sched.cpp:703 ] framework regist 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306363 1761 hierarchical.cpp:1403 ] no resourc avail alloc ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306401 1761 hierarchical.cpp:1498 ] no invers offer send ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306432 1761 hierarchical.cpp:1096 ] perform alloc 0 slave 126082n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306447 1759 sched.cpp:717 ] schedul : :regist took 67960n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306437 1757 master.cpp:4237 ] regist slave slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) id 112363e2-c680-4946-8fee-d0626ed8b21e-s0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.306884 1759 registrar.cpp:439 ] appli 1 oper 63175n ; attempt updat 'registry' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.307592 1756 log.cpp:683 ] attempt append 396 byte log [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.307724 1760 coordinator.cpp:348 ] coordin attempt write append action posit 3 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.308398 1760 replica.cpp:537 ] replica receiv write request posit 3 ( 13622 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.308473 1755 slave.cpp:1321 ] will retri registr 37.671741m necessari [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.308627 1758 master.cpp:4225 ] ignor regist slave messag slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) admiss alreadi progress [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.310000 1760 leveldb.cpp:341 ] persist action ( 415 byte ) leveldb took 1.556814m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.310025 1760 replica.cpp:712 ] persist action 3 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.310541 1755 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.311928 1755 leveldb.cpp:341 ] persist action ( 417 byte ) leveldb took 1.357404m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.311950 1755 replica.cpp:712 ] persist action 3 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.311966 1755 replica.cpp:697 ] replica learn append action posit 3 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.313117 1755 registrar.cpp:484 ] success updat 'registri ' 6.16704m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.313297 1758 log.cpp:702 ] attempt truncat log 3 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.313391 1755 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.313807 1761 slave.cpp:3482 ] receiv ping slave-observ ( 360 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.313946 1754 master.cpp:4305 ] regist slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314067 1756 slave.cpp:971 ] regist master master @ 172.30.2.239:39785 ; given slave id 112363e2-c680-4946-8fee-d0626ed8b21e-s0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314095 1756 fetcher.cpp:81 ] clear fetcher cach [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314102 1760 replica.cpp:537 ] replica receiv write request posit 4 ( 13623 ) @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314164 1758 hierarchical.cpp:473 ] ad slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 ( ip-172-30-2-239.mesosphere.io ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314219 1761 status_update_manager.cpp:181 ] resum send statu updat [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314370 1756 slave.cpp:994 ] checkpoint slaveinfo '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/meta/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/slave.info' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314579 1756 slave.cpp:1030 ] forward total oversubscrib resourc [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314707 1756 master.cpp:4646 ] receiv updat slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) total oversubscrib resourc [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314818 1758 hierarchical.cpp:1498 ] no invers offer send ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.314848 1758 hierarchical.cpp:1116 ] perform alloc slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 654176n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315137 1758 hierarchical.cpp:531 ] slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 ( ip-172-30-2-239.mesosphere.io ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315217 1756 master.cpp:5352 ] send 1 offer framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315238 1758 hierarchical.cpp:1403 ] no resourc avail alloc ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315268 1758 hierarchical.cpp:1498 ] no invers offer send ! [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315285 1758 hierarchical.cpp:1116 ] perform alloc slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 118646n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.315635 1755 sched.cpp:873 ] schedul : :resourceoff took 99802n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.317126 1755 master.cpp:3138 ] process accept call offer : [ 112363e2-c680-4946-8fee-d0626ed8b21e-o0 ] slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.317163 1755 master.cpp:2825 ] author framework princip 'test-princip ' launch task 1 user 'root' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.317229 1760 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 3.089068m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.317261 1760 replica.cpp:712 ] persist action 4 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.317845 1759 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.318722 1755 master.hpp:176 ] ad task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 ( ip-172-30-2-239.mesosphere.io ) [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.318886 1755 master.cpp:3623 ] launch task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319195 1757 slave.cpp:1361 ] got assign task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319305 1759 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.430044m [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319349 1757 resources.cpp:576 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 18:06:25 ] [ step 8/8 ] tri semicolon-delimit string format instead [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319363 1759 leveldb.cpp:399 ] delet ~2 key leveldb took 34738n [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319380 1759 replica.cpp:712 ] persist action 4 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.319396 1759 replica.cpp:697 ] replica learn truncat action posit 4 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.320034 1757 slave.cpp:1480 ] launch task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.320127 1757 resources.cpp:576 ] pars resourc json fail : cpus:0.1 ; mem:32 [ 18:06:25 ] [ step 8/8 ] tri semicolon-delimit string format instead [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.320725 1757 paths.cpp:474 ] tri chown '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08 ' user 'root' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.325739 1757 slave.cpp:5351 ] launch executor 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.326493 1757 slave.cpp:1698 ] queu task ' 1 ' executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.326633 1757 slave.cpp:749 ] success attach file '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.331328 1761 docker.cpp:803 ] start contain '4f58cd28-7ac7-4960-b3e3-8d28918e6d08 ' task ' 1 ' ( executor ' 1 ' ) framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000' [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.331699 1761 docker.cpp:1053 ] run docker -h unix : ///var/run/docker.sock inspect alpin : latest [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.449668 1758 docker.cpp:384 ] docker pull alpin complet [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.511905 1760 slave.cpp:2643 ] got registr executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.513098 1759 docker.cpp:1077 ] ignor updat contain '4f58cd28-7ac7-4960-b3e3-8d28918e6d08 ' resourc pass updat ident exist resourc [ 18:06:25 ] [ step 8/8 ] i0215 17:06:25.513494 1756 slave.cpp:1863 ] send queu task ' 1 ' executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] 2016-02-15 17:06:25,981:1740 ( 0x7f870b7fe700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:36716 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.227973 1757 slave.cpp:3002 ] handl statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.228302 1757 slave.cpp:3002 ] handl statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.228734 1754 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.228790 1754 status_update_manager.cpp:497 ] creat statusupd stream task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.228837 1757 slave.cpp:5661 ] termin task 1 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.229243 1754 status_update_manager.cpp:374 ] forward updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 slave [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.229346 1755 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-s0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232147 1758 slave.cpp:3400 ] forward updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 master @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232383 1758 slave.cpp:3294 ] statu updat manag success handl statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232419 1758 slave.cpp:3310 ] send acknowledg statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232631 1759 master.cpp:4791 ] statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232681 1759 master.cpp:4839 ] forward statu updat task_run ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.232911 1759 master.cpp:6447 ] updat state task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( latest state : task_fail , statu updat state : task_run ) [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.233170 1756 sched.cpp:981 ] schedul : :statusupd took 100304n [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.233613 1754 hierarchical.cpp:892 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.233642 1759 master.cpp:3949 ] process acknowledg call b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.233944 1759 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.234294 1761 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.264482 1759 hierarchical.cpp:1498 ] no invers offer send ! [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.264554 1759 hierarchical.cpp:1096 ] perform alloc 1 slave 1.210209m [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.264837 1757 master.cpp:5352 ] send 1 offer framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.265275 1760 sched.cpp:873 ] schedul : :resourceoff took 26245n [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.357859 1756 status_update_manager.cpp:320 ] receiv statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358085 1756 status_update_manager.cpp:374 ] forward updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 slave [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358330 1758 slave.cpp:3400 ] forward updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 master @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358554 1758 slave.cpp:3294 ] statu updat manag success handl statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358594 1758 slave.cpp:3310 ] send acknowledg statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358687 1761 master.cpp:4791 ] statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358724 1761 master.cpp:4839 ] forward statu updat task_fail ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.358921 1761 master.cpp:6447 ] updat state task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( latest state : task_fail , statu updat state : task_fail ) [ 18:06:26 ] [ step 8/8 ] .. / .. /src/tests/containerizer/docker_containerizer_tests.cpp:1269 : failur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.359128 1754 sched.cpp:981 ] schedul : :statusupd took 108209n [ 18:06:26 ] [ step 8/8 ] valu : statusfinished.get ( ) .state ( ) [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.359400 1759 master.cpp:3949 ] process acknowledg call 05810f46-10e7-4d50-a83d-d05bf79dd8e2 task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 [ 18:06:26 ] [ step 8/8 ] actual : task_fail [ 18:06:26 ] [ step 8/8 ] expect : task_finish [ 18:06:26 ] [ step 8/8 ] .. / .. /src/tests/containerizer/docker_containerizer_tests.cpp:1278 : failur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.359470 1759 master.cpp:6513 ] remov task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 slave ( 393 ) @ 172.30.2.239:39785 ( ip-172-30-2-239.mesosphere.io ) [ 18:06:26 ] [ step 8/8 ] valu : containslin ( line , `` err '' + uuid ) [ 18:06:26 ] [ step 8/8 ] actual : fals [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.359928 1761 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] expect : true [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.359931 1740 sched.cpp:1903 ] ask stop driver [ 18:06:26 ] [ step 8/8 ] .. / .. /src/tests/containerizer/docker_containerizer_tests.cpp:1286 : failur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360031 1759 sched.cpp:1143 ] stop framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000' [ 18:06:26 ] [ step 8/8 ] valu : containslin ( line , `` '' + uuid ) [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360080 1761 status_update_manager.cpp:528 ] clean statu updat stream task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] actual : fals [ 18:06:26 ] [ step 8/8 ] expect : true [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360213 1760 master.cpp:5923 ] process teardown call framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360239 1760 master.cpp:5935 ] remov framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 ( default ) scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18 @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360481 1755 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : 05810f46-10e7-4d50-a83d-d05bf79dd8e2 ) task 1 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360522 1755 slave.cpp:5702 ] complet task 1 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360539 1756 hierarchical.cpp:375 ] deactiv framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360586 1755 slave.cpp:2079 ] ask shut framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 master @ 172.30.2.239:39785 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360610 1755 slave.cpp:2104 ] shut framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.360666 1755 slave.cpp:4198 ] shut executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.361110 1761 hierarchical.cpp:892 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.361395 1756 hierarchical.cpp:326 ] remov framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.361397 1760 master.cpp:1027 ] master termin [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.361800 1755 hierarchical.cpp:505 ] remov slave 112363e2-c680-4946-8fee-d0626ed8b21e-s0 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.362174 1756 slave.cpp:3528 ] master @ 172.30.2.239:39785 exit [ 18:06:26 ] [ step 8/8 ] w0215 17:06:26.362200 1756 slave.cpp:3531 ] master disconnect ! wait new master elect [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.367146 1758 docker.cpp:1455 ] destroy contain '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.367168 1758 docker.cpp:1515 ] send sigterm executor pid : 10194 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.383013 1758 docker.cpp:1557 ] run docker stop contain '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.384843 1757 slave.cpp:3528 ] executor ( 1 ) @ 172.30.2.239:38602 exit [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.458639 1761 docker.cpp:1654 ] executor contain '4f58cd28-7ac7-4960-b3e3-8d28918e6d08 ' exit [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.458806 1757 slave.cpp:3886 ] executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 termin signal termin [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.458852 1757 slave.cpp:3990 ] clean executor ' 1 ' framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 executor ( 1 ) @ 172.30.2.239:38602 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459166 1754 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08 ' gc 6.99999468745481day futur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459262 1757 slave.cpp:4078 ] clean framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459336 1754 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1 ' gc 6.99999468505778day futur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459372 1761 status_update_manager.cpp:282 ] close statu updat stream framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459481 1754 gc.cpp:54 ] schedul '/tmp/dockercontainerizertest_root_docker_logs_a4ns2n/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-s0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000 ' gc 6.99999468310222day futur [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.459915 1759 slave.cpp:668 ] slave termin [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.463074 1740 docker.cpp:885 ] run docker -h unix : ///var/run/docker.sock ps -a [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.560154 1760 docker.cpp:766 ] run docker -h unix : ///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-s0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08 [ 18:06:26 ] [ step 8/8 ] i0215 17:06:26.666368 1740 docker.cpp:727 ] run docker -h unix : ///var/run/docker.sock rm -f -v c7f89c245f256c03222551178f6a0c26413bb91dfb3c843bff837b365d7c7432 [ 18:06:26 ] [ step 8/8 ] [ fail ] dockercontainerizertest.root_docker_log ( 1519 ms ) { noformat }",MESOS-4676,2.0
"can disabl systemd support on certain platform systemd init system avail , use . not abl disabl meso systemd integr platform make hard oper use differ init / monit system .",MESOS-4675,1.0
"statu updat executor forward order agent . previous , statu updat messag executor forward agent master order receiv . howev , seem longer valid due recent introduc chang agent : { code } // befor send updat , need retriev contain statu . containerizer- > statu ( executor- > containerid ) .onani ( defer ( self ( ) , & slave : :_statusupd , updat , pid , executor- > id , lambda : :_1 ) ) ; { code } thi sometim lead statu updat sent order depend order { { futur } } fulfil call { { statu ( ... ) } } .",MESOS-4671,1.0
"` cgroup_info ` expos state.json composingcontainer use . the composingcontainer current ` statu ` method . thi result ` containerstatu ` updat agent , use ` composingcontainer ` launch contain . thi would specif happen agent launch ` -- containerizer=dock , meso `",MESOS-4670,1.0
add common compress util we need gzip uncompress util appc imag fetch function . the imag tar + gzip' need first uncompress comput sha 512 checksum .,MESOS-4669,2.0
logrot contain logger die agent unit systemd .,MESOS-4640,1.0
posix process executor die agent unit systemd .,MESOS-4639,1.0
docker process executor die agent unit systemd .,MESOS-4637,1.0
"test derefer stack alloc master object upon assertion/expect failur . test use { { startmast } } test helper gener fragil test fail assert/expect middl test . thi { { startmast } } helper take raw pointer argument , may stack-alloc . in case assert failur , test immedi exit ( destroy stack alloc object ) proce onto test cleanup . the test cleanup may derefer destroy object , lead test crash like : { code } [ 18:27:36 ] [ step 8/8 ] f0204 18:27:35.981302 23085 logging.cpp:64 ] raw : pure virtual method call [ 18:27:36 ] [ step 8/8 ] @ 0x7f7077055e1c googl : :logmessag : :fail ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f707705ba6f googl : :rawlog__ ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f70760f76c9 __cxa_pure_virtu [ 18:27:36 ] [ step 8/8 ] @ 0xa9423c meso : :intern : :test : :cluster : :slave : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074e45 meso : :intern : :test : :mesostest : :shutdownslav ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074de4 meso : :intern : :test : :mesostest : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1070ec7 meso : :intern : :test : :mesostest : :teardown ( ) { code } the { { startmast } } helper take { { shared_ptr } } argument instead . thi also mean remov { { shutdown } } helper test .",MESOS-4634,5.0
"test derefer stack alloc agent object upon assertion/expect failur . test use { { startslav } } test helper gener fragil test fail assert/expect middl test . thi { { startslav } } helper take raw pointer argument , may stack-alloc . in case assert failur , test immedi exit ( destroy stack alloc object ) proce onto test cleanup . the test cleanup may derefer destroy object , lead test crash like : { code } [ 18:27:36 ] [ step 8/8 ] f0204 18:27:35.981302 23085 logging.cpp:64 ] raw : pure virtual method call [ 18:27:36 ] [ step 8/8 ] @ 0x7f7077055e1c googl : :logmessag : :fail ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f707705ba6f googl : :rawlog__ ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f70760f76c9 __cxa_pure_virtu [ 18:27:36 ] [ step 8/8 ] @ 0xa9423c meso : :intern : :test : :cluster : :slave : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074e45 meso : :intern : :test : :mesostest : :shutdownslav ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074de4 meso : :intern : :test : :mesostest : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1070ec7 meso : :intern : :test : :mesostest : :teardown ( ) { code } the { { startslav } } helper take { { shared_ptr } } argument instead . thi also mean remov { { shutdown } } helper test .",MESOS-4633,5.0
"containerloggertest.defaulttosandbox flaki just saw failur asf ci : { code } [ run ] containerloggertest.defaulttosandbox i0206 01:25:03.766458 2824 leveldb.cpp:174 ] open db 72.979786m i0206 01:25:03.811712 2824 leveldb.cpp:181 ] compact db 45.162067m i0206 01:25:03.811810 2824 leveldb.cpp:196 ] creat db iter 26090n i0206 01:25:03.811828 2824 leveldb.cpp:202 ] seek begin db 3173n i0206 01:25:03.811839 2824 leveldb.cpp:271 ] iter 0 key db 497n i0206 01:25:03.811900 2824 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0206 01:25:03.812785 2849 recover.cpp:447 ] start replica recoveri i0206 01:25:03.813043 2849 recover.cpp:473 ] replica empti statu i0206 01:25:03.814668 2854 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 371 ) @ 172.17.0.8:37843 i0206 01:25:03.815210 2849 recover.cpp:193 ] receiv recov respons replica empti statu i0206 01:25:03.815732 2854 recover.cpp:564 ] updat replica statu start i0206 01:25:03.819664 2857 master.cpp:376 ] master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8d ( 74ef606c4063 ) start 172.17.0.8:37843 i0206 01:25:03.819703 2857 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/h5vu5i/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.28.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/h5vu5i/mast '' -- zk_session_timeout= '' 10sec '' i0206 01:25:03.820241 2857 master.cpp:423 ] master allow authent framework regist i0206 01:25:03.820257 2857 master.cpp:428 ] master allow authent slave regist i0206 01:25:03.820269 2857 credentials.hpp:35 ] load credenti authent '/tmp/h5vu5i/credentials' i0206 01:25:03.821110 2857 master.cpp:468 ] use default 'crammd5 ' authent i0206 01:25:03.821311 2857 master.cpp:537 ] use default 'basic ' http authent i0206 01:25:03.821636 2857 master.cpp:571 ] author enabl i0206 01:25:03.821979 2846 hierarchical.cpp:144 ] initi hierarch alloc process i0206 01:25:03.822057 2846 whitelist_watcher.cpp:77 ] no whitelist given i0206 01:25:03.825460 2847 master.cpp:1712 ] the newli elect leader master @ 172.17.0.8:37843 id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8d i0206 01:25:03.825512 2847 master.cpp:1725 ] elect lead master ! i0206 01:25:03.825533 2847 master.cpp:1470 ] recov registrar i0206 01:25:03.825835 2847 registrar.cpp:307 ] recov registrar i0206 01:25:03.848212 2854 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 32.226093m i0206 01:25:03.848299 2854 replica.cpp:320 ] persist replica statu start i0206 01:25:03.848702 2854 recover.cpp:473 ] replica start statu i0206 01:25:03.850728 2858 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 373 ) @ 172.17.0.8:37843 i0206 01:25:03.851230 2854 recover.cpp:193 ] receiv recov respons replica start statu i0206 01:25:03.852018 2854 recover.cpp:564 ] updat replica statu vote i0206 01:25:03.881681 2854 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 29.184163m i0206 01:25:03.881772 2854 replica.cpp:320 ] persist replica statu vote i0206 01:25:03.882058 2854 recover.cpp:578 ] success join paxo group i0206 01:25:03.882258 2854 recover.cpp:462 ] recov process termin i0206 01:25:03.883076 2854 log.cpp:659 ] attempt start writer i0206 01:25:03.885040 2854 replica.cpp:493 ] replica receiv implicit promis request ( 374 ) @ 172.17.0.8:37843 propos 1 i0206 01:25:03.915132 2854 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 29.980589m i0206 01:25:03.915215 2854 replica.cpp:342 ] persist promis 1 i0206 01:25:03.916038 2856 coordinator.cpp:238 ] coordin attempt fill miss posit i0206 01:25:03.917659 2856 replica.cpp:388 ] replica receiv explicit promis request ( 375 ) @ 172.17.0.8:37843 posit 0 propos 2 i0206 01:25:03.948698 2856 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 30.974607m i0206 01:25:03.948786 2856 replica.cpp:712 ] persist action 0 i0206 01:25:03.950920 2849 replica.cpp:537 ] replica receiv write request posit 0 ( 376 ) @ 172.17.0.8:37843 i0206 01:25:03.951011 2849 leveldb.cpp:436 ] read posit leveldb took 44263n i0206 01:25:03.982026 2849 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 30.947321m i0206 01:25:03.982225 2849 replica.cpp:712 ] persist action 0 i0206 01:25:03.983867 2849 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0206 01:25:04.015499 2849 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 30.957888m i0206 01:25:04.015591 2849 replica.cpp:712 ] persist action 0 i0206 01:25:04.015682 2849 replica.cpp:697 ] replica learn nop action posit 0 i0206 01:25:04.016666 2849 log.cpp:675 ] writer start end posit 0 i0206 01:25:04.017881 2855 leveldb.cpp:436 ] read posit leveldb took 56779n i0206 01:25:04.018934 2852 registrar.cpp:340 ] success fetch registri ( 0b ) 193.048064m i0206 01:25:04.019076 2852 registrar.cpp:439 ] appli 1 oper 38180n ; attempt updat 'registry' i0206 01:25:04.020100 2844 log.cpp:683 ] attempt append 170 byte log i0206 01:25:04.020288 2855 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0206 01:25:04.021323 2844 replica.cpp:537 ] replica receiv write request posit 1 ( 377 ) @ 172.17.0.8:37843 i0206 01:25:04.054726 2844 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 33.309419m i0206 01:25:04.054818 2844 replica.cpp:712 ] persist action 1 i0206 01:25:04.055933 2844 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0206 01:25:04.088142 2844 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 32.116643m i0206 01:25:04.088230 2844 replica.cpp:712 ] persist action 1 i0206 01:25:04.088265 2844 replica.cpp:697 ] replica learn append action posit 1 i0206 01:25:04.090070 2856 registrar.cpp:484 ] success updat 'registri ' 70.90816m i0206 01:25:04.090338 2851 log.cpp:702 ] attempt truncat log 1 i0206 01:25:04.090358 2856 registrar.cpp:370 ] success recov registrar i0206 01:25:04.090507 2847 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0206 01:25:04.090867 2858 master.cpp:1522 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i0206 01:25:04.091449 2858 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0206 01:25:04.092280 2857 replica.cpp:537 ] replica receiv write request posit 2 ( 378 ) @ 172.17.0.8:37843 i0206 01:25:04.125702 2857 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 33.192265m i0206 01:25:04.125804 2857 replica.cpp:712 ] persist action 2 i0206 01:25:04.127400 2857 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0206 01:25:04.157727 2857 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 30.268594m i0206 01:25:04.157905 2857 leveldb.cpp:399 ] delet ~1 key leveldb took 88436n i0206 01:25:04.157941 2857 replica.cpp:712 ] persist action 2 i0206 01:25:04.157984 2857 replica.cpp:697 ] replica learn truncat action posit 2 i0206 01:25:04.166174 2824 containerizer.cpp:149 ] use isol : posix/cpu , posix/mem , filesystem/posix w0206 01:25:04.166954 2824 backend.cpp:48 ] fail creat 'bind ' backend : bindbackend requir root privileg i0206 01:25:04.172008 2844 slave.cpp:193 ] slave start 9 ) @ 172.17.0.8:37843 i0206 01:25:04.172046 2844 slave.cpp:194 ] flag startup : -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/containerloggertest_defaulttosandbox_fmaksw/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' http : //auth.docker.io '' -- docker_kill_orphans= '' true '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/containerloggertest_defaulttosandbox_fmaksw/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.28.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/containerloggertest_defaulttosandbox_fmaksw '' i0206 01:25:04.172569 2844 credentials.hpp:83 ] load credenti authent '/tmp/containerloggertest_defaulttosandbox_fmaksw/credential' i0206 01:25:04.172886 2844 slave.cpp:324 ] slave use credenti : test-princip i0206 01:25:04.173141 2844 resources.cpp:564 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0206 01:25:04.173620 2844 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0206 01:25:04.173686 2844 slave.cpp:472 ] slave attribut : [ ] i0206 01:25:04.173702 2844 slave.cpp:477 ] slave hostnam : 74ef606c4063 i0206 01:25:04.174816 2847 state.cpp:58 ] recov state '/tmp/containerloggertest_defaulttosandbox_fmaksw/meta' i0206 01:25:04.175441 2847 status_update_manager.cpp:200 ] recov statu updat manag i0206 01:25:04.175678 2858 containerizer.cpp:397 ] recov container i0206 01:25:04.177573 2858 provisioner.cpp:245 ] provision recoveri complet i0206 01:25:04.178231 2847 slave.cpp:4496 ] finish recoveri i0206 01:25:04.178834 2847 slave.cpp:4668 ] queri resourc estim oversubscrib resourc i0206 01:25:04.179405 2847 slave.cpp:796 ] new master detect master @ 172.17.0.8:37843 i0206 01:25:04.179500 2847 slave.cpp:859 ] authent master master @ 172.17.0.8:37843 i0206 01:25:04.179525 2847 slave.cpp:864 ] use default cram-md5 authenticate i0206 01:25:04.179656 2858 status_update_manager.cpp:174 ] paus send statu updat i0206 01:25:04.179798 2847 slave.cpp:832 ] detect new master i0206 01:25:04.179891 2852 authenticatee.cpp:121 ] creat new client sasl connect i0206 01:25:04.179916 2847 slave.cpp:4682 ] receiv oversubscrib resourc resourc estim i0206 01:25:04.180286 2847 master.cpp:5523 ] authent slave ( 9 ) @ 172.17.0.8:37843 i0206 01:25:04.180569 2847 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 32 ) @ 172.17.0.8:37843 i0206 01:25:04.181000 2847 authenticator.cpp:98 ] creat new server sasl connect i0206 01:25:04.181315 2847 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0206 01:25:04.181387 2847 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0206 01:25:04.181562 2847 authenticator.cpp:203 ] receiv sasl authent start i0206 01:25:04.181648 2847 authenticator.cpp:325 ] authent requir step i0206 01:25:04.181843 2847 authenticatee.cpp:258 ] receiv sasl authent step i0206 01:25:04.182034 2853 authenticator.cpp:231 ] receiv sasl authent step i0206 01:25:04.182071 2853 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '74ef606c4063 ' server fqdn : '74ef606c4063 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0206 01:25:04.182093 2853 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0206 01:25:04.182145 2853 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0206 01:25:04.182173 2853 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '74ef606c4063 ' server fqdn : '74ef606c4063 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0206 01:25:04.182185 2853 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0206 01:25:04.182193 2853 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0206 01:25:04.182211 2853 authenticator.cpp:317 ] authent success i0206 01:25:04.182333 2849 authenticatee.cpp:298 ] authent success i0206 01:25:04.182422 2853 master.cpp:5553 ] success authent princip 'test-princip ' slave ( 9 ) @ 172.17.0.8:37843 i0206 01:25:04.182510 2853 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 32 ) @ 172.17.0.8:37843 i0206 01:25:04.182945 2849 slave.cpp:927 ] success authent master master @ 172.17.0.8:37843 i0206 01:25:04.183178 2849 slave.cpp:1321 ] will retri registr 9.87937m necessari i0206 01:25:04.183466 2852 master.cpp:4237 ] regist slave slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 i0206 01:25:04.184039 2845 registrar.cpp:439 ] appli 1 oper 89453n ; attempt updat 'registry' i0206 01:25:04.185288 2856 log.cpp:683 ] attempt append 339 byte log i0206 01:25:04.185672 2850 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0206 01:25:04.186674 2846 replica.cpp:537 ] replica receiv write request posit 3 ( 392 ) @ 172.17.0.8:37843 i0206 01:25:04.195863 2856 slave.cpp:1321 ] will retri registr 11.038094m necessari i0206 01:25:04.196233 2856 master.cpp:4225 ] ignor regist slave messag slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) admiss alreadi progress i0206 01:25:04.208094 2856 slave.cpp:1321 ] will retri registr 27.881223m necessari i0206 01:25:04.208472 2856 master.cpp:4225 ] ignor regist slave messag slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) admiss alreadi progress i0206 01:25:04.216698 2846 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 29.961291m i0206 01:25:04.216789 2846 replica.cpp:712 ] persist action 3 i0206 01:25:04.218246 2845 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0206 01:25:04.237861 2846 slave.cpp:1321 ] will retri registr 1.006941m necessari i0206 01:25:04.238221 2846 master.cpp:4225 ] ignor regist slave messag slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) admiss alreadi progress i0206 01:25:04.239858 2856 slave.cpp:1321 ] will retri registr 167.305686m necessari i0206 01:25:04.240044 2856 master.cpp:4225 ] ignor regist slave messag slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) admiss alreadi progress i0206 01:25:04.241482 2845 leveldb.cpp:341 ] persist action ( 360 byte ) leveldb took 23.193162m i0206 01:25:04.241524 2845 replica.cpp:712 ] persist action 3 i0206 01:25:04.241557 2845 replica.cpp:697 ] replica learn append action posit 3 i0206 01:25:04.243746 2844 registrar.cpp:484 ] success updat 'registri ' 59.587072m i0206 01:25:04.244210 2857 log.cpp:702 ] attempt truncat log 3 i0206 01:25:04.244344 2845 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0206 01:25:04.244597 2856 master.cpp:4305 ] regist slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0206 01:25:04.244746 2843 slave.cpp:3436 ] receiv ping slave-observ ( 8 ) @ 172.17.0.8:37843 i0206 01:25:04.244976 2845 hierarchical.cpp:473 ] ad slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 ( 74ef606c4063 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0206 01:25:04.245072 2843 slave.cpp:971 ] regist master master @ 172.17.0.8:37843 ; given slave id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 i0206 01:25:04.245121 2843 fetcher.cpp:81 ] clear fetcher cach i0206 01:25:04.245146 2845 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:04.245178 2845 hierarchical.cpp:1116 ] perform alloc slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 159744n i0206 01:25:04.245465 2846 status_update_manager.cpp:181 ] resum send statu updat i0206 01:25:04.245776 2843 slave.cpp:994 ] checkpoint slaveinfo '/tmp/containerloggertest_defaulttosandbox_fmaksw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/slave.info' i0206 01:25:04.245745 2846 replica.cpp:537 ] replica receiv write request posit 4 ( 393 ) @ 172.17.0.8:37843 i0206 01:25:04.246273 2843 slave.cpp:1030 ] forward total oversubscrib resourc i0206 01:25:04.246507 2850 master.cpp:4646 ] receiv updat slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) total oversubscrib resourc i0206 01:25:04.247180 2824 sched.cpp:222 ] version : 0.28.0 i0206 01:25:04.247155 2850 hierarchical.cpp:531 ] slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 ( 74ef606c4063 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0206 01:25:04.247357 2850 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:04.247406 2850 hierarchical.cpp:1116 ] perform alloc slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 183250n i0206 01:25:04.247938 2854 sched.cpp:326 ] new master detect master @ 172.17.0.8:37843 i0206 01:25:04.248157 2854 sched.cpp:382 ] authent master master @ 172.17.0.8:37843 i0206 01:25:04.248265 2854 sched.cpp:389 ] use default cram-md5 authenticate i0206 01:25:04.248769 2854 authenticatee.cpp:121 ] creat new client sasl connect i0206 01:25:04.249311 2854 master.cpp:5523 ] authent scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:04.249646 2854 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 33 ) @ 172.17.0.8:37843 i0206 01:25:04.250114 2854 authenticator.cpp:98 ] creat new server sasl connect i0206 01:25:04.250453 2854 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0206 01:25:04.250525 2854 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0206 01:25:04.250814 2853 authenticator.cpp:203 ] receiv sasl authent start i0206 01:25:04.250881 2853 authenticator.cpp:325 ] authent requir step i0206 01:25:04.250982 2853 authenticatee.cpp:258 ] receiv sasl authent step i0206 01:25:04.251092 2853 authenticator.cpp:231 ] receiv sasl authent step i0206 01:25:04.251128 2853 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '74ef606c4063 ' server fqdn : '74ef606c4063 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0206 01:25:04.251144 2853 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0206 01:25:04.251200 2853 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0206 01:25:04.251242 2853 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '74ef606c4063 ' server fqdn : '74ef606c4063 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0206 01:25:04.251260 2853 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0206 01:25:04.251269 2853 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0206 01:25:04.251288 2853 authenticator.cpp:317 ] authent success i0206 01:25:04.251471 2853 authenticatee.cpp:298 ] authent success i0206 01:25:04.251574 2853 master.cpp:5553 ] success authent princip 'test-princip ' scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:04.251669 2851 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 33 ) @ 172.17.0.8:37843 i0206 01:25:04.252162 2854 sched.cpp:471 ] success authent master master @ 172.17.0.8:37843 i0206 01:25:04.252188 2854 sched.cpp:776 ] send subscrib call master @ 172.17.0.8:37843 i0206 01:25:04.252286 2854 sched.cpp:809 ] will retri registr 1.575999657sec necessari i0206 01:25:04.252583 2853 master.cpp:2280 ] receiv subscrib call framework 'default ' scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:04.252694 2853 master.cpp:1751 ] author framework princip 'test-princip ' receiv offer role ' * ' i0206 01:25:04.253110 2853 master.cpp:2351 ] subscrib framework default checkpoint disabl capabl [ ] i0206 01:25:04.253703 2843 hierarchical.cpp:265 ] ad framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:04.255300 2843 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:04.255367 2843 hierarchical.cpp:1096 ] perform alloc 1 slave 1.621522m i0206 01:25:04.255820 2844 sched.cpp:703 ] framework regist 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:04.256006 2844 sched.cpp:717 ] schedul : :regist took 105156n i0206 01:25:04.256572 2853 master.cpp:5352 ] send 1 offer framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:04.257524 2853 sched.cpp:873 ] schedul : :resourceoff took 173470n i0206 01:25:04.260818 2855 master.cpp:3138 ] process accept call offer : [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-o0 ] slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:04.260968 2855 master.cpp:2825 ] author framework princip 'test-princip ' launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c user 'mesos' i0206 01:25:04.264458 2844 master.hpp:176 ] ad task 0e7267ed-c5ed-4914-9042-5970b2aaec1c resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 ( 74ef606c4063 ) i0206 01:25:04.264796 2844 master.cpp:3623 ] launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) i0206 01:25:04.265341 2855 slave.cpp:1361 ] got assign task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:04.265941 2855 resources.cpp:564 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0206 01:25:04.267323 2855 slave.cpp:1480 ] launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:04.267627 2855 resources.cpp:564 ] pars resourc json fail : cpus:0.1 ; mem:32 tri semicolon-delimit string format instead i0206 01:25:04.268705 2855 paths.cpp:474 ] tri chown '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7 ' user 'mesos' i0206 01:25:04.274116 2855 slave.cpp:5282 ] launch executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 resourc cpu ( * ) :0.1 ; mem ( * ) :32 work directori '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' i0206 01:25:04.275185 2844 containerizer.cpp:656 ] start contain '5c952202-44cf-427a-8452-0f501140a4b7 ' executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' i0206 01:25:04.275311 2846 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 29.403837m i0206 01:25:04.275390 2846 replica.cpp:712 ] persist action 4 i0206 01:25:04.275511 2855 slave.cpp:1698 ] queu task '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:04.275832 2855 slave.cpp:749 ] success attach file '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' i0206 01:25:04.276707 2855 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0206 01:25:04.284708 2844 launcher.cpp:132 ] fork child pid '2872 ' contain '5c952202-44cf-427a-8452-0f501140a4b7' i0206 01:25:04.301365 2855 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 24.497489m i0206 01:25:04.301528 2855 leveldb.cpp:399 ] delet ~2 key leveldb took 92156n i0206 01:25:04.301563 2855 replica.cpp:712 ] persist action 4 i0206 01:25:04.301640 2855 replica.cpp:697 ] replica learn truncat action posit 4 i0206 01:25:04.823314 2854 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:04.823387 2854 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:04.823420 2854 hierarchical.cpp:1096 ] perform alloc 1 slave 327509n i0206 01:25:05.825943 2850 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:05.826027 2850 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:05.826066 2850 hierarchical.cpp:1096 ] perform alloc 1 slave 362856n i0206 01:25:06.827154 2857 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:06.827235 2857 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:06.827275 2857 hierarchical.cpp:1096 ] perform alloc 1 slave 328221n i0206 01:25:07.828547 2843 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:07.828753 2843 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:07.828907 2843 hierarchical.cpp:1096 ] perform alloc 1 slave 624979n i0206 01:25:08.829737 2855 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:08.829918 2855 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:08.830070 2855 hierarchical.cpp:1096 ] perform alloc 1 slave 596793n i0206 01:25:09.831233 2856 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:09.831316 2856 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:09.831352 2856 hierarchical.cpp:1096 ] perform alloc 1 slave 353864n i0206 01:25:10.832953 2849 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:10.833307 2849 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:10.833411 2849 hierarchical.cpp:1096 ] perform alloc 1 slave 731864n i0206 01:25:11.834967 2847 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 01:25:11.835149 2847 hierarchical.cpp:1498 ] no invers offer send ! i0206 01:25:11.835294 2847 hierarchical.cpp:1096 ] perform alloc 1 slave 586988n i0206 01:25:12.174247 2853 slave.cpp:2643 ] got registr executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.179061 2844 slave.cpp:1863 ] send queu task '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.194753 2858 slave.cpp:3002 ] handl statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.195852 2858 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.196094 2858 status_update_manager.cpp:497 ] creat statusupd stream task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.197000 2858 status_update_manager.cpp:374 ] forward updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 slave i0206 01:25:12.197739 2855 slave.cpp:3354 ] forward updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 master @ 172.17.0.8:37843 i0206 01:25:12.198442 2855 master.cpp:4791 ] statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) i0206 01:25:12.198673 2855 master.cpp:4839 ] forward statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.199038 2855 master.cpp:6447 ] updat state task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( latest state : task_run , statu updat state : task_run ) i0206 01:25:12.199581 2854 sched.cpp:981 ] schedul : :statusupd took 159022n i0206 01:25:12.200568 2854 master.cpp:3949 ] process acknowledg call 9d924a5b-76ab-4886-8091-7af3428ff179 task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 i0206 01:25:12.201513 2858 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 .. / .. /src/tests/container_logger_tests.cpp:350 : failur valu : string : :contain ( stdout.get ( ) , `` hello world ! '' ) actual : fals expect : true i0206 01:25:12.201702 2824 sched.cpp:1903 ] ask stop driver i0206 01:25:12.202831 2848 sched.cpp:1143 ] stop framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' i0206 01:25:12.203284 2848 master.cpp:5923 ] process teardown call framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:12.203321 2848 master.cpp:5935 ] remov framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( default ) scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6 @ 172.17.0.8:37843 i0206 01:25:12.201762 2854 slave.cpp:3248 ] statu updat manag success handl statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.203384 2854 slave.cpp:3264 ] send acknowledg statu updat task_run ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.204712 2843 hierarchical.cpp:375 ] deactiv framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.204953 2848 master.cpp:6447 ] updat state task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ( latest state : task_kil , statu updat state : task_kil ) i0206 01:25:12.205885 2854 slave.cpp:2412 ] statu updat manag success handl statu updat acknowledg ( uuid : 9d924a5b-76ab-4886-8091-7af3428ff179 ) task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.206082 2854 slave.cpp:2079 ] ask shut framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 master @ 172.17.0.8:37843 i0206 01:25:12.206125 2854 slave.cpp:2104 ] shut framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.206331 2854 slave.cpp:4129 ] shut executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.206408 2843 hierarchical.cpp:892 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.207352 2848 master.cpp:6513 ] remov task 0e7267ed-c5ed-4914-9042-5970b2aaec1c resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 slave ( 9 ) @ 172.17.0.8:37843 ( 74ef606c4063 ) i0206 01:25:12.208258 2848 master.cpp:1027 ] master termin i0206 01:25:12.208703 2857 hierarchical.cpp:326 ] remov framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.209658 2857 hierarchical.cpp:505 ] remov slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0 i0206 01:25:12.212208 2848 slave.cpp:3482 ] master @ 172.17.0.8:37843 exit w0206 01:25:12.212261 2848 slave.cpp:3485 ] master disconnect ! wait new master elect i0206 01:25:12.224596 2854 containerizer.cpp:1318 ] destroy contain '5c952202-44cf-427a-8452-0f501140a4b7' i0206 01:25:12.241466 2852 slave.cpp:3482 ] executor ( 1 ) @ 172.17.0.8:43659 exit i0206 01:25:12.250931 2856 containerizer.cpp:1534 ] executor contain '5c952202-44cf-427a-8452-0f501140a4b7 ' exit i0206 01:25:12.253350 2850 provisioner.cpp:306 ] ignor destroy request unknown contain 5c952202-44cf-427a-8452-0f501140a4b7 i0206 01:25:12.253885 2850 slave.cpp:3817 ] executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 termin signal kill i0206 01:25:12.254125 2850 slave.cpp:3921 ] clean executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c ' framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 executor ( 1 ) @ 172.17.0.8:43659 i0206 01:25:12.254545 2847 gc.cpp:54 ] schedul '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7 ' gc 6.99999705530074day futur i0206 01:25:12.254803 2850 slave.cpp:4009 ] clean framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.254822 2847 gc.cpp:54 ] schedul '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c ' gc 6.99999705202667day futur i0206 01:25:12.255084 2857 status_update_manager.cpp:282 ] close statu updat stream framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.255143 2856 gc.cpp:54 ] schedul '/tmp/containerloggertest_defaulttosandbox_fmaksw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-s0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ' gc 6.99999704808day futur i0206 01:25:12.255190 2857 status_update_manager.cpp:528 ] clean statu updat stream task 0e7267ed-c5ed-4914-9042-5970b2aaec1c framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 i0206 01:25:12.255192 2850 slave.cpp:668 ] slave termin [ fail ] containerloggertest.defaulttosandbox ( 8566 ms ) { code }",MESOS-4615,1.0
"slaverecoverytest/0.cleanuphttpexecutor flaki just saw failur asf ci : { code } [ run ] slaverecoverytest/0.cleanuphttpexecutor i0206 00:22:44.791671 2824 leveldb.cpp:174 ] open db 2.539372m i0206 00:22:44.792459 2824 leveldb.cpp:181 ] compact db 740473n i0206 00:22:44.792510 2824 leveldb.cpp:196 ] creat db iter 24164n i0206 00:22:44.792532 2824 leveldb.cpp:202 ] seek begin db 1831n i0206 00:22:44.792548 2824 leveldb.cpp:271 ] iter 0 key db 342n i0206 00:22:44.792605 2824 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0206 00:22:44.793256 2847 recover.cpp:447 ] start replica recoveri i0206 00:22:44.793480 2847 recover.cpp:473 ] replica empti statu i0206 00:22:44.794538 2847 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 9472 ) @ 172.17.0.2:43484 i0206 00:22:44.795040 2848 recover.cpp:193 ] receiv recov respons replica empti statu i0206 00:22:44.795644 2848 recover.cpp:564 ] updat replica statu start i0206 00:22:44.796519 2850 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 752810n i0206 00:22:44.796545 2850 replica.cpp:320 ] persist replica statu start i0206 00:22:44.796725 2848 recover.cpp:473 ] replica start statu i0206 00:22:44.797828 2857 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 9473 ) @ 172.17.0.2:43484 i0206 00:22:44.798355 2850 recover.cpp:193 ] receiv recov respons replica start statu i0206 00:22:44.799193 2850 recover.cpp:564 ] updat replica statu vote i0206 00:22:44.799583 2855 master.cpp:376 ] master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca ( 6632562f1ade ) start 172.17.0.2:43484 i0206 00:22:44.799609 2855 master.cpp:378 ] flag startup : -- acls= '' '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' true '' -- authenticate_http= '' true '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/n2fxqv/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- http_authenticators= '' basic '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_completed_frameworks= '' 50 '' -- max_completed_tasks_per_framework= '' 1000 '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 100sec '' -- registry_strict= '' true '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.28.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/n2fxqv/master '' -- zk_session_timeout= '' 10sec '' i0206 00:22:44.799991 2855 master.cpp:423 ] master allow authent framework regist i0206 00:22:44.800009 2855 master.cpp:428 ] master allow authent slave regist i0206 00:22:44.800020 2855 credentials.hpp:35 ] load credenti authent '/tmp/n2fxqv/credentials' i0206 00:22:44.800245 2850 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 679345n i0206 00:22:44.800370 2850 replica.cpp:320 ] persist replica statu vote i0206 00:22:44.800397 2855 master.cpp:468 ] use default 'crammd5 ' authent i0206 00:22:44.800693 2855 master.cpp:537 ] use default 'basic ' http authent i0206 00:22:44.800815 2855 master.cpp:571 ] author enabl i0206 00:22:44.801216 2850 recover.cpp:578 ] success join paxo group i0206 00:22:44.801604 2850 recover.cpp:462 ] recov process termin i0206 00:22:44.801759 2856 whitelist_watcher.cpp:77 ] no whitelist given i0206 00:22:44.801725 2847 hierarchical.cpp:144 ] initi hierarch alloc process i0206 00:22:44.803982 2855 master.cpp:1712 ] the newli elect leader master @ 172.17.0.2:43484 id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca i0206 00:22:44.804026 2855 master.cpp:1725 ] elect lead master ! i0206 00:22:44.804059 2855 master.cpp:1470 ] recov registrar i0206 00:22:44.804424 2855 registrar.cpp:307 ] recov registrar i0206 00:22:44.805202 2855 log.cpp:659 ] attempt start writer i0206 00:22:44.806782 2856 replica.cpp:493 ] replica receiv implicit promis request ( 9475 ) @ 172.17.0.2:43484 propos 1 i0206 00:22:44.807368 2856 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 547939n i0206 00:22:44.807395 2856 replica.cpp:342 ] persist promis 1 i0206 00:22:44.808375 2856 coordinator.cpp:238 ] coordin attempt fill miss posit i0206 00:22:44.809460 2848 replica.cpp:388 ] replica receiv explicit promis request ( 9476 ) @ 172.17.0.2:43484 posit 0 propos 2 i0206 00:22:44.809929 2848 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 427561n i0206 00:22:44.809967 2848 replica.cpp:712 ] persist action 0 i0206 00:22:44.811035 2850 replica.cpp:537 ] replica receiv write request posit 0 ( 9477 ) @ 172.17.0.2:43484 i0206 00:22:44.811149 2850 leveldb.cpp:436 ] read posit leveldb took 36452n i0206 00:22:44.811532 2850 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 318924n i0206 00:22:44.811615 2850 replica.cpp:712 ] persist action 0 i0206 00:22:44.812532 2850 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0206 00:22:44.813117 2850 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 476530n i0206 00:22:44.813143 2850 replica.cpp:712 ] persist action 0 i0206 00:22:44.813166 2850 replica.cpp:697 ] replica learn nop action posit 0 i0206 00:22:44.813984 2848 log.cpp:675 ] writer start end posit 0 i0206 00:22:44.815549 2848 leveldb.cpp:436 ] read posit leveldb took 31800n i0206 00:22:44.817061 2848 registrar.cpp:340 ] success fetch registri ( 0b ) 12.591104m i0206 00:22:44.817319 2848 registrar.cpp:439 ] appli 1 oper 63480n ; attempt updat 'registry' i0206 00:22:44.818780 2845 log.cpp:683 ] attempt append 170 byte log i0206 00:22:44.818981 2845 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0206 00:22:44.819941 2845 replica.cpp:537 ] replica receiv write request posit 1 ( 9478 ) @ 172.17.0.2:43484 i0206 00:22:44.820582 2845 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 600949n i0206 00:22:44.820608 2845 replica.cpp:712 ] persist action 1 i0206 00:22:44.821552 2845 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0206 00:22:44.821934 2845 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 352813n i0206 00:22:44.821960 2845 replica.cpp:712 ] persist action 1 i0206 00:22:44.821979 2845 replica.cpp:697 ] replica learn append action posit 1 i0206 00:22:44.823447 2845 registrar.cpp:484 ] success updat 'registri ' 5.987072m i0206 00:22:44.823580 2845 registrar.cpp:370 ] success recov registrar i0206 00:22:44.823833 2845 log.cpp:702 ] attempt truncat log 1 i0206 00:22:44.824203 2845 master.cpp:1522 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i0206 00:22:44.824291 2845 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0206 00:22:44.824645 2845 hierarchical.cpp:171 ] skip recoveri hierarch alloc : noth recov i0206 00:22:44.825222 2850 replica.cpp:537 ] replica receiv write request posit 2 ( 9479 ) @ 172.17.0.2:43484 i0206 00:22:44.825742 2850 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 481617n i0206 00:22:44.825772 2850 replica.cpp:712 ] persist action 2 i0206 00:22:44.826748 2852 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0206 00:22:44.827368 2852 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 588591n i0206 00:22:44.827432 2852 leveldb.cpp:399 ] delet ~1 key leveldb took 33059n i0206 00:22:44.827450 2852 replica.cpp:712 ] persist action 2 i0206 00:22:44.827468 2852 replica.cpp:697 ] replica learn truncat action posit 2 i0206 00:22:44.838011 2824 containerizer.cpp:149 ] use isol : posix/cpu , posix/mem , filesystem/posix w0206 00:22:44.838873 2824 backend.cpp:48 ] fail creat 'bind ' backend : bindbackend requir root privileg i0206 00:22:44.843785 2857 slave.cpp:193 ] slave start 172.17.0.2:43484 i0206 00:22:44.843819 2857 slave.cpp:194 ] flag startup : -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' http : //auth.docker.io '' -- docker_kill_orphans= '' true '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.28.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw '' i0206 00:22:44.844292 2857 credentials.hpp:83 ] load credenti authent '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/credential' i0206 00:22:44.844518 2857 slave.cpp:324 ] slave use credenti : test-princip i0206 00:22:44.844696 2857 resources.cpp:564 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk:1024 ; port : [ 31000-32000 ] tri semicolon-delimit string format instead i0206 00:22:44.845243 2857 slave.cpp:464 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0206 00:22:44.845326 2857 slave.cpp:472 ] slave attribut : [ ] i0206 00:22:44.845342 2857 slave.cpp:477 ] slave hostnam : 6632562f1ade i0206 00:22:44.845953 2824 sched.cpp:222 ] version : 0.28.0 i0206 00:22:44.846853 2848 sched.cpp:326 ] new master detect master @ 172.17.0.2:43484 i0206 00:22:44.846936 2848 sched.cpp:382 ] authent master master @ 172.17.0.2:43484 i0206 00:22:44.846958 2848 sched.cpp:389 ] use default cram-md5 authenticate i0206 00:22:44.847692 2858 state.cpp:58 ] recov state '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta' i0206 00:22:44.848108 2850 status_update_manager.cpp:200 ] recov statu updat manag i0206 00:22:44.848325 2852 containerizer.cpp:397 ] recov container i0206 00:22:44.848603 2845 authenticatee.cpp:121 ] creat new client sasl connect i0206 00:22:44.849719 2845 master.cpp:5523 ] authent scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:44.850052 2852 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 662 ) @ 172.17.0.2:43484 i0206 00:22:44.850227 2854 provisioner.cpp:245 ] provision recoveri complet i0206 00:22:44.850410 2852 authenticator.cpp:98 ] creat new server sasl connect i0206 00:22:44.850692 2852 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0206 00:22:44.850720 2852 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0206 00:22:44.850805 2852 authenticator.cpp:203 ] receiv sasl authent start i0206 00:22:44.850862 2852 authenticator.cpp:325 ] authent requir step i0206 00:22:44.850939 2852 authenticatee.cpp:258 ] receiv sasl authent step i0206 00:22:44.851027 2852 authenticator.cpp:231 ] receiv sasl authent step i0206 00:22:44.851052 2852 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '6632562f1ade ' server fqdn : '6632562f1ade ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0206 00:22:44.851063 2852 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0206 00:22:44.851102 2852 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0206 00:22:44.851121 2852 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '6632562f1ade ' server fqdn : '6632562f1ade ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0206 00:22:44.851130 2852 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0206 00:22:44.851136 2852 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0206 00:22:44.851150 2852 authenticator.cpp:317 ] authent success i0206 00:22:44.851219 2850 authenticatee.cpp:298 ] authent success i0206 00:22:44.851310 2850 master.cpp:5553 ] success authent princip 'test-princip ' scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:44.851485 2849 slave.cpp:4496 ] finish recoveri i0206 00:22:44.852154 2843 sched.cpp:471 ] success authent master master @ 172.17.0.2:43484 i0206 00:22:44.852175 2843 sched.cpp:776 ] send subscrib call master @ 172.17.0.2:43484 i0206 00:22:44.852262 2843 sched.cpp:809 ] will retri registr 939.183679m necessari i0206 00:22:44.852375 2844 master.cpp:2280 ] receiv subscrib call framework 'default ' scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:44.852448 2844 master.cpp:1751 ] author framework princip 'test-princip ' receiv offer role ' * ' i0206 00:22:44.852699 2852 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 662 ) @ 172.17.0.2:43484 i0206 00:22:44.852782 2844 master.cpp:2351 ] subscrib framework default checkpoint enabl capabl [ ] i0206 00:22:44.853056 2849 slave.cpp:4668 ] queri resourc estim oversubscrib resourc i0206 00:22:44.853421 2856 hierarchical.cpp:265 ] ad framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:44.853513 2856 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:44.853582 2844 sched.cpp:703 ] framework regist 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:44.853613 2852 slave.cpp:4682 ] receiv oversubscrib resourc resourc estim i0206 00:22:44.853663 2844 sched.cpp:717 ] schedul : :regist took 53762n i0206 00:22:44.853899 2843 slave.cpp:796 ] new master detect master @ 172.17.0.2:43484 i0206 00:22:44.853955 2854 status_update_manager.cpp:174 ] paus send statu updat i0206 00:22:44.853997 2856 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:44.853960 2843 slave.cpp:859 ] authent master master @ 172.17.0.2:43484 i0206 00:22:44.854035 2843 slave.cpp:864 ] use default cram-md5 authenticate i0206 00:22:44.854030 2856 hierarchical.cpp:1096 ] perform alloc 0 slave 581355n i0206 00:22:44.854182 2843 slave.cpp:832 ] detect new master i0206 00:22:44.854277 2854 authenticatee.cpp:121 ] creat new client sasl connect i0206 00:22:44.854517 2843 master.cpp:5523 ] authent slave @ 172.17.0.2:43484 i0206 00:22:44.854603 2854 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 663 ) @ 172.17.0.2:43484 i0206 00:22:44.854836 2855 authenticator.cpp:98 ] creat new server sasl connect i0206 00:22:44.855013 2852 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0206 00:22:44.855044 2852 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0206 00:22:44.855139 2855 authenticator.cpp:203 ] receiv sasl authent start i0206 00:22:44.855186 2855 authenticator.cpp:325 ] authent requir step i0206 00:22:44.855263 2855 authenticatee.cpp:258 ] receiv sasl authent step i0206 00:22:44.855352 2855 authenticator.cpp:231 ] receiv sasl authent step i0206 00:22:44.855381 2855 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '6632562f1ade ' server fqdn : '6632562f1ade ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0206 00:22:44.855389 2855 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0206 00:22:44.855419 2855 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0206 00:22:44.855438 2855 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '6632562f1ade ' server fqdn : '6632562f1ade ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0206 00:22:44.855448 2855 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0206 00:22:44.855453 2855 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0206 00:22:44.855464 2855 authenticator.cpp:317 ] authent success i0206 00:22:44.855540 2851 authenticatee.cpp:298 ] authent success i0206 00:22:44.855721 2851 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 663 ) @ 172.17.0.2:43484 i0206 00:22:44.855832 2852 slave.cpp:927 ] success authent master master @ 172.17.0.2:43484 i0206 00:22:44.855615 2855 master.cpp:5553 ] success authent princip 'test-princip ' slave @ 172.17.0.2:43484 i0206 00:22:44.855973 2852 slave.cpp:1321 ] will retri registr 9.327708m necessari i0206 00:22:44.856145 2854 master.cpp:4237 ] regist slave slave @ 172.17.0.2:43484 ( 6632562f1ade ) id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 i0206 00:22:44.856598 2851 registrar.cpp:439 ] appli 1 oper 59112n ; attempt updat 'registry' i0206 00:22:44.857403 2851 log.cpp:683 ] attempt append 339 byte log i0206 00:22:44.857525 2855 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0206 00:22:44.858482 2844 replica.cpp:537 ] replica receiv write request posit 3 ( 9493 ) @ 172.17.0.2:43484 i0206 00:22:44.858755 2844 leveldb.cpp:341 ] persist action ( 358 byte ) leveldb took 228484n i0206 00:22:44.858855 2844 replica.cpp:712 ] persist action 3 i0206 00:22:44.859751 2852 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0206 00:22:44.860332 2852 leveldb.cpp:341 ] persist action ( 360 byte ) leveldb took 549638n i0206 00:22:44.860358 2852 replica.cpp:712 ] persist action 3 i0206 00:22:44.860411 2852 replica.cpp:697 ] replica learn append action posit 3 i0206 00:22:44.862709 2856 registrar.cpp:484 ] success updat 'registri ' 6.020864m i0206 00:22:44.863106 2850 log.cpp:702 ] attempt truncat log 3 i0206 00:22:44.863358 2850 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0206 00:22:44.864321 2850 slave.cpp:3436 ] receiv ping slave-observ ( 288 ) @ 172.17.0.2:43484 i0206 00:22:44.864706 2849 hierarchical.cpp:473 ] ad slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 ( 6632562f1ade ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0206 00:22:44.864716 2843 replica.cpp:537 ] replica receiv write request posit 4 ( 9494 ) @ 172.17.0.2:43484 i0206 00:22:44.865309 2843 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 410199n i0206 00:22:44.865337 2843 replica.cpp:712 ] persist action 4 i0206 00:22:44.866092 2849 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:44.866132 2848 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0206 00:22:44.866137 2849 hierarchical.cpp:1116 ] perform alloc slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 1.30657m i0206 00:22:44.866497 2856 master.cpp:4305 ] regist slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0206 00:22:44.866564 2843 slave.cpp:1321 ] will retri registr 32.803438m necessari i0206 00:22:44.866690 2843 slave.cpp:971 ] regist master master @ 172.17.0.2:43484 ; given slave id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 i0206 00:22:44.866716 2843 fetcher.cpp:81 ] clear fetcher cach i0206 00:22:44.867066 2856 master.cpp:5352 ] send 1 offer framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:44.867105 2843 slave.cpp:994 ] checkpoint slaveinfo '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/slave.info' i0206 00:22:44.867347 2856 master.cpp:4207 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) alreadi regist , resend acknowledg i0206 00:22:44.867441 2856 status_update_manager.cpp:181 ] resum send statu updat i0206 00:22:44.867465 2843 slave.cpp:1030 ] forward total oversubscrib resourc w0206 00:22:44.867547 2843 slave.cpp:1016 ] alreadi regist master master @ 172.17.0.2:43484 i0206 00:22:44.867574 2843 slave.cpp:1030 ] forward total oversubscrib resourc i0206 00:22:44.867710 2843 master.cpp:4646 ] receiv updat slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) total oversubscrib resourc i0206 00:22:44.867951 2856 sched.cpp:873 ] schedul : :resourceoff took 133371n i0206 00:22:44.867961 2843 master.cpp:4646 ] receiv updat slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) total oversubscrib resourc i0206 00:22:44.868484 2856 hierarchical.cpp:531 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 ( 6632562f1ade ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) i0206 00:22:44.868599 2848 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 2.418545m i0206 00:22:44.868700 2848 leveldb.cpp:399 ] delet ~2 key leveldb took 54053n i0206 00:22:44.868751 2848 replica.cpp:712 ] persist action 4 i0206 00:22:44.868811 2848 replica.cpp:697 ] replica learn truncat action posit 4 i0206 00:22:44.869241 2856 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:44.869287 2856 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:44.869321 2856 hierarchical.cpp:1116 ] perform alloc slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 782848n i0206 00:22:44.869840 2856 hierarchical.cpp:531 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 ( 6632562f1ade ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) i0206 00:22:44.869985 2856 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:44.870028 2856 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:44.870053 2856 hierarchical.cpp:1116 ] perform alloc slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 160104n i0206 00:22:44.871824 2853 master.cpp:3138 ] process accept call offer : [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-o0 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:44.871868 2853 master.cpp:2825 ] author framework princip 'test-princip ' launch task 1 user 'mesos' w0206 00:22:44.873613 2843 validation.cpp:404 ] executor http task 1 use less cpu ( none ) minimum requir ( 0.01 ) . pleas updat executor , mandatori futur releas . w0206 00:22:44.873667 2843 validation.cpp:416 ] executor http task 1 use less memori ( none ) minimum requir ( 32mb ) . pleas updat executor , mandatori futur releas . i0206 00:22:44.874035 2843 master.hpp:176 ] ad task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 ( 6632562f1ade ) i0206 00:22:44.874223 2843 master.cpp:3623 ] launch task 1 framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) i0206 00:22:44.874802 2843 slave.cpp:1361 ] got assign task 1 framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:44.874966 2843 slave.cpp:5202 ] checkpoint frameworkinfo '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info' i0206 00:22:44.875440 2843 slave.cpp:5213 ] checkpoint framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 ' '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid' i0206 00:22:44.876106 2843 slave.cpp:1480 ] launch task 1 framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:44.876644 2843 paths.cpp:474 ] tri chown '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17 ' user 'mesos' i0206 00:22:44.884089 2843 slave.cpp:5654 ] checkpoint executorinfo '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info' i0206 00:22:44.900928 2843 slave.cpp:5282 ] launch executor http framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 resourc work directori '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' i0206 00:22:44.901449 2853 containerizer.cpp:656 ] start contain 'fd4649a4-1c82-4eda-b663-b568b6110d17 ' executor 'http ' framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' i0206 00:22:44.901561 2843 slave.cpp:5677 ] checkpoint taskinfo '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info' i0206 00:22:44.902060 2843 slave.cpp:1698 ] queu task ' 1 ' executor 'http ' framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:44.902207 2843 slave.cpp:749 ] success attach file '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' i0206 00:22:44.907027 2850 launcher.cpp:132 ] fork child pid '8875 ' contain 'fd4649a4-1c82-4eda-b663-b568b6110d17' i0206 00:22:44.907229 2850 containerizer.cpp:1094 ] checkpoint executor 's fork pid 8875 '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid' warn : log initgooglelog ( ) written stderr i0206 00:22:45.080060 8875 process.cpp:991 ] libprocess initi 172.17.0.2:49724 16 cpu i0206 00:22:45.082499 8875 logging.cpp:193 ] log stderr i0206 00:22:45.082862 8875 executor.cpp:172 ] version : 0.28.0 i0206 00:22:45.087201 8903 executor.cpp:316 ] connect agent i0206 00:22:45.802878 2858 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:45.802969 2858 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:45.803014 2858 hierarchical.cpp:1096 ] perform alloc 1 slave 424120n 2016-02-06 00:22:45,982:2824 ( 0x7fd8c5ffb700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:40712 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client w0206 00:22:46.588022 2854 group.cpp:503 ] time wait connect zookeep . forc zookeep session ( sessionid=0 ) expir i0206 00:22:46.588969 2854 group.cpp:519 ] zookeep session expir 2016-02-06 00:22:46,589:2824 ( 0x7fd9fefd1700 ) : zoo_info @ zookeeper_clos @ 2522 : free zookeep resourc sessionid=0 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 712 : client environ : zookeeper.version=zookeep c client 3.4.5 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 716 : client environ : host.name=6632562f1ad 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 723 : client environ : os.name=linux 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 724 : client environ : os.arch=3.13.0-36-lowlat 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 725 : client environ : os.version= # 63-ubuntu smp preempt wed sep 3 21:56:12 utc 2014 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 733 : client environ : user.name= ( null ) 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 741 : client environ : user.home=/home/meso 2016-02-06 00:22:46,589:2824 ( 0x7fda03fdb700 ) : zoo_info @ log_env @ 753 : client environ : user.dir=/tmp/n2fxqv 2016-02-06 00:22:46,590:2824 ( 0x7fda03fdb700 ) : zoo_info @ zookeeper_init @ 786 : initi client connect , host=127.0.0.1:40712 sessiontimeout=10000 watcher=0x7fda10e9e520 sessionid=0 sessionpasswd= < null > context=0x7fd9d401bc10 flags=0 2016-02-06 00:22:46,590:2824 ( 0x7fd8c67fc700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:40712 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client i0206 00:22:46.804400 2844 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:46.804481 2844 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:46.804514 2844 hierarchical.cpp:1096 ] perform alloc 1 slave 347954n i0206 00:22:47.805842 2847 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:47.805934 2847 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:47.805980 2847 hierarchical.cpp:1096 ] perform alloc 1 slave 415449n i0206 00:22:48.807723 2851 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:48.807814 2851 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:48.807857 2851 hierarchical.cpp:1096 ] perform alloc 1 slave 442104n i0206 00:22:49.808733 2848 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:49.808816 2848 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:49.808856 2848 hierarchical.cpp:1096 ] perform alloc 1 slave 384959n 2016-02-06 00:22:49,926:2824 ( 0x7fd8c67fc700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:40712 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client i0206 00:22:50.810307 2847 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:50.810400 2847 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:50.810443 2847 hierarchical.cpp:1096 ] perform alloc 1 slave 389572n i0206 00:22:51.811586 2849 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:51.811681 2849 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:51.811722 2849 hierarchical.cpp:1096 ] perform alloc 1 slave 404450n i0206 00:22:52.812860 2851 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:52.812944 2851 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:52.812981 2851 hierarchical.cpp:1096 ] perform alloc 1 slave 359671n 2016-02-06 00:22:53,263:2824 ( 0x7fd8c67fc700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:40712 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client i0206 00:22:53.814512 2847 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:53.814599 2847 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:53.814651 2847 hierarchical.cpp:1096 ] perform alloc 1 slave 386669n i0206 00:22:54.815238 2852 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:54.815321 2852 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:54.815356 2852 hierarchical.cpp:1096 ] perform alloc 1 slave 376235n i0206 00:22:55.816453 2846 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:55.816550 2846 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:55.816596 2846 hierarchical.cpp:1096 ] perform alloc 1 slave 416350n w0206 00:22:56.592408 2849 group.cpp:503 ] time wait connect zookeep . forc zookeep session ( sessionid=0 ) expir i0206 00:22:56.593480 2849 group.cpp:519 ] zookeep session expir 2016-02-06 00:22:56,593:2824 ( 0x7fda017d6700 ) : zoo_info @ zookeeper_clos @ 2522 : free zookeep resourc sessionid=0 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 712 : client environ : zookeeper.version=zookeep c client 3.4.5 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 716 : client environ : host.name=6632562f1ad 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 723 : client environ : os.name=linux 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 724 : client environ : os.arch=3.13.0-36-lowlat 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 725 : client environ : os.version= # 63-ubuntu smp preempt wed sep 3 21:56:12 utc 2014 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 733 : client environ : user.name= ( null ) 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 741 : client environ : user.home=/home/meso 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ log_env @ 753 : client environ : user.dir=/tmp/n2fxqv 2016-02-06 00:22:56,594:2824 ( 0x7fda007d4700 ) : zoo_info @ zookeeper_init @ 786 : initi client connect , host=127.0.0.1:40712 sessiontimeout=10000 watcher=0x7fda10e9e520 sessionid=0 sessionpasswd= < null > context=0x7fd9e401f350 flags=0 2016-02-06 00:22:56,595:2824 ( 0x7fd8c5ffb700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:40712 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client i0206 00:22:56.817683 2848 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:56.817766 2848 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:56.817803 2848 hierarchical.cpp:1096 ] perform alloc 1 slave 374115n i0206 00:22:57.818447 2844 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:57.818526 2844 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:57.818562 2844 hierarchical.cpp:1096 ] perform alloc 1 slave 344545n i0206 00:22:58.819828 2851 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:58.819914 2851 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:58.819957 2851 hierarchical.cpp:1096 ] perform alloc 1 slave 376948n i0206 00:22:59.820874 2848 hierarchical.cpp:1403 ] no resourc avail alloc ! i0206 00:22:59.820957 2848 hierarchical.cpp:1498 ] no invers offer send ! i0206 00:22:59.820991 2848 hierarchical.cpp:1096 ] perform alloc 1 slave 344192n i0206 00:22:59.854698 2845 slave.cpp:4668 ] queri resourc estim oversubscrib resourc i0206 00:22:59.854991 2845 slave.cpp:4682 ] receiv oversubscrib resourc resourc estim i0206 00:22:59.864612 2857 slave.cpp:3436 ] receiv ping slave-observ ( 288 ) @ 172.17.0.2:43484 .. / .. /src/tests/slave_recovery_tests.cpp:1105 : failur fail wait 15sec updatecall1 i0206 00:22:59.876358 2852 master.cpp:1213 ] framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 disconnect i0206 00:22:59.876410 2852 master.cpp:2576 ] disconnect framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:59.876456 2852 master.cpp:2600 ] deactiv framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:59.876569 2852 master.cpp:1237 ] give framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 0n failov i0206 00:22:59.876981 2844 hierarchical.cpp:375 ] deactiv framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.877049 2844 master.cpp:5204 ] framework failov timeout , remov framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:59.877075 2844 master.cpp:5935 ] remov framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( default ) scheduler-63899759-d7fc-42b2-8371-57484f352895 @ 172.17.0.2:43484 i0206 00:22:59.877276 2844 master.cpp:6447 ] updat state task 1 framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ( latest state : task_kil , statu updat state : task_kil ) i0206 00:22:59.878051 2844 master.cpp:6513 ] remov task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) i0206 00:22:59.878433 2844 master.cpp:6542 ] remov executor 'http ' resourc framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 slave @ 172.17.0.2:43484 ( 6632562f1ade ) i0206 00:22:59.878667 2852 slave.cpp:2079 ] ask shut framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 master @ 172.17.0.2:43484 i0206 00:22:59.878733 2852 slave.cpp:2104 ] shut framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.878806 2852 slave.cpp:4129 ] shut executor 'http ' framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 w0206 00:22:59.878834 2852 slave.hpp:655 ] unabl send event executor 'http ' framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 : unknown connect type i0206 00:22:59.879550 2844 master.cpp:1027 ] master termin i0206 00:22:59.879703 2854 hierarchical.cpp:892 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.879947 2854 hierarchical.cpp:326 ] remov framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.880306 2854 hierarchical.cpp:505 ] remov slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0 i0206 00:22:59.880666 2852 slave.cpp:3482 ] master @ 172.17.0.2:43484 exit w0206 00:22:59.880695 2852 slave.cpp:3485 ] master disconnect ! wait new master elect i0206 00:22:59.885498 2857 containerizer.cpp:1318 ] destroy contain 'fd4649a4-1c82-4eda-b663-b568b6110d17' i0206 00:22:59.904532 2858 containerizer.cpp:1534 ] executor contain 'fd4649a4-1c82-4eda-b663-b568b6110d17 ' exit i0206 00:22:59.907024 2858 provisioner.cpp:306 ] ignor destroy request unknown contain fd4649a4-1c82-4eda-b663-b568b6110d17 i0206 00:22:59.907428 2858 slave.cpp:3817 ] executor 'http ' framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 termin signal kill i0206 00:22:59.907538 2858 slave.cpp:3921 ] clean executor 'http ' framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.908213 2858 slave.cpp:4009 ] clean framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.908555 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17 ' gc 6.99998949252444day futur i0206 00:22:59.908720 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http ' gc 6.99998949082074day futur i0206 00:22:59.908807 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17 ' gc 6.99998948980444day futur i0206 00:22:59.908927 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http ' gc 6.99998948890074day futur i0206 00:22:59.909009 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ' gc 6.99998948710518day futur i0206 00:22:59.909121 2858 gc.cpp:54 ] schedul '/tmp/slaverecoverytest_0_cleanuphttpexecutor_kaxwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-s0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 ' gc 6.99998948630815day futur i0206 00:22:59.909211 2858 status_update_manager.cpp:282 ] close statu updat stream framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 i0206 00:22:59.910423 2853 slave.cpp:668 ] slave termin .. / .. /3rdparty/libprocess/include/process/gmock.hpp:425 : failur actual function call count n't match expect_cal ( filter- > mock , filter ( test : :a < const httpevent & > ( ) ) ) ... expect arg : union http matcher ( 72-byte object < d0-11 44-12 da-7f 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 > , updat , 1-byte object < 1b > , 16-byte object < 1b-f4 34-01 00-00 00-00 00-00 00-00 da-7f 00-00 > ) expect : call actual : never call - unsatisfi activ .. / .. /3rdparty/libprocess/include/process/gmock.hpp:425 : failur actual function call count n't match expect_cal ( filter- > mock , filter ( test : :a < const httpevent & > ( ) ) ) ... expect arg : union http matcher ( 72-byte object < d0-11 44-12 da-7f 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 > , updat , 1-byte object < 1b > , 16-byte object < 1b-f4 34-01 00-00 00-00 00-00 00-00 da-7f 00-00 > ) expect : call actual : never call - unsatisfi activ [ fail ] slaverecoverytest/0.cleanuphttpexecutor , typeparam = meso : :intern : :slave : :mesoscontainer ( 15126 ms ) { code }",MESOS-4614,3.0
"subprocess intellig setting/inherit libprocess environ variabl mostli copi [ comment|http : //issues.apache.org/jira/browse/mesos-4598 ? focusedcommentid=15133497 & page=com.atlassian.jira.plugin.system.issuetabpanel : comment-tabpanel # comment-15133497 ] a subprocess inherit environ variabl { { libprocess_ * } } may run accident fatal : | || subprocess use libprocess || subprocess someth els || || subprocess sets/inherit { { port } } accid | bind failur - > exit | noth happen ( ? ) | || subprocess set differ { { port } } purpos | bind success ( ? ) | noth happen ( ? ) | ( ? ) = mean usual case , 100 % . a complet fix would look someth like : * if { { subprocess } } call get { { environ = none ( ) } } , automat remov { { libprocess_port } } inherit environ . * the part [ { { executorenviron } } |http : //github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp # l265 ] deal libprocess & libmeso refactor libprocess helper . we would use helper container , fetcher , containerlogg modul . * if { { subprocess } } call given { { libprocess_port == os : :getenv ( `` libprocess_port '' ) } } , log ( warn ) unset env var local .",MESOS-4609,2.0
"root_docker_dockerhealthytask flaki . log teamciti run { { sudo ./bin/mesos-tests.sh } } aw ec2 instanc : { noformat } [ 18:27:14 ] [ step 8/8 ] [ -- -- -- -- -- ] 8 test healthchecktest [ 18:27:14 ] [ step 8/8 ] [ run ] healthchecktest.healthytask [ 18:27:17 ] [ step 8/8 ] [ ok ] healthchecktest.healthytask ( 2222 ms ) [ 18:27:17 ] [ step 8/8 ] [ run ] healthchecktest.root_docker_dockerhealthytask [ 18:27:36 ] [ step 8/8 ] .. / .. /src/tests/health_check_tests.cpp:388 : failur [ 18:27:36 ] [ step 8/8 ] fail wait 15sec termin [ 18:27:36 ] [ step 8/8 ] f0204 18:27:35.981302 23085 logging.cpp:64 ] raw : pure virtual method call [ 18:27:36 ] [ step 8/8 ] @ 0x7f7077055e1c googl : :logmessag : :fail ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f707705ba6f googl : :rawlog__ ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x7f70760f76c9 __cxa_pure_virtu [ 18:27:36 ] [ step 8/8 ] @ 0xa9423c meso : :intern : :test : :cluster : :slave : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074e45 meso : :intern : :test : :mesostest : :shutdownslav ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1074de4 meso : :intern : :test : :mesostest : :shutdown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x1070ec7 meso : :intern : :test : :mesostest : :teardown ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16eb7b2 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16e61a9 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16c56aa test : :test : :run ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16c5e89 test : :testinfo : :run ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16c650a test : :testcas : :run ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16cd1f6 test : :intern : :unittestimpl : :runalltest ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16ec513 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16e6df1 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 18:27:36 ] [ step 8/8 ] @ 0x16cbe26 test : :unittest : :run ( ) [ 18:27:36 ] [ step 8/8 ] @ 0xe54c84 run_all_test ( ) [ 18:27:36 ] [ step 8/8 ] @ 0xe54867 main [ 18:27:36 ] [ step 8/8 ] @ 0x7f7071560a40 ( unknown ) [ 18:27:36 ] [ step 8/8 ] @ 0x9b52d9 _start [ 18:27:36 ] [ step 8/8 ] abort ( core dump ) [ 18:27:36 ] [ step 8/8 ] process exit code 134 { noformat } happen ubuntu 15.04 , cento 6 , cento 7 _quite_ often .",MESOS-4604,2.0
"logrot containerlogg remov ip environ . the { { logrotatecontainerlogg } } start libprocess-us subprocess . libprocess initi attempt resolv ip hostnam . if dn servic avail , step fail , termin logger subprocess prematur . sinc logger subprocess live agent , use { { libprocess_ip } } suppli agent .",MESOS-4598,1.0
renam ` examples/event_call_framework.cpp ` ` examples/test_http_framework.cpp ` we alreadi { { examples/test_framework.cpp } } test { { pid } } base framework . we would ideal want renam { { event_call_framework } } correctli reflect 's exampl http base framework .,MESOS-4583,1.0
"design doc schedul http stream id thi ticket design http stream id , use http schedul . these id allow meso distinguish differ instanc http framework schedul .",MESOS-4573,5.0
netclsisolatortest.root_cgroups_netclsisol fail cento 6 thi test fail cento 6 vm due cgroup issu : { code } [ run ] netclsisolatortest.root_cgroups_netclsisol i0127 19:15:06.637328 25347 exec.cpp:134 ] version : 0.28.0 i0127 19:15:06.648378 25378 exec.cpp:208 ] executor regist slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-s0 regist executor localhost start task b745d88e-3fbe-4af9-80b3-e43484e37acf sh -c 'sleep 1000' fork command 25385 .. / .. /src/tests/containerizer/isolator_tests.cpp:926 : failur pid : fail read cgroup control 'cgroup.proc ' : '/sys/fs/cgroup/net_cl ' valid hierarchi i0127 19:15:06.662083 25376 exec.cpp:381 ] executor ask shutdown shut send sigterm process tree pid 25385 [ fail ] netclsisolatortest.root_cgroups_netclsisol ( 335 ms ) { code },MESOS-4540,1.0
"logrot containerlogg may handl fd ownership correctli one patch [ mesos-4136 ] introduc { { fdtype : :own } } enum { { subprocess : :io : :fd } } . the way logrot modul use slightli incorrect : # the modul start subprocess output { { subprocess : :pipe ( ) } } . # that pipe 's fd pass anoth subprocess via { { subprocess : :io : :fd ( pipe , io : :own ) } } . # when second subprocess start , pipe 's fd close parent . # when first subprocess termin , exist code tri close pipe . thi effect close random fd .",MESOS-4535,1.0
"netclsisolatortest.root_cgroups_netclsisol flaki while run command { noformat } sudo ./bin/mesos-tests.sh -- gtest_filter= '' -cgroupsanyhierarchywithcpumemorytest.root_cgroups_listen : cgroupsanyhierarchymemorypressuretest.root_increaserss '' -- gtest_repeat=10 -- gtest_break_on_failur { noformat } one eventu get follow output : { noformat } [ run ] netclsisolatortest.root_cgroups_netclsisol .. / .. /src/tests/containerizer/isolator_tests.cpp:870 : failur container : could creat isol 'cgroups/net_cl ' : unexpect subsystem found attach hierarchi /sys/fs/cgroup/net_cl , net_prio [ fail ] netclsisolatortest.root_cgroups_netclsisol ( 75 ms ) { noformat }",MESOS-4530,1.0
"introduc docker runtim isol . current docker imag default configur includ ` provisioninfo ` . we grab necessari config ` provisioninfo ` ` containerinfo ` , handl runtim inform insid docker runtim isol . return ` containerlaunchinfo ` contain ` working_dir ` , ` env ` merg ` commandinfo ` , etc .",MESOS-4517,3.0
containerloggertest.logrotate_rotateinsandbox break run centos6 . { noformat } [ 17:24:58 ] [ step 7/7 ] logrot : bad argument -- version : unknown error [ 17:24:58 ] [ step 7/7 ] f0126 17:24:57.913729 4503 container_logger_tests.cpp:380 ] check_som ( container ) : fail creat contain logger : fail creat contain logger modul 'org_apache_mesos_logrotatecontainerlogg ' : error creat modul instanc 'org_apache_mesos_logrotatecontainerlogg ' [ 17:24:58 ] [ step 7/7 ] * * * check failur stack trace : * * * [ 17:24:58 ] [ step 7/7 ] @ 0x7f11ae0d2d40 googl : :logmessag : :fail ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x7f11ae0d2c9c googl : :logmessag : :sendtolog ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x7f11ae0d2692 googl : :logmessag : :flush ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x7f11ae0d544c googl : :logmessagefat : :~logmessagefat ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x983927 _checkfat : :~_checkfat ( ) [ 17:24:58 ] [ step 7/7 ] @ 0xa9a18b meso : :intern : :test : :containerloggertest_logrotate_rotateinsandbox_test : :testbodi ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x1623a4e test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x161eab2 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x15ffdfd test : :test : :run ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x160058b test : :testinfo : :run ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x1600bc6 test : :testcas : :run ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x1607515 test : :intern : :unittestimpl : :runalltest ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x16246dd test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x161f608 test : :intern : :handleexceptionsinmethodifsupport < > ( ) [ 17:24:58 ] [ step 7/7 ] @ 0x1606245 test : :unittest : :run ( ) [ 17:24:58 ] [ step 7/7 ] @ 0xde36b6 run_all_test ( ) [ 17:24:58 ] [ step 7/7 ] @ 0xde32cc main [ 17:24:58 ] [ step 7/7 ] @ 0x7f11a8896d5d __libc_start_main [ 17:24:58 ] [ step 7/7 ] @ 0x981fc9 ( unknown ) { noformat },MESOS-4515,1.0
"render quota statu consist endpoint . current quota statu endpoint return collect { { quotainfo } } proto convert json . an exampl respons look like : { code : xml } { `` info '' : [ { `` role '' : `` role1 '' , `` guarante '' : [ { `` name '' : `` cpu '' , `` role '' : `` * '' , `` type '' : `` scalar '' , `` scalar '' : { `` valu '' : 12 } } , { `` name '' : `` mem '' , `` role '' : `` * '' , `` type '' : `` scalar '' , `` scalar '' : { `` valu '' : 6144 } } ] } ] } { code } presenc field , e.g . `` role '' , mislead . to address issu make output inform , probabl introduc { { model ( ) } } function { { quotastatu } } .",MESOS-4512,3.0
"expos executorinfo taskinfo isol . current info isol . imag docker runtim isol , commandinfo necessari support either custom executor command executor .",MESOS-4500,2.0
"docker provision store reus exist layer cach . current , docker provision store download layer associ imag imag found local , even though layer might alreadi exist cach . thi problemat anytim user deploy new imag , meso fetch layer new imag , even though layer alreadi cach local .",MESOS-4499,5.0
delet ` os : :chown ` window,MESOS-4495,1.0
implement process querying/count window,MESOS-4471,2.0
implement ` waitpid ` window,MESOS-4466,5.0
creat common sha512 comput util function . add common util function comput digest . start ` sha512 ` sinc immedi need appc imag fetcher .,MESOS-4454,2.0
"instal 3rdparti packag boost , glog , protobuf picojson instal meso meso modul depend packag instal exact version meso compil .",MESOS-4434,3.0
"introduc filter test abstract http event libprocess we need test abstract { { httpevent } } similar alreadi exist one 's { { dispatchev } } , { { messageev } } libprocess . the abstract look similar semant alreadi exist { { future_dispatch } } / { { future_messag } } .",MESOS-4425,3.0
"prevent alloc crash success recoveri . there might bug may crash master point [ ~bmahler ] http : //reviews.apache.org/r/42222/ : { noformat } it look like trip resum call addslav , delay resum crash master due check ( paus ) current resid resum . { noformat }",MESOS-4417,3.0
travers role quota alloc . there might bug resourc alloc multipl quota' role one role 's quota met . we need investig behavior .,MESOS-4411,3.0
"synchron handl authz error schedul endpoint . current , authz error { { /schedul } } endpoint handl asynchron { { frameworkerrormessag } } . here exampl : { code } ( authorizationerror.issom ( ) ) { log ( info ) < < `` refus subscript framework '' < < `` ' '' < < frameworkinfo.nam ( ) < < `` ' '' < < `` : `` < < authorizationerror.get ( ) .messag ; frameworkerrormessag messag ; message.set_messag ( authorizationerror.get ( ) .messag ) ; http.send ( messag ) ; http.close ( ) ; return ; } { code } we would like handl error synchron request receiv similar endpoint like { { /reserv } } / { { /quota } } . we alreadi relev function { { authorizexxx } } etc { { master.cpp } } . we make request pass relev { { futur } } { { authorizexxx } } function fulfil .",MESOS-4398,5.0
add persist volum endpoint test princip there current persist volum endpoint test use princip ; ad .,MESOS-4395,1.0
"offer inverseoff accept accept call * problem * * in { { master : :accept } } , { { valid : :offer : :valid } } return error { { inverseoff } } includ list { { offerid } } { { accept } } call . * if { { offer } } part { { accept } } , master see { { error.issom ( ) } } return { { task_lost } } normal offer . ( http : //github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp # l3117 ) here 's regress test : http : //reviews.apache.org/r/42092/ * propros * the question whether want allow mix { { offer } } { { inverseoff } } . argument mix : * the design/structur mainten origin intend overload { { accept } } { { declin } } take invers offer . * enforc non-mix may requir break chang { { scheduler.proto } } . argument mix : * some semant difficult explain . what mean suppli { { inverseoff } } { { offer : :oper } } ? what { { declin } } { { offer } } { { inverseoff } } , includ `` reason '' ? * what happen presum add third type offer ? * doe make sens { { task_lost } } valid normal offer { { inverseoff } } invalid ?",MESOS-4385,2.0
support docker runtim configur env var imag . we need support env var configur return docker imag meso container .,MESOS-4383,2.0
"creat common tar/untar util function . as part refactor creat common place add command util , add * tar * * untar * first poc .",MESOS-4360,3.0
"gmock warn dockercontainerizertest.root_docker_dockerinspectdiscard the follow gmock warn seen cento 7.1 : { code } [ run ] dockercontainerizertest.root_docker_dockerinspectdiscard gmock warn : uninterest mock function call - return directli . function call : executorlost ( 0x7ffdd74f73e0 , @ 0x7f3e3c00fa20 e1 , @ 0x7f3e3c00f4b0 cf212bb4-c8c5-4a43-b71f-c17b27458627-s0 , -1 ) stack trace : [ ok ] dockercontainerizertest.root_docker_dockerinspectdiscard ( 405 ms ) { code }",MESOS-4359,2.0
"gmock warn ` offerrescind ` ` reservationtest ` fixtur sever test involv checkpoint resourc { { reservationtest } } fixtur throw gmock warn occasion . here output { { gtest_filter= '' reservationtest . * '' bin/mesos-tests.sh -- gtest_repeat=10000 -- gtest_break_on_failure=1 | grep -b 3 -a 6 warn } } : { code } -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - we run docker test : docker test support non-linux system -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - [ ok ] reservationtest.masterfailov ( 89 ms ) [ run ] reservationtest.compatiblecheckpointedresourc gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec320fab0 65537c10-285c-419e-b89f-191283402d85-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourc ( 52 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 45 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 46 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec796f220 bf4e1b52-02db-4763-8be0-3c759c80f1ba-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 63 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 45 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 42 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec7ad92b0 42a9f1ff-122e-4df7-9530-a96126e36f84-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 65 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 46 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 49 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec7af4310 d5e1005f-abb8-4bfd-92e0-3976ee150fbf-o1 ) stack trace : [ ok ] reservationtest.incompatiblecheckpointedresourc ( 94 ms ) [ run ] reservationtest.goodaclreservethenunreserv [ ok ] reservationtest.goodaclreservethenunreserv ( 57 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 43 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec7cdadc0 36e15f52-3299-46fa-850d-970097fef8e2-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 62 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 46 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 47 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feec8c1b580 c8dd35ab-7363-40e0-8e20-8c7dc76a8497-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 62 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 45 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 47 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feecbd9b5b0 031c2148-8a20-4532-b77f-b6200c3791c8-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 62 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 46 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 47 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feecd52adb0 edc5a322-b220-4b13-a39b-99a523b172ba-o1 ) stack trace : [ ok ] reservationtest.incompatiblecheckpointedresourc ( 76 ms ) [ run ] reservationtest.goodaclreservethenunreserv [ ok ] reservationtest.goodaclreservethenunreserv ( 63 ms ) -- -- [ ok ] reservationtest.sendingcheckpointresourcesmessag ( 45 ms ) [ run ] reservationtest.resourcescheckpoint gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f015df8 , @ 0x7feecfe16f00 09a90e67-a40f-4e42-8802-1a5644733a06-o1 ) stack trace : [ ok ] reservationtest.resourcescheckpoint ( 60 ms ) [ run ] reservationtest.masterfailov [ ok ] reservationtest.masterfailov ( 89 ms ) -- -- [ ok ] reservationtest.compatiblecheckpointedresourc ( 43 ms ) [ run ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum gmock warn : uninterest mock function call - return directli . function call : offerrescind ( 0x7fff5f014960 , @ 0x7feecacceba0 84965984-28cd-4bc8-b25b-746583477d09-o1 ) stack trace : [ ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolum ( 58 ms ) [ run ] reservationtest.incompatiblecheckpointedresourc [ ok ] reservationtest.incompatiblecheckpointedresourc ( 68 ms ) { code }",MESOS-4350,2.0
"gmock warn reservationtest.aclmultipleoper { noformat } [ run ] reservationtest.aclmultipleoper gmock warn : uninterest mock function call - return directli . function call : shutdown ( 0x7fa2a311b300 ) stack trace : [ ok ] reservationtest.aclmultipleoper ( 174 ms ) [ -- -- -- -- -- ] 1 test reservationtest ( 174 ms total ) { noformat } seem occur non-determinist , mayb per 50 run . osx 10.10",MESOS-4347,1.0
"creat util common shell command use . we spawn shell command line util like tar , untar , sha256 etc . would great resus creat common util class/fil util .",MESOS-4338,5.0
"slavetest.launchtaskinfowithcontainerinfo execut isol execut { { slavetest.launchtaskinfowithcontainerinfo } } { { 468b8ec } } os x 10.10.5 isol fail due miss cleanup , { code } % ./bin/mesos-tests.sh -- gtest_filter=slavetest.launchtaskinfowithcontainerinfo sourc directori : /abc/def/src/meso build directori : /abc/def/src/mesos/build -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - we run docker test : docker test support non-linux system -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - /usr/bin/nc /usr/bin/curl note : googl test filter = slavetest.launchtaskinfowithcontainerinfo-healthchecktest.root_docker_dockerhealthytask : healthchecktest.root_docker_dockerhealthstatuschang : hierarchicalallocator_benchmark_test.declineoff : hooktest.root_docker_verifyslaveprelaunchdockerhook : slavetest.root_runtaskwithcommandinfowithoutus : slavetest.disabled_root_runtaskwithcommandinfowithus : dockercontainerizertest.root_docker_launch : dockercontainerizertest.root_docker_kil : dockercontainerizertest.root_docker_usag : dockercontainerizertest.root_docker_recov : dockercontainerizertest.root_docker_skiprecovernondock : dockercontainerizertest.root_docker_log : dockercontainerizertest.root_docker_default_cmd : dockercontainerizertest.root_docker_default_cmd_overrid : dockercontainerizertest.root_docker_default_cmd_arg : dockercontainerizertest.root_docker_slaverecoverytaskcontain : dockercontainerizertest.disabled_root_docker_slaverecoveryexecutorcontain : dockercontainerizertest.root_docker_nc_portmap : dockercontainerizertest.root_docker_launchsandboxwithcolon : dockercontainerizertest.root_docker_destroywhilefetch : dockercontainerizertest.root_docker_destroywhilepul : dockercontainerizertest.root_docker_executorcleanupwhenlaunchfail : dockercontainerizertest.root_docker_fetchfailur : dockercontainerizertest.root_docker_dockerpullfailur : dockercontainerizertest.root_docker_dockerinspectdiscard : dockertest.root_docker_interfac : dockertest.root_docker_parsing_vers : dockertest.root_docker_checkcommandwithshel : dockertest.root_docker_checkportresourc : dockertest.root_docker_cancelpul : dockertest.root_docker_mountrel : dockertest.root_docker_mountabsolut : copybackendtest.root_copybackend : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/0 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/1 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/2 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/3 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/4 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/5 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/6 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/7 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/8 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/9 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/10 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/11 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/12 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/13 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/14 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/15 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/16 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/17 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/18 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/19 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/20 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/21 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/22 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/23 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/24 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/25 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/26 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/27 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/28 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/29 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/30 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/31 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/32 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/33 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/34 : slaveandframeworkcount/hierarchicalallocator_benchmark_test.addandupdateslave/35 : slavecount/registrar_benchmark_test.performance/0 : slavecount/registrar_benchmark_test.performance/1 : slavecount/registrar_benchmark_test.performance/2 : slavecount/registrar_benchmark_test.performance/3 [ ========== ] run 1 test 1 test case . [ -- -- -- -- -- ] global test environ set-up . [ -- -- -- -- -- ] 1 test slavetest [ run ] slavetest.launchtaskinfowithcontainerinfo [ ok ] slavetest.launchtaskinfowithcontainerinfo ( 79 ms ) [ -- -- -- -- -- ] 1 test slavetest ( 79 ms total ) [ -- -- -- -- -- ] global test environ tear-down .. / .. /src/tests/environment.cpp:569 : failur fail test complet child process remain : -+- 54487 /abc/def/src/mesos/build/src/.libs/mesos-test -- gtest_filter=slavetest.launchtaskinfowithcontainerinfo \ -- - 54503 /bin/sh /abc/def/src/mesos/build/src/mesos-container launch -- command= { `` shell '' : true , '' valu '' : '' \/abc\/def\/src\/mesos\/build\/src\/mesos-executor '' } -- commands= { `` command '' : [ ] } -- directory=/tmp -- help=fals -- pipe_read=10 -- pipe_write=13 -- user=test [ ========== ] 1 test 1 test case ran . ( 87 ms total ) [ pass ] 1 test . [ fail ] 0 test , list : 0 fail test { code }",MESOS-4329,1.0
"persistentvolumetest.badaclnoprincip flaki http : //builds.apache.org/job/mesos/1457/compiler=gcc , configuration= -- verbos % 20 -- enable-libev % 20 -- enable-ssl , os=centos:7 , label_exp=dock % 7c % 7chadoop/consoleful { noformat } [ run ] persistentvolumetest.badaclnoprincip i0108 01:13:16.117883 1325 leveldb.cpp:174 ] open db 2.614722m i0108 01:13:16.118650 1325 leveldb.cpp:181 ] compact db 706567n i0108 01:13:16.118702 1325 leveldb.cpp:196 ] creat db iter 24489n i0108 01:13:16.118723 1325 leveldb.cpp:202 ] seek begin db 2436n i0108 01:13:16.118738 1325 leveldb.cpp:271 ] iter 0 key db 397n i0108 01:13:16.118793 1325 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0108 01:13:16.119627 1348 recover.cpp:447 ] start replica recoveri i0108 01:13:16.120352 1348 recover.cpp:473 ] replica empti statu i0108 01:13:16.121750 1357 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 7084 ) @ 172.17.0.2:32801 i0108 01:13:16.122297 1353 recover.cpp:193 ] receiv recov respons replica empti statu i0108 01:13:16.122747 1350 recover.cpp:564 ] updat replica statu start i0108 01:13:16.123625 1354 master.cpp:365 ] master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 ( d9632dd1c41e ) start 172.17.0.2:32801 i0108 01:13:16.123946 1347 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 728242n i0108 01:13:16.123999 1347 replica.cpp:320 ] persist replica statu start i0108 01:13:16.123708 1354 master.cpp:367 ] flag startup : -- acls= '' create_volum { princip { valu : `` test-princip '' } volume_typ { type : ani } } create_volum { princip { type : ani } volume_typ { type : none } } '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' fals '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/f2ra75/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 25sec '' -- registry_strict= '' true '' -- roles= '' role1 '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.27.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/f2ra75/mast '' -- zk_session_timeout= '' 10sec '' i0108 01:13:16.124219 1354 master.cpp:414 ] master allow unauthent framework regist i0108 01:13:16.124236 1354 master.cpp:417 ] master allow authent slave regist i0108 01:13:16.124248 1354 credentials.hpp:35 ] load credenti authent '/tmp/f2ra75/credentials' i0108 01:13:16.124294 1358 recover.cpp:473 ] replica start statu i0108 01:13:16.124644 1354 master.cpp:456 ] use default 'crammd5 ' authent i0108 01:13:16.124820 1354 master.cpp:493 ] author enabl w0108 01:13:16.124843 1354 master.cpp:553 ] the ' -- role ' flag deprec . thi flag remov futur . see meso 0.27 upgrad note inform i0108 01:13:16.125154 1348 hierarchical.cpp:147 ] initi hierarch alloc process i0108 01:13:16.125334 1345 whitelist_watcher.cpp:77 ] no whitelist given i0108 01:13:16.126065 1346 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 7085 ) @ 172.17.0.2:32801 i0108 01:13:16.126806 1348 recover.cpp:193 ] receiv recov respons replica start statu i0108 01:13:16.128237 1354 recover.cpp:564 ] updat replica statu vote i0108 01:13:16.128402 1359 master.cpp:1629 ] the newli elect leader master @ 172.17.0.2:32801 id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 i0108 01:13:16.128489 1359 master.cpp:1642 ] elect lead master ! i0108 01:13:16.128523 1359 master.cpp:1387 ] recov registrar i0108 01:13:16.128756 1355 registrar.cpp:307 ] recov registrar i0108 01:13:16.129259 1344 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 531437n i0108 01:13:16.129292 1344 replica.cpp:320 ] persist replica statu vote i0108 01:13:16.129425 1358 recover.cpp:578 ] success join paxo group i0108 01:13:16.129680 1358 recover.cpp:462 ] recov process termin i0108 01:13:16.130187 1358 log.cpp:659 ] attempt start writer i0108 01:13:16.131613 1352 replica.cpp:493 ] replica receiv implicit promis request ( 7086 ) @ 172.17.0.2:32801 propos 1 i0108 01:13:16.131983 1352 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 333646n i0108 01:13:16.132004 1352 replica.cpp:342 ] persist promis 1 i0108 01:13:16.132627 1348 coordinator.cpp:238 ] coordin attempt fill miss posit i0108 01:13:16.133896 1349 replica.cpp:388 ] replica receiv explicit promis request ( 7087 ) @ 172.17.0.2:32801 posit 0 propos 2 i0108 01:13:16.134289 1349 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 349652n i0108 01:13:16.134317 1349 replica.cpp:712 ] persist action 0 i0108 01:13:16.135470 1351 replica.cpp:537 ] replica receiv write request posit 0 ( 7088 ) @ 172.17.0.2:32801 i0108 01:13:16.135537 1351 leveldb.cpp:436 ] read posit leveldb took 36181n i0108 01:13:16.135901 1351 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 308752n i0108 01:13:16.135924 1351 replica.cpp:712 ] persist action 0 i0108 01:13:16.136529 1347 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i0108 01:13:16.136889 1347 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 327106n i0108 01:13:16.136916 1347 replica.cpp:712 ] persist action 0 i0108 01:13:16.136943 1347 replica.cpp:697 ] replica learn nop action posit 0 i0108 01:13:16.137707 1359 log.cpp:675 ] writer start end posit 0 i0108 01:13:16.138844 1348 leveldb.cpp:436 ] read posit leveldb took 31371n i0108 01:13:16.139878 1356 registrar.cpp:340 ] success fetch registri ( 0b ) 0n i0108 01:13:16.140012 1356 registrar.cpp:439 ] appli 1 oper 42063n ; attempt updat 'registry' i0108 01:13:16.140797 1355 log.cpp:683 ] attempt append 170 byte log i0108 01:13:16.140974 1345 coordinator.cpp:348 ] coordin attempt write append action posit 1 i0108 01:13:16.141744 1354 replica.cpp:537 ] replica receiv write request posit 1 ( 7089 ) @ 172.17.0.2:32801 i0108 01:13:16.142226 1354 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 441971n i0108 01:13:16.142251 1354 replica.cpp:712 ] persist action 1 i0108 01:13:16.142860 1351 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i0108 01:13:16.143198 1351 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 305928n i0108 01:13:16.143223 1351 replica.cpp:712 ] persist action 1 i0108 01:13:16.143241 1351 replica.cpp:697 ] replica learn append action posit 1 i0108 01:13:16.144271 1354 registrar.cpp:484 ] success updat 'registri ' 0n i0108 01:13:16.144435 1354 registrar.cpp:370 ] success recov registrar i0108 01:13:16.144567 1359 log.cpp:702 ] attempt truncat log 1 i0108 01:13:16.144780 1359 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i0108 01:13:16.144989 1348 hierarchical.cpp:165 ] skip recoveri hierarch alloc : noth recov i0108 01:13:16.144928 1354 master.cpp:1439 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i0108 01:13:16.145690 1357 replica.cpp:537 ] replica receiv write request posit 2 ( 7090 ) @ 172.17.0.2:32801 i0108 01:13:16.146072 1357 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 345113n i0108 01:13:16.146097 1357 replica.cpp:712 ] persist action 2 i0108 01:13:16.146667 1358 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i0108 01:13:16.147060 1358 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 283648n i0108 01:13:16.147116 1358 leveldb.cpp:399 ] delet ~1 key leveldb took 32174n i0108 01:13:16.147135 1358 replica.cpp:712 ] persist action 2 i0108 01:13:16.147153 1358 replica.cpp:697 ] replica learn truncat action posit 2 i0108 01:13:16.166832 1325 containerizer.cpp:139 ] use isol : posix/cpu , posix/mem , filesystem/posix w0108 01:13:16.167556 1325 backend.cpp:48 ] fail creat 'bind ' backend : bindbackend requir root privileg i0108 01:13:16.170526 1349 slave.cpp:191 ] slave start 231 ) @ 172.17.0.2:32801 i0108 01:13:16.170718 1349 slave.cpp:192 ] flag startup : -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/persistentvolumetest_badaclnoprincipal_yqjjly/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' http : //auth.docker.io '' -- docker_kill_orphans= '' true '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' http : //registry-1.docker.io '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/persistentvolumetest_badaclnoprincipal_yqjjly/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.27.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk ( role1 ) :2048 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/persistentvolumetest_badaclnoprincipal_yqjjli '' i0108 01:13:16.171269 1349 credentials.hpp:83 ] load credenti authent '/tmp/persistentvolumetest_badaclnoprincipal_yqjjly/credential' i0108 01:13:16.171505 1349 slave.cpp:322 ] slave use credenti : test-princip i0108 01:13:16.171747 1349 resources.cpp:481 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk ( role1 ) :2048 tri semicolon-delimit string format instead i0108 01:13:16.172266 1349 slave.cpp:392 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] i0108 01:13:16.172327 1349 slave.cpp:400 ] slave attribut : [ ] i0108 01:13:16.172340 1349 slave.cpp:405 ] slave hostnam : d9632dd1c41e i0108 01:13:16.172353 1349 slave.cpp:410 ] slave checkpoint : true i0108 01:13:16.173418 1353 state.cpp:58 ] recov state '/tmp/persistentvolumetest_badaclnoprincipal_yqjjly/meta' i0108 01:13:16.173521 1325 sched.cpp:164 ] version : 0.27.0 i0108 01:13:16.174054 1345 status_update_manager.cpp:200 ] recov statu updat manag i0108 01:13:16.174289 1353 containerizer.cpp:387 ] recov container i0108 01:13:16.174295 1356 sched.cpp:268 ] new master detect master @ 172.17.0.2:32801 i0108 01:13:16.174387 1356 sched.cpp:278 ] no credenti provid . attempt regist without authent i0108 01:13:16.174409 1356 sched.cpp:722 ] send subscrib call master @ 172.17.0.2:32801 i0108 01:13:16.174515 1356 sched.cpp:755 ] will retri registr 1.699889272sec necessari i0108 01:13:16.174653 1349 master.cpp:2197 ] receiv subscrib call framework 'no-princip ' scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.174823 1349 master.cpp:1668 ] author framework princip `` receiv offer role 'role1' i0108 01:13:16.175250 1347 master.cpp:2268 ] subscrib framework no-princip checkpoint disabl capabl [ ] i0108 01:13:16.175359 1353 slave.cpp:4429 ] finish recoveri i0108 01:13:16.175715 1345 hierarchical.cpp:260 ] ad framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.175734 1351 sched.cpp:649 ] framework regist 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.175792 1345 hierarchical.cpp:1329 ] no resourc avail alloc ! i0108 01:13:16.175833 1345 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.175853 1353 slave.cpp:4601 ] queri resourc estim oversubscrib resourc i0108 01:13:16.175869 1345 hierarchical.cpp:1079 ] perform alloc 0 slave 127881n i0108 01:13:16.175923 1351 sched.cpp:663 ] schedul : :regist took 27956n i0108 01:13:16.176110 1353 slave.cpp:729 ] new master detect master @ 172.17.0.2:32801 i0108 01:13:16.176187 1353 slave.cpp:792 ] authent master master @ 172.17.0.2:32801 i0108 01:13:16.176216 1353 slave.cpp:797 ] use default cram-md5 authenticate i0108 01:13:16.176398 1357 status_update_manager.cpp:174 ] paus send statu updat i0108 01:13:16.176404 1353 slave.cpp:765 ] detect new master i0108 01:13:16.176463 1358 authenticatee.cpp:121 ] creat new client sasl connect i0108 01:13:16.176553 1353 slave.cpp:4615 ] receiv oversubscrib resourc resourc estim i0108 01:13:16.176709 1353 master.cpp:5445 ] authent slave ( 231 ) @ 172.17.0.2:32801 i0108 01:13:16.176823 1359 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 516 ) @ 172.17.0.2:32801 i0108 01:13:16.177135 1348 authenticator.cpp:98 ] creat new server sasl connect i0108 01:13:16.177373 1356 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i0108 01:13:16.177399 1356 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i0108 01:13:16.177502 1344 authenticator.cpp:203 ] receiv sasl authent start i0108 01:13:16.177563 1344 authenticator.cpp:325 ] authent requir step i0108 01:13:16.177680 1346 authenticatee.cpp:258 ] receiv sasl authent step i0108 01:13:16.177848 1354 authenticator.cpp:231 ] receiv sasl authent step i0108 01:13:16.177883 1354 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'd9632dd1c41e ' server fqdn : 'd9632dd1c41e ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0108 01:13:16.177894 1354 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i0108 01:13:16.177944 1354 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0108 01:13:16.177994 1354 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : 'd9632dd1c41e ' server fqdn : 'd9632dd1c41e ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0108 01:13:16.178014 1354 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0108 01:13:16.178040 1354 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0108 01:13:16.178066 1354 authenticator.cpp:317 ] authent success i0108 01:13:16.178256 1355 authenticatee.cpp:298 ] authent success i0108 01:13:16.178315 1354 master.cpp:5475 ] success authent princip 'test-princip ' slave ( 231 ) @ 172.17.0.2:32801 i0108 01:13:16.178356 1355 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 516 ) @ 172.17.0.2:32801 i0108 01:13:16.178710 1354 slave.cpp:860 ] success authent master master @ 172.17.0.2:32801 i0108 01:13:16.178865 1354 slave.cpp:1254 ] will retri registr 13.009431m necessari i0108 01:13:16.179138 1350 master.cpp:4154 ] regist slave slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 i0108 01:13:16.179628 1345 registrar.cpp:439 ] appli 1 oper 71663n ; attempt updat 'registry' i0108 01:13:16.180505 1356 log.cpp:683 ] attempt append 343 byte log i0108 01:13:16.180711 1352 coordinator.cpp:348 ] coordin attempt write append action posit 3 i0108 01:13:16.181499 1350 replica.cpp:537 ] replica receiv write request posit 3 ( 7103 ) @ 172.17.0.2:32801 i0108 01:13:16.182080 1350 leveldb.cpp:341 ] persist action ( 362 byte ) leveldb took 537757n i0108 01:13:16.182112 1350 replica.cpp:712 ] persist action 3 i0108 01:13:16.182749 1351 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i0108 01:13:16.183120 1351 leveldb.cpp:341 ] persist action ( 364 byte ) leveldb took 340999n i0108 01:13:16.183151 1351 replica.cpp:712 ] persist action 3 i0108 01:13:16.183177 1351 replica.cpp:697 ] replica learn append action posit 3 i0108 01:13:16.184787 1348 registrar.cpp:484 ] success updat 'registri ' 0n i0108 01:13:16.185287 1348 log.cpp:702 ] attempt truncat log 3 i0108 01:13:16.185484 1349 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i0108 01:13:16.186043 1353 slave.cpp:3371 ] receiv ping slave-observ ( 230 ) @ 172.17.0.2:32801 i0108 01:13:16.186074 1345 master.cpp:4222 ] regist slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] i0108 01:13:16.186224 1353 slave.cpp:904 ] regist master master @ 172.17.0.2:32801 ; given slave id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 i0108 01:13:16.186441 1353 fetcher.cpp:81 ] clear fetcher cach i0108 01:13:16.186486 1349 hierarchical.cpp:465 ] ad slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 ( d9632dd1c41e ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0108 01:13:16.186658 1346 status_update_manager.cpp:181 ] resum send statu updat i0108 01:13:16.186885 1353 slave.cpp:927 ] checkpoint slaveinfo '/tmp/persistentvolumetest_badaclnoprincipal_yqjjly/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0/slave.info' i0108 01:13:16.186905 1350 replica.cpp:537 ] replica receiv write request posit 4 ( 7104 ) @ 172.17.0.2:32801 i0108 01:13:16.187595 1350 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 645704n i0108 01:13:16.187628 1350 replica.cpp:712 ] persist action 4 i0108 01:13:16.188347 1349 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.188475 1349 hierarchical.cpp:1101 ] perform alloc slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 1.861833m i0108 01:13:16.188560 1348 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i0108 01:13:16.188385 1353 slave.cpp:963 ] forward total oversubscrib resourc i0108 01:13:16.189275 1344 master.cpp:5274 ] send 1 offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.189792 1344 master.cpp:4564 ] receiv updat slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) total oversubscrib resourc i0108 01:13:16.189851 1348 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 1.204958m i0108 01:13:16.190150 1348 leveldb.cpp:399 ] delet ~2 key leveldb took 62381n i0108 01:13:16.190265 1348 replica.cpp:712 ] persist action 4 i0108 01:13:16.190402 1348 replica.cpp:697 ] replica learn truncat action posit 4 i0108 01:13:16.191192 1349 sched.cpp:819 ] schedul : :resourceoff took 126783n i0108 01:13:16.191253 1359 hierarchical.cpp:521 ] slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 ( d9632dd1c41e ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ) i0108 01:13:16.191529 1359 hierarchical.cpp:1329 ] no resourc avail alloc ! i0108 01:13:16.191591 1359 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.191627 1359 hierarchical.cpp:1101 ] perform alloc slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 310808n i0108 01:13:16.195103 1349 hierarchical.cpp:1329 ] no resourc avail alloc ! i0108 01:13:16.195171 1349 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.195205 1349 hierarchical.cpp:1079 ] perform alloc 1 slave 368834n i0108 01:13:16.205402 1351 master.cpp:3055 ] process accept call offer : [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-o0 ] slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.205471 1351 master.cpp:2843 ] author princip 'ani ' creat volum e0108 01:13:16.206641 1351 master.cpp:1737 ] drop creat offer oper framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 : not author creat persist volum `` i0108 01:13:16.207283 1351 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.216485 1348 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.216562 1348 hierarchical.cpp:1079 ] perform alloc 1 slave 983574n i0108 01:13:16.216915 1345 master.cpp:5274 ] send 1 offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.217514 1345 sched.cpp:819 ] schedul : :resourceoff took 82354n i0108 01:13:16.227466 1348 master.cpp:3592 ] process declin call offer : [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-o1 ] framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.227843 1325 sched.cpp:164 ] version : 0.27.0 i0108 01:13:16.228489 1344 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.228989 1346 sched.cpp:268 ] new master detect master @ 172.17.0.2:32801 i0108 01:13:16.229118 1346 sched.cpp:278 ] no credenti provid . attempt regist without authent i0108 01:13:16.229143 1346 sched.cpp:722 ] send subscrib call master @ 172.17.0.2:32801 i0108 01:13:16.229277 1346 sched.cpp:755 ] will retri registr 1.383902465sec necessari i0108 01:13:16.229912 1348 master.cpp:2650 ] process suppress call framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.230171 1346 hierarchical.cpp:953 ] suppress offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.230262 1348 master.cpp:2197 ] receiv subscrib call framework 'default ' scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.230370 1348 master.cpp:1668 ] author framework princip 'test-princip ' receiv offer role 'role1' i0108 01:13:16.230788 1348 master.cpp:2268 ] subscrib framework default checkpoint disabl capabl [ ] i0108 01:13:16.231477 1346 hierarchical.cpp:260 ] ad framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:16.232698 1346 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.232795 1346 hierarchical.cpp:1079 ] perform alloc 1 slave 1.282992m i0108 01:13:16.233512 1348 master.cpp:5274 ] send 1 offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.233728 1351 sched.cpp:649 ] framework regist 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:16.233800 1351 sched.cpp:663 ] schedul : :regist took 29498n i0108 01:13:16.234381 1359 sched.cpp:819 ] schedul : :resourceoff took 113212n i0108 01:13:16.239941 1348 hierarchical.cpp:1329 ] no resourc avail alloc ! i0108 01:13:16.240223 1348 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.240275 1348 hierarchical.cpp:1079 ] perform alloc 1 slave 633949n i0108 01:13:16.251688 1357 master.cpp:3055 ] process accept call offer : [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-o2 ] slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.251785 1357 master.cpp:2843 ] author princip 'test-princip ' creat volum i0108 01:13:16.253445 1352 master.cpp:3384 ] appli creat oper volum disk ( role1 ) [ id1 : path1 ] :128 framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) i0108 01:13:16.253911 1352 master.cpp:6508 ] send checkpoint resourc disk ( role1 ) [ id1 : path1 ] :128 slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 slave ( 231 ) @ 172.17.0.2:32801 ( d9632dd1c41e ) i0108 01:13:16.255210 1352 slave.cpp:2277 ] updat checkpoint resourc disk ( role1 ) [ id1 : path1 ] :128 i0108 01:13:16.257128 1356 hierarchical.cpp:642 ] updat alloc framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 i0108 01:13:16.257844 1356 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :1920 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :128 , alloc : ) slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:16.262976 1344 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.263068 1344 hierarchical.cpp:1079 ] perform alloc 1 slave 1.435723m i0108 01:13:16.263535 1353 master.cpp:5274 ] send 1 offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.264181 1356 sched.cpp:819 ] schedul : :resourceoff took 139353n i0108 01:13:16.271931 1355 master.cpp:3671 ] process reviv call framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:16.272141 1359 hierarchical.cpp:973 ] remov offer filter framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:16.272177 1355 master.cpp:3592 ] process declin call offer : [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-o3 ] framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.272423 1359 hierarchical.cpp:1329 ] no resourc avail alloc ! i0108 01:13:16.272483 1359 hierarchical.cpp:1423 ] no invers offer send ! i0108 01:13:16.272514 1359 hierarchical.cpp:1079 ] perform alloc 1 slave 344563n i0108 01:13:16.272924 1355 master.cpp:2650 ] process suppress call framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:16.272989 1359 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :1920 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :128 , alloc : ) slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:16.273309 1359 hierarchical.cpp:953 ] suppress offer framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 2016-01-08 01:13:18,959:1325 ( 0x7fb7cd6ae700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:50826 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2016-01-08 01:13:22,295:1325 ( 0x7fb7cd6ae700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:50826 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2016-01-08 01:13:25,631:1325 ( 0x7fb7cd6ae700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:50826 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2016-01-08 01:13:28,968:1325 ( 0x7fb7cd6ae700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:50826 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client .. / .. /src/tests/persistent_volume_tests.cpp:1211 : failur fail wait 15sec offer i0108 01:13:31.277577 1354 master.cpp:1130 ] framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 disconnect i .. / .. /src/tests/persistent_volume_tests.cpp:1204 : failur actual function call count n't match expect_cal ( sched1 , resourceoff ( & driver1 , _ ) ) ... expect : call actual : never call - unsatisfi activ 0108 01:13:31.277909 1354 master.cpp:2493 ] disconnect framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:31.279088 1354 master.cpp:2517 ] deactiv framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 i0108 01:13:31.279496 1354 master.cpp:1154 ] give framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 ( default ) scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3 @ 172.17.0.2:32801 0n failov i0108 01:13:31.280046 1354 master.cpp:1130 ] framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 disconnect i0108 01:13:31.280603 1354 master.cpp:2493 ] disconnect framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:31.280644 1354 master.cpp:2517 ] deactiv framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 i0108 01:13:31.280863 1354 master.cpp:1154 ] give framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 ( no-princip ) scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbc @ 172.17.0.2:32801 0n failov i0108 01:13:31.280563 1348 hierarchical.cpp:366 ] deactiv framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:31.281056 1348 hierarchical.cpp:366 ] deactiv framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:31.281097 1354 master.cpp:930 ] master termin i0108 01:13:31.281910 1355 hierarchical.cpp:496 ] remov slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-s0 i0108 01:13:31.282516 1352 hierarchical.cpp:321 ] remov framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 i0108 01:13:31.282817 1352 hierarchical.cpp:321 ] remov framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 i0108 01:13:31.282985 1352 slave.cpp:3417 ] master @ 172.17.0.2:32801 exit w0108 01:13:31.283144 1352 slave.cpp:3420 ] master disconnect ! wait new master elect i0108 01:13:31.313812 1346 slave.cpp:601 ] slave termin [ fail ] persistentvolumetest.badaclnoprincip ( 15203 ms ) { noformat }",MESOS-4318,1.0
"accept invers offer print mislead log whenev schedul accept invers offer , meso print line like master log : { code } w1125 10:05:53.155109 29362 master.cpp:2897 ] accept call use invalid offer ' [ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-o2 ] ' : offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-o2 longer valid { code } invers offer trigger warn .",MESOS-4301,1.0
"fs : :enter ( rootf ) work 'rootf ' read . i notic i test unifi container bind mount backend volum . the current implement fs : :enter put old root /tmp/._old_root_.xxxxxx new rootf . it assum /tmp writabl new rootf , might true , especi bind mount backend use . to solv problem , mount tmpf /tmp new rootf umount pivot_root .",MESOS-4291,2.0
meso command task n't support volum imag current volum strip imag specifi run command task meso container .,MESOS-4285,3.0
"updat isol prepar function use containerlaunchinfo current isol 's prepar function return containerprepareinfo protobuf . we enabl containerlaunchinfo ( contain environ variabl , namespac , etc . ) return use meso container launch contain . by ( containerprepareinfo - > containerlaunchinfo ) , select necessari inform pass launcher .",MESOS-4282,2.0
correctli handl disk quota usag volum bind mount contain . in current implement disk quota enforc task sandbox work correctli disk volum bind mount task sandbox ( happen linux filesystem isol use ) .,MESOS-4281,3.0
report volum usag resourcestatist . posix disk isol current report volum usag resourcestatist . { { posixdiskisolatorprocess : :usag ( ) } } amend take account volum usag well .,MESOS-4263,3.0
"add mechan test recoveri http base executor current , slave process gener process id everi time initi via { { process : :id : :gener } } function call . thi problem test http executor ca n't retri disconnect agent restart sinc prefix increment . { code } agent pid : slave ( 1 ) @ 127.0.0.1:43915 agent pid restart : slave ( 2 ) @ 127.0.0.1:43915 { code } there coupl way fix : - add constructor { { slave } } exclus test pass fix { { id } } instead reli { { id : :gener } } . - current deleg slave ( 1 ) @ i.e . ( 1 ) noth specifi url libprocess i.e . { { 127.0.0.1:43915/api/v1/executor } } would deleg { { slave ( 1 ) @ 127.0.0.1:43915/api/v1/executor } } . instead default ( 1 ) , default last known activ id .",MESOS-4255,5.0
add ` dist ` target cmake solut,MESOS-4245,3.0
test quota statu endpoint,MESOS-4218,3.0
"persistentvolumetest.badacldropcreateanddestroy flaki { noformat } [ run ] persistentvolumetest.badacldropcreateanddestroy i1219 09:51:32.623245 31878 leveldb.cpp:174 ] open db 4.393596m i1219 09:51:32.624084 31878 leveldb.cpp:181 ] compact db 709447n i1219 09:51:32.624186 31878 leveldb.cpp:196 ] creat db iter 21252n i1219 09:51:32.624290 31878 leveldb.cpp:202 ] seek begin db 11391n i1219 09:51:32.624378 31878 leveldb.cpp:271 ] iter 0 key db 611n i1219 09:51:32.624505 31878 replica.cpp:779 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i1219 09:51:32.625195 31904 recover.cpp:447 ] start replica recoveri i1219 09:51:32.625641 31904 recover.cpp:473 ] replica empti statu i1219 09:51:32.627305 31904 replica.cpp:673 ] replica empti statu receiv broadcast recov request ( 6740 ) @ 172.17.0.3:36408 i1219 09:51:32.627749 31904 recover.cpp:193 ] receiv recov respons replica empti statu i1219 09:51:32.628330 31904 recover.cpp:564 ] updat replica statu start i1219 09:51:32.629068 31906 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 410494n i1219 09:51:32.629169 31906 replica.cpp:320 ] persist replica statu start i1219 09:51:32.629598 31906 recover.cpp:473 ] replica start statu i1219 09:51:32.630782 31912 replica.cpp:673 ] replica start statu receiv broadcast recov request ( 6741 ) @ 172.17.0.3:36408 i1219 09:51:32.631166 31901 recover.cpp:193 ] receiv recov respons replica start statu i1219 09:51:32.632467 31902 recover.cpp:564 ] updat replica statu vote i1219 09:51:32.633600 31907 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 311370n i1219 09:51:32.633627 31907 replica.cpp:320 ] persist replica statu vote i1219 09:51:32.633719 31907 recover.cpp:578 ] success join paxo group i1219 09:51:32.633874 31907 recover.cpp:462 ] recov process termin i1219 09:51:32.636409 31909 master.cpp:365 ] master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 ( 60ab6e727501 ) start 172.17.0.3:36408 i1219 09:51:32.636593 31909 master.cpp:367 ] flag startup : -- acls= '' create_volum { princip { valu : `` creator-princip '' } volume_typ { type : ani } } create_volum { princip { type : ani } volume_typ { type : none } } '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' fals '' -- authenticate_slaves= '' true '' -- authenticators= '' crammd5 '' -- authorizers= '' local '' -- credentials= '' /tmp/sppf7b/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- hostname_lookup= '' true '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 25sec '' -- registry_strict= '' true '' -- roles= '' role1 '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.27.0/_inst/share/mesos/webui '' -- work_dir= '' /tmp/sppf7b/master '' -- zk_session_timeout= '' 10sec '' i1219 09:51:32.637055 31909 master.cpp:414 ] master allow unauthent framework regist i1219 09:51:32.637068 31909 master.cpp:417 ] master allow authent slave regist i1219 09:51:32.637094 31909 credentials.hpp:35 ] load credenti authent '/tmp/sppf7b/credentials' i1219 09:51:32.637403 31909 master.cpp:456 ] use default 'crammd5 ' authent i1219 09:51:32.637555 31909 master.cpp:493 ] author enabl w1219 09:51:32.637575 31909 master.cpp:553 ] the ' -- role ' flag deprec . thi flag remov futur . see meso 0.27 upgrad note inform i1219 09:51:32.637806 31897 whitelist_watcher.cpp:77 ] no whitelist given i1219 09:51:32.637820 31910 hierarchical.cpp:147 ] initi hierarch alloc process i1219 09:51:32.639677 31909 master.cpp:1629 ] the newli elect leader master @ 172.17.0.3:36408 id bded856d-1c7f-4fad-a8bc-3629ba8c59d3 i1219 09:51:32.639768 31909 master.cpp:1642 ] elect lead master ! i1219 09:51:32.639892 31909 master.cpp:1387 ] recov registrar i1219 09:51:32.640136 31907 registrar.cpp:307 ] recov registrar i1219 09:51:32.640929 31901 log.cpp:659 ] attempt start writer i1219 09:51:32.642199 31912 replica.cpp:493 ] replica receiv implicit promis request ( 6742 ) @ 172.17.0.3:36408 propos 1 i1219 09:51:32.642719 31912 leveldb.cpp:304 ] persist metadata ( 8 byte ) leveldb took 445876n i1219 09:51:32.642755 31912 replica.cpp:342 ] persist promis 1 i1219 09:51:32.643478 31904 coordinator.cpp:238 ] coordin attempt fill miss posit i1219 09:51:32.645009 31909 replica.cpp:388 ] replica receiv explicit promis request ( 6743 ) @ 172.17.0.3:36408 posit 0 propos 2 i1219 09:51:32.645356 31909 leveldb.cpp:341 ] persist action ( 8 byte ) leveldb took 310064n i1219 09:51:32.645382 31909 replica.cpp:712 ] persist action 0 i1219 09:51:32.646662 31909 replica.cpp:537 ] replica receiv write request posit 0 ( 6744 ) @ 172.17.0.3:36408 i1219 09:51:32.646721 31909 leveldb.cpp:436 ] read posit leveldb took 29298n i1219 09:51:32.647047 31909 leveldb.cpp:341 ] persist action ( 14 byte ) leveldb took 283424n i1219 09:51:32.647073 31909 replica.cpp:712 ] persist action 0 i1219 09:51:32.647722 31909 replica.cpp:691 ] replica receiv learn notic posit 0 @ 0.0.0.0:0 i1219 09:51:32.648052 31909 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 300825n i1219 09:51:32.648077 31909 replica.cpp:712 ] persist action 0 i1219 09:51:32.648095 31909 replica.cpp:697 ] replica learn nop action posit 0 i1219 09:51:32.655295 31899 log.cpp:675 ] writer start end posit 0 i1219 09:51:32.656543 31905 leveldb.cpp:436 ] read posit leveldb took 32788n i1219 09:51:32.658164 31905 registrar.cpp:340 ] success fetch registri ( 0b ) 0n i1219 09:51:32.658604 31905 registrar.cpp:439 ] appli 1 oper 38183n ; attempt updat 'registry' i1219 09:51:32.660102 31905 log.cpp:683 ] attempt append 170 byte log i1219 09:51:32.660538 31906 coordinator.cpp:348 ] coordin attempt write append action posit 1 i1219 09:51:32.661872 31906 replica.cpp:537 ] replica receiv write request posit 1 ( 6745 ) @ 172.17.0.3:36408 i1219 09:51:32.662719 31906 leveldb.cpp:341 ] persist action ( 189 byte ) leveldb took 483018n i1219 09:51:32.663054 31906 replica.cpp:712 ] persist action 1 i1219 09:51:32.664008 31902 replica.cpp:691 ] replica receiv learn notic posit 1 @ 0.0.0.0:0 i1219 09:51:32.664330 31902 leveldb.cpp:341 ] persist action ( 191 byte ) leveldb took 287310n i1219 09:51:32.664355 31902 replica.cpp:712 ] persist action 1 i1219 09:51:32.664376 31902 replica.cpp:697 ] replica learn append action posit 1 i1219 09:51:32.665365 31902 registrar.cpp:484 ] success updat 'registri ' 0n i1219 09:51:32.665493 31902 registrar.cpp:370 ] success recov registrar i1219 09:51:32.665894 31902 master.cpp:1439 ] recov 0 slave registri ( 131b ) ; allow 10min slave re-regist i1219 09:51:32.665990 31902 hierarchical.cpp:165 ] skip recoveri hierarch alloc : noth recov i1219 09:51:32.666266 31902 log.cpp:702 ] attempt truncat log 1 i1219 09:51:32.666424 31902 coordinator.cpp:348 ] coordin attempt write truncat action posit 2 i1219 09:51:32.667181 31907 replica.cpp:537 ] replica receiv write request posit 2 ( 6746 ) @ 172.17.0.3:36408 i1219 09:51:32.667768 31907 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 335947n i1219 09:51:32.668067 31907 replica.cpp:712 ] persist action 2 i1219 09:51:32.668942 31906 replica.cpp:691 ] replica receiv learn notic posit 2 @ 0.0.0.0:0 i1219 09:51:32.669240 31906 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 266566n i1219 09:51:32.669292 31906 leveldb.cpp:399 ] delet ~1 key leveldb took 27852n i1219 09:51:32.669314 31906 replica.cpp:712 ] persist action 2 i1219 09:51:32.669334 31906 replica.cpp:697 ] replica learn truncat action posit 2 i1219 09:51:32.691251 31878 containerizer.cpp:141 ] use isol : posix/cpu , posix/mem , filesystem/posix w1219 09:51:32.691759 31878 backend.cpp:48 ] fail creat 'bind ' backend : bindbackend requir root privileg i1219 09:51:32.697428 31901 slave.cpp:191 ] slave start 228 ) @ 172.17.0.3:36408 i1219 09:51:32.697459 31901 slave.cpp:192 ] flag startup : -- appc_store_dir= '' /tmp/mesos/store/appc '' -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- credential= '' /tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc/credenti '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_auth_server= '' auth.docker.io '' -- docker_auth_server_port= '' 443 '' -- docker_kill_orphans= '' true '' -- docker_local_archives_dir= '' /tmp/mesos/images/dock '' -- docker_puller= '' local '' -- docker_puller_timeout= '' 60 '' -- docker_registry= '' registry-1.docker.io '' -- docker_registry_port= '' 443 '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- docker_store_dir= '' /tmp/mesos/store/dock '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- hostname_lookup= '' true '' -- image_provisioner_backend= '' copi '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.27.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 10m '' -- resources= '' cpus:2 ; mem:1024 ; disk ( role1 ) :2048 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- systemd_runtime_directory= '' /run/systemd/system '' -- version= '' fals '' -- work_dir= '' /tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc '' i1219 09:51:32.697963 31901 credentials.hpp:83 ] load credenti authent '/tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc/credential' i1219 09:51:32.698210 31901 slave.cpp:322 ] slave use credenti : test-princip i1219 09:51:32.698449 31901 resources.cpp:478 ] pars resourc json fail : cpus:2 ; mem:1024 ; disk ( role1 ) :2048 tri semicolon-delimit string format instead i1219 09:51:32.699065 31901 slave.cpp:392 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] i1219 09:51:32.699137 31901 slave.cpp:400 ] slave attribut : [ ] i1219 09:51:32.699151 31901 slave.cpp:405 ] slave hostnam : 60ab6e727501 i1219 09:51:32.699161 31901 slave.cpp:410 ] slave checkpoint : true i1219 09:51:32.699364 31878 sched.cpp:164 ] version : 0.27.0 i1219 09:51:32.700614 31911 sched.cpp:262 ] new master detect master @ 172.17.0.3:36408 i1219 09:51:32.700703 31911 sched.cpp:272 ] no credenti provid . attempt regist without authent i1219 09:51:32.700724 31911 sched.cpp:714 ] send subscrib call master @ 172.17.0.3:36408 i1219 09:51:32.700839 31911 sched.cpp:747 ] will retri registr 620.399428m necessari i1219 09:51:32.701244 31903 master.cpp:2197 ] receiv subscrib call framework 'default ' scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.701313 31903 master.cpp:1668 ] author framework princip 'test-princip ' receiv offer role 'role1' i1219 09:51:32.701625 31903 master.cpp:2268 ] subscrib framework default checkpoint disabl capabl [ ] i1219 09:51:32.702308 31903 hierarchical.cpp:260 ] ad framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.702386 31903 hierarchical.cpp:1329 ] no resourc avail alloc ! i1219 09:51:32.702422 31903 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.702448 31903 hierarchical.cpp:1079 ] perform alloc 0 slave 114358n i1219 09:51:32.702638 31903 sched.cpp:641 ] framework regist bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.702688 31903 sched.cpp:655 ] schedul : :regist took 25558n i1219 09:51:32.703553 31901 state.cpp:58 ] recov state '/tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc/meta' i1219 09:51:32.704118 31897 status_update_manager.cpp:200 ] recov statu updat manag i1219 09:51:32.704407 31907 containerizer.cpp:383 ] recov container i1219 09:51:32.705373 31907 slave.cpp:4427 ] finish recoveri i1219 09:51:32.705991 31907 slave.cpp:4599 ] queri resourc estim oversubscrib resourc i1219 09:51:32.706277 31907 slave.cpp:4613 ] receiv oversubscrib resourc resourc estim i1219 09:51:32.706666 31907 slave.cpp:729 ] new master detect master @ 172.17.0.3:36408 i1219 09:51:32.706738 31907 slave.cpp:792 ] authent master master @ 172.17.0.3:36408 i1219 09:51:32.706760 31907 slave.cpp:797 ] use default cram-md5 authenticate i1219 09:51:32.706886 31899 status_update_manager.cpp:174 ] paus send statu updat i1219 09:51:32.706941 31907 slave.cpp:765 ] detect new master i1219 09:51:32.707036 31899 authenticatee.cpp:121 ] creat new client sasl connect i1219 09:51:32.707291 31910 master.cpp:5423 ] authent slave ( 228 ) @ 172.17.0.3:36408 i1219 09:51:32.707479 31910 authenticator.cpp:413 ] start authent session crammd5_authenticate ( 510 ) @ 172.17.0.3:36408 i1219 09:51:32.707849 31910 authenticator.cpp:98 ] creat new server sasl connect i1219 09:51:32.708082 31910 authenticatee.cpp:212 ] receiv sasl authent mechan : cram-md5 i1219 09:51:32.708112 31910 authenticatee.cpp:238 ] attempt authent mechan 'cram-md5' i1219 09:51:32.708196 31910 authenticator.cpp:203 ] receiv sasl authent start i1219 09:51:32.708395 31910 authenticator.cpp:325 ] authent requir step i1219 09:51:32.708611 31902 authenticatee.cpp:258 ] receiv sasl authent step i1219 09:51:32.708773 31910 authenticator.cpp:231 ] receiv sasl authent step i1219 09:51:32.708889 31910 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '60ab6e727501 ' server fqdn : '60ab6e727501 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i1219 09:51:32.708976 31910 auxprop.cpp:179 ] look auxiliari properti ' * userpassword' i1219 09:51:32.709096 31910 auxprop.cpp:179 ] look auxiliari properti ' * cmusaslsecretcram-md5' i1219 09:51:32.709200 31910 auxprop.cpp:107 ] request lookup properti user : 'test-princip ' realm : '60ab6e727501 ' server fqdn : '60ab6e727501 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i1219 09:51:32.709285 31910 auxprop.cpp:129 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i1219 09:51:32.709363 31910 auxprop.cpp:129 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i1219 09:51:32.709452 31910 authenticator.cpp:317 ] authent success i1219 09:51:32.709707 31910 authenticatee.cpp:298 ] authent success i1219 09:51:32.710252 31910 slave.cpp:860 ] success authent master master @ 172.17.0.3:36408 i1219 09:51:32.710525 31910 slave.cpp:1254 ] will retri registr 17.44437m necessari i1219 09:51:32.709839 31908 master.cpp:5453 ] success authent princip 'test-princip ' slave ( 228 ) @ 172.17.0.3:36408 i1219 09:51:32.710985 31908 master.cpp:4132 ] regist slave slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 i1219 09:51:32.711645 31908 registrar.cpp:439 ] appli 1 oper 83191n ; attempt updat 'registry' i1219 09:51:32.709908 31912 authenticator.cpp:431 ] authent session cleanup crammd5_authenticate ( 510 ) @ 172.17.0.3:36408 i1219 09:51:32.713407 31908 log.cpp:683 ] attempt append 343 byte log i1219 09:51:32.713646 31912 coordinator.cpp:348 ] coordin attempt write append action posit 3 i1219 09:51:32.714884 31911 replica.cpp:537 ] replica receiv write request posit 3 ( 6758 ) @ 172.17.0.3:36408 i1219 09:51:32.715221 31911 leveldb.cpp:341 ] persist action ( 362 byte ) leveldb took 288909n i1219 09:51:32.715250 31911 replica.cpp:712 ] persist action 3 i1219 09:51:32.716145 31912 replica.cpp:691 ] replica receiv learn notic posit 3 @ 0.0.0.0:0 i1219 09:51:32.716689 31912 leveldb.cpp:341 ] persist action ( 364 byte ) leveldb took 512217n i1219 09:51:32.716716 31912 replica.cpp:712 ] persist action 3 i1219 09:51:32.716737 31912 replica.cpp:697 ] replica learn append action posit 3 i1219 09:51:32.718426 31911 registrar.cpp:484 ] success updat 'registri ' 0n i1219 09:51:32.719441 31902 slave.cpp:3371 ] receiv ping slave-observ ( 228 ) @ 172.17.0.3:36408 i1219 09:51:32.719843 31909 log.cpp:702 ] attempt truncat log 3 i1219 09:51:32.719908 31911 master.cpp:4200 ] regist slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] i1219 09:51:32.720064 31911 slave.cpp:904 ] regist master master @ 172.17.0.3:36408 ; given slave id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 i1219 09:51:32.720088 31911 fetcher.cpp:81 ] clear fetcher cach i1219 09:51:32.720491 31911 slave.cpp:927 ] checkpoint slaveinfo '/tmp/persistentvolumetest_badacldropcreateanddestroy_gwltnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0/slave.info' i1219 09:51:32.720844 31909 coordinator.cpp:348 ] coordin attempt write truncat action posit 4 i1219 09:51:32.720929 31911 slave.cpp:963 ] forward total oversubscrib resourc i1219 09:51:32.721017 31903 status_update_manager.cpp:181 ] resum send statu updat i1219 09:51:32.721099 31911 master.cpp:4542 ] receiv updat slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) total oversubscrib resourc i1219 09:51:32.721141 31905 hierarchical.cpp:465 ] ad slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 ( 60ab6e727501 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i1219 09:51:32.721879 31911 replica.cpp:537 ] replica receiv write request posit 4 ( 6759 ) @ 172.17.0.3:36408 i1219 09:51:32.722293 31905 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.722337 31905 hierarchical.cpp:1101 ] perform alloc slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 1.155563m i1219 09:51:32.722681 31905 hierarchical.cpp:521 ] slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 ( 60ab6e727501 ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ) i1219 09:51:32.722713 31909 master.cpp:5252 ] send 1 offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.723031 31905 hierarchical.cpp:1329 ] no resourc avail alloc ! i1219 09:51:32.723073 31905 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.723095 31905 hierarchical.cpp:1101 ] perform alloc slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 368889n i1219 09:51:32.723191 31909 sched.cpp:811 ] schedul : :resourceoff took 113921n i1219 09:51:32.723410 31911 leveldb.cpp:341 ] persist action ( 16 byte ) leveldb took 1.418243m i1219 09:51:32.723497 31911 replica.cpp:712 ] persist action 4 i1219 09:51:32.724326 31907 replica.cpp:691 ] replica receiv learn notic posit 4 @ 0.0.0.0:0 i1219 09:51:32.724758 31907 leveldb.cpp:341 ] persist action ( 18 byte ) leveldb took 329678n i1219 09:51:32.724917 31907 leveldb.cpp:399 ] delet ~2 key leveldb took 58317n i1219 09:51:32.725025 31907 replica.cpp:712 ] persist action 4 i1219 09:51:32.725127 31907 replica.cpp:697 ] replica learn truncat action posit 4 i1219 09:51:32.731515 31910 hierarchical.cpp:1329 ] no resourc avail alloc ! i1219 09:51:32.731564 31910 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.731591 31910 hierarchical.cpp:1079 ] perform alloc 1 slave 239271n i1219 09:51:32.741710 31910 master.cpp:3055 ] process accept call offer : [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-o0 ] slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.741770 31910 master.cpp:2843 ] author princip 'test-princip ' creat volum e1219 09:51:32.742707 31910 master.cpp:1737 ] drop creat offer oper framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 : not author creat persist volum 'test-principal' i1219 09:51:32.743219 31910 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.752542 31908 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.752590 31908 hierarchical.cpp:1079 ] perform alloc 1 slave 888401n i1219 09:51:32.753018 31908 master.cpp:5252 ] send 1 offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.753435 31908 sched.cpp:811 ] schedul : :resourceoff took 92252n i1219 09:51:32.761533 31878 sched.cpp:164 ] version : 0.27.0 i1219 09:51:32.761931 31897 master.cpp:3570 ] process declin call offer : [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-o1 ] framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.762373 31897 sched.cpp:262 ] new master detect master @ 172.17.0.3:36408 i1219 09:51:32.762451 31897 sched.cpp:272 ] no credenti provid . attempt regist without authent i1219 09:51:32.762470 31897 sched.cpp:714 ] send subscrib call master @ 172.17.0.3:36408 i1219 09:51:32.762543 31897 sched.cpp:747 ] will retri registr 465.481193m necessari i1219 09:51:32.762572 31898 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :2048 ; port ( * ) : [ 31000-32000 ] , alloc : ) slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.762722 31898 master.cpp:2197 ] receiv subscrib call framework 'creator-framework ' scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.762785 31898 master.cpp:1668 ] author framework princip 'creator-princip ' receiv offer role 'role1' i1219 09:51:32.763036 31897 master.cpp:2268 ] subscrib framework creator-framework checkpoint disabl capabl [ ] i1219 09:51:32.763464 31898 hierarchical.cpp:260 ] ad framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:32.763562 31897 sched.cpp:641 ] framework regist bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:32.763605 31897 sched.cpp:655 ] schedul : :regist took 20669n i1219 09:51:32.763804 31908 master.cpp:2650 ] process suppress call framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.764343 31898 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.764382 31898 hierarchical.cpp:1079 ] perform alloc 1 slave 893765n i1219 09:51:32.764428 31898 hierarchical.cpp:953 ] suppress offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.764746 31898 master.cpp:5252 ] send 1 offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.765127 31898 sched.cpp:811 ] schedul : :resourceoff took 83608n i1219 09:51:32.773298 31900 hierarchical.cpp:1329 ] no resourc avail alloc ! i1219 09:51:32.773339 31900 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.773365 31900 hierarchical.cpp:1079 ] perform alloc 1 slave 201759n i1219 09:51:32.782901 31898 master.cpp:3055 ] process accept call offer : [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-o2 ] slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.782961 31898 master.cpp:2843 ] author princip 'creator-princip ' creat volum i1219 09:51:32.784190 31904 master.cpp:3362 ] appli creat oper volum disk ( role1 ) [ id1 : path1 ] :128 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) i1219 09:51:32.784548 31904 master.cpp:6486 ] send checkpoint resourc disk ( role1 ) [ id1 : path1 ] :128 slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 slave ( 228 ) @ 172.17.0.3:36408 ( 60ab6e727501 ) i1219 09:51:32.786471 31904 hierarchical.cpp:642 ] updat alloc framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :2048 cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 i1219 09:51:32.786929 31904 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :1920 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :128 , alloc : ) slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:32.788035 31904 slave.cpp:2277 ] updat checkpoint resourc disk ( role1 ) [ id1 : path1 ] :128 i1219 09:51:32.795177 31902 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.795250 31902 hierarchical.cpp:1079 ] perform alloc 1 slave 1.357898m i1219 09:51:32.795897 31902 master.cpp:5252 ] send 1 offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.796540 31897 sched.cpp:811 ] schedul : :resourceoff took 138880n i1219 09:51:32.803026 31902 master.cpp:3570 ] process declin call offer : [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-o3 ] framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.804143 31902 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :1920 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :128 , alloc : ) slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:32.804622 31907 master.cpp:2650 ] process suppress call framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:32.804729 31907 hierarchical.cpp:953 ] suppress offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:32.805140 31897 master.cpp:3649 ] process reviv call framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:32.805250 31897 hierarchical.cpp:973 ] remov offer filter framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:32.806507 31897 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.806562 31897 hierarchical.cpp:1079 ] perform alloc 1 slave 1.284779m i1219 09:51:32.807067 31897 master.cpp:5252 ] send 1 offer framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 .. / .. /src/tests/persistent_volume_tests.cpp:1336 : failur mock function call time expect - return directli . function call : resourceoff ( 0x7ffff9edb3a0 , @ 0x7f71079798f0 { 144-byte object < f0-1b 42-14 71-7f 00-00 00-00 00-00 00-00 00-00 d0-96 02-f0 70-7f 00-00 50-97 02-f0 70-7f 00-00 20-a1 02-f0 70-7f 00-00 50-e0 01-f0 70-7f 00-00 b0-9f 02-f0 70-7f 00-00 00-32 01-f0 70-7f 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7f 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1f-00 00-00 > } ) expect : call actual : call twice - over-satur activ i1219 09:51:32.807899 31897 sched.cpp:811 ] schedul : :resourceoff took 406435n i1219 09:51:32.820523 31909 hierarchical.cpp:1329 ] no resourc avail alloc ! i1219 09:51:32.820611 31909 hierarchical.cpp:1423 ] no invers offer send ! i1219 09:51:32.820642 31909 hierarchical.cpp:1079 ] perform alloc 1 slave 448034n 2015-12-19 09:51:33,146:31878 ( 0x7f6ff6ffd700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:39991 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2015-12-19 09:51:36,482:31878 ( 0x7f6ff6ffd700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:39991 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2015-12-19 09:51:39,818:31878 ( 0x7f6ff6ffd700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:39991 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2015-12-19 09:51:43,155:31878 ( 0x7f6ff6ffd700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:39991 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client 2015-12-19 09:51:46,490:31878 ( 0x7f6ff6ffd700 ) : zoo_error @ handle_socket_error_msg @ 1697 : socket [ 127.0.0.1:39991 ] zk retcode=-4 , errno=111 ( connect refus ) : server refus accept client .. / .. /src/tests/persistent_volume_tests.cpp:1411 : failur fail wait 15sec offer i1219 09:51:47.829073 31909 master.cpp:1130 ] framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 disconnect i1219 09:51:47.829169 31909 master.cpp:2493 ] disconnect framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:47.829200 31909 master.cpp:2517 ] deactiv framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:47.829366 31909 master.cpp:1154 ] give framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 0n failov i1219 09:51:47.829720 31909 hierarchical.cpp:366 ] deactiv framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:47.831614 31907 master.cpp:5100 ] framework failov timeout , remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:47.831748 31907 master.cpp:5835 ] remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 ( creator-framework ) scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5c @ 172.17.0.3:36408 i1219 09:51:47.833314 31897 slave.cpp:2012 ] ask shut framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 master @ 172.17.0.3:36408 w1219 09:51:47.833421 31897 slave.cpp:2027 ] can shut unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:47.834002 31897 hierarchical.cpp:321 ] remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 i1219 09:51:47.843332 31908 master.cpp:1130 ] framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 disconnect i1219 09:51:47.843521 31908 master.cpp:2493 ] disconnect framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:47.843663 31908 master.cpp:2517 ] deactiv framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 w1219 09:51:47.844665 31908 master.hpp:1758 ] master attempt send messag disconnect framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:47.845077 31908 master.cpp:1154 ] give framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 0n failov i1219 09:51:47.844887 31903 hierarchical.cpp:366 ] deactiv framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:47.845728 31903 hierarchical.cpp:880 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) :1920 ; disk ( role1 ) [ id1 : path1 ] :128 ( total : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( role1 ) :1920 ; port ( * ) : [ 31000-32000 ] ; disk ( role1 ) [ id1 : path1 ] :128 , alloc : ) slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 .. / .. /src/tests/persistent_volume_tests.cpp:1404 : failur actual function call count n't match expect_cal ( sched1 , resourceoff ( & driver1 , _ ) ) ... expect : call actual : never call - unsatisfi activ i1219 09:51:47.847968 31902 master.cpp:5100 ] framework failov timeout , remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:47.848068 31902 master.cpp:5835 ] remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 ( default ) scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879 @ 172.17.0.3:36408 i1219 09:51:47.848553 31902 slave.cpp:2012 ] ask shut framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 master @ 172.17.0.3:36408 w1219 09:51:47.848644 31902 slave.cpp:2027 ] can shut unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:47.848999 31902 hierarchical.cpp:321 ] remov framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 i1219 09:51:47.849782 31912 master.cpp:930 ] master termin i1219 09:51:47.851934 31899 hierarchical.cpp:496 ] remov slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-s0 i1219 09:51:47.855919 31907 slave.cpp:3417 ] master @ 172.17.0.3:36408 exit w1219 09:51:47.856021 31907 slave.cpp:3420 ] master disconnect ! wait new master elect i1219 09:51:47.908278 31878 slave.cpp:601 ] slave termin [ fail ] persistentvolumetest.badacldropcreateanddestroy ( 15298 ms ) { noformat }",MESOS-4208,1.0
"race ssl socket shutdown libprocess socket share ownership file descriptor libev . in destructor libprocess libevent_ssl socket , call ssl shutdown execut asynchron . thi caus libprocess socket file descriptor tobe close ( possibl reus ) file descriptor could use bylibevent/ssl . sinc set shutdown option ssl_received_shutdown , leav write oper continu possibl close file descriptor . thi issu manifest junk charact written file handl close socket file descriptor ( os ) issu .",MESOS-4202,5.0
"disk resourc reserv not enforc persist volum if i creat persist volum reserv disk resourc , i abl write data excess reserv size . disk resourc reserv enforc `` cpu '' `` mem '' reserv enforc .",MESOS-4198,3.0
port ` process/file.hpp `,MESOS-4193,3.0
"add document api version current , n't document : - how meso implement api version ? - how protobuf version meso handl intern ? - what contributor need make chang extern user face protobuf ? the relev design doc : http : //docs.google.com/document/d/1-iqjo6778h_fu_1zi_yk6szg8qj-wqygvgnx7u3h6ou/edit # heading=h.2gkbjz6amn7b",MESOS-4192,3.0
extend ` master ` author persist volum thi ticket second seri add author support persist volum . method { { master : :authorizecreatevolum ( ) } } { { master : :authorizedestroyvolum } } must ad allow master author oper .,MESOS-4179,1.0
"add persist volum support author thi ticket first seri add author support persist volum creation destruct . persist volum author { { princip } } reserv entiti ( framework master ) . the idea introduc { { creat } } { { destroy } } acl . { code } messag creat { // subject . requir entiti princip = 1 ; // object ? perhap kind volum ? allow permiss ? } messag destroy { // subject . requir entiti princip = 1 ; // object . requir entiti creator_princip = 2 ; } { code } acl volum creation destruct must ad { { authorizer.proto } } , appropri function overload must ad author .",MESOS-4178,1.0
creat user doc executor http api we need user doc similar correspond one schedul http api .,MESOS-4177,3.0
"reserve/unreserv dynam reserv endpoint allow reserv non-exist role when work dynam reserv via /reserv /unreserv endpoint , possibl reserv resourc role specifi via -- role flag master . howev , role usabl role defin , ad list role avail . per mail list , chang role fact possibl time . ( that may anoth jira ) , importantli , /reserv /unreserv end point allow reserv role specifi -- role .",MESOS-4143,2.0
"implement ` windowserror ` correspond ` errnoerror ` . in c standard librari , ` errno ` record last error thread . you pretty-print ` strerror ` . in stout , report error ` errnoerror ` . the window api someth similar , call ` getlasterror ( ) ` . the way pretty-print hilari unintuit terribl , case actual benefici wrap someth similar ` errnoerror ` , mayb call ` windowserror ` .",MESOS-4110,5.0
"implement ` os : :mkdtemp ` window use basic exclus test , insecur otherwise-not-quite-suitable-for-prod function need work run eventu becom fs test .",MESOS-4108,5.0
` os : :strerror_r ` break window build ` os : :strerror_r ` exist window .,MESOS-4107,1.0
"quota n't alloc resourc slave join . see attach patch . { { framework1 } } alloc resourc , despit fact resourc { { agent2 } } safe alloc without risk violat { { quota1 } } . if i understand intend quota behavior correctli , n't seem intend . note framework ad _after_ slave ad , resourc { { agent2 } } alloc { { framework1 } } .",MESOS-4102,5.0
parallel make test build test target when insid 3rdparty/libprocess : run { { make -j8 test } } clean build yield { { libprocess-test } } binari . run subsequ time trigger compil end yield { { libprocess-test } } binari . thi suggest { { test } } target built correctli .,MESOS-4099,1.0
libevent_ssl_socket assert fail have see follow socket receiv error frequent : { code } f1204 11:12:47.301839 54104 libevent_ssl_socket.cpp:245 ] check fail : length > 0 * * * check failur stack trace : * * * @ 0x7f73227fe5a6 googl : :logmessag : :fail ( ) @ 0x7f73227fe4f2 googl : :logmessag : :sendtolog ( ) @ 0x7f73227fdef4 googl : :logmessag : :flush ( ) @ 0x7f7322800e08 googl : :logmessagefat : :~logmessagefat ( ) @ 0x7f73227b93e2 process : :network : :libeventsslsocketimpl : :recv_callback ( ) @ 0x7f73227b9182 process : :network : :libeventsslsocketimpl : :recv_callback ( ) @ 0x7f731cbc75cc bufferevent_run_deferred_callbacks_lock @ 0x7f731cbbdc5d event_base_loop @ 0x7f73227d9ded process : :eventloop : :run ( ) @ 0x7f73227a3101 _znst12_bind_simpleifpfvvevee9_m_invokeijeeevst12_index_tupleijxspt_ee @ 0x7f73227a305b std : :_bind_simpl < > : :oper ( ) ( ) @ 0x7f73227a2ff4 std : :thread : :_impl < > : :_m_run ( ) @ 0x7f731e0d1a40 ( unknown ) @ 0x7f731de0a182 start_thread @ 0x7f731db3730d ( unknown ) @ ( nil ) ( unknown ) { code } in case http get ssl . the url : http : //dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data ? expires=1449259252 & signature=q4cqdr1lbxsiyyvebmetrx~lqdgqfhvkgxpbmm3poisn6r07dxizbx6~tl1izx9uxdfr~5awh8kxwh-y8b0dtv3mltzavlnezlhbhbax9qbymd180-qvuvrfezwolsmx4b3idvo-zk0caruu3ev1hbjz5y3olwe2zc~rxhewzkq_ & key-pair-id=apkajech5m7vwis5yz6q * step reproduc : * 1 . run master 2 . run slave build directori : { code } glog_v=1 ; ssl_enabled=1 ; ssl_key_file= < path_to_key > ; ssl_cert_file= < path_to_cert > ; sudo -e ./bin/mesos-slave.sh \ -- master=127.0.0.1:5050 \ -- executor_registration_timeout=5min \ -- containerizers=meso \ -- isolation=filesystem/linux \ -- image_providers=dock \ -- docker_puller_timeout=600 \ -- launcher_dir= $ mesos_build_dir/src/.lib \ -- switch_user= '' fals '' \ -- docker_puller= '' registri '' { code } 3 . run mesos-execut build directori : { code } ./src/mesos-execut \ -- master=127.0.0.1:5050 \ -- command= '' unam -a '' \ -- name=test \ -- docker_image=ubuntu { code },MESOS-4069,8.0
"investig remain flaki mastermaintenancetest.inverseoffersfilt per comment mesos-3916 , fix issu decreas degre flaki , seem intermitt test failur occur -- investig . * flaki task acknowledg * { code } i1203 18:25:04.609817 28732 status_update_manager.cpp:392 ] receiv statu updat acknowledg ( uuid : 6afd012e-8e88-41b2-8239-a9b852d07ca1 ) task 26305fdd-edb0-4764-8b8a-2558f2b2d81b framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000 w1203 18:25:04.610076 28732 status_update_manager.cpp:762 ] unexpect statu updat acknowledg ( receiv 6afd012e-8e88-41b2-8239-a9b852d07ca1 , expect 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6 ) updat task_run ( uuid : 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6 ) task 26305fdd-edb0-4764-8b8a-2558f2b2d81b framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000 e1203 18:25:04.610339 28736 slave.cpp:2339 ] fail handl statu updat acknowledg ( uuid : 6afd012e-8e88-41b2-8239-a9b852d07ca1 ) task 26305fdd-edb0-4764-8b8a-2558f2b2d81b framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000 : duplic acknowledgemen { code } thi race [ launch acknowledg two tasks|http : //github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp # l1486-l1517 ] . the statu updat task necessarili receiv order launch task . * flaki first invers offer filter * see [ comment mesos-3916|http : //issues.apache.org/jira/browse/mesos-3916 ? focusedcommentid=15027478 & page=com.atlassian.jira.plugin.system.issuetabpanel : comment-tabpanel # comment-15027478 ] explan . the relat log comment .",MESOS-4059,1.0
"memorypressuremesostest.cgroups_root_slaverecoveri flaki { code : title=output pass test } [ -- -- -- -- -- ] 1 test memorypressuremesostest 1+0 record 1+0 record 1048576 byte ( 1.0 mb ) copi , 0.000430889 , 2.4 gb/ [ run ] memorypressuremesostest.cgroups_root_slaverecoveri i1202 11:09:14.319327 5062 exec.cpp:134 ] version : 0.27.0 i1202 11:09:14.333317 5079 exec.cpp:208 ] executor regist slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-s0 regist executor ubuntu start task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162 sh -c 'while true ; dd count=512 bs=1m if=/dev/zero of=./temp ; done' fork command 5085 i1202 11:09:14.391739 5077 exec.cpp:254 ] receiv reconnect request slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-s0 i1202 11:09:14.398598 5082 exec.cpp:231 ] executor re-regist slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-s0 re-regist executor ubuntu shut send sigterm process tree pid 5085 kill follow process tree : [ -+- 5085 sh -c true ; dd count=512 bs=1m if=/dev/zero of=./temp ; done \ -- - 5086 dd count=512 bs=1m if=/dev/zero of=./temp ] [ ok ] memorypressuremesostest.cgroups_root_slaverecoveri ( 1096 ms ) { code } { code : title=output fail test } [ -- -- -- -- -- ] 1 test memorypressuremesostest 1+0 record 1+0 record 1048576 byte ( 1.0 mb ) copi , 0.000404489 , 2.6 gb/ [ run ] memorypressuremesostest.cgroups_root_slaverecoveri i1202 11:09:15.509950 5109 exec.cpp:134 ] version : 0.27.0 i1202 11:09:15.568183 5123 exec.cpp:208 ] executor regist slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-s0 regist executor ubuntu start task 14b6bab9-9f60-4130-bdc4-44efba262bc6 fork command 5132 sh -c 'while true ; dd count=512 bs=1m if=/dev/zero of=./temp ; done' i1202 11:09:15.665498 5129 exec.cpp:254 ] receiv reconnect request slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-s0 i1202 11:09:15.670995 5123 exec.cpp:381 ] executor ask shutdown shut send sigterm process tree pid 5132 .. / .. /src/tests/containerizer/memory_pressure_tests.cpp:283 : failur ( usag ) .failur ( ) : unknown contain : ebe90e15-72fa-4519-837b-62f43052c913 * * * abort 1449083355 ( unix time ) tri `` date -d @ 1449083355 '' use gnu date * * * { code } notic fail test , executor ask shutdown tri reconnect agent .",MESOS-4047,1.0
"contenttype/schedulertest flaki . ssl build , [ ubuntu 14.04|http : //github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh ] , non-root test run . { noformat } [ -- -- -- -- -- ] 22 test contenttype/schedulertest [ run ] contenttype/schedulertest.subscribe/0 [ ok ] contenttype/schedulertest.subscribe/0 ( 48 ms ) * * * abort 1448928007 ( unix time ) tri `` date -d @ 1448928007 '' use gnu date * * * [ run ] contenttype/schedulertest.subscribe/1 pc : @ 0x1451b8e test : :intern : :untypedfunctionmockerbas : :untypedinvokewith ( ) * * * sigsegv ( @ 0x100000030 ) receiv pid 21320 ( tid 0x2b549e5d4700 ) pid 48 ; stack trace : * * * @ 0x2b54c95940b7 os : :linux : :chained_handl ( ) @ 0x2b54c9598219 jvm_handle_linux_sign @ 0x2b5496300340 ( unknown ) @ 0x1451b8e test : :intern : :untypedfunctionmockerbas : :untypedinvokewith ( ) @ 0xe2ea6d _zn7testing8internal18functionmockerbaseifvrkst5queuein5mesos2v19scheduler5eventest5dequeis6_sais6_eeeee10invokewitherkst5tupleijsc_e @ 0xe2b1bc test : :intern : :functionmock < > : :invok ( ) @ 0x1118a meso : :intern : :test : :schedulertest : :callback : :receiv ( ) @ 0x111c453 _znkst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins0_2v19scheduler5eventest5dequeis8_sais8_eeeeeclijse_eveevrs4_dpot_ @ 0x111c001 _znst5_bindifst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins1_2v19scheduler5eventest5dequeis9_sais9_eeeeest17reference_wrapperis5_est12_placeholderili1eeee6__callivjsf_ejlm0elm1eeeet_ost5tupleijdpt0_eest12_index_tupleijxspt1_ee @ 0x111b90d _znst5_bindifst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins1_2v19scheduler5eventest5dequeis9_sais9_eeeeest17reference_wrapperis5_est12_placeholderili1eeeeclijsf_eveet0_dpot_ @ 0x111ae09 std : :_function_handl < > : :_m_invok ( ) @ 0x2b5493c6da09 std : :function < > : :oper ( ) ( ) @ 0x2b5493c688ee process : :asyncexecutorprocess : :execut < > ( ) @ 0x2b5493c6db2a _zzn7process8dispatchi7nothingns_20asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeis8_sais8_eeeeesc_pvsg_sc_sj_eens_6futureit_eerkns_3pidit0_eemso_fsl_t1_t2_t3_et4_t5_t6_enkulpns_11processbaseee_cles11_ @ 0x2b5493c765a4 _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchi7nothingns0_20asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeisc_saisc_eeeeesg_pvsk_sg_sn_eens0_6futureit_eerkns0_3pidit0_eemss_fsp_t1_t2_t3_et4_t5_t6_euls2_e_e9_m_invokeerkst9_any_datas2_ @ 0x2b54946b1201 std : :function < > : :oper ( ) ( ) @ 0x2b549469960f process : :processbas : :visit ( ) @ 0x2b549469d480 process : :dispatchev : :visit ( ) @ 0x9dc0ba process : :processbas : :serv ( ) @ 0x2b54946958cc process : :processmanag : :resum ( ) @ 0x2b5494692a9c _zzn7process14processmanager12init_threadsevenkulrkst11atomic_boole_cles3_ @ 0x2b549469ccac _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eee6__callivieilm0eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_ee @ 0x2b549469cc5c _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eeecliieveet0_dpot_ @ 0x2b549469cbee _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeevee9_m_invokeiieeevst12_index_tupleiixspt_ee @ 0x2b549469cb45 _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeeveeclev @ 0x2b549469cade _znst6thread5_implist12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis6_eeeveee6_m_runev @ 0x2b5495b81a40 ( unknown ) @ 0x2b54962f8182 start_thread @ 0x2b549660847d ( unknown ) make [ 3 ] : * * * [ check-loc ] segment fault make [ 3 ] : leav directori ` /home/vagrant/mesos/build/src' make [ 2 ] : * * * [ check-am ] error 2 make [ 2 ] : leav directori ` /home/vagrant/mesos/build/src' make [ 1 ] : * * * [ check ] error 2 make [ 1 ] : leav directori ` /home/vagrant/mesos/build/src' make : * * * [ check-recurs ] error 1 { noformat }",MESOS-4029,3.0
"pass agent work_dir isol modul some isol modul benefit access agent 's { { work_dir } } . for exampl , dvd isol ( http : //github.com/emccode/mesos-module-dvdi ) current forc mount extern volum hard-cod directori . make { { work_dir } } access isol via { { isol : :recov ( ) } } would allow isol mount volum within agent 's { { work_dir } } . thi accomplish simpli ad overload signatur { { isol : :recov ( ) } } includ { { work_dir } } paramet .",MESOS-4003,1.0
"c++ http schedul librari work ssl enabl the c++ http schedul librari work meso ssl enabl ( without downgrad ) . the fix simpl : * the librari detect ssl enabl . * if ssl enabl , connect made http instead http .",MESOS-3976,3.0
"ensur resourc ` quotainfo ` protobuf contain ` role ` { { quotainfo } } protobuf current store per-rol quota , includ { { resourc } } object . these resourc neither static dynam reserv , henc may contain { { role } } field . we ensur field unset , well updat valid routin { { quotainfo } }",MESOS-3965,3.0
"/reserv /unreserv permiss master without authent . current , { { /reserv } } { { /unreserv } } endpoint work without authent enabl master . when authent disabl master , endpoint permiss .",MESOS-3940,1.0
"implement authn handl master schedul endpoint if authent ( authn ) enabl master , framework attempt use http schedul api ca n't regist . { code } $ cat /tmp/subscribe-943257503176798091.bin | http -- print=hhbb -- stream -- pretty=color -- auth verif : password1 post :5050/api/v1/schedul accept : application/x-protobuf content-typ : application/x-protobuf post /api/v1/schedul http/1.1 connect : keep-al content-typ : application/x-protobuf accept-encod : gzip , deflat accept : application/x-protobuf content-length : 126 user-ag : httpie/0.9.0 host : localhost:5050 author : basic dmvyawzpy2f0aw9uonbhc3n3b3jkmq== + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | note : binari data shown termin | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ http/1.1 401 unauthor date : fri , 13 nov 2015 20:00:45 gmt www-authent : basic realm= '' meso master '' content-length : 65 http schedul support authent requir { code } author ( authz ) alreadi support http base framework .",MESOS-3923,5.0
identifi implement test case handl race optimist lender tenant offer . an exampl lender launch task agent follow borrow launch task agent optimist offer rescind .,MESOS-3898,13.0
identifi implement test case verifi evict logic agent,MESOS-3897,13.0
"add account reserv slack alloc . mesos-xxx : optimsist account { code } class hierarchicalallocatorprocess { struct slave { ... struct optimist { resourc total ; // the total alloc slack resourc resourc alloc ; // the alloc alloc slack resourc } ; optimist optimist ; } ; } { code } mesos-4146 : flatten & allocationslack optimist offer { code } class resourc { // return resourc object amount resourc // type resourc , resourc object mark // specifi ` revocableinfo : :type ` ; attribut // affect . resourc flatten ( resourc : :revocableinfo : :type type ) ; // return resourc object : // - role given , resourc includ role 's reserv // resourc . // - resourc 's revoc type ` allocation_slack ` // - role resourc set `` * '' resourc allocationslack ( option < string > role = none ( ) ) ; } { code } mesos-xxx : alloc allocation_slack resourc framework { code } void hierarchicalallocatorprocess : :alloc ( const hashset < slaveid > & slaveids_ ) { foreach slave ; foreach role ; foreach framework { resourc optimist ; ( framework.revoc ) { resourc total = slave [ slaveid ] .optimistic.total.allocationslack ( role ) ; optimist = total - slave [ slaveid ] .optimistic.alloc ; } ... offer [ frameworkid ] [ slaveid ] += resourc + optimist ; ... slave [ slaveid ] .optimistic.alloc += optimist ; } } { code } here 's consider ` allocation_slack ` : 1 . 'old ' resourc ( available/tot ) includ allocation_slack 2 . after ` quota ` , ` remainingclusterresources.contain ` check allocation_slack ; enough resourc , master still offer allocation_salck resourc . 3 . in sorter , 'll includ allocation_slack ; resourc borrow role/framework 4 . if either normal resourc allocation_slack resourc allocable/ ! filter , offer framework 5 . current , alloc assign allocation_salck resourc slave one framework mesos-xxx : updat allocation_slack dynam reserv ( updatealloc ) { code } void hierarchicalallocatorprocess : :updatealloc ( const frameworkid & frameworkid , const slaveid & slaveid , const vector < offer : :oper > & oper ) { ... tri < resourc > updatedoptimist = slave [ slaveid ] .optimistic.total.appli ( oper ) ; check_som ( updatedtot ) ; slave [ slaveid ] .optimistic.tot = updatedoptimistic.get ( ) .stateless ( ) .reserv ( ) .flatten ( allocation_slack ) ; ... } { code } mesos-xxx : add allocation_slack slaver register/re-regist ( addslav ) { code } void hierarchicalallocatorprocess : :addslav ( const slaveid & slaveid , const slaveinfo & slaveinfo , const option < unavail > & unavail , const resourc & total , const hashmap < frameworkid , resourc > & use ) { ... slave [ slaveid ] .optimistic.tot = total.stateless ( ) .reserv ( ) .flatten ( allocation_slack ) ; ... } { code } no need handl ` removeslav ` , 'll relat info ` slave ` includ ` optimist ` . mesos-xxx : return resourc alloc ( recoverresourc ) { code } void hierarchicalallocatorprocess : :recoverresourc ( const frameworkid & frameworkid , const slaveid & slaveid , const resourc & resourc , const option < filter > & filter ) { ( slaves.contain ( slaveid ) ) { ... slave [ slaveid ] .optimistic.alloc -= resources.allocationslack ( ) ; ... } } { code }",MESOS-3896,13.0
updat reserv slack alloc state agent failov .,MESOS-3895,13.0
rebuild reserv slack alloc state master failov .,MESOS-3894,13.0
implement test verifi alloc resourc math . write test ensur alloc perform reserv slack calcul correctli .,MESOS-3893,8.0
"add helper function agent retriev list executor use optimist offer , revoc resourc . in agent , add helper function get list exeuctor use allocation_slack . it 's short term solut differ design document , master executor command line executor . send evicatbl executor master slave addess post-mvp mesos-1718 . { noformat } class slave { ... // if executor use revoc resourc , add ` evictableexecutor ` // list . void addevictableexecutor ( executor * executor ) ; // if executor use revoc resourc , remov // ` evictableexecutor ` list . void removeevictableexecutor ( executor * executor ) ; // get evict executor id list ` request resourc ` . the return valu ` result < list < executor * > > ` : // - ` iserror ( ) ` , 's enough resourc launch task // - ` isnon ( ) ` , evict exectuor need termin // - ! ` isnon ( ) ` , list executor need evict resourc result < std : :list < executor * > > getevictableexecutor ( const resourc & request ) ; ... // the map evict executor list . if 's enough resourc , // evict executor termin slave releas resourc . hashmap < frameworkid , std : :set < executorid > > evictableexecutor ; ... } { noformat }",MESOS-3892,5.0
"add helper function agent check avail resourc launch task . launch task use revoc resourc funnel account system : * if task launch use revoc resourc , resourc must use launch task . if use , task fail start . * if task launch use reserv resourc , resourc must made avail . thi mean potenti evict task use revoc resourc . both case could implement ad check slave : :runtask , like new helper method : { noformat } class slave { ... // check given resourc avail ( i.e . util ) // start task . if , task either fail // start result evict revoc resourc . virtual process : :futur < bool > checkavailableresourc ( const resourc & resourc ) ; ... } { noformat }",MESOS-3891,5.0
add notion evict task runtaskmessag { code } // evict resourc launch task . messag revoc { option frameworkid framework_id = 1 ; requir string role = 2 ; repeat resourc revocable_resourc = 3 ; } repeat revoc revoc = 5 ; { code },MESOS-3890,2.0
"modifi oversubscript document explicitli forbid qo control kill executor run optimist offer resourc . the oversubcript document current assum oversubscrib resourc ( { { usage_slack } } ) type revoc resourc . optimist offer add second type revoc resourc ( { { allocation_slack } } ) act upon oversubscript compon . for exampl , [ oversubscript doc|http : //mesos.apache.org/documentation/latest/oversubscription/ ] say follow : { quot } note : if resourc use task executor revoc , whole contain treat revoc contain therefor kill throttl qo control . { quot } may amend someth like : { quot } note : if resourc use task executor revoc usag slack , whole contain treat oversubscrib contain therefor kill throttl qo control . { quot }",MESOS-3889,2.0
"support distinguish revoc resourc resourc protobuf . add enum type revocableinfo : * framework need assign revocableinfo launch task ; ’ assign , use reserv resourc . framework need identifi resourc ’ use * oversubscript resourc need assign type agent ( mesos-3930 ) * updat oversubscript document oo over-subscrib alloc slack recommend qo handl usag slack . ( mesos-3889 ) { code } messag resourc { ... messag revocableinfo { enum type { // under-util , alloc resourc . control // oversubscript ( qoscontrol & resourceestim ) . usage_slack = 1 ; // unalloc , reserv resourc . // control optimist offer ( alloc ) . allocation_slack = 2 ; } option type type = 1 ; } ... option revocableinfo revoc = 9 ; } { code }",MESOS-3888,2.0
add flag master enabl optimist offer .,MESOS-3887,3.0
implement ` stout/os/pstree.hpp ` window,MESOS-3881,2.0
add mtime-rel fetcher test,MESOS-3856,2.0
"meso set content-typ 400 bad request while integr http schedul api i encount follow scenario . the messag serial protobuf sent post bodi { code : title=messag } call { type : acknowledg , acknowledg : { uuid : < byte > , agentid : { valu : `` 20151012-182734-16777343-5050-8978-s2 '' } , taskid : { valu : `` task-1 '' } } } { code } { code : title=request header } post /api/v1/schedul http/1.1 content-typ : application/x-protobuf accept : application/x-protobuf content-length : 73 host : localhost:5050 user-ag : rxnetti client { code } i receiv follow respons { code : title=respons header } http/1.1 400 bad request date : wed , 14 oct 2015 23:21:36 gmt content-length : 74 fail valid schedul : :call : expect 'framework_id ' present { code } even though accept header made mention { { text/plain } } messag bodi return { { text/plain } } . addit , { { content-typ } } header set respons i ca n't even anyth intellig respons handler .",MESOS-3739,2.0
"implement quota support alloc the built-in hierarch drf alloc support quota . thi includ ( limit ) : ad , updat , remov satisfi quota ; avoid overcomit resourc hand non-quota' role presenc master failov . a [ design doc quota support allocator|http : //issues.apache.org/jira/browse/mesos-2937 ] provid overview featur set requir implement .",MESOS-3718,5.0
"updat alloc interfac support quota an alloc notifi quota set/upd remov . also support master failov presenc quota , alloc notifi reregist agent alloc toward quota .",MESOS-3716,3.0
"make schedul librari use http pipelin abstract libprocess current , schedul librari send call order chain send receiv respons earlier call . thi done http pipelin abstract libprocess { { process : :post } } . howev { { mesos-3332 } } resolv , abl use new abstract .",MESOS-3570,8.0
"revoc task cpu show zero /state.json the slave 's state.json report revoc task resourc zero : { noformat } resourc : { cpu : 0 , disk : 3071 , mem : 1248 , port : `` [ 31715-31715 ] '' } , { noformat } also , indic task use revoc cpu . it would great type info .",MESOS-3563,2.0
"valid slave 's work_dir share mount peer group linuxfilesystemisol use . to address todo code : { noformat } src/slave/containerizer/isolators/filesystem/linux.cpp +122 // todo ( jieyu ) : current , n't check slave 's work_dir // mount share mount . we assum . we // simpli mark slave share creat // new peer group mount . thi temporari workaround // think fix . { noformat }",MESOS-3539,3.0
add support expos accept/declin respons invers offer current implement mainten primit support expos accept/declin respons framework cluster oper . thi function necessari provid visibl oper whether given framework readi compli post mainten schedul .,MESOS-3489,2.0
"linuxfilesystemisol make slave 's work_dir share mount . so user task fork , hold extra refer sandbox mount provision bind backend mount . if n't , could get follow error messag clean bind backend mount point sandbox mount point . { noformat } e0921 17:35:57.268159 47010 bind.cpp:182 ] fail remov rootf mount point '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a/backends/bind/rootfses/30f7e5e2-55d0-4d4d-a662-f8aad0d56b33 ' : devic resourc busi e0921 17:35:57.268349 47010 provisioner.cpp:403 ] fail remov provis contain directori '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a ' : devic resourc busi { noformat }",MESOS-3483,3.0
segfault accept declin invers offer discov write test filter ( regard invers offer ) . fix : http : //reviews.apache.org/r/38470/,MESOS-3458,1.0
support run filesystem isol command executor mesoscontainer,MESOS-3428,4.0
"docker container symlink persist volum sandbox for arangodb framework i tri use persist primit . nearli work , i miss crucial piec end : i success creat persist disk resourc set persist volum inform diskinfo messag . howev , i see way find directori host meso slave reserv us . i know $ { mesos_slave_workdir } /volumes/roles/ < myrol > / < name > _ < uuid > way queri inform anywher . the docker container automat mount directori docker contain , symlink sandbox . therefor , i essenti access . note meso container ( i use reason ) seem creat symlink sandbox actual path persist volum . with , i could mount volum docker contain would well .",MESOS-3413,5.0
"remov mount point fail ebusi linuxfilesystemisol . when run test root , found persistentvolumetest.accesspersistentvolum fail consist platform . { noformat } [ run ] persistentvolumetest.accesspersistentvolum i0901 02:17:26.435140 39432 exec.cpp:133 ] version : 0.25.0 i0901 02:17:26.442129 39461 exec.cpp:207 ] executor regist slave 20150901-021726-1828659978-52102-32604-s0 regist executor hostnam start task d8ff1f00-e720-4a61-b440-e111009dfdc3 sh -c 'echo abc > path1/file' fork command 39484 command exit statu 0 ( pid : 39484 ) .. / .. /src/tests/persistent_volume_tests.cpp:579 : failur valu : os : :exist ( path : :join ( directori , `` path1 '' ) ) actual : true expect : fals [ fail ] persistentvolumetest.accesspersistentvolum ( 777 ms ) { noformat } turn 'rmdir ' 'umount ' fail ebusi 's still refer mount . fyi [ ~jieyu ] [ ~mcypark ]",MESOS-3349,5.0
"dynam reserv count use resourc master dynam reserv resourc consid use alloc henc reflect meso bookkeep structur { { state.json } } . i expand { { reservationtest.reservethenunreserv } } test follow section : { code } // check master count reserv use resourc . { futur < process : :http : :respons > respons = process : :http : :get ( master.get ( ) , `` state.json '' ) ; await_readi ( respons ) ; tri < json : :object > pars = json : :pars < json : :object > ( response.get ( ) .bodi ) ; assert_som ( pars ) ; result < json : :number > cpu = parse.get ( ) .find < json : :number > ( `` slave [ 0 ] .used_resources.cpu '' ) ; assert_some_eq ( json : :number ( 1 ) , cpu ) ; } { code } got { noformat } .. / .. / .. /src/tests/reservation_tests.cpp:168 : failur valu : ( cpu ) .get ( ) actual : 0 expect : json : :number ( 1 ) which : 1 { noformat } idea new resourc state : http : //docs.google.com/drawings/d/1aquviqpy8d_mr-cqjzu-wz5nnn3cyp3jxqeguhl-kzc/edit",MESOS-3338,3.0
"make use c++11 atom now requir c++11 , make use std : :atom . for exampl : * libprocess/process.cpp use bare int + __sync_synchron ( ) `` run '' * __sync_synchron ( ) use logging.hpp libprocess fork.hpp stout * sched/sched.cpp use volatil int `` run '' -- wrong , `` volatil '' suffici ensur safe concurr access * `` volatil '' use place -- probabl dubiou i n't look close",MESOS-3326,2.0
"master drop http call 's recov much like pid base framework , master drop http call 's leader and/or still recov .",MESOS-3290,3.0
"eventcal test framework flaki observ asf ci . h/t [ ~haosdent @ gmail.com ] look like http schedul never sent subscrib request master . { code } [ run ] examplestest.eventcallframework use temporari directori '/tmp/examplestest_eventcallframework_k4vxkx' i0813 19:55:15.643579 26085 exec.cpp:443 ] ignor exit event driver abort ! shut send sigterm process tree pid 26061 kill follow process tree : [ ] shut send sigterm process tree pid 26062 shut kill follow process tree : [ ] send sigterm process tree pid 26063 kill follow process tree : [ ] shut send sigterm process tree pid 26098 kill follow process tree : [ ] shut send sigterm process tree pid 26099 kill follow process tree : [ ] warn : log initgooglelog ( ) written stderr i0813 19:55:17.161726 26100 process.cpp:1012 ] libprocess initi 172.17.2.10:60249 16 cpu i0813 19:55:17.161888 26100 logging.cpp:177 ] log stderr i0813 19:55:17.163625 26100 scheduler.cpp:157 ] version : 0.24.0 i0813 19:55:17.175302 26100 leveldb.cpp:176 ] open db 3.167446m i0813 19:55:17.176393 26100 leveldb.cpp:183 ] compact db 1.047996m i0813 19:55:17.176496 26100 leveldb.cpp:198 ] creat db iter 77155n i0813 19:55:17.176518 26100 leveldb.cpp:204 ] seek begin db 8429n i0813 19:55:17.176527 26100 leveldb.cpp:273 ] iter 0 key db 4219n i0813 19:55:17.176708 26100 replica.cpp:744 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0813 19:55:17.178951 26136 recover.cpp:449 ] start replica recoveri i0813 19:55:17.179934 26136 recover.cpp:475 ] replica empti statu i0813 19:55:17.181970 26126 master.cpp:378 ] master 20150813-195517-167907756-60249-26100 ( 297daca2d01a ) start 172.17.2.10:60249 i0813 19:55:17.182317 26126 master.cpp:380 ] flag startup : -- acls= '' permiss : fals register_framework { princip { type : some valu : `` test-princip '' } role { type : some valu : `` * '' } } run_task { princip { type : some valu : `` test-princip '' } user { type : some valu : `` meso '' } } '' -- allocation_interval= '' 1sec '' -- allocator= '' hierarchicaldrf '' -- authenticate= '' fals '' -- authenticate_slaves= '' fals '' -- authenticators= '' crammd5 '' -- credentials= '' /tmp/examplestest_eventcallframework_k4vxkx/credenti '' -- framework_sorter= '' drf '' -- help= '' fals '' -- initialize_driver_logging= '' true '' -- log_auto_initialize= '' true '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- max_slave_ping_timeouts= '' 5 '' -- quiet= '' fals '' -- recovery_slave_removal_limit= '' 100 % '' -- registry= '' replicated_log '' -- registry_fetch_timeout= '' 1min '' -- registry_store_timeout= '' 5sec '' -- registry_strict= '' fals '' -- root_submissions= '' true '' -- slave_ping_timeout= '' 15sec '' -- slave_reregister_timeout= '' 10min '' -- user_sorter= '' drf '' -- version= '' fals '' -- webui_dir= '' /mesos/mesos-0.24.0/src/webui '' -- work_dir= '' /tmp/mesos-ii8gua '' -- zk_session_timeout= '' 10sec '' i0813 19:55:17.183475 26126 master.cpp:427 ] master allow unauthent framework regist i0813 19:55:17.183536 26126 master.cpp:432 ] master allow unauthent slave regist i0813 19:55:17.183615 26126 credentials.hpp:37 ] load credenti authent '/tmp/examplestest_eventcallframework_k4vxkx/credentials' w0813 19:55:17.183859 26126 credentials.hpp:52 ] permiss credenti file '/tmp/examplestest_eventcallframework_k4vxkx/credenti ' open . it recommend credenti file not access other . i0813 19:55:17.183969 26123 replica.cpp:641 ] replica empti statu receiv broadcast recov request i0813 19:55:17.184306 26126 master.cpp:469 ] use default 'crammd5 ' authent i0813 19:55:17.184661 26126 authenticator.cpp:512 ] initi server sasl i0813 19:55:17.185104 26138 recover.cpp:195 ] receiv recov respons replica empti statu i0813 19:55:17.185972 26100 containerizer.cpp:143 ] use isol : posix/cpu , posix/mem , filesystem/posix i0813 19:55:17.186058 26135 recover.cpp:566 ] updat replica statu start i0813 19:55:17.187001 26138 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 654586n i0813 19:55:17.187037 26138 replica.cpp:323 ] persist replica statu start i0813 19:55:17.187499 26134 recover.cpp:475 ] replica start statu i0813 19:55:17.187605 26126 auxprop.cpp:66 ] initi in-memori auxiliari properti plugin i0813 19:55:17.187710 26126 master.cpp:506 ] author enabl i0813 19:55:17.188657 26138 replica.cpp:641 ] replica start statu receiv broadcast recov request i0813 19:55:17.188853 26131 hierarchical.hpp:346 ] initi hierarch alloc process i0813 19:55:17.189252 26132 whitelist_watcher.cpp:79 ] no whitelist given i0813 19:55:17.189321 26134 recover.cpp:195 ] receiv recov respons replica start statu i0813 19:55:17.190001 26125 recover.cpp:566 ] updat replica statu vote i0813 19:55:17.190696 26124 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 357331n i0813 19:55:17.190775 26124 replica.cpp:323 ] persist replica statu vote i0813 19:55:17.190970 26133 recover.cpp:580 ] success join paxo group i0813 19:55:17.192183 26129 recover.cpp:464 ] recov process termin i0813 19:55:17.192699 26123 slave.cpp:190 ] slave start 1 ) @ 172.17.2.10:60249 i0813 19:55:17.192741 26123 slave.cpp:191 ] flag startup : -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/mesos/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.24.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 1sec '' -- resource_monitoring_interval= '' 1sec '' -- resources= '' cpus:2 ; mem:10240 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- version= '' fals '' -- work_dir= '' /tmp/mesos-ii8gua/0 '' i0813 19:55:17.194514 26100 containerizer.cpp:143 ] use isol : posix/cpu , posix/mem , filesystem/posix i0813 19:55:17.194658 26123 slave.cpp:354 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.194854 26123 slave.cpp:384 ] slave hostnam : 297daca2d01a i0813 19:55:17.194877 26123 slave.cpp:389 ] slave checkpoint : true i0813 19:55:17.196751 26132 master.cpp:1524 ] the newli elect leader master @ 172.17.2.10:60249 id 20150813-195517-167907756-60249-26100 i0813 19:55:17.196797 26132 master.cpp:1537 ] elect lead master ! i0813 19:55:17.196815 26132 master.cpp:1307 ] recov registrar i0813 19:55:17.197032 26138 registrar.cpp:311 ] recov registrar i0813 19:55:17.197845 26132 slave.cpp:190 ] slave start 2 ) @ 172.17.2.10:60249 i0813 19:55:17.198420 26125 log.cpp:661 ] attempt start writer i0813 19:55:17.197948 26132 slave.cpp:191 ] flag startup : -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/mesos/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.24.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 1sec '' -- resource_monitoring_interval= '' 1sec '' -- resources= '' cpus:2 ; mem:10240 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- version= '' fals '' -- work_dir= '' /tmp/mesos-ii8gua/1 '' i0813 19:55:17.199121 26132 slave.cpp:354 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.199235 26138 state.cpp:54 ] recov state '/tmp/mesos-ii8gua/0/meta' i0813 19:55:17.199322 26132 slave.cpp:384 ] slave hostnam : 297daca2d01a i0813 19:55:17.199345 26132 slave.cpp:389 ] slave checkpoint : true i0813 19:55:17.199676 26100 containerizer.cpp:143 ] use isol : posix/cpu , posix/mem , filesystem/posix i0813 19:55:17.200085 26135 state.cpp:54 ] recov state '/tmp/mesos-ii8gua/1/meta' i0813 19:55:17.200317 26132 status_update_manager.cpp:202 ] recov statu updat manag i0813 19:55:17.200371 26129 status_update_manager.cpp:202 ] recov statu updat manag i0813 19:55:17.202003 26129 replica.cpp:477 ] replica receiv implicit promis request propos 1 i0813 19:55:17.202585 26131 slave.cpp:190 ] slave start 3 ) @ 172.17.2.10:60249 i0813 19:55:17.202596 26129 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 523191n i0813 19:55:17.202756 26129 replica.cpp:345 ] persist promis 1 i0813 19:55:17.202770 26132 containerizer.cpp:379 ] recov container i0813 19:55:17.203061 26135 containerizer.cpp:379 ] recov container i0813 19:55:17.202663 26131 slave.cpp:191 ] flag startup : -- authenticatee= '' crammd5 '' -- cgroups_cpu_enable_pids_and_tids_count= '' fals '' -- cgroups_enable_cfs= '' fals '' -- cgroups_hierarchy= '' /sys/fs/cgroup '' -- cgroups_limit_swap= '' fals '' -- cgroups_root= '' meso '' -- container_disk_watch_interval= '' 15sec '' -- containerizers= '' meso '' -- default_role= '' * '' -- disk_watch_interval= '' 1min '' -- docker= '' docker '' -- docker_kill_orphans= '' true '' -- docker_remove_delay= '' 6hr '' -- docker_socket= '' /var/run/docker.sock '' -- docker_stop_timeout= '' 0n '' -- enforce_container_disk_quota= '' fals '' -- executor_registration_timeout= '' 1min '' -- executor_shutdown_grace_period= '' 5sec '' -- fetcher_cache_dir= '' /tmp/mesos/fetch '' -- fetcher_cache_size= '' 2gb '' -- frameworks_home= '' '' -- gc_delay= '' 1week '' -- gc_disk_headroom= '' 0.1 '' -- hadoop_home= '' '' -- help= '' fals '' -- initialize_driver_logging= '' true '' -- isolation= '' posix/cpu , posix/mem '' -- launcher_dir= '' /mesos/mesos-0.24.0/_build/src '' -- logbufsecs= '' 0 '' -- logging_level= '' info '' -- oversubscribed_resources_interval= '' 15sec '' -- perf_duration= '' 10sec '' -- perf_interval= '' 1min '' -- qos_correction_interval_min= '' 0n '' -- quiet= '' fals '' -- recover= '' reconnect '' -- recovery_timeout= '' 15min '' -- registration_backoff_factor= '' 1sec '' -- resource_monitoring_interval= '' 1sec '' -- resources= '' cpus:2 ; mem:10240 '' -- revocable_cpu_low_priority= '' true '' -- sandbox_directory= '' /mnt/mesos/sandbox '' -- strict= '' true '' -- switch_user= '' true '' -- version= '' fals '' -- work_dir= '' /tmp/mesos-ii8gua/2 '' i0813 19:55:17.203819 26131 slave.cpp:354 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.203930 26131 slave.cpp:384 ] slave hostnam : 297daca2d01a i0813 19:55:17.203948 26131 slave.cpp:389 ] slave checkpoint : true i0813 19:55:17.204674 26137 state.cpp:54 ] recov state '/tmp/mesos-ii8gua/2/meta' i0813 19:55:17.205178 26135 status_update_manager.cpp:202 ] recov statu updat manag i0813 19:55:17.205323 26135 containerizer.cpp:379 ] recov container i0813 19:55:17.205521 26136 slave.cpp:4069 ] finish recoveri i0813 19:55:17.206074 26136 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:17.206424 26128 slave.cpp:4069 ] finish recoveri i0813 19:55:17.206722 26137 status_update_manager.cpp:176 ] paus send statu updat i0813 19:55:17.206858 26136 slave.cpp:684 ] new master detect master @ 172.17.2.10:60249 i0813 19:55:17.206902 26138 slave.cpp:4069 ] finish recoveri i0813 19:55:17.206962 26128 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:17.208312 26134 scheduler.cpp:272 ] new master detect master @ 172.17.2.10:60249 i0813 19:55:17.208364 26136 slave.cpp:709 ] no credenti provid . attempt regist without authent i0813 19:55:17.208608 26136 slave.cpp:720 ] detect new master i0813 19:55:17.208839 26138 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:17.209216 26123 coordinator.cpp:231 ] coordin attemp fill miss posit i0813 19:55:17.209247 26127 status_update_manager.cpp:176 ] paus send statu updat i0813 19:55:17.209259 26128 slave.cpp:684 ] new master detect master @ 172.17.2.10:60249 i0813 19:55:17.209322 26127 status_update_manager.cpp:176 ] paus send statu updat i0813 19:55:17.209364 26128 slave.cpp:709 ] no credenti provid . attempt regist without authent i0813 19:55:17.209344 26138 slave.cpp:684 ] new master detect master @ 172.17.2.10:60249 i0813 19:55:17.209455 26128 slave.cpp:720 ] detect new master i0813 19:55:17.209492 26138 slave.cpp:709 ] no credenti provid . attempt regist without authent i0813 19:55:17.209573 26128 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:17.209601 26138 slave.cpp:720 ] detect new master i0813 19:55:17.209730 26138 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:17.209883 26136 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:17.211266 26136 replica.cpp:378 ] replica receiv explicit promis request posit 0 propos 2 i0813 19:55:17.211771 26136 leveldb.cpp:343 ] persist action ( 8 byte ) leveldb took 462128n i0813 19:55:17.211797 26136 replica.cpp:679 ] persist action 0 i0813 19:55:17.212980 26130 replica.cpp:511 ] replica receiv write request posit 0 i0813 19:55:17.213124 26130 leveldb.cpp:438 ] read posit leveldb took 67075n i0813 19:55:17.213580 26130 leveldb.cpp:343 ] persist action ( 14 byte ) leveldb took 301649n i0813 19:55:17.213603 26130 replica.cpp:679 ] persist action 0 i0813 19:55:17.214284 26123 replica.cpp:658 ] replica receiv learn notic posit 0 i0813 19:55:17.214622 26123 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 284547n i0813 19:55:17.214648 26123 replica.cpp:679 ] persist action 0 i0813 19:55:17.214675 26123 replica.cpp:664 ] replica learn nop action posit 0 i0813 19:55:17.215420 26136 log.cpp:677 ] writer start end posit 0 i0813 19:55:17.217463 26133 leveldb.cpp:438 ] read posit leveldb took 47943n i0813 19:55:17.220762 26125 registrar.cpp:344 ] success fetch registri ( 0b ) 23.649024m i0813 19:55:17.221081 26125 registrar.cpp:443 ] appli 1 oper 136902n ; attempt updat 'registry' i0813 19:55:17.223667 26133 log.cpp:685 ] attempt append 174 byte log i0813 19:55:17.223778 26125 coordinator.cpp:341 ] coordin attempt write append action posit 1 i0813 19:55:17.224516 26127 replica.cpp:511 ] replica receiv write request posit 1 i0813 19:55:17.225009 26127 leveldb.cpp:343 ] persist action ( 193 byte ) leveldb took 466230n i0813 19:55:17.225042 26127 replica.cpp:679 ] persist action 1 i0813 19:55:17.225653 26126 replica.cpp:658 ] replica receiv learn notic posit 1 i0813 19:55:17.225953 26126 leveldb.cpp:343 ] persist action ( 195 byte ) leveldb took 286966n i0813 19:55:17.225975 26126 replica.cpp:679 ] persist action 1 i0813 19:55:17.226013 26126 replica.cpp:664 ] replica learn append action posit 1 i0813 19:55:17.227545 26137 registrar.cpp:488 ] success updat 'registri ' 6.328064m i0813 19:55:17.227722 26137 registrar.cpp:374 ] success recov registrar i0813 19:55:17.227918 26124 log.cpp:704 ] attempt truncat log 1 i0813 19:55:17.228024 26133 coordinator.cpp:341 ] coordin attempt write truncat action posit 2 i0813 19:55:17.228193 26131 master.cpp:1334 ] recov 0 slave registri ( 135b ) ; allow 10min slave re-regist i0813 19:55:17.228659 26127 replica.cpp:511 ] replica receiv write request posit 2 i0813 19:55:17.228972 26127 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 297903n i0813 19:55:17.229004 26127 replica.cpp:679 ] persist action 2 i0813 19:55:17.229565 26127 replica.cpp:658 ] replica receiv learn notic posit 2 i0813 19:55:17.229837 26127 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 260326n i0813 19:55:17.229899 26127 leveldb.cpp:401 ] delet ~1 key leveldb took 48697n i0813 19:55:17.229923 26127 replica.cpp:679 ] persist action 2 i0813 19:55:17.229956 26127 replica.cpp:664 ] replica learn truncat action posit 2 i0813 19:55:17.325634 26138 slave.cpp:1209 ] will retri registr 445.955946m necessari i0813 19:55:17.326088 26124 master.cpp:3635 ] regist slave slave ( 2 ) @ 172.17.2.10:60249 ( 297daca2d01a ) id 20150813-195517-167907756-60249-26100-s0 i0813 19:55:17.327446 26124 registrar.cpp:443 ] appli 1 oper 231072n ; attempt updat 'registry' i0813 19:55:17.330252 26136 log.cpp:685 ] attempt append 344 byte log i0813 19:55:17.330407 26132 coordinator.cpp:341 ] coordin attempt write append action posit 3 i0813 19:55:17.331418 26128 replica.cpp:511 ] replica receiv write request posit 3 i0813 19:55:17.331753 26128 leveldb.cpp:343 ] persist action ( 363 byte ) leveldb took 264140n i0813 19:55:17.331778 26128 replica.cpp:679 ] persist action 3 i0813 19:55:17.332324 26133 replica.cpp:658 ] replica receiv learn notic posit 3 i0813 19:55:17.332809 26133 leveldb.cpp:343 ] persist action ( 365 byte ) leveldb took 313064n i0813 19:55:17.332834 26133 replica.cpp:679 ] persist action 3 i0813 19:55:17.332865 26133 replica.cpp:664 ] replica learn append action posit 3 i0813 19:55:17.334211 26132 registrar.cpp:488 ] success updat 'registri ' 6.668032m i0813 19:55:17.334430 26127 log.cpp:704 ] attempt truncat log 3 i0813 19:55:17.334566 26132 coordinator.cpp:341 ] coordin attempt write truncat action posit 4 i0813 19:55:17.335283 26129 replica.cpp:511 ] replica receiv write request posit 4 i0813 19:55:17.335615 26127 slave.cpp:3058 ] receiv ping slave-observ ( 1 ) @ 172.17.2.10:60249 i0813 19:55:17.335816 26129 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 458268n i0813 19:55:17.335908 26137 master.cpp:3698 ] regist slave 20150813-195517-167907756-60249-26100-s0 slave ( 2 ) @ 172.17.2.10:60249 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.335983 26129 replica.cpp:679 ] persist action 4 i0813 19:55:17.336019 26136 slave.cpp:859 ] regist master master @ 172.17.2.10:60249 ; given slave id 20150813-195517-167907756-60249-26100-s0 i0813 19:55:17.336073 26136 fetcher.cpp:77 ] clear fetcher cach i0813 19:55:17.336220 26127 status_update_manager.cpp:183 ] resum send statu updat i0813 19:55:17.336328 26128 hierarchical.hpp:540 ] ad slave 20150813-195517-167907756-60249-26100-s0 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0813 19:55:17.336599 26138 replica.cpp:658 ] replica receiv learn notic posit 4 i0813 19:55:17.336910 26128 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.336957 26128 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s0 580663n i0813 19:55:17.337016 26136 slave.cpp:882 ] checkpoint slaveinfo '/tmp/mesos-ii8gua/1/meta/slaves/20150813-195517-167907756-60249-26100-s0/slave.info' i0813 19:55:17.337035 26138 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 403607n i0813 19:55:17.337138 26138 leveldb.cpp:401 ] delet ~2 key leveldb took 77040n i0813 19:55:17.337167 26138 replica.cpp:679 ] persist action 4 i0813 19:55:17.337208 26138 replica.cpp:664 ] replica learn truncat action posit 4 i0813 19:55:17.337514 26136 slave.cpp:918 ] forward total oversubscrib resourc i0813 19:55:17.337745 26131 master.cpp:3997 ] receiv updat slave 20150813-195517-167907756-60249-26100-s0 slave ( 2 ) @ 172.17.2.10:60249 ( 297daca2d01a ) total oversubscrib resourc i0813 19:55:17.338240 26131 hierarchical.hpp:600 ] slave 20150813-195517-167907756-60249-26100-s0 ( 297daca2d01a ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0813 19:55:17.338479 26131 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.338505 26131 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s0 216259n i0813 19:55:17.504086 26124 slave.cpp:1209 ] will retri registr 1.92618421sec necessari i0813 19:55:17.504408 26124 master.cpp:3635 ] regist slave slave ( 3 ) @ 172.17.2.10:60249 ( 297daca2d01a ) id 20150813-195517-167907756-60249-26100-s1 i0813 19:55:17.505203 26124 registrar.cpp:443 ] appli 1 oper 144314n ; attempt updat 'registry' i0813 19:55:17.507616 26124 log.cpp:685 ] attempt append 511 byte log i0813 19:55:17.507796 26132 coordinator.cpp:341 ] coordin attempt write append action posit 5 i0813 19:55:17.508735 26128 replica.cpp:511 ] replica receiv write request posit 5 i0813 19:55:17.509291 26128 leveldb.cpp:343 ] persist action ( 530 byte ) leveldb took 527776n i0813 19:55:17.509328 26128 replica.cpp:679 ] persist action 5 i0813 19:55:17.509945 26124 replica.cpp:658 ] replica receiv learn notic posit 5 i0813 19:55:17.510393 26124 leveldb.cpp:343 ] persist action ( 532 byte ) leveldb took 438543n i0813 19:55:17.510416 26124 replica.cpp:679 ] persist action 5 i0813 19:55:17.510437 26124 replica.cpp:664 ] replica learn append action posit 5 i0813 19:55:17.511907 26125 registrar.cpp:488 ] success updat 'registri ' 6624u i0813 19:55:17.512225 26138 log.cpp:704 ] attempt truncat log 5 i0813 19:55:17.512305 26136 coordinator.cpp:341 ] coordin attempt write truncat action posit 6 i0813 19:55:17.513066 26133 slave.cpp:3058 ] receiv ping slave-observ ( 2 ) @ 172.17.2.10:60249 i0813 19:55:17.513242 26133 slave.cpp:859 ] regist master master @ 172.17.2.10:60249 ; given slave id 20150813-195517-167907756-60249-26100-s1 i0813 19:55:17.513221 26126 master.cpp:3698 ] regist slave 20150813-195517-167907756-60249-26100-s1 slave ( 3 ) @ 172.17.2.10:60249 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.513089 26129 replica.cpp:511 ] replica receiv write request posit 6 i0813 19:55:17.513393 26133 fetcher.cpp:77 ] clear fetcher cach i0813 19:55:17.513380 26138 hierarchical.hpp:540 ] ad slave 20150813-195517-167907756-60249-26100-s1 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0813 19:55:17.513805 26132 status_update_manager.cpp:183 ] resum send statu updat i0813 19:55:17.513949 26129 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 340511n i0813 19:55:17.514046 26138 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.514050 26129 replica.cpp:679 ] persist action 6 i0813 19:55:17.514195 26133 slave.cpp:882 ] checkpoint slaveinfo '/tmp/mesos-ii8gua/2/meta/slaves/20150813-195517-167907756-60249-26100-s1/slave.info' i0813 19:55:17.514140 26138 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s1 417609n i0813 19:55:17.514704 26133 slave.cpp:918 ] forward total oversubscrib resourc i0813 19:55:17.514708 26138 replica.cpp:658 ] replica receiv learn notic posit 6 i0813 19:55:17.514880 26133 master.cpp:3997 ] receiv updat slave 20150813-195517-167907756-60249-26100-s1 slave ( 3 ) @ 172.17.2.10:60249 ( 297daca2d01a ) total oversubscrib resourc i0813 19:55:17.515244 26127 hierarchical.hpp:600 ] slave 20150813-195517-167907756-60249-26100-s1 ( 297daca2d01a ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0813 19:55:17.515454 26138 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 640882n i0813 19:55:17.515522 26138 leveldb.cpp:401 ] delet ~2 key leveldb took 56550n i0813 19:55:17.515547 26138 replica.cpp:679 ] persist action 6 i0813 19:55:17.515581 26138 replica.cpp:664 ] replica learn truncat action posit 6 i0813 19:55:17.515802 26127 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.515866 26127 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s1 591007n i0813 19:55:17.984196 26135 slave.cpp:1209 ] will retri registr 1.542495291sec necessari i0813 19:55:17.984391 26138 master.cpp:3635 ] regist slave slave ( 1 ) @ 172.17.2.10:60249 ( 297daca2d01a ) id 20150813-195517-167907756-60249-26100-s2 i0813 19:55:17.985170 26133 registrar.cpp:443 ] appli 1 oper 202126n ; attempt updat 'registry' i0813 19:55:17.987498 26133 log.cpp:685 ] attempt append 678 byte log i0813 19:55:17.987656 26123 coordinator.cpp:341 ] coordin attempt write append action posit 7 i0813 19:55:17.988704 26138 replica.cpp:511 ] replica receiv write request posit 7 i0813 19:55:17.989223 26138 leveldb.cpp:343 ] persist action ( 697 byte ) leveldb took 490422n i0813 19:55:17.989248 26138 replica.cpp:679 ] persist action 7 i0813 19:55:17.989972 26126 replica.cpp:658 ] replica receiv learn notic posit 7 i0813 19:55:17.990401 26126 leveldb.cpp:343 ] persist action ( 699 byte ) leveldb took 404333n i0813 19:55:17.990420 26126 replica.cpp:679 ] persist action 7 i0813 19:55:17.990440 26126 replica.cpp:664 ] replica learn append action posit 7 i0813 19:55:17.994066 26123 registrar.cpp:488 ] success updat 'registri ' 8.788224m i0813 19:55:17.994436 26134 log.cpp:704 ] attempt truncat log 7 i0813 19:55:17.994575 26123 coordinator.cpp:341 ] coordin attempt write truncat action posit 8 i0813 19:55:17.995070 26134 slave.cpp:3058 ] receiv ping slave-observ ( 3 ) @ 172.17.2.10:60249 i0813 19:55:17.995291 26134 slave.cpp:859 ] regist master master @ 172.17.2.10:60249 ; given slave id 20150813-195517-167907756-60249-26100-s2 i0813 19:55:17.995319 26134 fetcher.cpp:77 ] clear fetcher cach i0813 19:55:17.995246 26129 master.cpp:3698 ] regist slave 20150813-195517-167907756-60249-26100-s2 slave ( 1 ) @ 172.17.2.10:60249 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] i0813 19:55:17.995565 26123 status_update_manager.cpp:183 ] resum send statu updat i0813 19:55:17.995579 26129 replica.cpp:511 ] replica receiv write request posit 8 i0813 19:55:17.996016 26134 slave.cpp:882 ] checkpoint slaveinfo '/tmp/mesos-ii8gua/0/meta/slaves/20150813-195517-167907756-60249-26100-s2/slave.info' i0813 19:55:17.996039 26129 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 440511n i0813 19:55:17.996067 26129 replica.cpp:679 ] persist action 8 i0813 19:55:17.996294 26128 hierarchical.hpp:540 ] ad slave 20150813-195517-167907756-60249-26100-s2 ( 297daca2d01a ) cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] ( alloc : ) i0813 19:55:17.996556 26134 slave.cpp:918 ] forward total oversubscrib resourc i0813 19:55:17.996623 26133 replica.cpp:658 ] replica receiv learn notic posit 8 i0813 19:55:17.997095 26134 master.cpp:3997 ] receiv updat slave 20150813-195517-167907756-60249-26100-s2 slave ( 1 ) @ 172.17.2.10:60249 ( 297daca2d01a ) total oversubscrib resourc i0813 19:55:17.997263 26133 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 442619n i0813 19:55:17.997385 26133 leveldb.cpp:401 ] delet ~2 key leveldb took 95741n i0813 19:55:17.997413 26133 replica.cpp:679 ] persist action 8 i0813 19:55:17.997465 26133 replica.cpp:664 ] replica learn truncat action posit 8 i0813 19:55:17.997756 26128 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.997925 26128 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s2 1.14489m i0813 19:55:17.998159 26128 hierarchical.hpp:600 ] slave 20150813-195517-167907756-60249-26100-s2 ( 297daca2d01a ) updat oversubscrib resourc ( total : cpu ( * ) :2 ; mem ( * ) :10240 ; disk ( * ) :3.70122e+06 ; port ( * ) : [ 31000-32000 ] , alloc : ) i0813 19:55:17.998445 26128 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:17.998471 26128 hierarchical.hpp:926 ] perform alloc slave 20150813-195517-167907756-60249-26100-s2 218856n i0813 19:55:18.190146 26133 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:18.190217 26133 hierarchical.hpp:908 ] perform alloc 3 slave 637042n i0813 19:55:19.191346 26131 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:19.191915 26131 hierarchical.hpp:908 ] perform alloc 3 slave 1.215355m i0813 19:55:20.193631 26135 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:20.193709 26135 hierarchical.hpp:908 ] perform alloc 3 slave 834491n i0813 19:55:21.194805 26134 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:21.194870 26134 hierarchical.hpp:908 ] perform alloc 3 slave 536547n i0813 19:55:22.196143 26137 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:22.196216 26137 hierarchical.hpp:908 ] perform alloc 3 slave 755140n i0813 19:55:23.197412 26132 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:23.197979 26132 hierarchical.hpp:908 ] perform alloc 3 slave 1.223984m i0813 19:55:24.199429 26132 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:24.199735 26132 hierarchical.hpp:908 ] perform alloc 3 slave 904654n i0813 19:55:25.200978 26127 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:25.201206 26127 hierarchical.hpp:908 ] perform alloc 3 slave 939979n i0813 19:55:26.203023 26132 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:26.203101 26132 hierarchical.hpp:908 ] perform alloc 3 slave 721178n i0813 19:55:27.204815 26126 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:27.204888 26126 hierarchical.hpp:908 ] perform alloc 3 slave 767983n i0813 19:55:28.206374 26126 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:28.206444 26126 hierarchical.hpp:908 ] perform alloc 3 slave 745214n i0813 19:55:29.207515 26124 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:29.207579 26124 hierarchical.hpp:908 ] perform alloc 3 slave 551217n i0813 19:55:30.208966 26136 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:30.209053 26136 hierarchical.hpp:908 ] perform alloc 3 slave 649887n i0813 19:55:31.210078 26123 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:31.210144 26123 hierarchical.hpp:908 ] perform alloc 3 slave 558919n i0813 19:55:32.211027 26130 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:32.211045 26129 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:32.211084 26132 slave.cpp:4226 ] queri resourc estim oversubscrib resourc i0813 19:55:32.211386 26129 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:32.211688 26132 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:32.211853 26133 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:32.212035 26133 hierarchical.hpp:908 ] perform alloc 3 slave 898985n i0813 19:55:32.212169 26133 slave.cpp:4240 ] receiv oversubscrib resourc resourc estim i0813 19:55:32.336745 26135 slave.cpp:3058 ] receiv ping slave-observ ( 1 ) @ 172.17.2.10:60249 i0813 19:55:32.514333 26129 slave.cpp:3058 ] receiv ping slave-observ ( 2 ) @ 172.17.2.10:60249 i0813 19:55:32.996134 26128 slave.cpp:3058 ] receiv ping slave-observ ( 3 ) @ 172.17.2.10:60249 i0813 19:55:33.213248 26128 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:33.213326 26128 hierarchical.hpp:908 ] perform alloc 3 slave 827511n i0813 19:55:34.214326 26125 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:34.214391 26125 hierarchical.hpp:908 ] perform alloc 3 slave 546422n i0813 19:55:35.215909 26123 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:35.215973 26123 hierarchical.hpp:908 ] perform alloc 3 slave 627190n i0813 19:55:36.217156 26134 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:36.217339 26134 hierarchical.hpp:908 ] perform alloc 3 slave 906249n i0813 19:55:37.218739 26132 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:37.219169 26132 hierarchical.hpp:908 ] perform alloc 3 slave 1.102465m i0813 19:55:38.220641 26133 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:38.220711 26133 hierarchical.hpp:908 ] perform alloc 3 slave 643146n i0813 19:55:39.221976 26133 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:39.222118 26133 hierarchical.hpp:908 ] perform alloc 3 slave 845334n i0813 19:55:40.223338 26129 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:40.223546 26129 hierarchical.hpp:908 ] perform alloc 3 slave 849995n i0813 19:55:41.225558 26138 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:41.225752 26138 hierarchical.hpp:908 ] perform alloc 3 slave 958480n i0813 19:55:42.227176 26131 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:42.227378 26131 hierarchical.hpp:908 ] perform alloc 3 slave 927048n i0813 19:55:43.228813 26137 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:43.229441 26137 hierarchical.hpp:908 ] perform alloc 3 slave 1.310118m i0813 19:55:44.230828 26135 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:44.231142 26135 hierarchical.hpp:908 ] perform alloc 3 slave 896369n i0813 19:55:45.232656 26135 hierarchical.hpp:1008 ] no resourc avail alloc ! i0813 19:55:45.232903 26135 hierarchical.hpp:908 ] perform alloc 3 slave 1.357693m i0813 19:55:46.234973 26137 hierarchical.hpp:1008 { code }",MESOS-3273,5.0
"http request nest path properli handl libprocess for exampl , master add rout `` /api/v1/schedul '' , handler name `` api/v1/schedul '' ad 'master ' libprocess . but request post path , process : :visit ( ) look http handler name `` api '' instead `` api/v1/schedul '' . ideal libprocess look handler follow prefer order : '' api/v1/schedul '' -- > `` api/v1 '' -- > `` api ''",MESOS-3237,2.0
"fix master metric schedul call current master increment metric old style messag driver receiv call . sinc driver send call , master updat metric correctli .",MESOS-3195,3.0
"containerinfo : :imag : :appc : :id option as i comment : http : //reviews.apache.org/r/34136/ current containerinfo : :imag : :appc defin follow { noformat : title= } messag appc { requir string name = 1 ; requir string id = 2 ; option label label = 3 ; } { noformat } in { { id } } requir field . when user specifi imag task like use imag id ( much like use docker rkt launch contain , often use { { ubuntu } } { { ubuntu : latest } } seldom sha512 id ) chang option . the motiv scenario : framework meso use someth like { { image=ubuntu:14.04 '' } } run task { { image=ubuntu } } default { { image=ubuntu : latest } } , oper swap latest version new task request { { image=ubuntu } } . if allow user specifi { { image=ubuntu : live } } , swap live version cover well . thi allow oper releas import imag updat ( e.g. , secur patch ) pick new task cluster without ask user updat job/task config .",MESOS-3192,1.0
"perform self bind mount rootf fs : :chroot : :enter . syscal 'pivot_root ' requir old new root filesystem . otherwis , user receiv `` devic resourc busi '' error . current , reli provision prepar rootf proper bind mount need pivot_root succeed . the drawback approach potenti pollut host mount tabl requir cleanup logic . for instanc , test , creat test rootf copi host file . we need self bind mount pivot_root . that pollut host mount tabl might leak mount test crash lazi umount : http : //github.com/apache/mesos/blob/master/src/tests/containerizer/launch_tests.cpp # l96-l102 what i propos alway perform recurs self bind mount rootf fs : :chroot : :enter ( enter new mount namespac ) . seem also done libcontain : http : //github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go # l402",MESOS-3178,2.0
design doc docker imag registri client creat design document docker registri authent compon baselin implement .,MESOS-3166,3.0
"port bootstrap cmake bootstrap lot signific thing , like set git commit hook . we want someth like bootstrap run also system n't bash -- ideal run cmake .",MESOS-3134,5.0
"updat persist volum slave restart problemat . just realiz review http : //reviews.apache.org/r/34135 sinc n't checkpoint 'resourc ' meso container , slave restart recov , 'resourc ' contain struct empti , symlink exist sandbox . we 'll end tri creat alreadi exist symlink ( fail ) . i think ignor creation alreadi exist .",MESOS-3124,3.0
"alway disabl sslv2 the ssl protocol mismatch test fail centos7 match sslv2 sslv2 . sinc version protocol highli discourag anyway , let 's disabl complet unless request otherwis .",MESOS-3121,2.0
"master n't properli handl subscrib call master : :subscrib ( ) incorrectli handl re-registr . it handl registr request ( `` re-registr '' ) bug loop ( ! frameworkinfo.has_id ( ) ) . { code } void master : :subscrib ( const upid & , const schedul : :call : :subscrib & subscrib ) { const frameworkinfo & frameworkinfo = subscribe.framework_info ( ) ; // todo ( vinod ) : instead call ' ( re- ) registerframework ( ) ' // refactor method call 'subscrib ( ) ' . ( frameworkinfo.has_id ( ) || frameworkinfo.id ( ) == `` '' ) { registerframework ( , frameworkinfo ) ; } els { reregisterframework ( , frameworkinfo , subscribe.forc ( ) ) ; } } { code }",MESOS-3055,2.0
"ssl test fail depend hostnam configur depend /etc/host configur , ssl test fail bad hostnam match certif . we avoid explicitli match hostnam certif ip use test .",MESOS-3005,3.0
"creat `` demo '' http api client we want creat simpl `` demo '' http api client ( java , python go ) serv `` exampl framework '' peopl want use new api framework . the scope fairli limit ( eg , launch simpl contain task ? ) suffici exercis new api endpoint messages/cap . scope : tbd non-goal : - creat `` best-of-bre '' framework deliv specif function ; - creat integr test http api .",MESOS-3001,8.0
ssl connect failur caus fail check . { code } [ run ] ssltest.basicsameprocess f0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507 ] check fail : 'self- > bev ' must non null { code },MESOS-2997,3.0
ssl test n't work -- gtest_shuffl,MESOS-2975,3.0
ssl test n't work -- gtest_repeat commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f author : jori van remoorter < joris.van.remoorter @ gmail.com > date : wed jul 1 16:16:52 2015 -0700 mesos-2973 : allow ssl test run use gtest_repeat . the ssl ctx object carri set reiniti ( ) call . re-construct object avoid state transit . review : http : //reviews.apache.org/r/36074,MESOS-2973,3.0
meso fail compil mac libssl libev enabl .. /configur -- enable-debug -- enable-libev -- enable-ssl & & make produc follow error : poll.cpp ' || echo ' .. / .. / .. /3rdparty/libprocess/ ' ` src/libevent_poll.cpp libtool : compil : g++ -dpackage_name=\ '' libprocess\ '' -dpackage_tarname=\ '' libprocess\ '' -dpackage_version=\ '' 0.0.1\ '' `` -dpackage_string=\ '' libprocess 0.0.1\ '' '' -dpackage_bugreport=\ '' \ '' -dpackage_url=\ '' \ '' -dpackage=\ '' libprocess\ '' -dversion=\ '' 0.0.1\ '' -dstdc_headers=1 -dhave_sys_types_h=1 -dhave_sys_stat_h=1 -dhave_stdlib_h=1 -dhave_string_h=1 -dhave_memory_h=1 -dhave_strings_h=1 -dhave_inttypes_h=1 -dhave_stdint_h=1 -dhave_unistd_h=1 -dhave_dlfcn_h=1 -dlt_objdir=\ '' .libs/\ '' -dhave_apr_pools_h=1 -dhave_libapr_1=1 -dhave_svn_version_h=1 -dhave_libsvn_subr_1=1 -dhave_svn_delta_h=1 -dhave_libsvn_delta_1=1 -dhave_libcurl=1 -dhave_event2_event_h=1 -dhave_libevent=1 -dhave_event2_thread_h=1 -dhave_libevent_pthreads=1 -dhave_openssl_ssl_h=1 -dhave_libssl=1 -dhave_libcrypto=1 -dhave_event2_bufferevent_ssl_h=1 -dhave_libevent_openssl=1 -duse_ssl_socket=1 -dhave_pthread_prio_inherit=1 -dhave_pthread=1 -dhave_libz=1 -dhave_libdl=1 -i . -i .. / .. / .. /3rdparty/libprocess -i .. / .. / .. /3rdparty/libprocess/includ -i .. / .. / .. /3rdparty/libprocess/3rdparty/stout/includ -i3rdparty/boost-1.53.0 -i3rdparty/libev-4.15 -i3rdparty/picojson-4f93734 -i3rdparty/glog-0.3.3/src -i3rdparty/ry-http-parser-1c3624a -i/usr/local/opt/openssl/includ -i/usr/local/opt/libevent/includ -i/usr/local/opt/subversion/include/subversion-1 -i/usr/include/apr-1 -i/usr/include/apr-1.0 -g1 -o0 -std=c++11 -stdlib=libc++ -dgtest_use_own_tr1_tuple=1 -mt libprocess_la-libevent_poll.lo -md -mp -mf .deps/libprocess_la-libevent_poll.tpo -c .. / .. / .. /3rdparty/libprocess/src/libevent_poll.cpp -fno-common -dpic -o libprocess_la-libevent_poll.o mv -f .deps/libprocess_la-socket.tpo .deps/libprocess_la-socket.plo mv -f .deps/libprocess_la-subprocess.tpo .deps/libprocess_la-subprocess.plo mv -f .deps/libprocess_la-libevent.tpo .deps/libprocess_la-libevent.plo mv -f .deps/libprocess_la-metrics.tpo .deps/libprocess_la-metrics.plo in file includ .. / .. / .. /3rdparty/libprocess/src/libevent_ssl_socket.cpp:11 : in file includ .. / .. / .. /3rdparty/libprocess/include/process/queue.hpp:9 : .. / .. / .. /3rdparty/libprocess/include/process/future.hpp:849:7 : error : viabl convers 'const process : :futur < const process : :futur < process : :network : :socket > > ' 'const process : :network : :socket' set ( u ) ; ^ .. / .. / .. /3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10 : note : instanti function templat special 'process : :futur < process : :network : :socket > : :futur < process : :futur < const process : :futur < process : :network : :socket > > > ' request return accept_queue.get ( ) ^ .. / .. / .. /3rdparty/libprocess/include/process/socket.hpp:21:7 : note : candid constructor ( implicit move constructor ) viabl : known convers 'const process : :futur < const process : :futur < process : :network : :socket > > ' 'process : :network : :socket & & ' 1st argument class socket ^ .. / .. / .. /3rdparty/libprocess/include/process/socket.hpp:21:7 : note : candid constructor ( implicit copi constructor ) viabl : known convers 'const process : :futur < const process : :futur < process : :network : :socket > > ' 'const process : :network : :socket & ' 1st argument class socket ^ .. / .. / .. /3rdparty/libprocess/include/process/future.hpp:411:21 : note : pass argument paramet '_t ' bool set ( const t & _t ) ; ^ 1 error gener . make [ 4 ] : * * * [ libprocess_la-libevent_ssl_socket.lo ] error 1 make [ 4 ] : * * * wait unfinish job .... mv -f .deps/libprocess_la-libevent_poll.tpo .deps/libprocess_la-libevent_poll.plo mv -f .deps/libprocess_la-openssl.tpo .deps/libprocess_la-openssl.plo mv -f .deps/libprocess_la-process.tpo .deps/libprocess_la-process.plo make [ 3 ] : * * * [ all-recurs ] error 1 make [ 2 ] : * * * [ all-recurs ] error 1 make [ 1 ] : * * * [ ] error 2 make : * * * [ all-recurs ] error 1,MESOS-2943,2.0
"framework overcommit oversubscrib resourc master failov . thi due bug hierarch alloc . here sequenc event : 1 ) slave use fix resourc estim advertis 4 revoc cpu 2 ) framework a launch task use 4 revoc cpu 3 ) master fail 4 ) slave re-regist new master , send updateslavemessag 4 revoc cpu oversubscrib resourc 5 ) framework a n't regist yet , therefor , slave 's avail resourc 4 revoc cpu 6 ) framework a regist receiv addit 4 revoc cpu . so launch anoth task 4 revoc cpu ( mean 8 total ! ) the problem due way calcul 'alloc ' resourc alloc 'updateslav ' . if framework regist , 'alloc ' accur ( check block 'addslav ' ) . { code } templat < class rolesort , class frameworksort > void hierarchicalallocatorprocess < rolesort , frameworksort > : :updateslav ( const slaveid & slaveid , const resourc & oversubscrib ) { check ( initi ) ; check ( slaves.contain ( slaveid ) ) ; // check oversubscrib resourc revoc . check_eq ( oversubscrib , oversubscribed.revoc ( ) ) ; // updat total resourc . // first remov old oversubscrib resourc total . slave [ slaveid ] .total -= slave [ slaveid ] .total.revoc ( ) ; // now add new estim oversubscrib resourc . slave [ slaveid ] .total += oversubscrib ; // now , updat total resourc role sorter . rolesorter- > updat ( slaveid , slave [ slaveid ] .total.unreserv ( ) ) ; // calcul current alloc oversubscrib resourc . resourc alloc ; foreachkey ( const std : :string & role , role ) { alloc += rolesorter- > alloc ( role , slaveid ) .revoc ( ) ; } // updat avail resourc . // first remov old oversubscrib resourc avail . slave [ slaveid ] .avail -= slave [ slaveid ] .available.revoc ( ) ; // now add new estim avail oversubscrib resourc . slave [ slaveid ] .avail += oversubscrib - alloc ; log ( info ) < < `` slave `` < < slaveid < < `` ( `` < < slave [ slaveid ] .hostnam < < `` ) updat oversubscrib resourc `` < < oversubscrib < < `` ( total : `` < < slave [ slaveid ] .total < < `` , avail : `` < < slave [ slaveid ] .avail < < `` ) '' ; alloc ( slaveid ) ; } templat < class rolesort , class frameworksort > void hierarchicalallocatorprocess < rolesort , frameworksort > : :addslav ( const slaveid & slaveid , const slaveinfo & slaveinfo , const resourc & total , const hashmap < frameworkid , resourc > & use ) { check ( initi ) ; check ( ! slaves.contain ( slaveid ) ) ; rolesorter- > add ( slaveid , total.unreserv ( ) ) ; foreachpair ( const frameworkid & frameworkid , const resourc & alloc , use ) { ( frameworks.contain ( frameworkid ) ) { const std : :string & role = framework [ frameworkid ] .role ; // todo ( bmahler ) : valid reserv resourc // framework 's role . rolesorter- > alloc ( role , slaveid , allocated.unreserv ( ) ) ; frameworksort [ role ] - > add ( slaveid , alloc ) ; frameworksort [ role ] - > alloc ( frameworkid.valu ( ) , slaveid , alloc ) ; } } ... } { code }",MESOS-2919,3.0
sandbox url n't work web-ui use ssl the link sandbox web ui n't work ssl enabl . thi happen certif master slave match . thi consequ redirect happen serv file . the resolut current set certif serv hostnam master slave .,MESOS-2890,3.0
add ssl switch python configur the python egg requir explicit depend ssl . add python configur ssl enabl .,MESOS-2889,3.0
convert portmappingstatist use automat json encoding/decod simplifi portmappingstatist use json : :protocol protobuf : :pars convert resourcestatist to/from line format . thi chang simplifi implement mesos-2332 .,MESOS-2874,2.0
"oversubscriptiontest.fixedresourceestim flaki came http : //reviews.apache.org/r/35395/ { code } [ run ] oversubscriptiontest.fixedresourceestim i0613 13:41:02.604904 19367 exec.cpp:132 ] version : 0.23.0 i0613 13:41:02.610995 19398 exec.cpp:206 ] executor regist slave 20150613-134102-3142697795-48295-13678-s0 regist executor pomona.apache.org start task 7d78a3ef-2de9-46c9-811c-b2c0e2d50578 fork command 19410 sh -c 'sleep 1000' .. / .. /src/tests/oversubscription_tests.cpp:579 : failur mock function call time expect - return directli . function call : statusupd ( 0x7ffffbc0c4e0 , @ 0x2ade2bffa910 96-byte object < 50-3e d7-22 de-2a 00-00 00-00 00-00 00-00 00-00 d0-c4 00-48 de-2a 00-00 50-71 ac-01 00-00 00-00 01-00 00-00 02-00 00-00 50-71 ac-01 00-00 00-00 b0-66 00-48 de-2a 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-2a 00-00 e7-17 a8-bb 0c-5f d5-41 10-31 01-48 de-2a 00-00 00-00 00-00 4b-03 00-00 > ) expect : call actual : call twice - over-satur activ [ fail ] oversubscriptiontest.fixedresourceestim ( 714 ms ) { code }",MESOS-2869,1.0
"slave send oversubscrib resourc inform master failov . after master failov , total amount oversubscrib resourc chang , slave send updateslav messag new master . the slave need send inform new master regardless .",MESOS-2866,3.0
master crash framework chang princip re-registr the master updat avoid crash framework re-regist differ princip .,MESOS-2842,5.0
"flaki test : fetchercachehttptest.httpcachedseri fetchercachehttptest.httpcachedseri observ fail ( far ) , normal work fine . here failur output : [ run ] fetchercachehttptest.httpcachedseri gmock warn : uninterest mock function call - return directli . function call : resourceoff ( 0x3cca8e0 , @ 0x2b1053422b20 { 128-byte object < d0-e1 59-49 10-2b 00-00 00-00 00-00 00-00 00-00 10-a1 00-68 10-2b 00-00 a0-d 00-68 10-2b 00-00 20-df 00-68 10-2b 00-00 40-9c 00-68 10-2b 00-00 90-2d 00-68 10-2b 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2b 00-00 00-00 00-00 0f-00 00-00 > } ) stack trace : f0604 13:08:16.377907 6813 fetcher_cache_tests.cpp:354 ] check_readi ( offer ) : pend fail wait resourc offer * * * check failur stack trace : * * * @ 0x2b10488ff6c0 googl : :logmessag : :fail ( ) @ 0x2b10488ff60c googl : :logmessag : :sendtolog ( ) @ 0x2b10488ff00e googl : :logmessag : :flush ( ) @ 0x2b1048901f22 googl : :logmessagefat : :~logmessagefat ( ) @ 0x9721e4 _checkfat : :~_checkfat ( ) @ 0xb4da86 meso : :intern : :test : :fetchercachetest : :launchtask ( ) @ 0xb53f8d meso : :intern : :test : :fetchercachehttptest_httpcachedserialized_test : :testbodi ( ) @ 0x116ac21 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1165e1e test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x114e1df test : :test : :run ( ) @ 0x114e902 test : :testinfo : :run ( ) @ 0x114ee8a test : :testcas : :run ( ) @ 0x1153b54 test : :intern : :unittestimpl : :runalltest ( ) @ 0x116ba93 test : :intern : :handlesehexceptionsinmethodifsupport < > ( ) @ 0x1166b0f test : :intern : :handleexceptionsinmethodifsupport < > ( ) @ 0x1152a60 test : :unittest : :run ( ) @ 0xcbc50f main @ 0x2b104af78ec5 ( unknown ) @ 0x867559 ( unknown ) make [ 4 ] : * * * [ check-loc ] abort make [ 4 ] : leav directori ` /home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make [ 3 ] : * * * [ check-am ] error 2 make [ 3 ] : leav directori ` /home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make [ 2 ] : * * * [ check ] error 2 make [ 2 ] : leav directori ` /home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make [ 1 ] : * * * [ check-recurs ] error 1 make [ 1 ] : leav directori ` /home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build' make : * * * [ distcheck ] error 1",MESOS-2815,2.0
"slave call resourc estim whenev want forward oversubscrib resourc current , poll resourc estim decoupl loop slave forward oversubscrib resourc . now slave send updat chang previou estim , poll resourc estim whenev want send estim . one advantag estim slow respond , slave n't keep forward estim stale 'oversubscrib ' valu caus revoc task unintent launch .",MESOS-2808,3.0
"allow resourc estim get resourc usag inform . thi includ two thing : 1 ) we need expos resourcemonitor : :usag modul writer access . we could defin protobuf messag . 2 ) we need allow resourceestim call 'resourcemonitor : :usag ( ) ' . we could either expos resourcemonitor , pass lambda resourc estim .",MESOS-2764,5.0
extend queue disciplin wrapper expos network isol statist export traffic control statist queue librari enabl report impact network bandwidth statist .,MESOS-2750,3.0
"expos resourc along resourcestatist resourc monitor right , resourc monitor return usag contain containerid , executorinfo resourcestatist . in order resourc estimator/qo control calcul usag slack , tell contain use revok resourc , need expos resourc current assign contain . thi requir us chang container interfac get resourc well call 'usag ( ) ' .",MESOS-2741,5.0
"chang interact slave resourc estim poll push thi make semant clear . the resourc estim control speed send resourc estim slave . to avoid cyclic depend , slave regist callback resourc estim resourc estim simpli invok callback 's new estim readi . the callback defer slave 's main event queue .",MESOS-2735,3.0
design doc executor http api thi track design executor http api .,MESOS-2708,2.0
"c++ schedul librari send http call master onc schedul librari send call messag , updat send call http request `` /call '' endpoint master .",MESOS-2552,3.0
"chang default leaf qdisc fq_codel insid contain when enabl bandwidth cap , htb use egress side insid contain , howev , default leaf qdisc htb class still pfifo_fast , known buffer bloat . chang default leaf qdisc fq_codel : ` tc qd add dev eth0 parent 1:1 fq_codel ` i longer see packet drop chang .",MESOS-2514,1.0
creat synchron valid call /call endpoint return 202 accept code basic valid . in case invalid return 4xx code . we creat mechan valid 'request ' send back appropri code .,MESOS-2497,8.0
"refactor valid master . there sever motiv . we process ad dynam reserv persist volum support master . to , master need valid relev oper framework ( see offer : :oper mesos.proto ) . the exist valid style master hard extend , compos re-us . anoth motiv unit test ( mesos-1064 ) . right , write integr test valid unfortun .",MESOS-2305,3.0
"hooktest.verifyslavelaunchexecutorhook flaki observ intern ci { code } [ run ] hooktest.verifyslavelaunchexecutorhook use temporari directori '/tmp/hooktest_verifyslavelaunchexecutorhook_gjbgme' i0114 18:51:34.659353 4720 leveldb.cpp:176 ] open db 1.255951m i0114 18:51:34.662112 4720 leveldb.cpp:183 ] compact db 596090n i0114 18:51:34.662364 4720 leveldb.cpp:198 ] creat db iter 177877n i0114 18:51:34.662719 4720 leveldb.cpp:204 ] seek begin db 19709n i0114 18:51:34.663010 4720 leveldb.cpp:273 ] iter 0 key db 18208n i0114 18:51:34.663312 4720 replica.cpp:744 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0114 18:51:34.664266 4735 recover.cpp:449 ] start replica recoveri i0114 18:51:34.664908 4735 recover.cpp:475 ] replica empti statu i0114 18:51:34.667842 4734 replica.cpp:641 ] replica empti statu receiv broadcast recov request i0114 18:51:34.669117 4735 recover.cpp:195 ] receiv recov respons replica empti statu i0114 18:51:34.677913 4735 recover.cpp:566 ] updat replica statu start i0114 18:51:34.683157 4735 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 137939n i0114 18:51:34.683507 4735 replica.cpp:323 ] persist replica statu start i0114 18:51:34.684013 4735 recover.cpp:475 ] replica start statu i0114 18:51:34.685554 4738 replica.cpp:641 ] replica start statu receiv broadcast recov request i0114 18:51:34.696512 4736 recover.cpp:195 ] receiv recov respons replica start statu i0114 18:51:34.700552 4735 recover.cpp:566 ] updat replica statu vote i0114 18:51:34.701128 4735 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 115624n i0114 18:51:34.701478 4735 replica.cpp:323 ] persist replica statu vote i0114 18:51:34.701817 4735 recover.cpp:580 ] success join paxo group i0114 18:51:34.702569 4735 recover.cpp:464 ] recov process termin i0114 18:51:34.716439 4736 master.cpp:262 ] master 20150114-185134-2272962752-57018-4720 ( fedora-19 ) start 192.168.122.135:57018 i0114 18:51:34.716913 4736 master.cpp:308 ] master allow authent framework regist i0114 18:51:34.717136 4736 master.cpp:313 ] master allow authent slave regist i0114 18:51:34.717488 4736 credentials.hpp:36 ] load credenti authent '/tmp/hooktest_verifyslavelaunchexecutorhook_gjbgme/credentials' i0114 18:51:34.718077 4736 master.cpp:357 ] author enabl i0114 18:51:34.719238 4738 whitelist_watcher.cpp:65 ] no whitelist given i0114 18:51:34.719755 4737 hierarchical_allocator_process.hpp:285 ] initi hierarch alloc process i0114 18:51:34.722584 4736 master.cpp:1219 ] the newli elect leader master @ 192.168.122.135:57018 id 20150114-185134-2272962752-57018-4720 i0114 18:51:34.722865 4736 master.cpp:1232 ] elect lead master ! i0114 18:51:34.723310 4736 master.cpp:1050 ] recov registrar i0114 18:51:34.723760 4734 registrar.cpp:313 ] recov registrar i0114 18:51:34.725229 4740 log.cpp:660 ] attempt start writer i0114 18:51:34.727893 4739 replica.cpp:477 ] replica receiv implicit promis request propos 1 i0114 18:51:34.728425 4739 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 114781n i0114 18:51:34.728662 4739 replica.cpp:345 ] persist promis 1 i0114 18:51:34.731271 4741 coordinator.cpp:230 ] coordin attemp fill miss posit i0114 18:51:34.733223 4734 replica.cpp:378 ] replica receiv explicit promis request posit 0 propos 2 i0114 18:51:34.734076 4734 leveldb.cpp:343 ] persist action ( 8 byte ) leveldb took 87441n i0114 18:51:34.734441 4734 replica.cpp:679 ] persist action 0 i0114 18:51:34.740272 4739 replica.cpp:511 ] replica receiv write request posit 0 i0114 18:51:34.740910 4739 leveldb.cpp:438 ] read posit leveldb took 59846n i0114 18:51:34.741672 4739 leveldb.cpp:343 ] persist action ( 14 byte ) leveldb took 189259n i0114 18:51:34.741919 4739 replica.cpp:679 ] persist action 0 i0114 18:51:34.743000 4739 replica.cpp:658 ] replica receiv learn notic posit 0 i0114 18:51:34.746844 4739 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 328487n i0114 18:51:34.747118 4739 replica.cpp:679 ] persist action 0 i0114 18:51:34.747553 4739 replica.cpp:664 ] replica learn nop action posit 0 i0114 18:51:34.751344 4737 log.cpp:676 ] writer start end posit 0 i0114 18:51:34.753504 4734 leveldb.cpp:438 ] read posit leveldb took 61183n i0114 18:51:34.762962 4737 registrar.cpp:346 ] success fetch registri ( 0b ) 38.907904m i0114 18:51:34.763610 4737 registrar.cpp:445 ] appli 1 oper 67206n ; attempt updat 'registry' i0114 18:51:34.766079 4736 log.cpp:684 ] attempt append 130 byte log i0114 18:51:34.766769 4736 coordinator.cpp:340 ] coordin attempt write append action posit 1 i0114 18:51:34.768215 4741 replica.cpp:511 ] replica receiv write request posit 1 i0114 18:51:34.768759 4741 leveldb.cpp:343 ] persist action ( 149 byte ) leveldb took 87970n i0114 18:51:34.768995 4741 replica.cpp:679 ] persist action 1 i0114 18:51:34.770691 4736 replica.cpp:658 ] replica receiv learn notic posit 1 i0114 18:51:34.771273 4736 leveldb.cpp:343 ] persist action ( 151 byte ) leveldb took 83590n i0114 18:51:34.771579 4736 replica.cpp:679 ] persist action 1 i0114 18:51:34.771917 4736 replica.cpp:664 ] replica learn append action posit 1 i0114 18:51:34.773252 4738 log.cpp:703 ] attempt truncat log 1 i0114 18:51:34.773756 4735 coordinator.cpp:340 ] coordin attempt write truncat action posit 2 i0114 18:51:34.775552 4736 replica.cpp:511 ] replica receiv write request posit 2 i0114 18:51:34.775846 4736 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 71503n i0114 18:51:34.776695 4736 replica.cpp:679 ] persist action 2 i0114 18:51:34.785259 4739 replica.cpp:658 ] replica receiv learn notic posit 2 i0114 18:51:34.786252 4737 registrar.cpp:490 ] success updat 'registri ' 22.340864m i0114 18:51:34.787094 4737 registrar.cpp:376 ] success recov registrar i0114 18:51:34.787749 4737 master.cpp:1077 ] recov 0 slave registri ( 94b ) ; allow 10min slave re-regist i0114 18:51:34.787282 4739 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 707150n i0114 18:51:34.788692 4739 leveldb.cpp:401 ] delet ~1 key leveldb took 60262n i0114 18:51:34.789048 4739 replica.cpp:679 ] persist action 2 i0114 18:51:34.789329 4739 replica.cpp:664 ] replica learn truncat action posit 2 i0114 18:51:34.819548 4738 slave.cpp:173 ] slave start 171 ) @ 192.168.122.135:57018 i0114 18:51:34.820530 4738 credentials.hpp:84 ] load credenti authent '/tmp/hooktest_verifyslavelaunchexecutorhook_ayxnqe/credential' i0114 18:51:34.820952 4738 slave.cpp:282 ] slave use credenti : test-princip i0114 18:51:34.821516 4738 slave.cpp:300 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0114 18:51:34.822217 4738 slave.cpp:329 ] slave hostnam : fedora-19 i0114 18:51:34.822502 4738 slave.cpp:330 ] slave checkpoint : fals w0114 18:51:34.822857 4738 slave.cpp:332 ] disabl checkpoint deprec -- checkpoint flag remov futur releas . pleas avoid use flag i0114 18:51:34.824998 4737 state.cpp:33 ] recov state '/tmp/hooktest_verifyslavelaunchexecutorhook_ayxnqe/meta' i0114 18:51:34.834015 4738 status_update_manager.cpp:197 ] recov statu updat manag i0114 18:51:34.834810 4738 slave.cpp:3519 ] finish recoveri i0114 18:51:34.835906 4734 status_update_manager.cpp:171 ] paus send statu updat i0114 18:51:34.836423 4738 slave.cpp:613 ] new master detect master @ 192.168.122.135:57018 i0114 18:51:34.836908 4738 slave.cpp:676 ] authent master master @ 192.168.122.135:57018 i0114 18:51:34.837190 4738 slave.cpp:681 ] use default cram-md5 authenticate i0114 18:51:34.837820 4737 authenticatee.hpp:138 ] creat new client sasl connect i0114 18:51:34.838784 4738 slave.cpp:649 ] detect new master i0114 18:51:34.839306 4740 master.cpp:4130 ] authent slave ( 171 ) @ 192.168.122.135:57018 i0114 18:51:34.839957 4740 master.cpp:4141 ] use default cram-md5 authent i0114 18:51:34.841236 4740 authenticator.hpp:170 ] creat new server sasl connect i0114 18:51:34.842681 4741 authenticatee.hpp:229 ] receiv sasl authent mechan : cram-md5 i0114 18:51:34.843118 4741 authenticatee.hpp:255 ] attempt authent mechan 'cram-md5' i0114 18:51:34.843581 4740 authenticator.hpp:276 ] receiv sasl authent start i0114 18:51:34.843962 4740 authenticator.hpp:398 ] authent requir step i0114 18:51:34.844357 4740 authenticatee.hpp:275 ] receiv sasl authent step i0114 18:51:34.844780 4740 authenticator.hpp:304 ] receiv sasl authent step i0114 18:51:34.845113 4740 auxprop.cpp:99 ] request lookup properti user : 'test-princip ' realm : 'fedora-19 ' server fqdn : 'fedora-19 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0114 18:51:34.845507 4740 auxprop.cpp:171 ] look auxiliari properti ' * userpassword' i0114 18:51:34.845835 4740 auxprop.cpp:171 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0114 18:51:34.846238 4740 auxprop.cpp:99 ] request lookup properti user : 'test-princip ' realm : 'fedora-19 ' server fqdn : 'fedora-19 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0114 18:51:34.846542 4740 auxprop.cpp:121 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0114 18:51:34.846806 4740 auxprop.cpp:121 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0114 18:51:34.847110 4740 authenticator.hpp:390 ] authent success i0114 18:51:34.847808 4734 authenticatee.hpp:315 ] authent success i0114 18:51:34.851029 4734 slave.cpp:747 ] success authent master master @ 192.168.122.135:57018 i0114 18:51:34.851608 4737 master.cpp:4188 ] success authent princip 'test-princip ' slave ( 171 ) @ 192.168.122.135:57018 i0114 18:51:34.854962 4720 sched.cpp:151 ] version : 0.22.0 i0114 18:51:34.856674 4734 slave.cpp:1075 ] will retri registr 3.085482m necessari i0114 18:51:34.857434 4739 sched.cpp:248 ] new master detect master @ 192.168.122.135:57018 i0114 18:51:34.861433 4739 sched.cpp:304 ] authent master master @ 192.168.122.135:57018 i0114 18:51:34.861693 4739 sched.cpp:311 ] use default cram-md5 authenticate i0114 18:51:34.857795 4737 master.cpp:3276 ] regist slave slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) id 20150114-185134-2272962752-57018-4720-s0 i0114 18:51:34.862951 4737 authenticatee.hpp:138 ] creat new client sasl connect i0114 18:51:34.863919 4735 registrar.cpp:445 ] appli 1 oper 120272n ; attempt updat 'registry' i0114 18:51:34.864645 4738 master.cpp:4130 ] authent scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.865033 4738 master.cpp:4141 ] use default cram-md5 authent i0114 18:51:34.866904 4738 authenticator.hpp:170 ] creat new server sasl connect i0114 18:51:34.868840 4737 authenticatee.hpp:229 ] receiv sasl authent mechan : cram-md5 i0114 18:51:34.869125 4737 authenticatee.hpp:255 ] attempt authent mechan 'cram-md5' i0114 18:51:34.869523 4737 authenticator.hpp:276 ] receiv sasl authent start i0114 18:51:34.869835 4737 authenticator.hpp:398 ] authent requir step i0114 18:51:34.870213 4737 authenticatee.hpp:275 ] receiv sasl authent step i0114 18:51:34.870622 4737 authenticator.hpp:304 ] receiv sasl authent step i0114 18:51:34.870946 4737 auxprop.cpp:99 ] request lookup properti user : 'test-princip ' realm : 'fedora-19 ' server fqdn : 'fedora-19 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0114 18:51:34.871219 4737 auxprop.cpp:171 ] look auxiliari properti ' * userpassword' i0114 18:51:34.871554 4737 auxprop.cpp:171 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0114 18:51:34.871968 4737 auxprop.cpp:99 ] request lookup properti user : 'test-princip ' realm : 'fedora-19 ' server fqdn : 'fedora-19 ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0114 18:51:34.872297 4737 auxprop.cpp:121 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0114 18:51:34.872655 4737 auxprop.cpp:121 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0114 18:51:34.873024 4737 authenticator.hpp:390 ] authent success i0114 18:51:34.873428 4737 authenticatee.hpp:315 ] authent success i0114 18:51:34.873632 4739 master.cpp:4188 ] success authent princip 'test-princip ' scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.875006 4740 sched.cpp:392 ] success authent master master @ 192.168.122.135:57018 i0114 18:51:34.875319 4740 sched.cpp:515 ] send registr request master @ 192.168.122.135:57018 i0114 18:51:34.876200 4740 sched.cpp:548 ] will retri registr 1.952991346sec necessari i0114 18:51:34.876729 4738 master.cpp:1417 ] receiv registr request framework 'default ' scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.877040 4738 master.cpp:1298 ] author framework princip 'test-princip ' receiv offer role ' * ' i0114 18:51:34.878059 4738 master.cpp:1481 ] regist framework 20150114-185134-2272962752-57018-4720-0000 ( default ) scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.878473 4739 log.cpp:684 ] attempt append 300 byte log i0114 18:51:34.879464 4737 coordinator.cpp:340 ] coordin attempt write append action posit 3 i0114 18:51:34.880116 4734 hierarchical_allocator_process.hpp:319 ] ad framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.880470 4734 hierarchical_allocator_process.hpp:839 ] no resourc avail alloc ! i0114 18:51:34.882331 4734 hierarchical_allocator_process.hpp:746 ] perform alloc 0 slave 1.901284m i0114 18:51:34.884024 4741 sched.cpp:442 ] framework regist 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.884454 4741 sched.cpp:456 ] schedul : :regist took 44320n i0114 18:51:34.881965 4737 replica.cpp:511 ] replica receiv write request posit 3 i0114 18:51:34.885218 4737 leveldb.cpp:343 ] persist action ( 319 byte ) leveldb took 134480n i0114 18:51:34.885716 4737 replica.cpp:679 ] persist action 3 i0114 18:51:34.886034 4739 slave.cpp:1075 ] will retri registr 22.947772m necessari i0114 18:51:34.886291 4740 master.cpp:3264 ] ignor regist slave messag slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) admiss alreadi progress i0114 18:51:34.894690 4736 replica.cpp:658 ] replica receiv learn notic posit 3 i0114 18:51:34.898638 4736 leveldb.cpp:343 ] persist action ( 321 byte ) leveldb took 215501n i0114 18:51:34.899055 4736 replica.cpp:679 ] persist action 3 i0114 18:51:34.899416 4736 replica.cpp:664 ] replica learn append action posit 3 i0114 18:51:34.911782 4736 registrar.cpp:490 ] success updat 'registri ' 46.176768m i0114 18:51:34.912286 4740 log.cpp:703 ] attempt truncat log 3 i0114 18:51:34.913108 4740 coordinator.cpp:340 ] coordin attempt write truncat action posit 4 i0114 18:51:34.915027 4736 master.cpp:3330 ] regist slave 20150114-185134-2272962752-57018-4720-s0 slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0114 18:51:34.915642 4735 hierarchical_allocator_process.hpp:453 ] ad slave 20150114-185134-2272962752-57018-4720-s0 ( fedora-19 ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] avail ) i0114 18:51:34.917809 4735 hierarchical_allocator_process.hpp:764 ] perform alloc slave 20150114-185134-2272962752-57018-4720-s0 514027n i0114 18:51:34.916689 4738 replica.cpp:511 ] replica receiv write request posit 4 i0114 18:51:34.915784 4741 slave.cpp:781 ] regist master master @ 192.168.122.135:57018 ; given slave id 20150114-185134-2272962752-57018-4720-s0 i0114 18:51:34.919293 4741 slave.cpp:2588 ] receiv ping slave-observ ( 156 ) @ 192.168.122.135:57018 i0114 18:51:34.919775 4740 status_update_manager.cpp:178 ] resum send statu updat i0114 18:51:34.920374 4736 master.cpp:4072 ] send 1 offer framework 20150114-185134-2272962752-57018-4720-0000 ( default ) scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.920569 4738 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 1.540136m i0114 18:51:34.921092 4738 replica.cpp:679 ] persist action 4 i0114 18:51:34.927111 4735 replica.cpp:658 ] replica receiv learn notic posit 4 i0114 18:51:34.927299 4734 sched.cpp:605 ] schedul : :resourceoff took 1.335524m i0114 18:51:34.930418 4735 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 1.596377m i0114 18:51:34.930882 4735 leveldb.cpp:401 ] delet ~2 key leveldb took 67578n i0114 18:51:34.931115 4735 replica.cpp:679 ] persist action 4 i0114 18:51:34.931529 4735 replica.cpp:664 ] replica learn truncat action posit 4 i0114 18:51:34.930356 4734 master.cpp:2541 ] process repli offer : [ 20150114-185134-2272962752-57018-4720-o0 ] slave 20150114-185134-2272962752-57018-4720-s0 slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) framework 20150114-185134-2272962752-57018-4720-0000 ( default ) scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 i0114 18:51:34.932834 4734 master.cpp:2647 ] author framework princip 'test-princip ' launch task 1 user 'jenkins' w0114 18:51:34.934442 4736 master.cpp:2124 ] executor default task 1 use less cpu ( none ) minimum requir ( 0.01 ) . pleas updat executor , mandatori futur releas . w0114 18:51:34.934960 4736 master.cpp:2136 ] executor default task 1 use less memori ( none ) minimum requir ( 32mb ) . pleas updat executor , mandatori futur releas . i0114 18:51:34.935878 4736 master.hpp:766 ] ad task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20150114-185134-2272962752-57018-4720-s0 ( fedora-19 ) i0114 18:51:34.939453 4738 hierarchical_allocator_process.hpp:610 ] updat alloc framework 20150114-185134-2272962752-57018-4720-0000 slave 20150114-185134-2272962752-57018-4720-s0 cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0114 18:51:34.939950 4736 master.cpp:2897 ] launch task 1 framework 20150114-185134-2272962752-57018-4720-0000 ( default ) scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f @ 192.168.122.135:57018 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20150114-185134-2272962752-57018-4720-s0 slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) i0114 18:51:34.940467 4736 test_hook_module.cpp:52 ] execut 'masterlaunchtasklabeldecor ' hook i0114 18:51:34.941490 4740 slave.cpp:1130 ] got assign task 1 framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.942873 4740 slave.cpp:1245 ] launch task 1 framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.943469 4740 test_hook_module.cpp:71 ] execut 'slavelaunchexecutorenvironmentdecor ' hook i0114 18:51:34.946705 4740 slave.cpp:3921 ] launch executor default framework 20150114-185134-2272962752-57018-4720-0000 work directori '/tmp/hooktest_verifyslavelaunchexecutorhook_ayxnqe/slaves/20150114-185134-2272962752-57018-4720-s0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d' i0114 18:51:34.956496 4740 exec.cpp:147 ] version : 0.22.0 i0114 18:51:34.960752 4737 exec.cpp:197 ] executor start : executor ( 56 ) @ 192.168.122.135:57018 pid 4720 i0114 18:51:34.964501 4740 slave.cpp:1368 ] queu task ' 1 ' executor default framework '20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.965133 4740 slave.cpp:566 ] success attach file '/tmp/hooktest_verifyslavelaunchexecutorhook_ayxnqe/slaves/20150114-185134-2272962752-57018-4720-s0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d' i0114 18:51:34.965605 4740 slave.cpp:1912 ] got registr executor 'default ' framework 20150114-185134-2272962752-57018-4720-0000 executor ( 56 ) @ 192.168.122.135:57018 i0114 18:51:34.966933 4734 exec.cpp:221 ] executor regist slave 20150114-185134-2272962752-57018-4720-s0 i0114 18:51:34.968889 4740 slave.cpp:2031 ] flush queu task 1 executor 'default ' framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.969743 4740 slave.cpp:2890 ] monitor executor 'default ' framework '20150114-185134-2272962752-57018-4720-0000 ' contain 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d' i0114 18:51:34.973484 4734 exec.cpp:233 ] executor : :regist took 4.814445m i0114 18:51:34.974081 4734 exec.cpp:308 ] executor ask run task ' 1' i0114 18:51:34.974431 4734 exec.cpp:317 ] executor : :launchtask took 184910n i0114 18:51:34.975292 4720 sched.cpp:1471 ] ask stop driver i0114 18:51:34.975817 4738 sched.cpp:808 ] stop framework '20150114-185134-2272962752-57018-4720-0000' i0114 18:51:34.975697 4720 master.cpp:654 ] master termin w0114 18:51:34.976610 4720 master.cpp:4980 ] remov task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] framework 20150114-185134-2272962752-57018-4720-0000 slave 20150114-185134-2272962752-57018-4720-s0 slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) non-termin state task_stag i0114 18:51:34.977880 4720 master.cpp:5023 ] remov executor 'default ' resourc framework 20150114-185134-2272962752-57018-4720-0000 slave 20150114-185134-2272962752-57018-4720-s0 slave ( 171 ) @ 192.168.122.135:57018 ( fedora-19 ) i0114 18:51:34.978196 4741 hierarchical_allocator_process.hpp:653 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total allocat : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) slave 20150114-185134-2272962752-57018-4720-s0 framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:34.982658 4735 slave.cpp:2673 ] master @ 192.168.122.135:57018 exit w0114 18:51:34.983065 4735 slave.cpp:2676 ] master disconnect ! wait new master elect i0114 18:51:35.029485 4720 slave.cpp:495 ] slave termin i0114 18:51:35.034024 4720 slave.cpp:1585 ] ask shut framework 20150114-185134-2272962752-57018-4720-0000 @ 0.0.0.0:0 i0114 18:51:35.034335 4720 slave.cpp:1610 ] shut framework 20150114-185134-2272962752-57018-4720-0000 i0114 18:51:35.034857 4720 slave.cpp:3198 ] shut executor 'default ' framework 20150114-185134-2272962752-57018-4720-0000 tests/hook_tests.cpp:271 : failur valu : os : :isfil ( path.get ( ) ) actual : true expect : fals [ fail ] hooktest.verifyslavelaunchexecutorhook ( 412 ms ) { code }",MESOS-2226,3.0
"larg number connect slow statistics.json respons . we observ product environ network monitor turn . if mani connect ( > 10^4 ) contain , get socket inform expens . it might take 1min process socket inform . one reason librari use ( libnl ) optim . cong wang alreadi submit patch : http : //lists.infradead.org/pipermail/libnl/2014-november/001715.html",MESOS-2147,2.0
"contain network stat report port map isol revers actual network stat . look like tx/rx network stat report revers actual network stat . the reason simpli get tx/rx data veth host . sinc veth pair tunnel , ingress veth host egress eth0 contain ( vice versa ) . therefor , need flip data got veth . { noformat } [ jyu @ ... ~ ] $ sudo ip netn exec 24926 /sbin/ip -s link show dev eth0 2 : eth0 : < broadcast , multicast , up , lower_up > mtu 1500 qdisc pfifo_fast state up mode default qlen 1000 link/eth f0:4d : a2:75:74:05 brd ff : ff : ff : ff : ff : ff rx : byte packet error drop overrun mcast 46030857691178 12561038581 0 0 0 0 tx : byte packet error drop carrier collsn 29792886058561 15036798198 0 0 0 0 [ jyu @ ... ~ ] $ ip -s link show dev mesos24926 7412 : mesos24926 : < broadcast , multicast , up , lower_up > mtu 1500 qdisc pfifo_fast state up mode default qlen 1000 link/eth f0:4d : a2:75:74:05 brd ff : ff : ff : ff : ff : ff rx : byte packet error drop overrun mcast 29793066979551 15036894749 0 0 0 0 tx : byte packet error drop carrier collsn 46031126366116 12561113732 0 0 0 0 { noformat }",MESOS-1989,1.0
"complet task remain task_run framework disconnect we run problem caus task complet , framework disconnect fail-ov time , remain run state even though task actual finish . thi hog cluster give user inconsist view cluster state . go slave , task finish . go master , task still non-termin state . when schedul reattach failov timeout expir , task finish correctli . the current workflow schedul long fail-ov timeout , may hand never reattach . here test framework abl reproduc issu : http : //gist.github.com/nqn/9b9b1de9123a6e836f54 it launch mani short-liv task ( 1 second sleep ) kill framework instanc , master report task run even sever minut : http : //cl.ly/image/2r3719461e0t/screen % 20shot % 202014-09-10 % 20at % 203.19.39 % 20pm.png when click one slave , exampl , task 49 run ; slave know complet : http : //cl.ly/image/2p410l3m1o1n/screen % 20shot % 202014-09-10 % 20at % 203.21.29 % 20pm.png here log mesos-loc instanc i reproduc : http : //gist.github.com/nqn/f7ee20601199d70787c0 ( here task 10 19 stuck run state ) . there lot output , filter log task 10 : http : //gist.github.com/nqn/a53e5ea05c5e41cd5a7d the problem turn issu ack-cycl statu updat : if framework disconnect ( failov timeout set ) , statu updat manag slave keep tri send front statu updat stream master ( turn forward framework ) . if first statu updat disconnect termin , thing work fine ; master pick termin state , remov task releas resourc . if , hand , one non-termin statu stream . the master never know task finish ( fail ) framework reconnect . dure discuss dev mail list ( http : //mail-archives.apache.org/mod_mbox/mesos-dev/201409.mbox/ % 3ccadkthhavr5mrq1s9hxw1bb_xfalxwwxjutp7mv4y3wp-bh=awg @ mail.gmail.com % 3e ) enumer coupl option solv problem . first , two ack-cycl : one master slave one master framework , would ideal . we would abl replay status order keep master state current . howev , requir us persist master state replic storag . as first pass , make sure task caught run state n't hog cluster complet framework disconnect . here proof-of-concept work : http : //github.com/nqn/mesos/tree/niklas/status-update-disconnect/ a new ( option ) field ad intern statu updat messag : http : //github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/messages/messages.proto # l68 which make possibl statu updat manag set field , latest statu termin : http : //github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/slave/status_update_manager.cpp # l501 i ad test high-light issu well : http : //github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/tests/fault_tolerance_tests.cpp # l2478 i would love input approach move . there rough edg poc ( cours ) address bring review .",MESOS-1817,2.0
"healthchecktest.healthstatuschang flaki jenkin . http : //builds.apache.org/job/mesos-trunk-ubuntu-build-out-of-src-disable-java-disable-python-disable-webui/2374/consoleful { noformat } [ run ] healthchecktest.healthstatuschang use temporari directori '/tmp/healthchecktest_healthstatuschange_iynlu2' i0916 22:56:14.034612 21026 leveldb.cpp:176 ] open db 2.155713m i0916 22:56:14.034965 21026 leveldb.cpp:183 ] compact db 332489n i0916 22:56:14.034984 21026 leveldb.cpp:198 ] creat db iter 3710n i0916 22:56:14.034996 21026 leveldb.cpp:204 ] seek begin db 642n i0916 22:56:14.035006 21026 leveldb.cpp:273 ] iter 0 key db 343n i0916 22:56:14.035023 21026 replica.cpp:741 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0916 22:56:14.035200 21054 recover.cpp:425 ] start replica recoveri i0916 22:56:14.035403 21041 recover.cpp:451 ] replica empti statu i0916 22:56:14.035888 21045 replica.cpp:638 ] replica empti statu receiv broadcast recov request i0916 22:56:14.035969 21052 recover.cpp:188 ] receiv recov respons replica empti statu i0916 22:56:14.036118 21042 recover.cpp:542 ] updat replica statu start i0916 22:56:14.036603 21046 master.cpp:286 ] master 20140916-225614-3125920579-47865-21026 ( penates.apache.org ) start 67.195.81.186:47865 i0916 22:56:14.036634 21046 master.cpp:332 ] master allow authent framework regist i0916 22:56:14.036648 21046 master.cpp:337 ] master allow authent slave regist i0916 22:56:14.036659 21046 credentials.hpp:36 ] load credenti authent '/tmp/healthchecktest_healthstatuschange_iynlu2/credentials' i0916 22:56:14.036686 21045 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 480322n i0916 22:56:14.036700 21045 replica.cpp:320 ] persist replica statu start i0916 22:56:14.036769 21046 master.cpp:366 ] author enabl i0916 22:56:14.036826 21045 recover.cpp:451 ] replica start statu i0916 22:56:14.036944 21052 master.cpp:120 ] no whitelist given . advertis offer slave i0916 22:56:14.036968 21049 hierarchical_allocator_process.hpp:299 ] initi hierarch alloc process master : master @ 67.195.81.186:47865 i0916 22:56:14.037284 21054 replica.cpp:638 ] replica start statu receiv broadcast recov request i0916 22:56:14.037312 21046 master.cpp:1212 ] the newli elect leader master @ 67.195.81.186:47865 id 20140916-225614-3125920579-47865-21026 i0916 22:56:14.037333 21046 master.cpp:1225 ] elect lead master ! i0916 22:56:14.037345 21046 master.cpp:1043 ] recov registrar i0916 22:56:14.037504 21040 registrar.cpp:313 ] recov registrar i0916 22:56:14.037505 21053 recover.cpp:188 ] receiv recov respons replica start statu i0916 22:56:14.037681 21047 recover.cpp:542 ] updat replica statu vote i0916 22:56:14.038072 21052 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 330251n i0916 22:56:14.038087 21052 replica.cpp:320 ] persist replica statu vote i0916 22:56:14.038127 21053 recover.cpp:556 ] success join paxo group i0916 22:56:14.038202 21053 recover.cpp:440 ] recov process termin i0916 22:56:14.038364 21048 log.cpp:656 ] attempt start writer i0916 22:56:14.038812 21053 replica.cpp:474 ] replica receiv implicit promis request propos 1 i0916 22:56:14.038925 21053 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 92623n i0916 22:56:14.038944 21053 replica.cpp:342 ] persist promis 1 i0916 22:56:14.039201 21052 coordinator.cpp:230 ] coordin attemp fill miss posit i0916 22:56:14.039676 21047 replica.cpp:375 ] replica receiv explicit promis request posit 0 propos 2 i0916 22:56:14.039836 21047 leveldb.cpp:343 ] persist action ( 8 byte ) leveldb took 144215n i0916 22:56:14.039850 21047 replica.cpp:676 ] persist action 0 i0916 22:56:14.040243 21047 replica.cpp:508 ] replica receiv write request posit 0 i0916 22:56:14.040267 21047 leveldb.cpp:438 ] read posit leveldb took 10323n i0916 22:56:14.040362 21047 leveldb.cpp:343 ] persist action ( 14 byte ) leveldb took 79471n i0916 22:56:14.040375 21047 replica.cpp:676 ] persist action 0 i0916 22:56:14.040556 21054 replica.cpp:655 ] replica receiv learn notic posit 0 i0916 22:56:14.040658 21054 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 83975n i0916 22:56:14.040676 21054 replica.cpp:676 ] persist action 0 i0916 22:56:14.040689 21054 replica.cpp:661 ] replica learn nop action posit 0 i0916 22:56:14.041023 21043 log.cpp:672 ] writer start end posit 0 i0916 22:56:14.041342 21052 leveldb.cpp:438 ] read posit leveldb took 10642n i0916 22:56:14.042325 21050 registrar.cpp:346 ] success fetch registri ( 0b ) i0916 22:56:14.042346 21050 registrar.cpp:422 ] attempt updat 'registry' i0916 22:56:14.043306 21054 log.cpp:680 ] attempt append 140 byte log i0916 22:56:14.043354 21050 coordinator.cpp:340 ] coordin attempt write append action posit 1 i0916 22:56:14.043637 21047 replica.cpp:508 ] replica receiv write request posit 1 i0916 22:56:14.044042 21047 leveldb.cpp:343 ] persist action ( 159 byte ) leveldb took 386690n i0916 22:56:14.044057 21047 replica.cpp:676 ] persist action 1 i0916 22:56:14.044271 21040 replica.cpp:655 ] replica receiv learn notic posit 1 i0916 22:56:14.044435 21040 leveldb.cpp:343 ] persist action ( 161 byte ) leveldb took 145186n i0916 22:56:14.044448 21040 replica.cpp:676 ] persist action 1 i0916 22:56:14.044456 21040 replica.cpp:661 ] replica learn append action posit 1 i0916 22:56:14.044729 21055 registrar.cpp:479 ] success updat 'registry' i0916 22:56:14.044776 21047 log.cpp:699 ] attempt truncat log 1 i0916 22:56:14.044795 21055 registrar.cpp:372 ] success recov registrar i0916 22:56:14.044831 21051 coordinator.cpp:340 ] coordin attempt write truncat action posit 2 i0916 22:56:14.044899 21053 master.cpp:1070 ] recov 0 slave registri ( 102b ) ; allow 10min slave re-regist i0916 22:56:14.045133 21055 replica.cpp:508 ] replica receiv write request posit 2 i0916 22:56:14.045450 21055 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 300867n i0916 22:56:14.045465 21055 replica.cpp:676 ] persist action 2 i0916 22:56:14.045725 21052 replica.cpp:655 ] replica receiv learn notic posit 2 i0916 22:56:14.045925 21052 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 182657n i0916 22:56:14.045948 21052 leveldb.cpp:401 ] delet ~1 key leveldb took 10733n i0916 22:56:14.045958 21052 replica.cpp:676 ] persist action 2 i0916 22:56:14.045964 21052 replica.cpp:661 ] replica learn truncat action posit 2 i0916 22:56:14.055306 21026 containerizer.cpp:89 ] use isol : posix/cpu , posix/mem i0916 22:56:14.057139 21048 slave.cpp:169 ] slave start 102 ) @ 67.195.81.186:47865 i0916 22:56:14.057178 21048 credentials.hpp:84 ] load credenti authent '/tmp/healthchecktest_healthstatuschange_cgktig/credential' i0916 22:56:14.057283 21048 slave.cpp:276 ] slave use credenti : test-princip i0916 22:56:14.057354 21048 slave.cpp:289 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0916 22:56:14.057457 21048 slave.cpp:317 ] slave hostnam : penates.apache.org i0916 22:56:14.057468 21048 slave.cpp:318 ] slave checkpoint : fals i0916 22:56:14.057754 21043 state.cpp:33 ] recov state '/tmp/healthchecktest_healthstatuschange_cgktig/meta' i0916 22:56:14.057864 21042 status_update_manager.cpp:193 ] recov statu updat manag i0916 22:56:14.057958 21042 containerizer.cpp:252 ] recov container i0916 22:56:14.058226 21042 slave.cpp:3219 ] finish recoveri i0916 22:56:14.058452 21047 slave.cpp:600 ] new master detect master @ 67.195.81.186:47865 i0916 22:56:14.058485 21047 slave.cpp:674 ] authent master master @ 67.195.81.186:47865 i0916 22:56:14.058506 21042 status_update_manager.cpp:167 ] new master detect master @ 67.195.81.186:47865 i0916 22:56:14.058539 21047 slave.cpp:647 ] detect new master i0916 22:56:14.058555 21042 authenticatee.hpp:128 ] creat new client sasl connect i0916 22:56:14.058656 21043 master.cpp:3653 ] authent slave ( 102 ) @ 67.195.81.186:47865 i0916 22:56:14.058737 21040 authenticator.hpp:156 ] creat new server sasl connect i0916 22:56:14.058830 21047 authenticatee.hpp:219 ] receiv sasl authent mechan : cram-md5 i0916 22:56:14.058852 21047 authenticatee.hpp:245 ] attempt authent mechan 'cram-md5' i0916 22:56:14.058884 21047 authenticator.hpp:262 ] receiv sasl authent start i0916 22:56:14.058936 21047 authenticator.hpp:384 ] authent requir step i0916 22:56:14.058981 21047 authenticatee.hpp:265 ] receiv sasl authent step i0916 22:56:14.059052 21040 authenticator.hpp:290 ] receiv sasl authent step i0916 22:56:14.059074 21040 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'penates.apache.org ' server fqdn : 'penates.apache.org ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0916 22:56:14.059087 21040 auxprop.cpp:153 ] look auxiliari properti ' * userpassword' i0916 22:56:14.059101 21040 auxprop.cpp:153 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0916 22:56:14.059111 21040 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'penates.apache.org ' server fqdn : 'penates.apache.org ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0916 22:56:14.059118 21040 auxprop.cpp:103 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0916 22:56:14.059123 21040 auxprop.cpp:103 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0916 22:56:14.059135 21040 authenticator.hpp:376 ] authent success i0916 22:56:14.059182 21047 authenticatee.hpp:305 ] authent success i0916 22:56:14.059192 21040 master.cpp:3693 ] success authent princip 'test-princip ' slave ( 102 ) @ 67.195.81.186:47865 i0916 22:56:14.059309 21047 slave.cpp:731 ] success authent master master @ 67.195.81.186:47865 i0916 22:56:14.059348 21047 slave.cpp:994 ] will retri registr 12.6149m necessari i0916 22:56:14.059396 21040 master.cpp:2843 ] regist slave slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) id 20140916-225614-3125920579-47865-21026-0 i0916 22:56:14.059495 21054 registrar.cpp:422 ] attempt updat 'registry' i0916 22:56:14.059558 21026 sched.cpp:137 ] version : 0.21.0 i0916 22:56:14.059710 21041 sched.cpp:233 ] new master detect master @ 67.195.81.186:47865 i0916 22:56:14.059730 21041 sched.cpp:283 ] authent master master @ 67.195.81.186:47865 i0916 22:56:14.059788 21052 authenticatee.hpp:128 ] creat new client sasl connect i0916 22:56:14.059890 21043 master.cpp:3653 ] authent scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4 @ 67.195.81.186:47865 i0916 22:56:14.059960 21055 authenticator.hpp:156 ] creat new server sasl connect i0916 22:56:14.060039 21040 authenticatee.hpp:219 ] receiv sasl authent mechan : cram-md5 i0916 22:56:14.060061 21040 authenticatee.hpp:245 ] attempt authent mechan 'cram-md5' i0916 22:56:14.060107 21055 authenticator.hpp:262 ] receiv sasl authent start i0916 22:56:14.060158 21055 authenticator.hpp:384 ] authent requir step i0916 22:56:14.060189 21055 authenticatee.hpp:265 ] receiv sasl authent step i0916 22:56:14.060220 21055 authenticator.hpp:290 ] receiv sasl authent step i0916 22:56:14.060236 21055 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'penates.apache.org ' server fqdn : 'penates.apache.org ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0916 22:56:14.060250 21055 auxprop.cpp:153 ] look auxiliari properti ' * userpassword' i0916 22:56:14.060277 21055 auxprop.cpp:153 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0916 22:56:14.060288 21055 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'penates.apache.org ' server fqdn : 'penates.apache.org ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0916 22:56:14.060295 21055 auxprop.cpp:103 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0916 22:56:14.060300 21055 auxprop.cpp:103 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0916 22:56:14.060312 21055 authenticator.hpp:376 ] authent success i0916 22:56:14.060349 21040 authenticatee.hpp:305 ] authent success i0916 22:56:14.060364 21055 master.cpp:3693 ] success authent princip 'test-princip ' scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4 @ 67.195.81.186:47865 i0916 22:56:14.060480 21046 sched.cpp:357 ] success authent master master @ 67.195.81.186:47865 i0916 22:56:14.060499 21046 sched.cpp:476 ] send registr request master @ 67.195.81.186:47865 i0916 22:56:14.060564 21050 master.cpp:1331 ] receiv registr request scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4 @ 67.195.81.186:47865 i0916 22:56:14.060593 21050 master.cpp:1291 ] author framework princip 'test-princip ' receiv offer role ' * ' i0916 22:56:14.060767 21053 master.cpp:1390 ] regist framework 20140916-225614-3125920579-47865-21026-0000 scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4 @ 67.195.81.186:47865 i0916 22:56:14.060797 21049 log.cpp:680 ] attempt append 337 byte log i0916 22:56:14.060873 21042 hierarchical_allocator_process.hpp:329 ] ad framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.060873 21040 coordinator.cpp:340 ] coordin attempt write append action posit 3 i0916 22:56:14.060899 21042 hierarchical_allocator_process.hpp:697 ] no resourc avail alloc ! i0916 22:56:14.060909 21042 hierarchical_allocator_process.hpp:659 ] perform alloc 0 slave 11862n i0916 22:56:14.061061 21044 sched.cpp:407 ] framework regist 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.061115 21044 sched.cpp:421 ] schedul : :regist took 34395n i0916 22:56:14.061173 21047 replica.cpp:508 ] replica receiv write request posit 3 i0916 22:56:14.061298 21047 leveldb.cpp:343 ] persist action ( 356 byte ) leveldb took 108843n i0916 22:56:14.061311 21047 replica.cpp:676 ] persist action 3 i0916 22:56:14.061553 21049 replica.cpp:655 ] replica receiv learn notic posit 3 i0916 22:56:14.061965 21049 leveldb.cpp:343 ] persist action ( 358 byte ) leveldb took 392670n i0916 22:56:14.061985 21049 replica.cpp:676 ] persist action 3 i0916 22:56:14.061996 21049 replica.cpp:661 ] replica learn append action posit 3 i0916 22:56:14.062268 21050 registrar.cpp:479 ] success updat 'registry' i0916 22:56:14.062331 21051 log.cpp:699 ] attempt truncat log 3 i0916 22:56:14.062355 21040 master.cpp:2883 ] regist slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) i0916 22:56:14.062386 21043 coordinator.cpp:340 ] coordin attempt write truncat action posit 4 i0916 22:56:14.062376 21040 master.cpp:4126 ] ad slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0916 22:56:14.062510 21045 slave.cpp:765 ] regist master master @ 67.195.81.186:47865 ; given slave id 20140916-225614-3125920579-47865-21026-0 i0916 22:56:14.062573 21045 slave.cpp:2346 ] receiv ping slave-observ ( 98 ) @ 67.195.81.186:47865 i0916 22:56:14.062599 21049 hierarchical_allocator_process.hpp:442 ] ad slave 20140916-225614-3125920579-47865-21026-0 ( penates.apache.org ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] avail ) i0916 22:56:14.062669 21049 hierarchical_allocator_process.hpp:734 ] offer cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.062764 21041 replica.cpp:508 ] replica receiv write request posit 4 i0916 22:56:14.062788 21049 hierarchical_allocator_process.hpp:679 ] perform alloc slave 20140916-225614-3125920579-47865-21026-0 145691n i0916 22:56:14.062839 21050 master.hpp:861 ] ad offer 20140916-225614-3125920579-47865-21026-0 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 ( penates.apache.org ) i0916 22:56:14.062891 21041 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 110169n i0916 22:56:14.062907 21041 replica.cpp:676 ] persist action 4 i0916 22:56:14.062911 21050 master.cpp:3600 ] send 1 offer framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.063065 21043 sched.cpp:544 ] schedul : :resourceoff took 39808n i0916 22:56:14.063163 21046 replica.cpp:655 ] replica receiv learn notic posit 4 i0916 22:56:14.063272 21046 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 89981n i0916 22:56:14.063308 21046 leveldb.cpp:401 ] delet ~2 key leveldb took 18542n i0916 22:56:14.063323 21046 replica.cpp:676 ] persist action 4 i0916 22:56:14.063333 21046 replica.cpp:661 ] replica learn truncat action posit 4 i0916 22:56:14.063482 21044 master.hpp:871 ] remov offer 20140916-225614-3125920579-47865-21026-0 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 ( penates.apache.org ) i0916 22:56:14.063535 21044 master.cpp:2201 ] process repli offer : [ 20140916-225614-3125920579-47865-21026-0 ] slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.063561 21044 master.cpp:2284 ] author framework princip 'test-princip ' launch task 1 user 'jenkins' i0916 22:56:14.063824 21040 master.hpp:833 ] ad task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 ( penates.apache.org ) i0916 22:56:14.063860 21040 master.cpp:2350 ] launch task 1 framework 20140916-225614-3125920579-47865-21026-0000 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) i0916 22:56:14.063943 21050 slave.cpp:1025 ] got assign task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.064158 21050 slave.cpp:1135 ] launch task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.065439 21050 slave.cpp:1248 ] queu task ' 1 ' executor 1 framework '20140916-225614-3125920579-47865-21026-0000 i0916 22:56:14.065460 21041 containerizer.cpp:394 ] start contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd ' executor ' 1 ' framework '20140916-225614-3125920579-47865-21026-0000' i0916 22:56:14.065477 21050 slave.cpp:554 ] success attach file '/tmp/healthchecktest_healthstatuschange_cgktig/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd' i0916 22:56:14.066735 21055 launcher.cpp:137 ] fork child pid '21858 ' contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' i0916 22:56:14.067486 21044 containerizer.cpp:510 ] fetch uri contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd ' use command '/home/jenkins/jenkins-slave/workspace/mesos-trunk-ubuntu-build-out-of-src-disable-java-disable-python-disable-webui/build/src/mesos-fetcher' i0916 22:56:15.037449 21050 hierarchical_allocator_process.hpp:659 ] perform alloc 1 slave 43708n i0916 22:56:15.038743 21054 slave.cpp:2559 ] monitor executor ' 1 ' framework '20140916-225614-3125920579-47865-21026-0000 ' contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' i0916 22:56:15.078441 21053 slave.cpp:1758 ] got registr executor ' 1 ' framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.078866 21053 slave.cpp:1876 ] flush queu task 1 executor ' 1 ' framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.084800 21043 slave.cpp:2110 ] handl statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:15.084969 21041 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.084995 21041 status_update_manager.cpp:499 ] creat statusupd stream task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.085160 21041 status_update_manager.cpp:373 ] forward statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 master @ 67.195.81.186:47865 i0916 22:56:15.085314 21043 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.085332 21041 master.cpp:3212 ] forward statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.085335 21043 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:15.085435 21041 master.cpp:3178 ] statu updat task_run ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) i0916 22:56:15.085675 21044 sched.cpp:635 ] schedul : :statusupd took 113998n i0916 22:56:15.085888 21052 master.cpp:2693 ] forward statu updat acknowledg a16d2819-e9f4-4119-bde6-f00ad33033e5 task 1 framework 20140916-225614-3125920579-47865-21026-0000 slave 20140916-225614-3125920579-47865-21026-0 slave ( 102 ) @ 67.195.81.186:47865 ( penates.apache.org ) i0916 22:56:15.086109 21051 status_update_manager.cpp:398 ] receiv statu updat acknowledg ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:15.086205 21051 slave.cpp:1698 ] statu updat manag success handl statu updat acknowledg ( uuid : a16d2819-e9f4-4119-bde6-f00ad33033e5 ) task 1 framework 20140916-225614-3125920579-47865-21026-0000 i .. / .. /src/tests/health_check_tests.cpp:330 : failur fail wait 10sec statushealth1 0916 22:56:16.038705 21049 hierarchical_allocator_process.hpp:659 ] perform alloc 1 slave 40061n i0916 22:56:16.126260 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 792b8e42-0d72-451b-978a-7d1f29a15751 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.190274 21045 master.cpp:741 ] framework 20140916-225614-3125920579-47865-21026-0000 disconnect i0916 22:56:28.190304 21045 master.cpp:1687 ] deactiv framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:19.037235 21050 master.cpp:120 ] no whitelist given . advertis offer slave i0916 22:56:28.190394 21045 master.cpp:763 ] give framework 20140916-225614-3125920579-47865-21026-0000 0n failov .. / .. /src/tests/health_check_tests.cpp:319 : failur actual function call count n't match expect_cal ( sched , statusupd ( & driver , _ ) ) ... expect : call 4 time actual : call - unsatisfi activ i0916 22:56:28.190624 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 5783bb6f-112f-4434-a160-a336e890398a ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.190757 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : b4a9f647-3894-47f3-b55e-49d0355b20f9 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.190773 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 792b8e42-0d72-451b-978a-7d1f29a15751 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.190831 21040 hierarchical_allocator_process.hpp:405 ] deactiv framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.190856 21054 master.cpp:3471 ] framework failov timeout , remov framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.190846 21046 status_update_manager.cpp:373 ] forward statu updat task_run ( uuid : 792b8e42-0d72-451b-978a-7d1f29a15751 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 master @ 67.195.81.186:47865 i0916 22:56:28.190887 21054 master.cpp:3976 ] remov framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.190887 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : b5d5d6c7-e92c-4ca0-ab72-656542c14ad ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.190994 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : c0225c5c-b15e-4b5e-a063-07a29703ea12 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.190996 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 5783bb6f-112f-4434-a160-a336e890398a ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.190999 21054 master.hpp:851 ] remov task 1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140916-225614-3125920579-47865-21026-0 ( penates.apache.org ) i0916 22:56:28.191090 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : b4a9f647-3894-47f3-b55e-49d0355b20f9 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 w0916 22:56:28.191141 21054 master.cpp:4419 ] remov task 1 framework 20140916-225614-3125920579-47865-21026-0000 slave 20140916-225614-3125920579-47865-21026-0 non-termin state task_run i0916 22:56:28.191093 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : e9e3fdb1-d8e0-4bfc-970b-fcd098cace13 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.191181 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : b5d5d6c7-e92c-4ca0-ab72-656542c14ad ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.191256 21054 master.cpp:650 ] master termin i0916 22:56:28.191258 21043 hierarchical_allocator_process.hpp:563 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total allocat : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) slave 20140916-225614-3125920579-47865-21026-0 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369088 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : c0225c5c-b15e-4b5e-a063-07a29703ea12 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.191319 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 780d211b-6ecc-478d-93e9-6744ed0a2d33 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369132 21043 hierarchical_allocator_process.hpp:360 ] remov framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369225 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : e9e3fdb1-d8e0-4bfc-970b-fcd098cace13 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369283 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : f41475b5-9b45-478e-8cd9-2cf7854627dd ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369323 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 780d211b-6ecc-478d-93e9-6744ed0a2d33 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369415 21046 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : f41475b5-9b45-478e-8cd9-2cf7854627dd ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369420 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 5f3a1b44-51f0-4deb-ba4c-e7238f63f856 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369536 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 4663a09b-147f-455c-a577-3d967ddf5256 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369642 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 89cbabd7-0169-4b58-8df7-d8fd4bc4a287 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369685 21055 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 5f3a1b44-51f0-4deb-ba4c-e7238f63f856 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369753 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 3c491f72-95f1-4c52-b7ca-c6470f748eb5 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369802 21055 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 4663a09b-147f-455c-a577-3d967ddf5256 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369884 21055 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 89cbabd7-0169-4b58-8df7-d8fd4bc4a287 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369889 21052 slave.cpp:2110 ] handl statu updat task_run ( uuid : 218be9bd-a229-4808-8fb6-1e507830cdaf ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.369943 21055 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 3c491f72-95f1-4c52-b7ca-c6470f748eb5 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.369978 21052 slave.cpp:1431 ] ask shut framework 20140916-225614-3125920579-47865-21026-0000 master @ 67.195.81.186:47865 i0916 22:56:28.369998 21052 slave.cpp:1456 ] shut framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370009 21055 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 218be9bd-a229-4808-8fb6-1e507830cdaf ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370018 21052 slave.cpp:2899 ] shut executor ' 1 ' framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370183 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 792b8e42-0d72-451b-978a-7d1f29a15751 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370206 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 792b8e42-0d72-451b-978a-7d1f29a15751 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.370426 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 5783bb6f-112f-4434-a160-a336e890398a ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370447 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 5783bb6f-112f-4434-a160-a336e890398a ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.370635 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : b4a9f647-3894-47f3-b55e-49d0355b20f9 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370657 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : b4a9f647-3894-47f3-b55e-49d0355b20f9 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.370815 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : b5d5d6c7-e92c-4ca0-ab72-656542c14ad ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.370837 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : b5d5d6c7-e92c-4ca0-ab72-656542c14ad ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.370972 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : c0225c5c-b15e-4b5e-a063-07a29703ea12 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.371000 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : c0225c5c-b15e-4b5e-a063-07a29703ea12 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.371155 21052 slave.cpp:2378 ] master @ 67.195.81.186:47865 exit w0916 22:56:28.371177 21052 slave.cpp:2381 ] master disconnect ! wait new master elect i0916 22:56:28.371202 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : e9e3fdb1-d8e0-4bfc-970b-fcd098cace13 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540035 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : e9e3fdb1-d8e0-4bfc-970b-fcd098cace13 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.371701 21053 containerizer.cpp:882 ] destroy contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' i0916 22:56:28.540177 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 780d211b-6ecc-478d-93e9-6744ed0a2d33 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540196 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 780d211b-6ecc-478d-93e9-6744ed0a2d33 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.540324 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : f41475b5-9b45-478e-8cd9-2cf7854627dd ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540350 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : f41475b5-9b45-478e-8cd9-2cf7854627dd ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.540403 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 5f3a1b44-51f0-4deb-ba4c-e7238f63f856 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540421 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 5f3a1b44-51f0-4deb-ba4c-e7238f63f856 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.540530 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 4663a09b-147f-455c-a577-3d967ddf5256 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540556 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 4663a09b-147f-455c-a577-3d967ddf5256 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.540664 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 89cbabd7-0169-4b58-8df7-d8fd4bc4a287 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540681 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 89cbabd7-0169-4b58-8df7-d8fd4bc4a287 ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.540889 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 3c491f72-95f1-4c52-b7ca-c6470f748eb5 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.540918 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 3c491f72-95f1-4c52-b7ca-c6470f748eb5 ) task 1 health state unhealthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:28.541082 21052 slave.cpp:2267 ] statu updat manag success handl statu updat task_run ( uuid : 218be9bd-a229-4808-8fb6-1e507830cdaf ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:28.541111 21052 slave.cpp:2273 ] send acknowledg statu updat task_run ( uuid : 218be9bd-a229-4808-8fb6-1e507830cdaf ) task 1 health state healthi framework 20140916-225614-3125920579-47865-21026-0000 executor ( 1 ) @ 67.195.81.186:35510 i0916 22:56:29.047708 21053 containerizer.cpp:997 ] executor contain 'd383a013-89cf-47c6-ad8e-39e2f3e971fd ' exit i0916 22:56:29.048037 21050 slave.cpp:2617 ] executor ' 1 ' framework 20140916-225614-3125920579-47865-21026-0000 termin signal kill i0916 22:56:29.048197 21050 slave.cpp:2753 ] clean executor ' 1 ' framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:29.048373 21050 slave.cpp:2828 ] clean framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:29.048444 21043 status_update_manager.cpp:282 ] close statu updat stream framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:29.048457 21050 slave.cpp:477 ] slave termin i0916 22:56:29.048476 21043 status_update_manager.cpp:530 ] clean statu updat stream task 1 framework 20140916-225614-3125920579-47865-21026-0000 i0916 22:56:29.048462 21041 gc.cpp:56 ] schedul '/tmp/healthchecktest_healthstatuschange_cgktig/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd ' gc 6.99999944121481day futur i0916 22:56:29.048568 21041 gc.cpp:56 ] schedul '/tmp/healthchecktest_healthstatuschange_cgktig/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1 ' gc 6.99999944031111day futur i0916 22:56:29.048607 21041 gc.cpp:56 ] schedul '/tmp/healthchecktest_healthstatuschange_cgktig/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000 ' gc 6.99999943939852day futur [ fail ] healthchecktest.healthstatuschang ( 15019 ms ) { noformat }",MESOS-1802,5.0
"reconcili send out-of-ord updat . when slave re-regist master , current send latest task state task termin acknowledg . howev , reconcili assum alway latest unacknowledg state task repres master . as result , out-of-ord updat possibl , e.g . ( 1 ) slave task t task_finish , unacknowledg updat : [ task_run , task_finish ] . ( 2 ) master fail . ( 3 ) new master re-regist slave t task_finish . ( 4 ) reconcili request arriv , master send task_finish . ( 5 ) slave send task_run master , master send task_run . i think fix preserv task state invari master , name , master latest unacknowledg state task . thi mean slave re-regist , instead send latest acknowledg state task .",MESOS-1799,3.0
"slave send exit executor messag executor never launch . when slave send task_lost launch executor task , slave send exit executor messag master . sinc master receiv exit executor messag , still think executor 's resourc consum slave . one possibl fix would send exit executor messag master case .",MESOS-1720,8.0
"the slave send pend task re-registr . in look like oversight , pend task executor slave ( framework : :pend ) sent re-registr messag . for task , lead spuriou task_lost notif gener master fals think task present slave .",MESOS-1715,3.0
"improv reconcili master slave . as updat master keep task memori termin acknowledg ( mesos-1410 ) , lifetim task meso look follow : { code } master slave { } { } { tn } { } // master receiv task t , non-termin . forward slave . { tn } { tn } // slave receiv task t , non-termin . { tn } { tt } // task becom termin slave . updat forward . { tt } { tt } // master receiv updat , forward framework . { } { tt } // master receiv ack , forward slave . { } { } // slave receiv ack . { code } in current form reconcili , slave send master task termin acknowledg . at point lifecycl , slave 's re-registr messag reach master . note follow properti : * ( 1 ) * the master may non-termin task , present slave 's re-registr messag . * ( 2 ) * the master may non-termin task , present slave 's re-registr messag differ state . * ( 3 ) * the slave 's re-registr messag may contain termin unacknowledg task unknown master . in current master / slave [ reconciliation|http : //github.com/apache/mesos/blob/0.19.1/src/master/master.cpp # l3146 ] code , master assum case ( 1 ) launch task messag drop , send task_lost . we 've seen ( 1 ) happen even task reach slave correctli , lead inconsist ! after chat [ ~vinodkon ] , 're consid updat reconcili occur follow : → slave send task termin acknowledg , re-registr . thi . → if master see task miss slave , master send task need reconcil slave task . thi piggy-back re-registr messag . → the slave send task_lost task known . prefer retri manner , unless updat socket closur slave forc re-registr .",MESOS-1696,3.0
"handl temporari one-way master -- > slave socket closur . in mesos-1529 , realiz 's possibl slave remain disconnect master follow occur : → master slave connect oper normal . → temporari one-way network failur , master→slav link break . → master mark slave disconnect . → network restor health check continu normal , slave remov result . slave attempt re-regist sinc receiv ping . → slave remain disconnect accord master , slave tri re-regist . bad ! we origin think use failov timeout master remov slave n't re-regist . howev , danger zookeep issu prevent slave re-regist master ; want remov ton slave situat . rather , slave health check correctli re-regist within timeout , could send registr request master slave , tell slave must re-regist . thi messag could also use receiv statu updat ( messag ) slave disconnect master .",MESOS-1668,2.0
"network isol toler slave crash isolate/cleanup . a slave may crash installing/remov filter . the slave recoveri network isol toler partial instal filter . also , want avoid leak filter host eth0 host lo . the current code toler , thu may caus follow error : { noformat } fail perform recoveri : collect fail : fail recov contain d409a100-2afb-497c-864f-fe3002cf65d9 pid 50405 : no ephemer port found to remedi follow : step 1 : rm -f /var/lib/mesos/meta/slaves/latest thi ensur slave n't recov old live executor . step 2 : restart slave . { noformat }",MESOS-1649,3.0
"slaverecoverytest/0.reconcilekilltask flaki observ jenkin . { code } [ run ] slaverecoverytest/0.reconcilekilltask use temporari directori '/tmp/slaverecoverytest_0_reconcilekilltask_3zj6dg' i0714 15:08:43.915114 27216 leveldb.cpp:176 ] open db 474.695188m i0714 15:08:43.933645 27216 leveldb.cpp:183 ] compact db 18.068942m i0714 15:08:43.934129 27216 leveldb.cpp:198 ] creat db iter 7860n i0714 15:08:43.934439 27216 leveldb.cpp:204 ] seek begin db 2560n i0714 15:08:43.934779 27216 leveldb.cpp:273 ] iter 0 key db 1400n i0714 15:08:43.935098 27216 replica.cpp:741 ] replica recov log posit 0 - > 0 1 hole 0 unlearn i0714 15:08:43.936027 27238 recover.cpp:425 ] start replica recoveri i0714 15:08:43.936225 27238 recover.cpp:451 ] replica empti statu i0714 15:08:43.936867 27238 replica.cpp:638 ] replica empti statu receiv broadcast recov request i0714 15:08:43.937049 27238 recover.cpp:188 ] receiv recov respons replica empti statu i0714 15:08:43.937232 27238 recover.cpp:542 ] updat replica statu start i0714 15:08:43.945600 27235 master.cpp:288 ] master 20140714-150843-16842879-55850-27216 ( quantal ) start 127.0.1.1:55850 i0714 15:08:43.945643 27235 master.cpp:325 ] master allow authent framework regist i0714 15:08:43.945651 27235 master.cpp:330 ] master allow authent slave regist i0714 15:08:43.945658 27235 credentials.hpp:36 ] load credenti authent '/tmp/slaverecoverytest_0_reconcilekilltask_3zj6dg/credentials' i0714 15:08:43.945808 27235 master.cpp:359 ] author enabl i0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301 ] initi hierarch alloc process master : master @ 127.0.1.1:55850 i0714 15:08:43.946419 27235 master.cpp:122 ] no whitelist given . advertis offer slave i0714 15:08:43.946614 27235 master.cpp:1128 ] the newli elect leader master @ 127.0.1.1:55850 id 20140714-150843-16842879-55850-27216 i0714 15:08:43.946630 27235 master.cpp:1141 ] elect lead master ! i0714 15:08:43.946637 27235 master.cpp:959 ] recov registrar i0714 15:08:43.946707 27235 registrar.cpp:313 ] recov registrar i0714 15:08:43.957895 27238 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 20.529301m i0714 15:08:43.957978 27238 replica.cpp:320 ] persist replica statu start i0714 15:08:43.958142 27238 recover.cpp:451 ] replica start statu i0714 15:08:43.958664 27238 replica.cpp:638 ] replica start statu receiv broadcast recov request i0714 15:08:43.958762 27238 recover.cpp:188 ] receiv recov respons replica start statu i0714 15:08:43.958945 27238 recover.cpp:542 ] updat replica statu vote i0714 15:08:43.975685 27238 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 16.646136m i0714 15:08:43.976367 27238 replica.cpp:320 ] persist replica statu vote i0714 15:08:43.976824 27241 recover.cpp:556 ] success join paxo group i0714 15:08:43.977072 27242 recover.cpp:440 ] recov process termin i0714 15:08:43.980590 27236 log.cpp:656 ] attempt start writer i0714 15:08:43.981385 27236 replica.cpp:474 ] replica receiv implicit promis request propos 1 i0714 15:08:43.999141 27236 leveldb.cpp:306 ] persist metadata ( 8 byte ) leveldb took 17.705787m i0714 15:08:43.999222 27236 replica.cpp:342 ] persist promis 1 i0714 15:08:44.004451 27240 coordinator.cpp:230 ] coordin attemp fill miss posit i0714 15:08:44.004914 27240 replica.cpp:375 ] replica receiv explicit promis request posit 0 propos 2 i0714 15:08:44.021456 27240 leveldb.cpp:343 ] persist action ( 8 byte ) leveldb took 16.499775m i0714 15:08:44.021533 27240 replica.cpp:676 ] persist action 0 i0714 15:08:44.022006 27240 replica.cpp:508 ] replica receiv write request posit 0 i0714 15:08:44.022043 27240 leveldb.cpp:438 ] read posit leveldb took 21376n i0714 15:08:44.035969 27240 leveldb.cpp:343 ] persist action ( 14 byte ) leveldb took 13.885907m i0714 15:08:44.036365 27240 replica.cpp:676 ] persist action 0 i0714 15:08:44.040156 27238 replica.cpp:655 ] replica receiv learn notic posit 0 i0714 15:08:44.058082 27238 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 17.860707m i0714 15:08:44.058161 27238 replica.cpp:676 ] persist action 0 i0714 15:08:44.058176 27238 replica.cpp:661 ] replica learn nop action posit 0 i0714 15:08:44.058526 27238 log.cpp:672 ] writer start end posit 0 i0714 15:08:44.058872 27238 leveldb.cpp:438 ] read posit leveldb took 25660n i0714 15:08:44.060556 27238 registrar.cpp:346 ] success fetch registri ( 0b ) i0714 15:08:44.060845 27238 registrar.cpp:422 ] attempt updat 'registry' i0714 15:08:44.062304 27238 log.cpp:680 ] attempt append 120 byte log i0714 15:08:44.062866 27236 coordinator.cpp:340 ] coordin attempt write append action posit 1 i0714 15:08:44.063154 27236 replica.cpp:508 ] replica receiv write request posit 1 i0714 15:08:44.082813 27236 leveldb.cpp:343 ] persist action ( 137 byte ) leveldb took 19.61683m i0714 15:08:44.082890 27236 replica.cpp:676 ] persist action 1 i0714 15:08:44.083256 27236 replica.cpp:655 ] replica receiv learn notic posit 1 i0714 15:08:44.097398 27236 leveldb.cpp:343 ] persist action ( 139 byte ) leveldb took 14.104796m i0714 15:08:44.097475 27236 replica.cpp:676 ] persist action 1 i0714 15:08:44.097488 27236 replica.cpp:661 ] replica learn append action posit 1 i0714 15:08:44.098569 27236 registrar.cpp:479 ] success updat 'registry' i0714 15:08:44.098906 27240 log.cpp:699 ] attempt truncat log 1 i0714 15:08:44.099608 27240 coordinator.cpp:340 ] coordin attempt write truncat action posit 2 i0714 15:08:44.100005 27240 replica.cpp:508 ] replica receiv write request posit 2 i0714 15:08:44.100566 27236 registrar.cpp:372 ] success recov registrar i0714 15:08:44.101227 27239 master.cpp:986 ] recov 0 slave registri ( 84b ) ; allow 10min slave re-regist i0714 15:08:44.118376 27240 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 18.329495m i0714 15:08:44.118455 27240 replica.cpp:676 ] persist action 2 i0714 15:08:44.122258 27242 replica.cpp:655 ] replica receiv learn notic posit 2 i0714 15:08:44.137336 27242 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 15.023553m i0714 15:08:44.137460 27242 leveldb.cpp:401 ] delet ~1 key leveldb took 55049n i0714 15:08:44.137480 27242 replica.cpp:676 ] persist action 2 i0714 15:08:44.137492 27242 replica.cpp:661 ] replica learn truncat action posit 2 i0714 15:08:44.143729 27216 containerizer.cpp:124 ] use isol : posix/cpu , posix/mem i0714 15:08:44.145934 27242 slave.cpp:168 ] slave start 43 ) @ 127.0.1.1:55850 i0714 15:08:44.145953 27242 credentials.hpp:84 ] load credenti authent '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/credential' i0714 15:08:44.146040 27242 slave.cpp:266 ] slave use credenti : test-princip i0714 15:08:44.146136 27242 slave.cpp:279 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0714 15:08:44.146198 27242 slave.cpp:324 ] slave hostnam : quantal i0714 15:08:44.146209 27242 slave.cpp:325 ] slave checkpoint : true i0714 15:08:44.146708 27242 state.cpp:33 ] recov state '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta' i0714 15:08:44.146824 27242 status_update_manager.cpp:193 ] recov statu updat manag i0714 15:08:44.146901 27242 containerizer.cpp:287 ] recov container i0714 15:08:44.147228 27242 slave.cpp:3126 ] finish recoveri i0714 15:08:44.147531 27242 slave.cpp:599 ] new master detect master @ 127.0.1.1:55850 i0714 15:08:44.147562 27242 slave.cpp:675 ] authent master master @ 127.0.1.1:55850 i0714 15:08:44.147614 27242 slave.cpp:648 ] detect new master i0714 15:08:44.147652 27242 status_update_manager.cpp:167 ] new master detect master @ 127.0.1.1:55850 i0714 15:08:44.147691 27242 authenticatee.hpp:128 ] creat new client sasl connect i0714 15:08:44.148533 27235 master.cpp:3507 ] authent slave ( 43 ) @ 127.0.1.1:55850 i0714 15:08:44.148666 27235 authenticator.hpp:156 ] creat new server sasl connect i0714 15:08:44.149054 27242 authenticatee.hpp:219 ] receiv sasl authent mechan : cram-md5 i0714 15:08:44.149447 27242 authenticatee.hpp:245 ] attempt authent mechan 'cram-md5' i0714 15:08:44.149917 27236 authenticator.hpp:262 ] receiv sasl authent start i0714 15:08:44.149974 27236 authenticator.hpp:384 ] authent requir step i0714 15:08:44.150208 27242 authenticatee.hpp:265 ] receiv sasl authent step i0714 15:08:44.150720 27239 authenticator.hpp:290 ] receiv sasl authent step i0714 15:08:44.150749 27239 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0714 15:08:44.150758 27239 auxprop.cpp:153 ] look auxiliari properti ' * userpassword' i0714 15:08:44.150771 27239 auxprop.cpp:153 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0714 15:08:44.150781 27239 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0714 15:08:44.150787 27239 auxprop.cpp:103 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0714 15:08:44.150792 27239 auxprop.cpp:103 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0714 15:08:44.150804 27239 authenticator.hpp:376 ] authent success i0714 15:08:44.150848 27239 master.cpp:3547 ] success authent princip 'test-princip ' slave ( 43 ) @ 127.0.1.1:55850 i0714 15:08:44.157696 27242 authenticatee.hpp:305 ] authent success i0714 15:08:44.158855 27242 slave.cpp:732 ] success authent master master @ 127.0.1.1:55850 i0714 15:08:44.158936 27242 slave.cpp:970 ] will retri registr 10.352612m necessari i0714 15:08:44.161813 27216 sched.cpp:139 ] version : 0.20.0 i0714 15:08:44.162608 27236 sched.cpp:235 ] new master detect master @ 127.0.1.1:55850 i0714 15:08:44.162637 27236 sched.cpp:285 ] authent master master @ 127.0.1.1:55850 i0714 15:08:44.162747 27236 authenticatee.hpp:128 ] creat new client sasl connect i0714 15:08:44.163506 27239 master.cpp:2789 ] regist slave slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) id 20140714-150843-16842879-55850-27216-0 i0714 15:08:44.164086 27238 registrar.cpp:422 ] attempt updat 'registry' i0714 15:08:44.165694 27238 log.cpp:680 ] attempt append 295 byte log i0714 15:08:44.166231 27240 coordinator.cpp:340 ] coordin attempt write append action posit 3 i0714 15:08:44.166517 27240 replica.cpp:508 ] replica receiv write request posit 3 i0714 15:08:44.167199 27239 master.cpp:3507 ] authent scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 i0714 15:08:44.167867 27241 authenticator.hpp:156 ] creat new server sasl connect i0714 15:08:44.168058 27241 authenticatee.hpp:219 ] receiv sasl authent mechan : cram-md5 i0714 15:08:44.168081 27241 authenticatee.hpp:245 ] attempt authent mechan 'cram-md5' i0714 15:08:44.168107 27241 authenticator.hpp:262 ] receiv sasl authent start i0714 15:08:44.168149 27241 authenticator.hpp:384 ] authent requir step i0714 15:08:44.168176 27241 authenticatee.hpp:265 ] receiv sasl authent step i0714 15:08:44.168215 27241 authenticator.hpp:290 ] receiv sasl authent step i0714 15:08:44.168233 27241 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0714 15:08:44.168793 27241 auxprop.cpp:153 ] look auxiliari properti ' * userpassword' i0714 15:08:44.168820 27241 auxprop.cpp:153 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0714 15:08:44.168834 27241 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0714 15:08:44.168840 27241 auxprop.cpp:103 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0714 15:08:44.168845 27241 auxprop.cpp:103 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0714 15:08:44.168858 27241 authenticator.hpp:376 ] authent success i0714 15:08:44.168895 27241 authenticatee.hpp:305 ] authent success i0714 15:08:44.168970 27241 sched.cpp:359 ] success authent master master @ 127.0.1.1:55850 i0714 15:08:44.168987 27241 sched.cpp:478 ] send registr request master @ 127.0.1.1:55850 i0714 15:08:44.169426 27239 master.cpp:1239 ] queu registr request scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 authent still progress i0714 15:08:44.169958 27239 master.cpp:3547 ] success authent princip 'test-princip ' scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 i0714 15:08:44.170440 27241 slave.cpp:970 ] will retri registr 8.76707m necessari i0714 15:08:44.175359 27239 master.cpp:2777 ] ignor regist slave messag slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) admiss alreadi progress i0714 15:08:44.175916 27239 master.cpp:1247 ] receiv registr request scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 i0714 15:08:44.176298 27239 master.cpp:1207 ] author framework princip 'test-princip ' receiv offer role ' * ' i0714 15:08:44.176858 27239 master.cpp:1306 ] regist framework 20140714-150843-16842879-55850-27216-0000 scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 i0714 15:08:44.177408 27236 sched.cpp:409 ] framework regist 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.177443 27236 sched.cpp:423 ] schedul : :regist took 12527n i0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331 ] ad framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724 ] no resourc avail alloc ! i0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686 ] perform alloc 0 slave 8120n i0714 15:08:44.179908 27241 slave.cpp:970 ] will retri registr 66.781028m necessari i0714 15:08:44.180007 27241 master.cpp:2777 ] ignor regist slave messag slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) admiss alreadi progress i0714 15:08:44.183082 27240 leveldb.cpp:343 ] persist action ( 314 byte ) leveldb took 16.533189m i0714 15:08:44.183125 27240 replica.cpp:676 ] persist action 3 i0714 15:08:44.183465 27240 replica.cpp:655 ] replica receiv learn notic posit 3 i0714 15:08:44.203276 27240 leveldb.cpp:343 ] persist action ( 316 byte ) leveldb took 19.768951m i0714 15:08:44.203376 27240 replica.cpp:676 ] persist action 3 i0714 15:08:44.203392 27240 replica.cpp:661 ] replica learn append action posit 3 i0714 15:08:44.204033 27240 registrar.cpp:479 ] success updat 'registry' i0714 15:08:44.204138 27240 log.cpp:699 ] attempt truncat log 3 i0714 15:08:44.204221 27240 master.cpp:2829 ] regist slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:44.204241 27240 master.cpp:3975 ] ad slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0714 15:08:44.204387 27240 coordinator.cpp:340 ] coordin attempt write truncat action posit 4 i0714 15:08:44.204489 27240 slave.cpp:766 ] regist master master @ 127.0.1.1:55850 ; given slave id 20140714-150843-16842879-55850-27216-0 i0714 15:08:44.204745 27240 slave.cpp:779 ] checkpoint slaveinfo '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info' i0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444 ] ad slave 20140714-150843-16842879-55850-27216-0 ( quantal ) cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] avail ) i0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750 ] offer cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706 ] perform alloc slave 20140714-150843-16842879-55850-27216-0 131192n i0714 15:08:44.205189 27240 slave.cpp:2323 ] receiv ping slave-observ ( 32 ) @ 127.0.1.1:55850 i0714 15:08:44.205258 27240 master.hpp:801 ] ad offer 20140714-150843-16842879-55850-27216-0 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:44.205303 27240 master.cpp:3454 ] send 1 offer framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.205469 27240 sched.cpp:546 ] schedul : :resourceoff took 23591n i0714 15:08:44.206351 27241 replica.cpp:508 ] replica receiv write request posit 4 i0714 15:08:44.208353 27237 master.hpp:811 ] remov offer 20140714-150843-16842879-55850-27216-0 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:44.208436 27237 master.cpp:2133 ] process repli offer : [ 20140714-150843-16842879-55850-27216-0 ] slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.208472 27237 master.cpp:2219 ] author framework princip 'test-princip ' launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 user 'jenkins' i0714 15:08:44.208909 27237 master.hpp:773 ] ad task 4a6783aa-8d07-46e3-8399-2a5d047f0021 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:44.208947 27237 master.cpp:2285 ] launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:44.209090 27237 slave.cpp:1001 ] got assign task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.209190 27237 slave.cpp:3398 ] checkpoint frameworkinfo '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info' i0714 15:08:44.209413 27237 slave.cpp:3405 ] checkpoint framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 ' '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid' i0714 15:08:44.209710 27237 slave.cpp:1111 ] launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.210978 27237 slave.cpp:3720 ] checkpoint executorinfo '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info' i0714 15:08:44.211520 27237 slave.cpp:3835 ] checkpoint taskinfo '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info' i0714 15:08:44.211714 27237 slave.cpp:1221 ] queu task '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework '20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.211937 27236 containerizer.cpp:427 ] start contain '19c466f8-bb5a-4842-a152-f585ff88762a ' executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework '20140714-150843-16842879-55850-27216-0000' i0714 15:08:44.212242 27236 slave.cpp:560 ] success attach file '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' i0714 15:08:44.216187 27236 launcher.cpp:137 ] fork child pid '28451 ' contain '19c466f8-bb5a-4842-a152-f585ff88762a' i0714 15:08:44.217281 27236 containerizer.cpp:705 ] checkpoint executor 's fork pid 28451 '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid' i0714 15:08:44.219408 27236 containerizer.cpp:537 ] fetch uri contain '19c466f8-bb5a-4842-a152-f585ff88762a ' use command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher' i0714 15:08:44.223963 27241 leveldb.cpp:343 ] persist action ( 16 byte ) leveldb took 17.554461m i0714 15:08:44.224501 27241 replica.cpp:676 ] persist action 4 i0714 15:08:44.225051 27241 replica.cpp:655 ] replica receiv learn notic posit 4 i0714 15:08:44.242923 27241 leveldb.cpp:343 ] persist action ( 18 byte ) leveldb took 17.806547m i0714 15:08:44.243057 27241 leveldb.cpp:401 ] delet ~2 key leveldb took 57154n i0714 15:08:44.243078 27241 replica.cpp:676 ] persist action 4 i0714 15:08:44.243096 27241 replica.cpp:661 ] replica learn truncat action posit 4 i0714 15:08:44.401140 27241 slave.cpp:2468 ] monitor executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework '20140714-150843-16842879-55850-27216-0000 ' contain '19c466f8-bb5a-4842-a152-f585ff88762a' warn : log initgooglelog ( ) written stderr i0714 15:08:44.434221 28486 process.cpp:1671 ] libprocess initi 127.0.1.1:34669 8 cpu i0714 15:08:44.436146 28486 exec.cpp:131 ] version : 0.20.0 i0714 15:08:44.438555 28500 exec.cpp:181 ] executor start : executor ( 1 ) @ 127.0.1.1:34669 pid 28486 i0714 15:08:44.440846 27241 slave.cpp:1732 ] got registr executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.440917 27241 slave.cpp:1817 ] checkpoint executor pid 'executor ( 1 ) @ 127.0.1.1:34669 ' '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid' i0714 15:08:44.442373 27243 process.cpp:1098 ] socket close receiv i0714 15:08:44.442790 27241 slave.cpp:1851 ] flush queu task 4a6783aa-8d07-46e3-8399-2a5d047f0021 executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.443192 27243 process.cpp:1098 ] socket close receiv i0714 15:08:44.443994 28508 process.cpp:1037 ] socket close receiv i0714 15:08:44.444144 28508 process.cpp:1037 ] socket close receiv i0714 15:08:44.444741 28500 exec.cpp:205 ] executor regist slave 20140714-150843-16842879-55850-27216-0 regist executor quantal i0714 15:08:44.446338 28500 exec.cpp:217 ] executor : :regist took 534236n i0714 15:08:44.446715 28500 exec.cpp:292 ] executor ask run task '4a6783aa-8d07-46e3-8399-2a5d047f0021' start task 4a6783aa-8d07-46e3-8399-2a5d047f0021 i0714 15:08:44.447548 28500 exec.cpp:301 ] executor : :launchtask took 584306n sh -c 'sleep 1000' fork command 28509 i0714 15:08:44.451202 28506 exec.cpp:524 ] executor send statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.452327 27239 slave.cpp:2086 ] handl statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 executor ( 1 ) @ 127.0.1.1:34669 i0714 15:08:44.452503 27239 status_update_manager.cpp:320 ] receiv statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.452520 27239 status_update_manager.cpp:499 ] creat statusupd stream task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.452775 27239 status_update_manager.hpp:342 ] checkpoint updat statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.472384 27239 status_update_manager.cpp:373 ] forward statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 master @ 127.0.1.1:55850 i0714 15:08:44.472764 27237 master.cpp:3115 ] statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:44.472854 27237 sched.cpp:637 ] schedul : :statusupd took 17656n i0714 15:08:44.472920 27237 master.cpp:2639 ] forward statu updat acknowledg 323fc20a-b5b8-475d-8752-b1f853797f55 task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:44.473122 27239 status_update_manager.cpp:398 ] receiv statu updat acknowledg ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.473146 27239 status_update_manager.hpp:342 ] checkpoint ack statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.473244 27237 slave.cpp:2244 ] statu updat manag success handl statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.473258 27237 slave.cpp:2250 ] send acknowledg statu updat task_run ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 executor ( 1 ) @ 127.0.1.1:34669 i0714 15:08:44.473567 27243 process.cpp:1098 ] socket close receiv i0714 15:08:44.474095 28508 process.cpp:1037 ] socket close receiv i0714 15:08:44.474676 28502 exec.cpp:338 ] executor receiv statu updat acknowledg 323fc20a-b5b8-475d-8752-b1f853797f55 task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.491111 27239 slave.cpp:1672 ] statu updat manag success handl statu updat acknowledg ( uuid : 323fc20a-b5b8-475d-8752-b1f853797f55 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.491761 27216 slave.cpp:484 ] slave termin i0714 15:08:44.492559 27216 containerizer.cpp:124 ] use isol : posix/cpu , posix/mem i0714 15:08:44.494635 27237 master.cpp:766 ] slave 20140714-150843-16842879-55850-27216-0 slave ( 43 ) @ 127.0.1.1:55850 ( quantal ) disconnect i0714 15:08:44.494663 27237 master.cpp:1608 ] disconnect slave 20140714-150843-16842879-55850-27216-0 i0714 15:08:44.495120 27237 slave.cpp:168 ] slave start 44 ) @ 127.0.1.1:55850 i0714 15:08:44.495133 27237 credentials.hpp:84 ] load credenti authent '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/credential' i0714 15:08:44.495226 27237 slave.cpp:266 ] slave use credenti : test-princip i0714 15:08:44.495322 27237 slave.cpp:279 ] slave resourc : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] i0714 15:08:44.495407 27237 slave.cpp:324 ] slave hostnam : quantal i0714 15:08:44.495419 27237 slave.cpp:325 ] slave checkpoint : true i0714 15:08:44.495939 27242 master.cpp:2469 ] ask kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.496207 27238 state.cpp:33 ] recov state '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta' i0714 15:08:44.498291 27240 slave.cpp:3194 ] recov framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.498325 27240 slave.cpp:3570 ] recov executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.498940 27240 status_update_manager.cpp:193 ] recov statu updat manag i0714 15:08:44.498956 27240 status_update_manager.cpp:201 ] recov executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.498975 27240 status_update_manager.cpp:499 ] creat statusupd stream task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.499092 27240 status_update_manager.hpp:306 ] replay statu updat stream task 4a6783aa-8d07-46e3-8399-2a5d047f0021 i0714 15:08:44.499241 27240 slave.cpp:560 ] success attach file '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' i0714 15:08:44.499433 27240 containerizer.cpp:287 ] recov container i0714 15:08:44.499457 27240 containerizer.cpp:329 ] recov contain '19c466f8-bb5a-4842-a152-f585ff88762a ' executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483 ] slave 20140714-150843-16842879-55850-27216-0 disconnect i0714 15:08:44.501255 27240 slave.cpp:3067 ] send reconnect request executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 executor ( 1 ) @ 127.0.1.1:34669 i0714 15:08:44.502030 28501 exec.cpp:251 ] receiv reconnect request slave 20140714-150843-16842879-55850-27216-0 i0714 15:08:44.502627 27243 process.cpp:1098 ] socket close receiv i0714 15:08:44.502681 28508 process.cpp:1037 ] socket close receiv i0714 15:08:44.503211 27240 slave.cpp:1911 ] re-regist executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:44.504238 28501 exec.cpp:228 ] executor re-regist slave 20140714-150843-16842879-55850-27216-0 i0714 15:08:44.505033 28501 exec.cpp:240 ] executor : :reregist took 45053n re-regist executor quantal i0714 15:08:44.505507 27243 process.cpp:1098 ] socket close receiv i0714 15:08:44.505558 28508 process.cpp:1037 ] socket close receiv i0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686 ] perform alloc 1 slave 124255n i0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686 ] perform alloc 1 slave 61521n i0714 15:08:46.503978 27238 slave.cpp:2035 ] clean un-reregist executor i0714 15:08:46.504050 27238 slave.cpp:3126 ] finish recoveri i0714 15:08:46.504590 27238 slave.cpp:599 ] new master detect master @ 127.0.1.1:55850 i0714 15:08:46.504639 27238 slave.cpp:675 ] authent master master @ 127.0.1.1:55850 i0714 15:08:46.504729 27238 slave.cpp:648 ] detect new master i0714 15:08:46.504772 27238 status_update_manager.cpp:167 ] new master detect master @ 127.0.1.1:55850 i0714 15:08:46.504863 27238 authenticatee.hpp:128 ] creat new client sasl connect i0714 15:08:46.505091 27238 master.cpp:3507 ] authent slave ( 44 ) @ 127.0.1.1:55850 i0714 15:08:46.505239 27238 authenticator.hpp:156 ] creat new server sasl connect i0714 15:08:46.505363 27238 authenticatee.hpp:219 ] receiv sasl authent mechan : cram-md5 i0714 15:08:46.505393 27238 authenticatee.hpp:245 ] attempt authent mechan 'cram-md5' i0714 15:08:46.505420 27238 authenticator.hpp:262 ] receiv sasl authent start i0714 15:08:46.505476 27238 authenticator.hpp:384 ] authent requir step i0714 15:08:46.505506 27238 authenticatee.hpp:265 ] receiv sasl authent step i0714 15:08:46.505542 27238 authenticator.hpp:290 ] receiv sasl authent step i0714 15:08:46.505558 27238 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : fals i0714 15:08:46.505566 27238 auxprop.cpp:153 ] look auxiliari properti ' * userpassword' i0714 15:08:46.505584 27238 auxprop.cpp:153 ] look auxiliari properti ' * cmusaslsecretcram-md5' i0714 15:08:46.505595 27238 auxprop.cpp:81 ] request lookup properti user : 'test-princip ' realm : 'quantal ' server fqdn : 'quantal ' sasl_auxprop_verify_against_hash : fals sasl_auxprop_overrid : fals sasl_auxprop_authzid : true i0714 15:08:46.505601 27238 auxprop.cpp:103 ] skip auxiliari properti ' * userpassword ' sinc sasl_auxprop_authzid == true i0714 15:08:46.505606 27238 auxprop.cpp:103 ] skip auxiliari properti ' * cmusaslsecretcram-md5 ' sinc sasl_auxprop_authzid == true i0714 15:08:46.505616 27238 authenticator.hpp:376 ] authent success i0714 15:08:46.505646 27238 authenticatee.hpp:305 ] authent success i0714 15:08:46.505671 27238 master.cpp:3547 ] success authent princip 'test-princip ' slave ( 44 ) @ 127.0.1.1:55850 i0714 15:08:46.505769 27238 slave.cpp:732 ] success authent master master @ 127.0.1.1:55850 i0714 15:08:46.505873 27238 slave.cpp:970 ] will retri registr 17.903094m necessari w0714 15:08:46.505991 27238 master.cpp:2904 ] slave slave ( 44 ) @ 127.0.1.1:55850 ( quantal ) allow re-regist alreadi use id ( 20140714-150843-16842879-55850-27216-0 ) w0714 15:08:46.506063 27238 master.cpp:3679 ] slave 20140714-150843-16842879-55850-27216-0 slave ( 44 ) @ 127.0.1.1:55850 ( quantal ) non-termin task 4a6783aa-8d07-46e3-8399-2a5d047f0021 suppos kill . kill ! i0714 15:08:46.506150 27238 slave.cpp:816 ] re-regist master master @ 127.0.1.1:55850 i0714 15:08:46.506186 27238 slave.cpp:1277 ] ask kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.507275 27241 hierarchical_allocator_process.hpp:497 ] slave 20140714-150843-16842879-55850-27216-0 reconnect i0714 15:08:46.508061 28504 exec.cpp:312 ] executor ask kill task '4a6783aa-8d07-46e3-8399-2a5d047f0021' i0714 15:08:46.508117 28504 exec.cpp:321 ] executor : :killtask took 24954n shut send sigterm process tree pid 28509 i0714 15:08:46.512238 27243 process.cpp:1098 ] socket close receiv i0714 15:08:46.512508 27238 slave.cpp:1582 ] updat framework 20140714-150843-16842879-55850-27216-0000 pid scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 i0714 15:08:46.512846 27238 slave.cpp:1590 ] checkpoint framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1 @ 127.0.1.1:55850 ' '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid' i0714 15:08:46.513419 28508 process.cpp:1037 ] socket close receiv kill follow process tree : [ -+- 28509 sh -c sleep 1000 \ -- - 28510 sleep 1000 ] command termin signal termin ( pid : 28509 ) i0714 15:08:46.940232 28506 exec.cpp:524 ] executor send statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.940918 27240 slave.cpp:2086 ] handl statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 executor ( 1 ) @ 127.0.1.1:34669 i0714 15:08:46.940979 27240 slave.cpp:3768 ] termin task 4a6783aa-8d07-46e3-8399-2a5d047f0021 i0714 15:08:46.941603 27240 status_update_manager.cpp:320 ] receiv statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.941644 27240 status_update_manager.hpp:342 ] checkpoint updat statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.949417 27236 hierarchical_allocator_process.hpp:686 ] perform alloc 1 slave 63926n i0714 15:08:46.965200 27240 status_update_manager.cpp:373 ] forward statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 master @ 127.0.1.1:55850 i0714 15:08:46.965625 27239 master.cpp:3115 ] statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 slave 20140714-150843-16842879-55850-27216-0 slave ( 44 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:46.965724 27239 master.hpp:791 ] remov task 4a6783aa-8d07-46e3-8399-2a5d047f0021 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:46.965903 27239 sched.cpp:637 ] schedul : :statusupd took 39326n i0714 15:08:46.966022 27239 hierarchical_allocator_process.hpp:635 ] recov cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ( total allocat : cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] ) slave 20140714-150843-16842879-55850-27216-0 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.966120 27239 master.cpp:2639 ] forward statu updat acknowledg e3a5f8fd-eefc-42c6-94a7-086c93c01968 task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 slave 20140714-150843-16842879-55850-27216-0 slave ( 44 ) @ 127.0.1.1:55850 ( quantal ) i0714 15:08:46.966501 27241 slave.cpp:2244 ] statu updat manag success handl statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.966519 27241 slave.cpp:2250 ] send acknowledg statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 executor ( 1 ) @ 127.0.1.1:34669 i0714 15:08:46.966754 27240 status_update_manager.cpp:398 ] receiv statu updat acknowledg ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.966785 27240 status_update_manager.hpp:342 ] checkpoint ack statu updat task_kil ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.967386 28500 exec.cpp:338 ] executor receiv statu updat acknowledg e3a5f8fd-eefc-42c6-94a7-086c93c01968 task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.967562 27243 process.cpp:1098 ] socket close receiv i0714 15:08:46.968147 28508 process.cpp:1037 ] socket close receiv i0714 15:08:46.984608 27240 status_update_manager.cpp:530 ] clean statu updat stream task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.985239 27236 slave.cpp:1672 ] statu updat manag success handl statu updat acknowledg ( uuid : e3a5f8fd-eefc-42c6-94a7-086c93c01968 ) task 4a6783aa-8d07-46e3-8399-2a5d047f0021 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:46.985280 27236 slave.cpp:3810 ] complet task 4a6783aa-8d07-46e3-8399-2a5d047f0021 i0714 15:08:47.940703 27243 process.cpp:1037 ] socket close receiv i0714 15:08:47.940984 27238 containerizer.cpp:1019 ] executor contain '19c466f8-bb5a-4842-a152-f585ff88762a ' exit i0714 15:08:47.941007 27238 containerizer.cpp:903 ] destroy contain '19c466f8-bb5a-4842-a152-f585ff88762a' i0714 15:08:47.950192 27241 hierarchical_allocator_process.hpp:750 ] offer cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:47.950405 27241 hierarchical_allocator_process.hpp:686 ] perform alloc 1 slave 320604n i0714 15:08:47.950518 27241 master.hpp:801 ] ad offer 20140714-150843-16842879-55850-27216-1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:47.950572 27241 master.cpp:3454 ] send 1 offer framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:47.950774 27241 sched.cpp:546 ] schedul : :resourceoff took 37944n i0714 15:08:47.951179 27216 master.cpp:625 ] master termin i0714 15:08:47.951263 27216 master.hpp:811 ] remov offer 20140714-150843-16842879-55850-27216-1 resourc cpu ( * ) :2 ; mem ( * ) :1024 ; disk ( * ) :1024 ; port ( * ) : [ 31000-32000 ] slave 20140714-150843-16842879-55850-27216-0 ( quantal ) i0714 15:08:47.953447 27242 sched.cpp:747 ] stop framework '20140714-150843-16842879-55850-27216-0000' i0714 15:08:47.953547 27242 slave.cpp:2330 ] master @ 127.0.1.1:55850 exit w0714 15:08:47.953567 27242 slave.cpp:2333 ] master disconnect ! wait new master elect i0714 15:08:47.964512 27238 slave.cpp:2526 ] executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 exit statu 0 i0714 15:08:47.968690 27238 slave.cpp:2660 ] clean executor '4a6783aa-8d07-46e3-8399-2a5d047f0021 ' framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:47.969348 27236 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a ' gc 6.99998878298667day futur i0714 15:08:47.969751 27241 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021 ' gc 6.99998877682963day futur i0714 15:08:47.970082 27239 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a ' gc 6.99998877336889day futur i0714 15:08:47.970379 27242 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021 ' gc 6.99998876968889day futur i0714 15:08:47.970587 27238 slave.cpp:2735 ] clean framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:47.970960 27237 status_update_manager.cpp:282 ] close statu updat stream framework 20140714-150843-16842879-55850-27216-0000 i0714 15:08:47.971225 27236 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000 ' gc 6.99998875966519day futur i0714 15:08:47.971549 27241 gc.cpp:56 ] schedul '/tmp/slaverecoverytest_0_reconcilekilltask_zl9dut/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000 ' gc 6.99998875612148day futur w0714 15:08:47.975971 27235 containerizer.cpp:893 ] ignor destroy unknown contain : 19c466f8-bb5a-4842-a152-f585ff88762a ./tests/cluster.hpp:530 : failur ( wait ) .failur ( ) : unknown contain : 19c466f8-bb5a-4842-a152-f585ff88762a # # a fatal error detect java runtim environ : # # sigsegv ( 0xb ) pc=0x00000000005a0299 , pid=27216 , tid=47907931709760 # # jre version : openjdk runtim environ ( 7.0_55-b14 ) ( build 1.7.0_55-b14 ) # java vm : openjdk 64-bit server vm ( 24.51-b03 mix mode linux-amd64 compress oop ) # problemat frame : # c [ lt-mesos-tests+0x1a0299 ] mlock @ @ glibc_2.2.5+0x1a0299 # # fail write core dump . core dump disabl . to enabl core dump , tri `` ulimit -c unlimit '' start java # # an error report file inform save : # /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/hs_err_pid27216.log # # if would like submit bug report , pleas includ # instruct reproduc bug visit : # http : //icedtea.classpath.org/bugzilla # the crash happen outsid java virtual machin nativ code . # see problemat frame report bug . # make [ 3 ] : * * * [ check-loc ] abort make [ 3 ] : leav directori ` /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make [ 2 ] : * * * [ check-am ] error 2 make [ 2 ] : leav directori ` /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make [ 1 ] : * * * [ check ] error 2 make [ 1 ] : leav directori ` /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make : * * * [ check-recurs ] error 1 build step 'execut shell ' mark build failur erreicht : 1854109 send e-mail : kernel-test @ twitter.com apache-meso @ twitter.com finish : failur help us local page page gener : jul 14 , 2014 5:57:17 pmrest { code }",MESOS-1594,1.0
improv framework rate limit impos max number outstand messag per framework princip * rate limit config take configur * capac * princip . * to ensur master maintain messag order framework 's import master send frameworkerrormessag back schedul ask abort .,MESOS-1578,5.0
race executor exit event launch task caus overcommit resourc the follow sequenc event caus overcommit -- > launch task call task whose executor alreadi run -- > executor 's resourc account master -- > executor exit event enqueu behind launch task master -- > master send task slave need commit resourc task ( new ) executor . -- > master process executor exit event re-off executor 's resourc caus overcommit resourc .,MESOS-1466,8.0
"slaverecoverytest/0.multipleframework flaki -- gtest_repeat=-1 -- gtest_shuffl -- gtest_break_on_failur { noformat } [ run ] slaverecoverytest/0.multipleframework warn : log initgooglelog ( ) written stderr i0513 15:42:05.931761 4320 exec.cpp:131 ] version : 0.19.0 i0513 15:42:05.936698 4340 exec.cpp:205 ] executor regist slave 20140513-154204-16842879-51872-13062-0 regist executor artoo start task 51991f97-f5fd-4905-ad0f-02668083af7c fork command 4367 sh -c 'sleep 1000' warn : log initgooglelog ( ) written stderr i0513 15:42:06.915061 4408 exec.cpp:131 ] version : 0.19.0 i0513 15:42:06.931149 4435 exec.cpp:205 ] executor regist slave 20140513-154204-16842879-51872-13062-0 regist executor artoo start task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83 sh -c 'sleep 1000' fork command 4439 i0513 15:42:06.998332 4340 exec.cpp:251 ] receiv reconnect request slave 20140513-154204-16842879-51872-13062-0 i0513 15:42:06.998414 4436 exec.cpp:251 ] receiv reconnect request slave 20140513-154204-16842879-51872-13062-0 i0513 15:42:07.006350 4437 exec.cpp:228 ] executor re-regist slave 20140513-154204-16842879-51872-13062-0 re-regist executor artoo i0513 15:42:07.027039 4337 exec.cpp:378 ] executor ask shutdown shut send sigterm process tree pid 4367 kill follow process tree : [ -+- 4367 sh -c sleep 1000 \ -- - 4368 sleep 1000 ] .. / .. /src/tests/slave_recovery_tests.cpp:2807 : failur valu : status1.get ( ) .state ( ) actual : task_fail expect : task_kil program receiv signal sigsegv , segment fault . test : :unittest : :addtestpartresult ( this=0x154dac0 < test : :unittest : :getinst ( ) : :instanc > , result_type=test : :testpartresult : :kfatalfailur , file_name=0xeb6b6c `` .. / .. /src/tests/slave_recovery_tests.cpp '' , line_number=2807 , message= ... , os_stack_trace= ... ) gmock-1.6.0/gtest/src/gtest.cc:3795 3795 * static_cast < volatil int * > ( null ) = 1 ; ( gdb ) bt # 0 test : :unittest : :addtestpartresult ( this=0x154dac0 < test : :unittest : :getinst ( ) : :instanc > , result_type=test : :testpartresult : :kfatalfailur , file_name=0xeb6b6c `` .. / .. /src/tests/slave_recovery_tests.cpp '' , line_number=2807 , message= ... , os_stack_trace= ... ) gmock-1.6.0/gtest/src/gtest.cc:3795 # 1 0x0000000000df98b9 test : :intern : :asserthelp : :operator= ( this=0x7fffffffb860 , message= ... ) gmock-1.6.0/gtest/src/gtest.cc:356 # 2 0x0000000000cdfa57 slaverecoverytest_multipleframeworks_test < meso : :intern : :slave : :mesoscontainer > : :testbodi ( this=0x1954db0 ) .. / .. /src/tests/slave_recovery_tests.cpp:2807 # 3 0x0000000000e22583 test : :intern : :handlesehexceptionsinmethodifsupport < test : :test , void > ( object=0x1954db0 , method= & virtual test : :test : :testbodi ( ) , location=0xed0af0 `` test bodi '' ) gmock-1.6.0/gtest/src/gtest.cc:2090 # 4 0x0000000000e12467 test : :intern : :handleexceptionsinmethodifsupport < test : :test , void > ( object=0x1954db0 , method= & virtual test : :test : :testbodi ( ) , location=0xed0af0 `` test bodi '' ) gmock-1.6.0/gtest/src/gtest.cc:2126 # 5 0x0000000000e010d5 test : :test : :run ( this=0x1954db0 ) gmock-1.6.0/gtest/src/gtest.cc:2161 # 6 0x0000000000e01ceb test : :testinfo : :run ( this=0x158cf80 ) gmock-1.6.0/gtest/src/gtest.cc:2338 # 7 0x0000000000e02387 test : :testcas : :run ( this=0x158a880 ) gmock-1.6.0/gtest/src/gtest.cc:2445 # 8 0x0000000000e079 test : :intern : :unittestimpl : :runalltest ( this=0x1558b40 ) gmock-1.6.0/gtest/src/gtest.cc:4237 # 9 0x0000000000e1ec83 test : :intern : :handlesehexceptionsinmethodifsupport < test : :intern : :unittestimpl , bool > ( object=0x1558b40 , method= ( bool ( test : :intern : :unittestimpl : : * ) ( test : :intern : :unittestimpl * const ) ) 0xe07700 < test : :intern : :unittestimpl : :runalltest ( ) > , location=0xed1219 `` auxiliari test code ( environ event listen ) '' ) gmock-1.6.0/gtest/src/gtest.cc:2090 # 10 0x0000000000e14217 test : :intern : :handleexceptionsinmethodifsupport < test : :intern : :unittestimpl , bool > ( object=0x1558b40 , method= ( bool ( test : :intern : :unittestimpl : : * ) ( test : :intern : :unittestimpl * const ) ) 0xe07700 < test : :intern : :unittestimpl : :runalltest ( ) > , location=0xed1219 `` auxiliari test code ( environ event listen ) '' ) gmock-1.6.0/gtest/src/gtest.cc:2126 # 11 0x0000000000e076d7 test : :unittest : :run ( this=0x154dac0 < test : :unittest : :getinst ( ) : :instanc > ) gmock-1.6.0/gtest/src/gtest.cc:3872 # 12 0x0000000000b99887 main ( argc=1 , argv=0x7fffffffd9f8 ) .. / .. /src/tests/main.cpp:107 ( gdb ) frame 2 # 2 0x0000000000cdfa57 slaverecoverytest_multipleframeworks_test < meso : :intern : :slave : :mesoscontainer > : :testbodi ( this=0x1954db0 ) .. / .. /src/tests/slave_recovery_tests.cpp:2807 2807 assert_eq ( task_kil , status1.get ( ) .state ( ) ) ; ( gdb ) p status1 $ 1 = { data = { < std : :__shared_ptr < process : :futur < meso : :taskstatu > : :data , 2 > > = { _m_ptr = 0x1963140 , _m_refcount = { _m_pi = 0x198a620 } } , < no data field > } } ( gdb ) p status1.get ( ) $ 2 = ( const meso : :taskstatu & ) @ 0x7fffdc5bf5f0 : { < googl : :protobuf : :messag > = { < googl : :protobuf : :messagelit > = { _vptr $ messagelit = 0x7ffff74bc940 < vtabl meso : :taskstatus+16 > } , < no data field > } , static ktaskidfieldnumb = 1 , static kstatefieldnumb = 2 , static kmessagefieldnumb = 4 , static kdatafieldnumb = 3 , static kslaveidfieldnumb = 5 , static ktimestampfieldnumb = 6 , _unknown_fields_ = { fields_ = 0x0 } , task_id_ = 0x7fffdc5ce9a0 , message_ = 0x7fffdc5f5880 , data_ = 0x154b4b0 < googl : :protobuf : :intern : :kemptystr > , slave_id_ = 0x7fffdc59c4f0 , timestamp_ = 1429688582.046252 , state_ = 3 , _cached_size_ = 0 , _has_bits_ = { 55 } , static default_instance_ = 0x0 } ( gdb ) p status1.get ( ) .state ( ) $ 3 = meso : :task_fail ( gdb ) list 2802 // kill task 1 . 2803 driver1.killtask ( task1.task_id ( ) ) ; 2804 2805 // wait task_kil updat . 2806 await_readi ( status1 ) ; 2807 assert_eq ( task_kil , status1.get ( ) .state ( ) ) ; 2808 2809 // kill task 2 . 2810 driver2.killtask ( task2.task_id ( ) ) ; 2811 { noformat }",MESOS-1365,1.0
"systemd.slic + cgroup enabl fail multipl way . when attempt configur meso use systemd slice 'rawhide/f21 ' machin , fail creat isol : i0407 12:39:28.035354 14916 containerizer.cpp:180 ] use isol : cgroups/cpu , cgroups/mem fail creat container : could creat isol cgroups/cpu : fail creat isol : the cpu subsystem co-mount /sys/fs/cgroup/cpu subsytem -- -- -- detail -- -- -- /sys/fs/cgroup total 0 drwxr-xr-x . 12 root root 280 mar 18 08:47 . drwxr-xr-x . 6 root root 0 mar 18 08:47 .. drwxr-xr-x . 2 root root 0 mar 18 08:47 blkio lrwxrwxrwx . 1 root root 11 mar 18 08:47 cpu - > cpu , cpuacct lrwxrwxrwx . 1 root root 11 mar 18 08:47 cpuacct - > cpu , cpuacct drwxr-xr-x . 2 root root 0 mar 18 08:47 cpu , cpuacct drwxr-xr-x . 2 root root 0 mar 18 08:47 cpuset drwxr-xr-x . 2 root root 0 mar 18 08:47 devic drwxr-xr-x . 2 root root 0 mar 18 08:47 freezer drwxr-xr-x . 2 root root 0 mar 18 08:47 hugetlb drwxr-xr-x . 3 root root 0 apr 3 11:26 memori drwxr-xr-x . 2 root root 0 mar 18 08:47 net_cl drwxr-xr-x . 2 root root 0 mar 18 08:47 perf_ev drwxr-xr-x . 4 root root 0 mar 18 08:47 systemd",MESOS-1195,3.0
"alloc make alloc decis per slave instead per framework/rol . current alloc : :alloc ( ) code loop role framework ( base drf sort ) alloc * * slave resourc first framework . thi logic bit invers . instead , slave go slave , alloc role/framework updat drf share .",MESOS-1119,2.0
updat semant framework regist ( ) /reregist ( ) get call current semant : 1 ) framework connect w/ master first time -- > regist ( ) 2 ) framework reconnect w/ master zk blip -- > reregist ( ) 3 ) framework reconnect w/ fail master -- > regist ( ) 4 ) fail framework connect w/ master -- > regist ( ) 5 ) fail framework connect w/ fail master -- > regist ( ) updat semant : everyth except 3 ) framework reconnect w/ fail master -- > reregist ( ),MESOS-786,3.0
[ front-end ] : minor issu connect local * 1 . * * precondit : * 1 . bill avail datalab web ui * step reproduc : * 1 . go 'list resourc ' page * actual result : * 1 . bill 'list resourc ' page local * expect result : * 1 . bill 'list resourc ' page local -- -- * 2 . * convert date period accord select languag 'bill report ' page -- -- * 3 . ( - ) * convey languag key back-end bill export ( branch back-end fix ) - done task 2089 .,DATALAB-2087,4.0
[ environ manag page ] : drop list valu broken * precondit : * 1 . user locat 'environ manag ' page * step reproduc : * # expand filter header # click 'project'/'endpoint ' drop list * actual result : * # valu dropdown list broken # a part dropdown list name visibl right side * expect : * # valu dropdown list broken # a part dropdown list name visibl right side,DATALAB-2073,8.0
librari instal due permiss absenc directori * precondit : * # dlab deploy k8 # notebook creat * step reproduc : * # go 'resources_list ' page # click action menu notebook # choos 'manag libraries' # choos avail resourc 'select resourc ' drop list # select apt/yum/python/oth group # choos avail librari # click instal * actual result : * # docker run ' 0' # librari instal fail * expect result : * # docker run ' 0' # librari instal success for java r packag group i could check issu side .,DATALAB-1250,4.0
"provis servic access respons file * precondit : * 1 . notebook creat * step reproduc : * 1 . go 'resources_list ' page 2 . click action menu notebook 3 . choos 'manag libraries' 4 . choos avail resourc 'select resourc ' drop list * actual result : * 1 . avail lib list get stuck ui 2 . user abl instal librari * expect result : * 1 . user abl instal librari could find file find dlab-us , json avail root .",DATALAB-1240,8.0
[ schedul ] : 'default timezone_offset ' rewritten switch schedul type notebook * precondit : * 1 . notebook creat ( 'run ' 'stop ' status ) * step reproduc : * # turn schedul inact notebook # click 'save ' button # turn schedul time * actual result : * 1 . valu 'select offset ' absent ( ' z ' f12 ) * expect result : * 1 . valu 'select offset ' default valu browser,DATALAB-1101,8.0
"[ terraform ] : it imposs git action via ungit * precondit : * # user locat ungit page # git account creat dlab * step reproduc : * # put < /home/dlab-us > ungit search # past http path git repositori ungit # click 'clone ' button * actual result : * # for everi git action usernam password requir ungit # requir credenti frequent , imposs git action * expect result : * # git action done without credenti requir ungit previous git credenti store notebook < .netrc > , file absent",DATALAB-1041,8.0
"preemptorservic failur trigger shutdown while observ aurora-1510 product i notic bug caus { { preemptorservic } } transit fail state . the { { /servic } } endpoint : { noformat } { name : `` preemptorservic '' , state : `` fail '' , failurecaus : `` java.util.concurrentmodificationexcept '' } , { noformat } howev schedul continu run . i believ bug .",AURORA-1511,2.0
"in-progress instanc updat page continu puls updat abort when updat abort , instanc progress still css class { { instance-upd } } , continu puls .",AURORA-1445,3.0
"remov duplic thermos_observ there current two thermos_observ python_binari codebas , one src/main/python/apache/aurora/tools/build ./src/main/python/apache/thermos/observer/bin/build . thi confus . let 's get rid one , i think latter one 's meant use .",AURORA-1381,2.0
referenti integr violat replay storag at startup vagrant environ i observ : { noformat } e0630 20:45:54.427 thread1 org.apache.aurora.scheduler.schedulerlifecycl $ 9.execut : caught uncheck except : org.apache.aurora.scheduler.storage.storag $ storageexc ption : # # # error updat databas . caus : org.h2.jdbc.jdbcsqlexcept : referenti integr constraint violat : `` constraint_4b : public.task foreign key ( slave_row_id ) r efer public.host_attribut ( id ) ( 1 ) '' ; sql statement : delet from host_attribut where host = ? [ 23503-187 ] # # # the error may involv defaultparametermap # # # the error occur set paramet # # # sql : delet from host_attribut where host = ? # # # caus : org.h2.jdbc.jdbcsqlexcept : referenti integr constraint violat : `` constraint_4b : public.task foreign key ( slave_row_id ) refer public.host_attr ibut ( id ) ( 1 ) '' ; sql statement : delet from host_attribut where host = ? [ 23503-187 ] org.apache.aurora.scheduler.storage.storag $ storageexcept : # # # error updat databas . caus : org.h2.jdbc.jdbcsqlexcept : referenti integr constraint violat : `` constraint_4b : public.task foreign key ( slave_row_id ) r efer public.host_attribut ( id ) ( 1 ) '' ; sql statement : delet from host_attribut where host = ? [ 23503-187 ] # # # the error may involv defaultparametermap # # # the error occur set paramet # # # sql : delet from host_attribut where host = ? # # # caus : org.h2.jdbc.jdbcsqlexcept : referenti integr constraint violat : `` constraint_4b : public.task foreign key ( slave_row_id ) refer public.host_attr ibut ( id ) ( 1 ) '' ; sql statement : delet from host_attribut where host = ? [ 23503-187 ] org.apache.aurora.scheduler.storage.db.dbstorage.writ ( dbstorage.java:144 ) org.mybatis.guice.transactional.transactionalmethodinterceptor.invok ( transactionalmethodinterceptor.java:101 ) com.twitter.common.inject.timedinterceptor.invok ( timedinterceptor.java:87 ) org.apache.aurora.scheduler.storage.log.logstorage.writ ( logstorage.java:632 ) org.apache.aurora.scheduler.storage.log.logstorag $ 3.execut ( logstorage.java:316 ) org.apache.aurora.scheduler.storage.log.logstorag $ 3.execut ( logstorage.java:313 ) org.apache.aurora.scheduler.storage.log.logstorage.replay ( logstorage.java:525 ) org.apache.aurora.scheduler.storage.log.logstorage.access $ 1200 ( logstorage.java:116 ) org.apache.aurora.scheduler.storage.log.logstorag $ 21 $ 1.execut ( logstorage.java:503 ) org.apache.aurora.scheduler.storage.log.logstorag $ 21 $ 1.execut ( logstorage.java:500 ) org.apache.aurora.scheduler.storage.log.streammanagerimpl.readfrombegin ( streammanagerimpl.java:127 ) org.apache.aurora.scheduler.storage.log.logstorag $ 21.execut ( logstorage.java:500 ) org.apache.aurora.scheduler.storage.storag $ mutatework $ noresult.appli ( storage.java:131 ) org.apache.aurora.scheduler.storage.db.dbstorage.bulkload ( dbstorage.java:165 ) com.twitter.common.inject.timedinterceptor.invok ( timedinterceptor.java:87 ) org.apache.aurora.scheduler.storage.log.logstorage.recov ( logstorage.java:496 ) com.twitter.common.inject.timedinterceptor.invok ( timedinterceptor.java:87 ) org.apache.aurora.scheduler.storage.log.logstorag $ 20.execut ( logstorage.java:477 ) org.apache.aurora.scheduler.storage.storag $ mutatework $ noresult.appli ( storage.java:131 ) org.apache.aurora.scheduler.storage.storag $ mutatework $ noresult $ quiet.appli ( storage.java:148 ) org.apache.aurora.scheduler.storage.db.dbstorage.writ ( dbstorage.java:142 ) org.mybatis.guice.transactional.transactionalmethodinterceptor.invok ( transactionalmethodinterceptor.java:101 ) com.twitter.common.inject.timedinterceptor.invok ( timedinterceptor.java:87 ) org.apache.aurora.scheduler.storage.log.logstorage.writ ( logstorage.java:632 ) org.apache.aurora.scheduler.storage.log.logstorage.start ( logstorage.java:471 ) org.apache.aurora.scheduler.storage.callorderenforcingstorage.start ( callorderenforcingstorage.java:92 ) org.apache.aurora.scheduler.schedulerlifecycl $ 6.execut ( schedulerlifecycle.java:252 ) { noformat },AURORA-1379,2.0
updat statu page larg job kill chrome when i say `` kill chrome '' i mean left open long enough page turn chrome `` aw snap '' page . thi presum due continu addit instanc event page caus oom chrome kill . one suggest made familiar angular add { { track } } { { ng-repeat } } use build dom instanc event .,AURORA-1345,5.0
"auth_modul instal child injector the recent patch enabl http basic authent move construct thriftauthmodul jetti child injector , user-suppli modul still instal parent injector . thi lead stack trace anyon use { { auth_modul } } : { noformat } info : connect master use authent ( princip : twitterschedul ) . except thread `` main '' com.google.inject.creationexcept : guic creation error : 1 ) no implement java.util.map < org.apache.aurora.auth.capabilityvalid $ capabl , java.lang.str > bound . locat java.util.map < org.apache.aurora.auth.capabilityvalid $ capabl , java.lang.str > paramet 1 com.twitter.aurora.internal.auth.twittercapabilityvalidator. < init > ( twittercapabilityvalidator.java:28 ) com.twitter.aurora.internal.auth.twitterauthmodule.configur ( twitterauthmodule.java:97 ) 2 ) no implement java.util.map < org.apache.aurora.auth.capabilityvalid $ capabl , java.lang.str > bound . com.twitter.aurora.internal.auth.twitterauthmodule.configur ( twitterauthmodule.java:94 ) 2 error com.google.inject.internal.errors.throwcreationexceptioniferrorsexist ( errors.java:435 ) com.google.inject.internal.internalinjectorcreator.initializestat ( internalinjectorcreator.java:154 ) com.google.inject.internal.internalinjectorcreator.build ( internalinjectorcreator.java:106 ) com.google.inject.guice.createinjector ( guice.java:95 ) com.google.inject.guice.createinjector ( guice.java:83 ) com.twitter.common.application.applauncher.configureinject ( applauncher.java:120 ) com.twitter.common.application.applauncher.run ( applauncher.java:87 ) com.twitter.common.application.applauncher.launch ( applauncher.java:181 ) com.twitter.common.application.applauncher.launch ( applauncher.java:142 ) org.apache.aurora.scheduler.app.schedulermain.main ( schedulermain.java:279 ) { noformat }",AURORA-1201,2.0
"the schedul synchron write backup write snapshot replic log in cours write snapshot replic log , schedul may block write snapshot disk . there need activ done synchron , caus write lock unnecessarili held addit period time . from storagebackup.java : { code } @ overrid public snapshot createsnapshot ( ) { snapshot snapshot = delegate.createsnapshot ( ) ; ( clock.nowmilli ( ) > = ( lastbackupm + backupintervalm ) ) { save ( snapshot ) ; } return snapshot ; } { code } { { storagebackup } } happen unqualifi bind { { snapshotstor < snapshot > } } use { { logstorag } } .",AURORA-1108,3.0
remov `` enable_legacy_constraint '' flag . as part aurora-184 ad flag call `` enable_legacy_constraint '' enabl behaviour inject host rack limit everi job . we deprec remov flag futur releas .,AURORA-1074,2.0
