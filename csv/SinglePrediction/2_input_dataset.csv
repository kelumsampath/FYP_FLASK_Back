Summary,Description,Number of developers,Number of comments,Issue key,Story point
ewererwf dsf,dfds  seff wefgf aeada,2,1,12,
ewererwf dsf,dfds  seff wefgf aeada,2,1,12,
Occational MIsses on Indexing into Elasticsearch,Occasionally applications still aren't being indexed into elasticsearch. Need to investigate why these might be occuring. ,0,0,USERGRID-1027,8.0
Re-index using AmazonAsyncEventService should support updated date,"see implemented example for InMemoryAsyncEventService

org/apache/usergrid/corepersistence/asyncevents/EventBuilderImpl.java:166

This filters out based on modified/updated date for the events to be indexed.  Need to ensure AmazonAsyncEventService consumers implement something similar OR look to have the producer filter out the events and not even drop on the AWS queues.",1,1,USERGRID-900,3.0
Fix and prune Usergrid tools in 2.1,"The Usergrid tools are broken in a variety of ways in the two-dot-o and two-dot-o-dev brands. The tools do not compile. Many rely on outdated 1.0 concepts. Many are obsolete.

1. Delete any obsolete tools
2. Make all tools compile",1,2,USERGRID-872,3.0
Notifications POST throwing NPE,"{code}curl -X POST -d '{ ""payloads"": { ""apple-dev"": ""Hello World!"" } }' 'https://api.usergrid.com/brandon.apigee/baas-integration-tests/devices/*/notifications?client_id=redacted&client_secret=redacted'{code}

throws:

{code}{
  ""error"": ""runtime"",
  ""timestamp"": 1436981233645,
  ""duration"": 0,
  ""exception"": ""java.lang.RuntimeException"",
  ""error_description"": ""java.lang.NullPointerException""
}{code}

1) According to docs, it should work: http://apigee.com/docs/app-services/content/creating-and-managing-notifications

2) If it doesn't and isn't supposed to, it shouldn't throw an NPE.

Note that a notifier with a sandbox cert has been created with the name 'apple-dev'.

---
UPDATE

It does work with {code}/devices;ql=select */notifications{code}

we should address this but also update the documentation to reflect the way in which it works.

---
UPDATE

It also appears that although the NPE is returned via the API, a notification *is* created in /notifications. In this particular test, there were no devices in the /devices collection:

{code}{
  ""uuid"": ""11c5e62a-2a84-11e5-af28-65b8f0f35f5f"",
  ""type"": ""notifications"",
  ""created"": 1436918242690,
  ""modified"": 1436918242690,
  ""payloads"": {
    ""apple-dev"": ""Hello World!""
  },
  ""debug"": false,
  ""state"": ""CREATED"",
  ""metadata"": {
    ""path"": ""/notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f"",
    ""collections"": {
      ""receipts"": ""/notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f/receipts""
    }
  }
}{code}",1,3,USERGRID-858,2.0
SNS<->SQS Subscriptions Not Created cross-region,,1,1,USERGRID-798,0.0
Org (and presumably App) credentials can't create connections by default,"If you try to create a new connection using org or app credentials, it will fail with:

{code}
{
    ""duration"": 1,
    ""error"": ""unauthorized"",
    ""error_description"": ""Subject does not have permission [applications:post:uuid:/col1/uuid/relationship/uid]"",
    ""exception"": ""org.apache.shiro.authz.UnauthorizedException"",
    ""timestamp"": ""....""
}
{code}

The only workaround seems to be to give {code}POST /**{code} to the Guest role.

Expected behavior: app and org credentials should (out of the box) both be able to create and delete connections, as well as retrieve all depths of them:

{code}GET /collection/uuid/verb
GET /collection/uuid/connecting/verb
GET /collection/uuid/connections/verb{code}
etc.

In short, all tests in the URAP integration_tests should pass on a net new instance without having to set Guest role permissions to {code}/**{code}",0,1,USERGRID-797,3.0
"'order by created desc' causing no results in location query (affects 1.0, not sure about 2.0)","Making the following API call returns no results:

{code}https://api.usergrid.com/org/app/collection?ql=location%20within%201000%20of%2051.73213%2C%20-1.20631%20order%20by%20created%20desc{code}

but this one works:

{code}https://api.usergrid.com/org/app/collection?ql=location%20within%201000%20of%2051.73213%2C%20-1.20631{code}

Definitely affects 1.0, not tested against 2.x",0,0,USERGRID-787,3.0
Limit on Queries results in a highly variable number of results,"This behavior is observed on a build from the two-dot-o branch with commit ID 2d1c8b8ac7b20b63a11d83adca56839d8b409cca.

For example, limit=2 gives you 1 sometimes, limit=750 gives anywhere from 625 to 749.  For example this script:

{code}
#!/bin/bash
for count in `seq 1 10`
do 
    curl -s ""https://example.com/appservices/testorg/sandbox/scmocks?limit=750"" > file${count}
    grep uuid file${count} | wc
    rm file${count}
done
{code}

Produces these results:

     685    2055   36305
     750    2250   39750
     749    2247   39697
     742    2226   39326
     750    2250   39750
     749    2247   39697
     747    2241   39591
     744    2232   39432
     750    2250   39750
     749    2247   39697

A different count every time.
",1,3,USERGRID-778,3.0
Refactor App Info Migration to make it functional,,1,0,USERGRID-777,3.0
Queries without 'ql' return a Null Pointer instead of a meaningful message,"http://api.usergrid.com/vta/sandbox/stops?select * where location within 3 of 37.415449732, -121.920367767  - this will return an 500/NPE when it should be a 4xx, and preferably have a better error message.",0,0,USERGRID-776,3.0
Exception being masked in ClientCredentialsSecurityFilter,"An exception is being masked and causing issues in a deployment.  Here is the class and line.  We should be more specific on what exceptions we catch and at least log them.

https://github.com/apache/incubator-usergrid/blob/49ae4ac5b8d5d77e90e6e6c6e9d8b299a5423863/stack/rest/src/main/java/org/apache/usergrid/rest/security/shiro/filters/ClientCredentialsSecurityFilter.java#L65
",0,0,USERGRID-774,3.0
Column Querying Issues,"From http://community.apigee.com/questions/2432/error-analysis-of-response-when-trying-to-curl-que.html#answer-2454.
I have seen similar issues. I am using the org devnexus and the app 2015, which should be publicly visible.
In the UI, if I set Path to /sessions and Query to select name,title,room where title contains 'Java', I get no response and no error. However, if I capture the query made by the browser (https://api.usergrid.com/devnexus/2015/sessions?ql=select%20name%2Ctitle%2Croom%20where%20title%20contains%20%27Java%27%20order%20by%20created%20desc&access_token=YWMtuzeubNpKEeSm49lERp5XKQAAAUylS2HCr3MkIzCtKZHIYLT1VUiiygN9uUs) using Chrome dev tools, the same query works with curl.
You can see the same result with something as simple as select title.",0,0,USERGRID-773,3.0
Tokens generated with app-level credentials cannot be used successfully in Header,They can be used in the access_token qparam but not a header.  They need to work in the header as well.,1,1,USERGRID-770,0.0
ACL not working as expected?,"When setting following ACL rule /assets/* user has access to /assets/xxx but also to /assets meaning that he can list all the assets.

I expected that when allowing /assets/* it would only allow /assets/xxx",0,0,USERGRID-769,3.0
Bad characters in query causes 500 response; expecting 400?,"If I make the following call:

https://api.usergrid.com/org/app/collection?ql=where%20thirdPropertyTypeInt%20%3E%2030000%20&&%20thirdPropertyTypeInt%20%3C%2040000&limit=10

I get the response:

{code}
{
  ""error"": ""null_pointer"",
  ""timestamp"": 1435176848310,
  ""duration"": 1,
  ""exception"": ""java.lang.NullPointerException""
}
{code}

The cause is that the ampersands in the query string aren't url encoded (should be %26 instead of &). I would expect that if the query string is bad, we should be returning a 400 Bad Request, rather than an ominous NullPointerException/500.",0,0,USERGRID-768,3.0
Portal does not show progress indicator for registration,When registering/signing up the form does not indicate any change in state when the 'Register' button is pressed.,0,0,USERGRID-763,3.0
[FIXED?] NPE in CpEntityManager.validate,"Need to be investigated before 2.1 release.

2015-06-16 22:47:20,361 [http-bio-8080-exec-7] ERROR org.apache.usergrid.corepersistence.CpEntityManager- Unable to load entity saparticles-fixed:e466f39a-cb97-11e4-b30c-f7cc05dca022
java.lang.NullPointerException
	at org.apache.usergrid.corepersistence.CpEntityManager.validate(CpEntityManager.java:956)
	at org.apache.usergrid.corepersistence.CpEntityManager.validate(CpEntityManager.java:942)
	at org.apache.usergrid.corepersistence.CpEntityManager.updateProperties(CpEntityManager.java:1030)
	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:456)
	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:444)
	at org.apache.usergrid.services.AbstractCollectionService.putItemByName(AbstractCollectionService.java:305)
	at org.apache.usergrid.services.AbstractService.invokeItemWithName(AbstractService.java:677)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:628)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:544)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:226)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:193)
	at org.apache.usergrid.rest.applications.ServiceResource.executeServiceRequest(ServiceResource.java:251)
	at org.apache.usergrid.rest.applications.ServiceResource.executePutWithMap(ServiceResource.java:371)
	at org.apache.usergrid.rest.applications.ServiceResource.executePut(ServiceResource.java:420)
	at sun.reflect.GeneratedMethodAccessor167.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)",0,1,USERGRID-756,3.0
Empty Response when using cursors after ~50 pages,,0,3,USERGRID-753,3.0
Test Not Passing: AssetResourceIT,,1,2,USERGRID-745,1.0
"Temp files from asset upload not removed, causing the file system to use all inodes","After running Usergrid 2.0 for over 1 month on a tomcat, we have saturated the inodes of the disk on the tomcat machine.  Our temporary files for uploads are not getting removed.  Files of this structure appear in 

{code}

 ls -lrt  /var/cache/tomcat7/temp


-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME377627531473294092.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME2634951521985073318.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME1066361445384372180.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME3554741621747313255.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME1805372544199883785.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME414525331725926400.tmp
{code}


We need to ensure that all temporary files are removed.  Note that all the files remaining are 0 bytes.  This seems to be caused by an edge case when a 0 byte file is uploaded, or possibly the upload is failing.

",1,4,USERGRID-740,5.0
2.0 does not correctly handle inferred type of Date with optional time,This causes issues if the field is blank.  We need to resolve this for a production installation.,1,0,USERGRID-738,3.0
SNS Queue Manager uses ARN to read instead of URL,There is a bug in org.apache.usergrid.persistence.queue.impl.SNSQueueManagerImpl which results in the ARN of the queue being used when the URL should be used.,1,0,USERGRID-729,1.0
Launcher url points to Apigee,"The launcher contains the url to: http://apigee.github.io/usergrid-portal/. 
This should be changed to reflect the new url.
Do we still host an admin portal on github like Apigee used to? ...  which we can point out local instance to by specifying an api_url argument in the query string?   

Rgds,
Malaka",0,2,USERGRID-728,3.0
Issue with getting started steps on github,"The getting started documents points to some Apigee links and hence don't provide the expected results. This could drive away potential users. Documentation needs to be updated.
link: https://github.com/apache/incubator-usergrid
section: Getting Started with the Admin Portal

Unable to access: 
-  https://github.com/apigee/usergrid-portal
- http://apigee.github.com/usergrid-portal/?api_url=http://localhost:8080

Rgds,
Malaka",0,3,USERGRID-727,3.0
SNSQueueManagerImpl needs better error handling and messaging,"It would be better if this was preceded by a message stating which queue cannot be found.

com.amazonaws.services.sqs.model.QueueDoesNotExistException: The specified queue does not exist for this wsdl version. (Service: AmazonSQS; Status Code: 400; Error Code: AWS.SimpleQueueService.NonExistentQueue; Request ID: 70c94af9-7d46-5d2b-9a7e-07155ddd1ed5)
	at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1160)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:748)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:467)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:302)
	at com.amazonaws.services.sqs.AmazonSQSClient.invoke(AmazonSQSClient.java:2422)
	at com.amazonaws.services.sqs.AmazonSQSClient.receiveMessage(AmazonSQSClient.java:1130)
	at org.apache.usergrid.persistence.queue.impl.SNSQueueManagerImpl.getMessages(SNSQueueManagerImpl.java:234)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService.take(AmazonAsyncEventService.java:153)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService.access$300(AmazonAsyncEventService.java:66)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService$2.call(AmazonAsyncEventService.java:379)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService$2.call(AmazonAsyncEventService.java:366)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable.unsafeSubscribe(Observable.java:7495)
	at rx.internal.operators.OperatorSubscribeOn$1$1.call(OperatorSubscribeOn.java:62)
	at rx.internal.schedulers.ScheduledAction.run(ScheduledAction.java:55)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",1,2,USERGRID-725,1.0
Test Not Passing: DevicesresourceIT ,seems to be a graph compaction issue DevicesResourceIT.putWithUUIDShouldCreateAfterDelete,1,0,USERGRID-704,1.0
Test Not Passing: ManagementResourceIT,"testSuperuserOnlyWhenValidateExternalTokensEnabled
and the other external test are broken.",1,7,USERGRID-703,2.0
Test Not Passing: StaleIndexCleanupTests,"Results :

Failed tests:
  StaleIndexCleanupTest.testCleanupOnUpdate:462 Expect candidates without earlier stale entities expected:<10> but was:<60>

Tests run: 258, Failures: 1, Errors: 0, Skipped: 25",1,1,USERGRID-700,1.0
Test Not Passing:  NotificationsIT.testPaging:90,,1,0,USERGRID-699,2.0
Test Not Passing: ConnectionResourceTest.connectionsLoopbackTest:103 NullPointer,,1,0,USERGRID-698,2.0
Test Not Passing: AdminUsersIT,AdminUsersIT.mgmtUserFeed:190 » UniformInterface GET http://localhost:10006/ma...,1,2,USERGRID-697,2.0
Test Not Passing: RegistrationIT," RegistrationIT.addExistingAdminUserToOrganization:304 » UniformInterface POST ...
  RegistrationIT.addNewAdminUserWithNoPwdToOrganization:245->postAddAdminToOrg:90 » ClientHandler
  RegistrationIT.postAddToOrganization:222->AbstractRestIT.getAdminToken:173 » UniformInterface
  RegistrationIT.postCreateOrgAndAdmin:134->AbstractRestIT.getAdminToken:173 » UniformInterface
  RegistrationIT.putAddToOrganizationFail:195->AbstractRestIT.getAdminToken:173 » UniformInterface",1,3,USERGRID-695,2.0
Test Not Passing: ContentTypeResourceIT,"ContentTypeResourceIT.formEncodedContentType:152 expected:<200> but was:<500>
  ContentTypeResourceIT.noAcceptGet:262 expected:<200> but was:<400>",1,0,USERGRID-693,2.0
Test Not Passing: PermissionsResourceIT,"  PermissionsResourceIT.applicationPermissions:262 expected:<[noca]> but was:<[4peaks]>
  PermissionsResourceIT.deleteUserGroup:157 null",1,0,USERGRID-692,2.0
Test Not Passing: OrganizationsIT,,1,0,USERGRID-690,2.0
Test Not Passing:  ManagementResourceIT,11 tests failing,1,0,USERGRID-689,2.0
Test Not Passing: AndOrQueryTest,"Fix AndOrQueryTest:
queryReturnCheck
queryReturnCheckWithShortHand",1,0,USERGRID-687,1.0
Test Not Passing: ConnectionsServiceIT.testEntityConnections,,1,0,USERGRID-685,3.0
UG 1.0 bug for DB setup with separate locks keyspace,"It seems the keyspace does not get created with /database/setup.

cassandra.system.keyspace=sso_ug10
#cassandra.application.keyspace=Usergrid_Applications
cassandra.application.keyspace=sso_ug10_apps
#cassandra.lock.keyspace=Locks
cassandra.lock.keyspace=sso_ug10_Locks",0,0,USERGRID-684,3.0
App Credentials do not work when accessing a collection within an app (2.0),In order to access collections it seems the org credentials are required as app credentials do not work.  ,1,7,USERGRID-670,5.0
Test Not Passing: RolesServiceIT.deleteRoles,,1,1,USERGRID-664,1.0
Test Not Passing: ConnectionsServiceIT,,1,0,USERGRID-663,1.0
Test Not Passing: AppInfoMigrationPluginTest,,1,0,USERGRID-662,1.0
Test Not Passing: ServiceInvocationIT,,1,0,USERGRID-661,1.0
Test Not Passing: GroupServiceIT - Services,,1,0,USERGRID-660,1.0
Test Not Passing: OrganizationIT Failing,,1,1,USERGRID-659,3.0
Issue with Unique Values - NPE,"Please ensure appropriate null checks and behavior happen here:

2015-05-12 17:05:37,811 [http-bio-8080-exec-14] ERROR org.apache.usergrid.rest.exceptions.AbstractExceptionMapper- java.lang.NullPointerException Server Error (500)
java.lang.NullPointerException
	at org.apache.usergrid.services.AbstractConnectionsService.getItemByName(AbstractConnectionsService.java:238)
	at org.apache.usergrid.services.AbstractService.invokeItemWithName(AbstractService.java:671)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:628)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:544)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:226)
	at org.apache.usergrid.services.ServiceRequest.invokeMultiple(ServiceRequest.java:262)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:229)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:193)
	at org.apache.usergrid.rest.applications.ServiceResource.executeServiceRequest(ServiceResource.java:251)
	at org.apache.usergrid.rest.applications.ServiceResource.executeGet(ServiceResource.java:297)
	at sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)",0,1,USERGRID-647,3.0
Test Not Passing: IteratingQueryIt failing due to bad serializer,"trying to deserialize a markededge into an edge

AbstractCursorSerializer line 47


2015-05-08 14:33:25,926 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205920001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a69ba0a-f5c1-11e4-ab26-47475f9887d5, type='test'}, version=7a69ba63-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,942 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205934001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a6bdcea-f5c1-11e4-8198-83878cd74a93, type='test'}, version=7a6c0456-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,957 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205949001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a6e26da-f5c1-11e4-8f8d-9b115b1298d8, type='test'}, version=7a6e2739-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,970 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205964001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a7070ca-f5c1-11e4-9deb-7d7bf418ca86, type='test'}, version=7a70712c-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,984 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205978001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a7293aa-f5c1-11e4-8cbc-6b63f058386d, type='test'}, version=7a72bb1f-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,998 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205991001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a748f7a-f5c1-11e4-b755-cfcc9cf90313, type='test'}, version=7a748fe2-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,013 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117206006001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a76d96a-f5c1-11e4-b6ba-5103774b6f21, type='test'}, version=7a7700e5-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,027 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117206021001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a79235a-f5c1-11e4-a640-1f069a6b43c9, type='test'}, version=7a7923c8-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,112 INFO (main) IndexRefreshCommandImpl - found record during refresh uuid: 7a7c0980-f5c1-11e4-b2bd-5994708e0639 took ms:75 
2015-05-08 14:33:26,112 INFO (main) IteratingQueryIT - Writes took 571 ms
Disconnected from the target VM, address: '127.0.0.1:49588', transport: 'socket'
2015-05-08 14:34:59,660 INFO (main) CoreApplication - Test allInConnectionNoType(org.apache.usergrid.persistence.query.IteratingQueryIT): finish with application

org.apache.usergrid.corepersistence.pipeline.cursor.CursorParseException: Unable to deserialize value
	at org.apache.usergrid.corepersistence.pipeline.cursor.AbstractCursorSerializer.fromJsonNode(AbstractCursorSerializer.java:51)
	at org.apache.usergrid.corepersistence.pipeline.cursor.RequestCursor.getCursor(RequestCursor.java:75)
	at org.apache.usergrid.corepersistence.pipeline.PipelineContext.getCursor(PipelineContext.java:68)
	at org.apache.usergrid.corepersistence.pipeline.read.AbstractPathFilter.getSeekValue(AbstractPathFilter.java:50)
	at org.apache.usergrid.corepersistence.pipeline.read.graph.AbstractReadGraphFilter.lambda$call$2(AbstractReadGraphFilter.java:73)
	at org.apache.usergrid.corepersistence.pipeline.read.graph.AbstractReadGraphFilter$$Lambda$100/1957269967.call(Unknown Source)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:55)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:55)
	at rx.internal.util.ScalarSynchronousObservable$1.call(ScalarSynchronousObservable.java:43)
	at rx.internal.util.ScalarSynchronousObservable$1.call(ScalarSynchronousObservable.java:32)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable.subscribe(Observable.java:7585)
	at rx.internal.operators.BlockingOperatorToIterator.toIterator(BlockingOperatorToIterator.java:53)
	at rx.observables.BlockingObservable.getIterator(BlockingObservable.java:156)
	at org.apache.usergrid.corepersistence.results.ObservableQueryExecutor.hasNext(ObservableQueryExecutor.java:114)
	at org.apache.usergrid.corepersistence.results.ObservableQueryExecutor.next(ObservableQueryExecutor.java:124)
	at org.apache.usergrid.corepersistence.CpRelationManager.searchConnectedEntities(CpRelationManager.java:948)
	at org.apache.usergrid.corepersistence.CpEntityManager.searchConnectedEntities(CpEntityManager.java:1546)
	at org.apache.usergrid.persistence.query.IteratingQueryIT$ConnectionNoTypeHelper.getResults(IteratingQueryIT.java:278)
	at org.apache.usergrid.persistence.query.IteratingQueryIT.allIn(IteratingQueryIT.java:1130)
	at org.apache.usergrid.persistence.query.IteratingQueryIT.allInConnectionNoType(IteratingQueryIT.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
Caused by: com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""deleted"" (class org.apache.usergrid.persistence.graph.impl.SimpleEdge), not marked as ignorable (4 known properties: ""type"", ""targetNode"", ""sourceNode"", ""timestamp""])
 at [Source: N/A; line: -1, column: -1] (through reference chain: org.apache.usergrid.persistence.graph.impl.SimpleEdge[""deleted""])
	at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:51)
	at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:671)
	at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:773)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1297)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1275)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:247)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:118)
	at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:2965)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:1587)
	at com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:1931)
	at org.apache.usergrid.corepersistence.pipeline.cursor.AbstractCursorSerializer.fromJsonNode(AbstractCursorSerializer.java:48)
	... 74 more
Caused by: rx.exceptions.OnErrorThrowable$OnNextValue: OnError while emitting onNext value: org.apache.usergrid.corepersistence.pipeline.read.FilterResult.class
	at rx.exceptions.OnErrorThrowable.addValueAsLastCause(OnErrorThrowable.java:101)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:58)
	... 68 more",1,0,USERGRID-644,2.0
Test Not Passing: StaleIndexCleanup Failing due to old version getting returned from search,"java.lang.AssertionError: 
Expected :e2283e4d-f504-11e4-b4ae-324ce75ff58b
Actual   :e2150469-f504-11e4-b4ae-324ce75ff58b
 <Click to see difference>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.usergrid.corepersistence.StaleIndexCleanupTest.testUpdateVersionMaxFirst(StaleIndexCleanupTest.java:187)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
",1,0,USERGRID-642,3.0
Entity does not return from collection when ql=select *,"Entity exists at /{org}/{app}/{collection}/{name} but is not returned from /{org}/{app}/{collection}?ql=select * 

For reference: https://apigeesc.atlassian.net/browse/APIBAAS-1560",0,0,USERGRID-639,3.0
Test Not Passing: NotSubPropertyIT,,1,0,USERGRID-637,2.0
Test Not Passing: IntersectionUnionPagingIT,,1,0,USERGRID-636,2.0
Test Not Passing: IntersectionTransitivePagingIT,,1,0,USERGRID-635,2.0
Test Not Passing: EntityManagerFactoryImplIT,,1,1,USERGRID-634,1.0
Test Not Passing: PermissionsIT,,1,1,USERGRID-633,0.0
Test Not Passing: PerformanceEntityRebuildIndexTest,,1,2,USERGRID-632,1.0
Test Not Passing: PathQueryIT,,1,0,USERGRID-631,2.0
Test Not Passing: IndexIT,,1,0,USERGRID-630,2.0
Test Not Passing: GeoQueryBooleanTest,,1,0,USERGRID-629,2.0
Test Not Passing: GeoIT,,1,4,USERGRID-628,1.0
Test Not Passing: EntityManagerIT,,1,1,USERGRID-627,0.0
Test Not Passing: EntityDictionaryIT,,1,0,USERGRID-626,2.0
Test Not Passing: CountingMutatorIT,,1,0,USERGRID-625,2.0
Test Not Passing: MessagesIT,,1,2,USERGRID-624,2.0
Test Not Passing: AllEntitiesInSystemObservableIT,,1,0,USERGRID-623,2.0
Test Not Passing: IndexServiceTest,,1,2,USERGRID-622,2.0
Test Not Passing: InMemoryAsyncIndexServiceTest,,1,1,USERGRID-621,2.0
Test Not Passing: IteratingQueryIT + collectionIT,,1,0,USERGRID-587,1.0
Test Not Passing: staleIndexCleanup,Seems like all of the stale entities aren't being cleaned up. ,1,2,USERGRID-584,1.0
ES Cursors not working in 2.1,,1,0,USERGRID-578,3.0
New Apps not in dropdown until you click on Org Administration Page,"When creating apps in the portal sometimes it takes a very long time to reflect them in the portal.  Even after /users/me returns the app in the list it is still not reflected in the portal.

When you use the '+ Add New App' at the top of the page the newly created app is not reflected in the App dropdown until you click on the Org Administration page.

See Jeff to get a URL for a recording of this issue being reproduced.

In addition, sometimes the immediate result is 'No Application Access Authorized'.",0,0,USERGRID-570,3.0
Updating default accesstoken ttl for an app gives 404,"PUT call to an organization's app to update accesstoken ttl gives 404. The REST call is documented here - http://apigee.com/docs/api-baas/content/changing-token-time-live-ttl

This fails in two-dot-o and two-dot-o-dev. The code which handles the api has been updated recently (past month).",0,1,USERGRID-566,3.0
[SPIKE] Cross-collection / graph filtering does not work,"REST calls of this nature do not work: GET /pets;ql=select * where type='cat'/belongsTo/owners;ql=select * where city='Dallas'
",0,0,USERGRID-551,5.0
Ensure SQS Consumers are robust and do not stop consuming messages,"In the past: For some reason the SQS queue consumers stop processing messages.  Please investigate, ensure that the code is robust, and add log messages.

Current: Test and confirm, review with Jeff and/or Todd that the loop for processing SQS messages is robust.",1,1,USERGRID-546,3.0
Configuring Notifier with APNS does not error when a non-p12 certificate is uploaded or used,"I uploaded a x509 certificate instead of a p12 certificate for configuring an APS notifier.  I experienced the following:

1) The UI did not error out when I uploaded a non p12 certificate.
-- Similar to Parse, the console should error out when a non-p12 certificate is attempted to be used.  If we accept an x509 certificate the docs and UI page should be updated.

2) When sending a push notification BaaS reports a success in the message history and notification entity but a push notification was not received
-- It seems that this is not possible to have worked since it was not a p12 certificate.  It looks as if there could be an exception getting swallowed.

3) I cannot delete the notifiers.
-- Dont know if this is related to the certificate or not
",1,1,USERGRID-519,3.0
BaaS Queries returning Entities which do not match the input Query,Error,1,1,USERGRID-516,3.0
Delete call with a limit of X returns >X resluts,"When doing a delete call with ql=select * and lmit=100, >100 results are returned.  This may or may not imply that >100 entities were deleted.",1,0,USERGRID-508,3.0
Broken link to contribution page,"On the usergrid homepage at http://usergrid.incubator.apache.org the ""Contribution Guidelines"" link points to

https://cwiki.apache.org/confluence/display/usergrid/GitHub+Based+Contribution+Workflow

but it should should really be

https://cwiki.apache.org/confluence/display/usergrid/Usergrid+External+Contributors+Guide",0,0,USERGRID-498,3.0
Long Pauses and Errors from ES on deleteByQuery,"When a single thread is doing PUTs against Usergrid, one after the other, ES can time out on deleteByQuery and block the caller.

The response time from Usergrid goes from 200ms to ~60s during this time.  Note that the error in ES logs indicates a 1 minute timeout.

[2015-03-10 17:37:42,947][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,947][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,948][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,948][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,949][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,949][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,007][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,012][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,072][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery",0,1,USERGRID-471,3.0
Rejected ES Index jobs are dropped.  ,"When the index queue is full and ES rejects index requests Usergrid drops them.  The result could be that entities cannot be returned.

We should handle this soon.  At a minimum, we should put them in an SQS queue or something similar for future handling.",1,2,USERGRID-466,5.0
Content length headers are not being sent on POSTS for ping identity,,1,9,USERGRID-457,3.0
Invalidate token after it has been used to activate a new user account,"Once token has been used to activate an account, it should be invalidated.",0,0,USERGRID-450,3.0
"Admin Portal - ${user} permissions not allowed, but they should be","In the admin portal, if you try to enter a permission like this:

/users/{$user}/has/stuff/mystuff

It gives this error:

""Path must begin with a slash, path only allows: /, a-z, 0-9, dot, and dash, paths of the format: /path/ or /path//path are not allowed""",0,0,USERGRID-449,3.0
Remove redundant appinfos collections in ManagementServiceImpl,"There is a flaw in the two-dot-o CpEntityManagerFactory.

The factory stores a collection of ""appinfo"" type entities, but the ManagementServiceImpl stores a redundant collection of ""application_info"" entities. 

The problem becomes evident when you try to delete an application. The application will only be deleted from the ""appinfos"" collection. When you call the management org/apps end-point you will still see the application because the end-point uses the ""application_infos"". 

To fix this:
- Ensure that only one collection is stored
- Add code to migrate the existing app information collections
",1,8,USERGRID-448,3.0
Investigate ElasticSearch timeout such that a long-running GC does not pause all ES and Tomcat,"I have seen as many as 16 tomcat nodes pausing at the same time across the cluster.  I took a stack trace and have attached it.  

This one jumped out to me:
""http-bio-8080-exec-2"" daemon prio=10 tid=0x00007f0cbc003800 nid=0x20ff waiting on condition [0x00007f0d370dd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007ee1de4d8> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
	at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
	at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
	at org.apache.usergrid.persistence.index.impl.EsEntityIndexImpl.search(EsEntityIndexImpl.java:357)
	at org.apache.usergrid.corepersistence.CpRelationManager.searchCollection(CpRelationManager.java:963)
	at org.apache.usergrid.corepersistence.CpEntityManager.searchCollection(CpEntityManager.java:624)
	at org.apache.usergrid.corepersistence.CpEntityManagerFactory.lookupApplication(CpEntityManagerFactory.java:433)
	at org.apache.usergrid.rest.organizations.OrganizationResource.getApplicationByName(OrganizationResource.java:137)
	at sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.dispatch(SubLocatorRule.java:197)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.invokeSubLocator(SubLocatorRule.java:183)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:110)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)
	- locked <0x00000007eda23278> (a org.apache.tomcat.util.net.SocketWrapper)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:722)

and Todd called out this one:
""collectiontasks-18"" daemon prio=10 tid=0x00007f0cfc004000 nid=0x21b7 waiting on condition [0x00007f0cb87c6000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000007ed32ccf0> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
at org.apache.usergrid.persistence.index.impl.EsEntityIndexImpl.deletePreviousVersions(EsEntityIndexImpl.java:533)
at org.apache.usergrid.corepersistence.events.EntityVersionCreatedHandler.versionCreated(EntityVersionCreatedHandler.java:67)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.fireEvents(EntityVersionCreatedTask.java:97)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.call(EntityVersionCreatedTask.java:83)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.call(EntityVersionCreatedTask.java:38)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)",0,5,USERGRID-442,3.0
Scheduler run failed when system idling,"This error happens consistently in some clusters when the system is largely idle:

2015-02-27 23:19:40,060 [JobSchedulerService] ERROR org.apache.usergrid.batch.service.JobSchedulerService- Scheduler run failed, error is
org.apache.usergrid.persistence.exceptions.QueueException: Unable to obtain a lock on queue '/jobs' after '5'seconds
	at org.apache.usergrid.mq.cassandra.io.ConsumerTransaction.getResults(ConsumerTransaction.java:206)
	at org.apache.usergrid.mq.cassandra.QueueManagerImpl.getFromQueue(QueueManagerImpl.java:412)
	at org.apache.usergrid.batch.service.SchedulerServiceImpl.getJobs(SchedulerServiceImpl.java:164)
	at org.apache.usergrid.batch.service.JobSchedulerService.runOneIteration(JobSchedulerService.java:111)
	at com.google.common.util.concurrent.AbstractScheduledService$1$1.run(AbstractScheduledService.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)",1,1,USERGRID-439,3.0
Admin Portal - connections listing on data page throws error,"Steps to reproduce:

1. Create entities in two different collections
2. Connect them
3. Read the connection in the data explorer

POST /reviews {""name"":""myreview""}
POST /users {""username"":""fred""}
POST /users/fred/wrote/reviews/myreview
GET /users/fred/wrote/reviews/ <== this call will give the error

This happens because the data explorer tries to make a /indexes call:

GET /users/fred/wrote/reviews/indexes

Which returns a 500 error",0,0,USERGRID-435,3.0
"[SPIKE] Verify that during an ES re-index, existing documents are force updated","When running a reindex last night, documents that appeared in ES were not returned via search in the CandidateResultSet.  Deleting the document from ES and then re-indexing resolved the issue. We need to ensure we're updating existing documents that exist in ES during a re-index to force an index update for existing entities.
",0,1,USERGRID-425,3.0
Cannot build Android SDK,"I cannot build the Android SDK in Usergrid 1.0.1.

Printouts with build_sdk_zip.sh modified with ""-U"" and ""-e"" options added (""mvn clean install -U -e""):

ubuntu@ubuntu-VirtualBox:~/Downloads/incubator-usergrid-master/sdks/android$ ./build_release_zip.sh 0.0.8
[INFO] Error stacktraces are turned on.
[INFO] Scanning for projects...
Downloading: http://repo.maven.apache.org/maven2/org/apache/usergrid/usergrid/1.0.0/usergrid-1.0.0.pom
[ERROR] The build could not read 1 project -> [Help 1]
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at wrong local POM @ line 27, column 10

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:364)
	at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:672)
	at org.apache.maven.DefaultMaven.getProjectsForMavenReactor(DefaultMaven.java:663)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:250)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]   
[ERROR]   The project org.apache.usergrid:usergrid-android:0.0.8 (/home/ubuntu/Downloads/incubator-usergrid-master/sdks/android/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at wrong local POM @ line 27, column 10 -> [Help 2]
org.apache.maven.model.resolution.UnresolvableModelException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:159)
	at org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:817)
	at org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:669)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:307)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:411)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:380)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:344)
	at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:672)
	at org.apache.maven.DefaultMaven.getProjectsForMavenReactor(DefaultMaven.java:663)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:250)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:459)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:262)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:239)
	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveArtifact(DefaultRepositorySystem.java:295)
	at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:155)
	... 21 more
Caused by: org.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$6.wrap(WagonRepositoryConnector.java:1012)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$6.wrap(WagonRepositoryConnector.java:1004)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(WagonRepositoryConnector.java:725)
	at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:67)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[ERROR] 
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
ubuntu@ubuntu-VirtualBox:~/Downloads/incubator-usergrid-master/sdks/android$ 



",0,2,USERGRID-350,3.0
Fix PropertiesResourceIT,,1,2,USERGRID-349,3.0
Fix AccessTokenIT,,1,2,USERGRID-347,3.0
Fix MatrixQueryTests,,0,0,USERGRID-346,3.0
Fix RetrieveUsersTest,,1,1,USERGRID-345,1.0
Fix ApplicationRequestCounterIT,,1,0,USERGRID-343,3.0
Fix AdminEmailEncodingIT,,1,2,USERGRID-342,1.0
Fix BrowserCompatibilityTest,,1,1,USERGRID-341,3.0
Fix RegistrationIT,,1,0,USERGRID-340,1.0
Fix OwnershipResourceIT,Failing the contextualConnectionOwnership test everytime. The others pass.,1,1,USERGRID-338,1.0
Fix ContentTypeResourceIT,,1,0,USERGRID-336,1.0
Test Not Passing: CollectionsResourceIT,,1,2,USERGRID-335,2.0
Fix AssetResourceIT,,1,1,USERGRID-334,3.0
Fix ApplicationResourceIT,,1,3,USERGRID-333,3.0
Fix ApplicationRequestCounterIT,,1,0,USERGRID-332,3.0
"Usergrid Launcher throws exception if ""initialize database"" unchecked","If you run the Launcher and you do not check the ""Initialize Database On Start"" button, then the first time you start the server the Launcher will exit with an error message like this:

2015-01-06 16:03:51,476 ERROR (pool-2-thread-1) [org.apache.cassandra.config.DatabaseDescriptor] - Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Cannot locate file:tmp/cassandra.yaml
	at org.apache.cassandra.config.DatabaseDescriptor.getStorageConfigURL(DatabaseDescriptor.java:123)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:140)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:132)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.usergrid.launcher.EmbeddedServerHelper$CassandraRunner.run(EmbeddedServerHelper.java:190)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",0,1,USERGRID-316,3.0
Build Application and Collection Delete Async Distributed Workflow,Akka and the Usergrid ActorSystem module is now used in Usergrid from 2.1.1 onwards.  We need to develop an asynch distributed workflow leveraging this ActorSystem for things like data cleanup after app/collection deletion.,1,0,USERGRID-286,3.0
Limit not honored on subsequent requests with cursors,"The following scenario worked in 1.0. We've broken this in 2.0.

Steps to reproduce.

1) Perform a query that returns a cursor, a limit may or may not be supplied.

curl -X GET ""http://localhost:8080/usergrid/test1/users?limit=20""

2) Perform the next page query and change the limit

curl -X GET ""http://localhost:8080/usergrid/test1/users?limit=40&cursor=cXVlcnlBbmRGZXRjaDsxOzk4NzA6UDFXa08zSVRTdGFXNncwRmZ2VkZudzswOw=="" 

*What should happen*

A response with  40 entities, assuming there are 40 to return.

*What actually happens*

A response with 20 entities, which is from the original query, but not what was requested.",0,0,USERGRID-263,3.0
ES cursor errors should be translated for our users,"When advancing beyond a cursor's end, or passing an invalid cursor, the following error is returned to the user.

{code}
body=
{""error"":""elasticsearch_illegal_argument"",""timestamp"":1417654353286,""duration"":0,""error_description"":""Failed to decode scrollId"",""exception"":""org.elasticsearch.ElasticsearchIllegalArgumentException""}

{code}

We should catch this error, and return a friendlier error message to our users.",0,1,USERGRID-262,3.0
Can't post on connection,"After setting up my roles as prescribed, I receive this error in the terminal when trying
to create a connection:

WARNING: Exception occurred during body skip
java.lang.IllegalStateException: Can not skip more bytes than available
	at org.glassfish.grizzly.http.server.io.InputBuffer.skip(InputBuffer.java:600)
	…

I tried issuing this command from the shell in the portal:

post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes { ""data"": ""Learn Usergrid” }

and

post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes [{ ""data"": ""Learn Usergrid""}]

Either command returns:

/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes
 
{
  ""action"": ""post"",
  ""application"": ""f2b952fa-22ee-11e4-9b4b-e9ea3d610fab"",
  ""params"": {
    ""access_token"": [
      ""YWMtmZWU0iQUEeScLt1PgUhfegAAAUf7J0uW32RkTiYpwSNVOHBVAtmkMnjFT3s""
    ]
  },
  ""path"": ""/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes"",
  ""uri"": ""http://localhost:8080/test.2/note-pad/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes"",
  ""entities"": [],
  ""timestamp"": 1408063432982,
  ""duration"": 6,
  ""organization"": ""test.2"",
  ""applicationName"": “note-pad""
}

However, no entities are being created. I’ve also tried using curl with the same results:

curl -H ""Authorization: Bearer YWMt2j3JaCQLEeSr1fvx65wFRAAAAUf67ffU8cqvWOyiAVXXIOea177UF05Noa8""
-X POST -d '[ {""data"":""Lear Usergrid""}]' http://localhost:8080/test.2/note-pad/users/me/mynotes

I did execute a post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes by itself as well,
which made no difference.

-charles
",0,1,USERGRID-249,3.0
Index Queue should not depend solely on AWS SQS ,"In the two-dot-o branch the faulty queue implementation has been replaced by a set of Queue interfaces and an AWS SQS implementation of those interfaces. 

Usergrid should not depend on a commercial service and the Usergrid Launcher should be able to run standalone and without external services.

So, what we need is a Queue implementation that we can embed in Usergrid for running JUnit tests that depend on the queue, for running the Usergrid Launcher and (ideally) one that can also be run remotely.

Apache Qpid seems to be a good candidate for our default queue implementation.",1,3,USERGRID-241,3.0
two-dot-o: Error in Portal after entity create,"After an entity is created, the Portal attempts to load the entity but gets a 500 error from the server because it forms a bad URL with two slashes after the application is specified.",0,1,USERGRID-237,3.0
Portal looks for helpJson.json in the wrong place,"If you deploy the portal to a path other than ""/"" then you will get a 404 or possible a 500 error when the portal loads pages because it will not be able to load helpJson.json.",0,0,USERGRID-236,3.0
Inconsistent missing resource status code,"Assuming valid credentials, I get a different status code depending on the authorization method.

Bearer token:

curl -X GET -i -H ""Accept: application/json"" -H ""Authorization: Bearer XXX"" 'http://localhost:8080/test-organization/test-app/dogs/fido'

404 error: Service resource not found


Client id & Secret:

curl -X GET -i -H ""Accept: application/json"" 'http://localhost:8080/test-organization/test-app/dogs/fido?client_id=XXX&client_secret=XXX'

HTTP/1.1 401 Unauthorized

Note: In both cases, the client receives the same body content:

{""error"":""service_resource_not_found"",""timestamp"":1412358355742,""duration"":0,""exception"":""org.apache.usergrid.services.exceptions.ServiceResourceNotFoundException"",""error_description"":""Service resource not found""}
",0,0,USERGRID-235,3.0
Default collections are not created by database setup,"Steps to reproduce:

1) setup a new Usergrid system
2) HTTP get the URL /system/database/setup
3) HTTP get the URL /system/superuser/setup
4) Login to the Portal
5) See that there is only a ""roles"" collection

But there should be these collections:
    /activities
    /assets
    /devices
    /folders
    /groups 
    /roles
    /tokens
    /users
",0,0,USERGRID-231,3.0
unable to run usergrid while off line from internet,"When trying to run the launcher and your not connected to the internet there is a spring bean error its unable to load the spring xsd e.g.:

http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd 

that is on line 22 in usergrid-standalone-context.xml in the launcher module.

Ive tried to run from source builds or from a-127 command and Its the same issue.",0,0,USERGRID-230,3.0
Possible bug in usergrid-lib.min.js TypeError: c in null,"Using UG (master) when I turn on firebug in firefox and log in to the UG portal, I see the following console error

usergrid-lib.min.js (line 27, col 92)

...==this&&c.$$nextSibling))for(;c!==this&&!(d=c.$$nextSibling);)c=c.$parent}while(...

First things first, can anyone else reproduce this? I have no idea about why this is as I have not looked into the JavaScript yet.

	
",0,0,USERGRID-215,3.0
"Fix ""start paging"" in the two-dot-o branch",,1,1,USERGRID-211,3.0
Java documentations are misleading the user(developer).,"Java classes in Usergrid SDKs still referring apigee SDKs classes.

ex: Through the method explanation of logOutAppUserAsync() method in Client.java of Usergrid android SDK, referring to a class called ""DataClient"". There is not a class called ""DataClient"" in Usergrid android SDK, while apigee SDK has.

Therefore it is difficult to figure out by going through the code base.",0,0,USERGRID-179,3.0
Lack of Documentation on available features and REST APIs,"There is not a proper documentation for available features and REST APIs. It is bit time consuming to do things with Apache Usergrid without documentation.
",0,0,USERGRID-178,3.0
Some required fields should not be required on the users page of the Admin Portal,"On the users page of the Admin portal, there are a several fields that are marked as required but should not be.",0,0,USERGRID-161,3.0
select (filter) queries with dot notation don't return results,"Steps to reproduce:

1. Create a collection and populate it with an entity with the following data:
{ ""foo.bar"": ""baz"" }

2. Run a GET query on this collection with a filter:

https://api.usergrid.com/org/app/collection/?ql=select foo.bar

3. No results are returned because the ""select"" filter isn't working with dot-notated key names.

Attached is a real-world use-case of where this is failing.",1,1,USERGRID-160,3.0
Cannot change custom entity name,"If you have a custom entity and attempt to update the entity's name (via PUT), the system will accept the request and return success (200), but the name will remain unchanged.

eg. using ugc:
ugc create foo ""name: 'bar'""
ugc update foo/bar ""name: 'baz'""

(Note: Referencing the entity in the PUT via UUID has the same effect.)",1,12,USERGRID-82,3.0
Application end-point returns incorrect counts,"Here's an example of the obviously bad counts:

        ""dogs"" : {
          ""title"" : ""Dogs"",
          ""count"" : -248,
          ""name"" : ""dogs"",
          ""type"" : ""dog""
        },
        ""medications"" : {
          ""title"" : ""Medications"",
          ""count"" : 3,
          ""name"" : ""medications"",
          ""type"" : ""medication""
        },
        ""somethings"" : {
          ""title"" : ""Somethings"",
          ""count"" : -12,
          ""name"" : ""somethings"",
          ""type"" : ""something""
        },

I suspect ",1,1,USERGRID-58,3.0
ActivityStreams should be indexed,"""top-level properties like category, content, title should be indexed by default, so you can do 

/activities?ql=category = 'Report' 


Objects such as actor, object, provider, generator & target should be deep-indexed so you can do queries like: 

/activities?ql=object.uuid = 'ca16c9a1-ab5b-11e1-8b99-1231381c404f' 

/activities?ql=author.name = 'Tim' 

etc.""",0,0,USERGRID-40,3.0
Cannot delete an entity that has a connection to it,"""To repro, attempt to delete an entity that has a connection to it such as: curl -X DELETE """"https://api.com/fdsafdsa/testapp/dog/Dachsund"""" 

Notice that you get error message: 
{""""error"""":""""class_cast"""",""""timestamp"""":1386200483978,""""duration"""":0,""""exception"""":""""java.lang.ClassCastException"""",""""error_description"""":""""org.usergrid.persistence.cassandra.ConnectedEntityRefImpl cannot be cast to org.usergrid.persistence.cassandra.ConnectionRefImpl""""} 

It is expected that the entity is deleted and the connection is deleted as well.""",1,4,USERGRID-35,3.0
"/collection/id/connected/* paths should either return an entity, an empty set or a 404","""Not clear what that path is supposed to do but itÕs accepted, and always returns an empty set of results or an error 

Examples: 

curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/followers 
=> returns empty set despite follower relationships existing 
curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/users 
=> returns empty set despite users being connected 
curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/d2740256-e312-11e1-ad17-12313d2b9232 
=> returns an error despite an entity with that UUID being connected 

1. Either we should accept a name or uuid at the end, and return that entity if it exists in the ÒconnectedÓ entities, or a 404 if it does not exist in the list of connected entities 
2. Or we should accept a relationship identifier there and return all connected entities through that identifier, or an empty set if there are none 
3. Or we should accept a collection name / type there and return all connected entities of that type, or an empty set if there are none 
4. Or you should tell me what is supposed to be provided there so we can put that in our docs and SDKs :p""",0,0,USERGRID-30,3.0
We don't seem to be parsing '++' correctly,"""I have made a simple app to test Usergrid's capabilities and I'm having trouble with updating entities that contain html entities. 

i.e. I add C++ as an entity in a collection called language and I also have C in the collection too. Whenever I try to update C++, which I grab via a collection query 

""""select * where owned='testuser' and lang='C++'"""" 

I get back the C entity. 

I have tried encoding the strings , but C%2B%2B 
gets double encoded to C%252B%252B 

This a typical call it makes without me trying to encode anything: 

https://api.usergrid.com/clydebyrdiii... 

Although it seems to encode correctly it returns the entity with lang='C' and not 
lang='C++'; 


And confirmed by Scott: 

Nope. If I create an entity with an attribute = 'c++', I can find it by querying for: 'c*' but not for 'c' or 'c+++'. 

""",1,1,USERGRID-29,3.0
Limit on connection query not working,"Also ""limit"" doesn't seems to be working for connections for e.g. https://api.usergrid.com/fdsafdsa/test/users/me/connecting/shares?limit=2&access_token=YWMtSQ92bBCsEeOYAQ1TRlcn_AAAAUDuPjgtlh06pENBKFRmoxta8jg1XwRuwVs. Returns all in our case is 3",1,1,USERGRID-27,3.0
Offer an option to delete all inbound and/or outbound connections on entity delete,"""Per Ed, all inbound connections should be cleaned up when an entity is deleted: 

For example, if you have entity rock and entity paper who both like entity scissors, both likes connections to scissors should be deleted if scissors is deleted. 


It would be great if there was an option to delete all inbound connections when an entity is deleted (the same flag should also be possible on deletes by queries).""",0,0,USERGRID-26,3.0
Collection counters on App endpoint are completely off,"""The counts of items in each collection returned in a call to /org/app are completely off (usually by a factor of 10). This is possibly related to USERGRID-17. Can we do something to fix this?

Possible solution in two parts:
1. Investigate all code paths to ensure that counters are being invoked when needed

2. Create a job that can be invoked automatically or manually that will recount all entities in a collection
""",0,3,USERGRID-24,3.0
Fresh Admin user token won't work on /management/users/me,"""After logging on:

curl -X POST """"https://api.usergrid.com/management/token"""" -d '{""""grant_type"""":""""password"""",""""username"""":""""fdsafdsa"""",""""password"""":""""fdsafdsafdsa""""}' 


The user is not able to log in with the token:

curl -X GET -i -H """"Authorization: Bearer YWMt997XKhAyEeO7oblzQPxkkAAAAUDrIyfHE8GIZlo8IhJztOX2JQ7kXpbFTHc"""" """"https://api.usergrid.com/management/usersfdsafdsa""""


But the token works with other endpoints.

***Update***: Looks like the problem is that the /management/users/<username or email> endpoint is not case-insensitive on the username or email address. We need to update that API call so that it is case-insensitive.




""",0,0,USERGRID-23,3.0
Bad geo query returns entire collection,"""When a badly formed geo query is sent with a GET, the API returns the entire collection, when it should return nothing, or return some sort of query parse error. 

For example, this is missing the 'location' param of the query statement: 

within 16000 of 37.774989,-122.419413 

and returns the first 10 entities in the collection even though the query can't be parsed. 
""",0,0,USERGRID-19,3.0
Random segmentation fault during training,,1,0,MXNET-1108,1.0
BUG in MultiBoxTargetForward when there is single box label,,1,0,MXNET-1033,3.0
Investigate roi_align operator issue,,1,0,MXNET-974,3.0
"Fix the bug for matrices of multiple dimension, with one dimension much larger ",,1,0,MXNET-921,3.0
InferShape return false not caught in Symbolic mode,,1,0,MXNET-865,3.0
how to convert parameters dtype from float32 to float64 in gluon?,,1,0,MXNET-816,2.0
different behaviour of customop in latest MXNet,,1,0,MXNET-812,5.0
test_arange failure - Jetson TX2 (CPU),,1,0,MXNET-811,1.0
src/operator/./bilinear_sampler-inl.h:105: Have not implemented the data req combinations! gdata_req=0 ggrid_req=1,,1,0,MXNET-810,2.0
Why does a tanh activation layer generates values greater than 1?,,1,0,MXNET-809,1.0
Undefined Behavior of mx.sym.where with shape-mismatched cond,,1,1,MXNET-806,2.0
mx.nd.topk does not work with ndarray of type float16,,1,0,MXNET-805,5.0
nd.softmax() doesn't support grad_req='add',,1,0,MXNET-804,5.0
Inconsistent / wrong output from sum,,1,0,MXNET-803,1.0
Autograd fails when using `take` operator repeatedly,,1,0,MXNET-802,1.0
non-deterministic backward of scatter_nd,,1,0,MXNET-801,2.0
Gradient function not returning enough gradient,,1,1,MXNET-799,5.0
Dangling outputs and dtype != float32: Gradient computation fails,,1,0,MXNET-798,1.0
Prelu activation compution fault in expand_shape function,,1,0,MXNET-797,5.0
Dropout may mask values even when ratio=0.0,,1,0,MXNET-792,1.0
Gluon raises error if the user does not call nd.waitall(),,1,1,MXNET-579,5.0
3D dilation support not working,,0,0,MXNET-578,8.0
Crash while running gluon image-classification.py example with float16,,1,1,MXNET-577,5.0
Symbolic .json file not compatible with .params file generated since MXNet 1.2,,1,0,MXNET-576,5.0
Reactivating a draining agent leaves the agent in draining state.,"When reactivating an agent that's in the draining state, the master erases it from its draining maps, and erases its estimated drain time.

However, it doesn't send any message to the agent, so if the agent is still draining and waiting for tasks to terminate, it will stay in that state, ultimately making any tasks that then get launched get DROPPED due to the agent still being in a draining state.

Seems like we should either:

* Disallow the user from reactivating if still in draining, or
* Send a message to the agent, and have the agent move itself out of draining.",1,2,MESOS-10096,3.0
Master's agent draining VLOG prints incorrect task counts.,"This logic is printing the framework counts of these maps rather than the task counts:

https://github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp#L6318-L6319

{code}
  // Check if the agent has any tasks running or operations pending.
  if (!slave->pendingTasks.empty() ||
      !slave->tasks.empty() ||
      !slave->operations.empty()) {
    VLOG(1)
      << ""DRAINING Agent "" << slaveId << "" has ""
      << slave->pendingTasks.size() << "" pending tasks, ""
      << slave->tasks.size() << "" tasks, and ""
      << slave->operations.size() << "" operations"";
    return;
  }
{code}

Since these are {{hashmap<FrameworkID, hashmap<TaskID, Task>>}}.",1,2,MESOS-10094,1.0
Libprocess does not properly escape subprocess argument strings on Windows,"When running some tests of Mesos on Windows, I discovered that the following command would not execute successfully when passed to the Docker containerizer in {{TaskInfo.command}}:
{noformat}
python -c ""print('hello world')""
{noformat}

The following error is found in the task sandbox:
{noformat}
  File ""<string>"", line 1
    ""print('hello
                ^
SyntaxError: EOL while scanning string literal
{noformat}",0,6,MESOS-10093,2.0
CSI plugins reporting duplicated volumes will crash the agent.,"The CSI spec requires volumes to be uniquely identifiable by ID, and thus SLRP currently assumes that a {{ListVolumes}} call does not return duplicated volumes. However, if a SLRP uses a non-conforming CSI plugin that reports duplicated volumes, these volumes would corrupt the SLRP checkpoint and cause the agent to crash at the next reconciliation:
{noformat}
 F0829 07:13:55.171332 12721 provider.cpp:1089] Check failed: !checkpointedMap.contains(resource.disk().source().id()){noformat}
MESOS-9254 introduces periodic reconciliation which make this problem much easier to manifest.",1,4,MESOS-9956,2.0
Master does not handle returning unreachable agents as draining/deactivated,"The master has two code paths for handling agent reregistration messages, one culminating in {{Master::___reregisterSlave}} and the other in {{Master::}}{{__reregisterSlave}}. The two paths are not continuations of each other.  Looks like we missed the double-underscore case in the initial implementation.  This is the path that unreachable agents take, when/if they come back to the cluster.  The result is that when unreachable agents are marked for draining, they do not get sent the appropriate message unless they are forced to reregister again (i.e. restarted manually).",1,2,MESOS-9934,3.0
SlaveTest.DrainingAgentRejectLaunch is flaky,"We saw {{SlaveTest.DrainingAgentRejectLaunch}} fail repeatedly on ASF Jenkins CI.
{noformat}
../../src/tests/slave_tests.cpp:12408: Failure
Failed to wait 15secs for runningUpdate2
{noformat}",1,2,MESOS-9895,1.0
Mesos.UpdateFrameworkV0Test.SuppressedRoles is flaky.,"Observed in CI, log attached.

{noformat}
mesos-ec2-ubuntu-14.04-SSL.Mesos.UpdateFrameworkV0Test.SuppressedRoles (from UpdateFrameworkV0Test)


Error Message
../../src/tests/master/update_framework_tests.cpp:1117
Mock function called more times than expected - returning directly.
    Function call: agentAdded(@0x7fb254001c40 32-byte object <90-7A 6C-85 B2-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 F0-85 00-54 B2-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
Stacktrace
../../src/tests/master/update_framework_tests.cpp:1117
Mock function called more times than expected - returning directly.
    Function call: agentAdded(@0x7fb254001c40 32-byte object <90-7A 6C-85 B2-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 F0-85 00-54 B2-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{noformat}
",1,1,MESOS-9882,1.0
Mesos did not respond correctly when operations should fail,"For testing persistent volumes with {{OPERATION_FAILED/ERROR}} feedbacks, we sshed into the mesos-agent and made it unable to create subdirectories in {{/srv/mesos/work/volumes}}, however, mesos did not respond any operation failed response. Instead, we received {{OPERATION_FINISHED}} feedback.

Steps to recreate the issue:

1. Ssh into a magent.
 2. Make it impossible to create a persistent volume (we expect the agent to crash and reregister, and the master to release that the operation is {{OPERATION_DROPPED}}):
 * cd /srv/mesos/work (if it doesn't exist mkdir /srv/mesos/work/volumes)
 * chattr -RV +i volumes (then no subdirectories can be created)

3. Launch a service with persistent volumes with the constraint of only using the magent modified above.

 

 

Logs for the scheduler for receiving `OPERATION_FINISHED`:

(Also see screenshot)

 

2019-06-27 21:57:11.879 [12768651|rdar://12768651] [Jarvis-mesos-dispatcher-105] INFO c.a.j.s.ServicePodInstance - Stored operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 and feedback=OPERATION_FINISHED in podInstanceID=4g3k02s1gjb0q on serviceID=yifan-badagents-1

 

* 2019-06-27 21:55:23: task reached state TASK_FAILED for mesos reason: REASON_CONTAINER_LAUNCH_FAILED with mesos message: Failed to launch container: Failed to change the ownership of the persistent volume at '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90' with uid 264 and gid 264: No such file or directory",1,7,MESOS-9875,8.0
Simultaneous adding/removal of a role from framework's roles and its suppressed roles crashes the master.,"Calling UPDATE_FRAMEWORK with a new role added both to 'FrameworkInfo.roles` and `suppressed_roles` crashes the master.

The first place which doesn't expect this is increasing a `suppressed` allocator metric:
[https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/hierarchical.cpp#L507]
[
https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/metrics.cpp#L255]

Probably there are other similar places.
Adding a new role in a suppressed state via re-subscribing  should also trigger this bug - haven't checked it",1,3,MESOS-9870,5.0
Make PushGauges support floating point stats.,"Currently, PushGauges are modeled against counters. Thus it does not support floating point stats. This prevents many existing PullGauges to use it. We need to add support for floating point stat.",1,2,MESOS-9861,1.0
REVIVE call with specified role(s) clears filters for all roles of a framework.,"As pointed out by [~asekretenko], the REVIVE implementation in the allocator incorrectly clears decline filters for all of the framework's roles, rather than only those that were specified in the REVIVE call:

https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1392

This should only clear filters for the roles specified in the REVIVE call.",1,1,MESOS-9856,3.0
/roles endpoint should return both guarantees and limits. ,,1,1,MESOS-9854,2.0
Migrate allocator metrics to PushGauge.,We should migrate all metrics in the master actor to use PushGauges instead of PullGauges for better performance.,0,0,MESOS-9851,5.0
Master should not report disconnected resource providers.,"MESOS-9384 attempted to make the master to garbage-collect disconnected resource providers. However, if there are disconnected resource providers but none of the connected ones changes, the following code snippet would make the master ignore the agent update and skip the garbage collection:
https://github.com/apache/mesos/blob/2ae1296c668686d234be92b00bd7abbc0a6194b0/src/master/master.cpp#L8186-L8234
The condition to ignore the agent update will be triggered in one of the following conditions:
1. The resource provider has no resource, so the agent's total resource remains the same.
2. When the agent restarts and reregisters, its resource provider resources will be reset.

As a result, the master will still keep records for the disconnected resource providers and report them.",1,2,MESOS-9831,2.0
Don't use reverse DNS for hostname validation,"Upon connection we first resolve the hostname and forget about it

https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp#L1462-L1504

then later use reverse DNS on the remote address to get back a hostname

https://github.com/apache/mesos/blob/4708c2a368e12a89669135f47777d0dd05d9b0b2/3rdparty/libprocess/src/posix/libevent/libevent_ssl_socket.cpp#L548-L556

and verify the server certificate against *that*.

Instead, we should verify the server certificate against the hostname that was used by t he client to initiate the connection.",1,2,MESOS-9811,5.0
Memory leak caused by an infinite chain of futures in `UriDiskProfileAdaptor`.,"Before MESOS-8906, {{UriDiskProfileAdaptor}} only update its promise for watchers if the polled profile matrix becomes larger in size, and this prevents the following code in the {{watch}} function from creating an infinite chain of futures when the profile matrix keeps the same:
https://github.com/apache/mesos/blob/fa410f2fb8efb988590f4da2d4cfffbb2ce70637/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L159-L160

However, the patch of MESOS-8906 removes the size check in the {{notify}} function to allow profile selectors to be updated. As a result, once the watch function is called, the returned future will be chained with a new promise every time a poll is made, hence creating a memory leak.

A jemalloc call graph for a 2hr trace is attached.",1,2,MESOS-9803,2.0
Unpublishing a volume that is failed to publish crashes the agent with CSI v1.,"The CSI v1 volume manager recovers a failed `publishVolume` call through `unpublishVolume`, which mistakenly assume that the target path, which is supposed to be created by the CSI plugin after a successful publishing, always exists. If volume publishing fails, a subsequent unpublishing would crash the agent with the following message:
{noformat}
F0412 20:20:12.254420  7540 v1_volume_manager.cpp:1161] Check failed: os::exists(targetPath){noformat}
 ",1,2,MESOS-9729,1.0
Agent crashes when SLRP recovers dropped operations.,"MESOS-9537 is fixed by persisting dropped operations in SLRP, but the recovery codepath doesn't account for that:
[https://github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp#L1278]
Which caused the agent to crash with the following message during SLRP recovery:
{noformat}
Reached unreachable statement at /pkg/src/mesos/src/resource_provider/storage/provider.cpp:1283{noformat}",1,2,MESOS-9661,1.0
Resource provider manager assumes all operations are triggered by frameworks,"When the agent tries to apply an operation to resource provider resources, it invokes {{ResourceProviderManager::applyOperation}} which in turn invokes {{ResourceProviderManagerProcess::applyOperation}}. That function currently assumes that the received message contains a valid {{FrameworkID}},
{noformat}
 void ResourceProviderManagerProcess::applyOperation(
      const ApplyOperationMessage& message)                                                                                                                                                                                                     {
    const Offer::Operation& operation = message.operation_info();                                                                                                                                                                                 
    const FrameworkID& frameworkId = message.framework_id(); // `framework_id` is `optional`.
{noformat}

Since {{FrameworkID}} is not a trivial proto types, but instead one with a {{required}} field {{value}}, the message composed with the {{frameworkId}} below cannot be serialized which leads to a failure below which in turn triggers a {{CHECK}} failure in the agent's function interfacing with the manager.

A typical scenario where we would want to support operator API calls here is to destroy leftover persistent volumes or reservations.",1,2,MESOS-9612,5.0
Removing a resource provider with consumers breaks resource publishing.,"Currently, the agent publishes all resources considered ""used"" via the resource provider manager whenever it is asked to publish a subportion. If a resource provider with active users (e.g., tasks or even just executors) was removed, but a user stays around this will fail _any resource publishing_ on that node since a ""used"" resource provider is not subscribed.

We should either update the agent code to just deltas, or provide a workaround of the same effect in the resource provider manager.",1,4,MESOS-9607,2.0
"Status update streams for operations affecting agent default resources should be stored under ""meta/slaves/<slave_id>/operations/""","The streams are currently created under {{meta/operations/}} but not recovered if {{meta/slaves/latest}} doesn't exist.

After discussing this with [~greggomann] and with [~kaysoky], we agreed that they should be created under {{meta/slaves/<slave_id>/operations/}} instead.

NOTE: don't forget to add the corresponding entry in the ascii drawing in {{slave/paths.hpp}.",1,2,MESOS-9597,2.0
Operation status update streams are not properly garbage collected.,"After successfully handling the acknowledgment of a terminal operation status update for an operation affecting agent's default resources, the agent should garbage collect the corresponding operation status update stream.",1,2,MESOS-9574,2.0
Agent should not try to recover operation status update streams that haven't been created yet.,"If the agent fails over after having checkpointed a new operation but before the operation status update stream is created, the recovery process will fail.

This happens because agent will try to recover the operation status update streams even if it hasn't been created yet.

In order to prevent recovery failures, the agent should obtain the ids of the streams to recover by walking the directory in which operation status updates streams are stored.

The agent should also garbage collect streams if the checkpointed state doesn't contain a corresponding operation.",1,2,MESOS-9573,2.0
SLRP does not clean up mount directories for destroyed MOUNT disks.,"When staging or publishing a CSI volume, SLRP will create the following mount points for these operations:
{noformat}
<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>/staging
<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>/target
{noformat}
These directories are cleaned up when the volume is unpublished/unstaged. However, their parent directory, namly {{<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>}} is never cleaned up.",1,2,MESOS-9568,2.0
OPERATION_UNREACHABLE and OPERATION_GONE_BY_OPERATOR updates don't include the agent/RP IDs,,1,2,MESOS-9559,2.0
Operations are leaked in Framework struct when agents are removed,"Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases.",1,2,MESOS-9557,2.0
SLRP does not clean up destroyed persistent volumes.,"When a persistent volume created on a {{ROOT}} disk is destroyed, the agent will clean up its data: https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4397
However, this is not the case for PVs on SLRP disks. The agent relies on the SLRP to do the cleanup:
https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4472
But SLRP simply updates its metadata and do nothing:
https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/resource_provider/storage/provider.cpp#L2805

This would lead to data leakage if the framework does not call `CREATE_DISK` but just unreserve it.",1,6,MESOS-9544,5.0
Hierarchical allocator check failure when an operation on a shutdown framework finishes,"When a non-speculated operation like e.g., {{CREATE_DISK}} becomes terminal after the originating framework was torn down, we run into an assertion failure in the allocator.
{noformat}
I0129 11:55:35.764394 57857 master.cpp:11373] Updating the state of operation 'operation' (uuid: 10a782bd-9e60-42da-90d6-c00997a25645) for framework a4d0499b-c0d3-4abf-8458-73e595d061ce-0000 (latest state: OPERATION_PENDING, status update state: OPERATION_FINISHED)
F0129 11:55:35.764744 57925 hierarchical.cpp:834] Check failed: frameworks.contains(frameworkId){noformat}
With non-speculated operations like e.g., {{CREATE_DISK}} it became possible that operations outlive their originating framework. This was not possible with speculated operations like {{RESERVE}} which were always applied immediately by the master.

The master does not take this into account, but instead unconditionally calls {{Allocator::updateAllocation}} which asserts that the framework is still known to the allocator.

Reproducer:
 * register a framework with the master.
 * add a master with a resource provider.
 * let the framework trigger a non-speculated operation like {{CREATE_DISK.}}
 * tear down the framework before a terminal operation status update reaches the master; this causes the master to e.g., remove the framework from the allocator.
 * let a terminal, successful operation status update reach the master
 * 💥 

To solve this we should cleanup the lifetimes of operations. Since operations can outlive their framework (unlike e.g., tasks), we probably need a different approach here.",1,3,MESOS-9542,5.0
SLRP sends inconsistent status updates for dropped operations.,"The bug manifests in the following scenario:
1. Upon receiving profile updates, the SLRP sends an {{UPDATE_STATE}} to the agent with a new resource version.
2. At the same time, the agent sends an {{APPLY_OPERATION}} to the SLRP with the original resource version.
3. The SLRP asks the status update manager (SUM) to reply with an {{OPERATION_DROPPED}} to the framework because of the resource version mismatch. The status update is required to be acked. Then, it simply discards the operation (i.e., no bookkeeping).
4. The agent finds a missing operation in the {{UPDATE_STATE}} so it sends a {{RECONCILE_OPERATIONS}}.
5. The SLRP asks the SUM to reply with an {{OPERATION_DROPPED}} to the agent (without a framework ID set) because it no longer knows about the operation.
6. The SUM returns an error because the latter {{OPERATION_DROPPED}} is inconsistent with the earlier one since it does not have a framework ID.",1,2,MESOS-9537,3.0
CniIsolatorTest.ROOT_CleanupAfterReboot is flaky.,"{noformat}
Error Message
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
Stacktrace
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
{noformat}

It was from this commit https://github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29",1,4,MESOS-9533,2.0
"SLRP should treat gRPC timeouts as non-terminal errors, instead of reporting OPERATION_FAILED.","1. framework executes a CREATE_DISK operation.
2. The SLRP issues a CreateVolume RPC to the plugin
3. The RPC call times out
4. The agent/SLRP translates non-terminal gRPC timeout errors (DeadlineExceeded) for ""CreateVolume"" calls into OPERATION_FAILED, which is terminal.
5. framework receives a *terminal* OPERATION_FAILED status, so it executes another CREATE_DISK operation.
6. The second CREATE_DISK operation does not timeout.
7. The first CREATE_DISK operation was actually completed by the plugin, unbeknownst to the SLRP.
8. There's now an orphan volume in the storage system that no one is tracking.

Proposed solution: the SLRP makes more intelligent decisions about non-terminal gRPC errors. For example, timeouts are likely expected for potentially long-running storage operations and should not be considered terminal. In such cases, the SLRP should NOT report OPERATION_FAILED and instead should re-issue the **same** (idempotent) CreateVolume call to the plugin to ascertain the status of the requested volume creation.

Agent logs for the 3 orphan vols above:
{code}
[jdefelice@ec101 DCOS-46889]$ grep -e 3bd1a1a9-43d3-485c-9275-59cebd64b07c agent.log
Jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:27.896306 13189 provider.cpp:1548] Received CREATE_DISK operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:27.904057 13190 provider.cpp:1605] Failed to apply operation (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c): Deadline Exceeded
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.904058 13192 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.904331 13192 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947286 13189 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947376 13189 slave.cpp:8034] Updating the state of operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947407 13189 slave.cpp:7890] Forwarding status update of operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (operation_uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.952689 13193 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for stream 3bd1a1a9-43d3-485c-9275-59cebd64b07c
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.952725 13193 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
[jdefelice@ec101 DCOS-46889]$ grep -e 4acf1495-1a36-4939-a71b-75ca5aa73657 agent.log
Jan 09 11:10:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:28.452811 13192 provider.cpp:1548] Received CREATE_DISK operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657)
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:28.460510 13190 provider.cpp:1605] Failed to apply operation (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657): Deadline Exceeded
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.460511 13186 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.460793 13186 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504062 13191 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504133 13191 slave.cpp:8034] Updating the state of operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504159 13191 slave.cpp:7890] Forwarding status update of operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (operation_uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.509495 13194 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for stream 4acf1495-1a36-4939-a71b-75ca5aa73657
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.509521 13194 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
[jdefelice@ec101 DCOS-46889]$ grep -e ca2bed2f-480e-4d35-af9e-1161a44c5b9b agent.log
Jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:27.458933 13186 provider.cpp:1548] Received CREATE_DISK operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:27.469853 13189 provider.cpp:1605] Failed to apply operation (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b): Deadline Exceeded
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.469859 13186 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.470120 13186 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513059 13192 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513129 13192 slave.cpp:8034] Updating the state of operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513147 13192 slave.cpp:7890] Forwarding status update of operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (operation_uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.518623 13191 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for stream ca2bed2f-480e-4d35-af9e-1161a44c5b9b
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.518656 13191 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
{code}",1,3,MESOS-9517,8.0
IOswitchboard cleanup could get stuck due to FD leak from a race.,"Our check container got stuck during destroy which in turned stucks the parent container. It is blocked by the I/O switchboard cleanup:

1223 18:04:41.000000 16269 switchboard.cpp:814] Sending SIGTERM to I/O switchboard server (pid: 62854) since container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e is being destroyed
....
1227 04:45:38.000000  5189 switchboard.cpp:916] I/O switchboard server process for container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e has terminated (status=N/A)

Note the timestamp.

*Root Cause:*
Fundamentally, this is caused by a race between *.discard()* triggered by Check Container TIMEOUT and IOSB extracting ContainerIO object. This race could be exposed by overloaded/slow agent process. Please see how this race be triggered below:

# Right after IOSB server process is running, Check container Timed out and the checker process returns a failure, which would close the HTTP connection with agent.
# From the agent side, if the connection breaks, the handler will trigger a discard on the returned future and that will result in containerizer->launch()'s future transitioned to DISCARDED state.
# In containerizer, the DISCARDED state will be propagated back to IOSB prepare(), which stop its continuation on *extracting the containerIO* (it implies the object being cleaned up and FDs(one end of pipes created in IOSB) being closed in its destructor).
# Agent starts to destroy the container due to its discarded launch result, and asks IOSB to cleanup the container.
# IOSB server is still running, so agent sends a SIGTERM.
# SIGTERM handler unblocks the IOSB from redirecting (to redirect stdout/stderr from container to logger before exiting).
# io::redirect() calls io::splice() and reads the other end of those pipes forever.

This issue is *not easy to reproduce unless* on a busy agent, because the timeout has to happen exactly *AFTER* IOSB server is running and *BEFORE* IOSB extracts containerIO.",1,3,MESOS-9502,8.0
SLRP does not set RP ID in produced OperationStatus.,,1,3,MESOS-9479,1.0
Master may send `FRAMEWORK_UPDATED` for a new framework ID in operator API.,"In the operator streaming API, the master only sends {{FRAMEWORK_ADDED}} if a framework is subscribed with no ID:
[https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2653-L2679]
[https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2951-L2988]

That means after a master failover, if a framework is recovered from an agent or subscribed with an ID, a {{FRAMEWORK_UPDATED}} with a framework ID that is previously unknown will be sent to the subscribers.",0,0,MESOS-9470,3.0
Completed framework update streams may retry forever,"Since the agent/RP currently does not GC operation status update streams when frameworks are torn down, it's possible that active update streams associated with completed frameworks may remain and continue retrying forever. We should add a mechanism to complete these streams when the framework becomes completed.

A couple options which have come up during discussion:
* Have the master acknowledge updates associated with completed frameworks. Note that since completed frameworks are currently only tracked by the master in memory, a master failover could prevent this from working perfectly.
* Extend the RP API to allow the GC of particular update streams, and have the agent GC streams associated with a framework when it receives a {{ShutdownFrameworkMessage}}. This would also require the addition of a new method to the status update manager.",1,5,MESOS-9434,2.0
Check failure on `StorageLocalResourceProviderProcess::applyCreateDisk`.,"Observed the following agent failure on one of our staging clusters:
{noformat}
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.641331 26684 http.cpp:1799] Processing GET_AGENT call
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.650429 26679 http.cpp:1117] HTTP POST for /slave(1)/api/v1/resource_provider from 172.31.8.65:57790
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.650629 26679 manager.cpp:672] Subscribing resource provider {""attributes"":[{""name"":""lvm-vg-name"",""text"":{""value"":""lvm-double-1540383639""},""type"":""SCALAR""},{""name"":""dss-asset-id"",""text"":{""value"":""6AbZV6W2DrK4YgcIR3ICVo""},""type"":""SCALAR""}],""default_reservations"":[{""principal"":""storage-principal"",""role"":""dcos-storage"",""type"":""DYNAMIC""}],""id"":{""value"":""8326e931-41f2-4f45-9174-13fe35c19300""},""name"":""rp_6AbZV6W2DrK4YgcIR3ICVo"",""storage"":{""plugin"":{""containers"":[{""command"":{""environment"":{""variables"":[{""name"":""PATH"",""type"":""VALUE"",""value"":""/opt/mesosphere/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin""},{""name"":""LD_LIBRARY_PATH"",""type"":""VALUE"",""value"":""/opt/mesosphere/lib""},{""name"":""CONTAINER_LOGGER_DESTINATION_TYPE"",""type"":""VALUE"",""value"":""journald+logrotate""},{""name"":""CONTAINER_LOGGER_EXTRA_LABELS"",""type"":""VALUE"",""value"":""{\""CSI_PLUGIN\"":\""csilvm\""}""}]},""shell"":true,""uris"":[{""executable"":true,""extract"":false,""value"":""<possibly-sensitive>""}],""value"":""echo \""a *:* rwm\"" > /sys/fs/cgroup/devices`cat /proc/self/cgroup | grep devices | cut -d : -f 3`/devices.allow; exec ./csilvm -devices=/dev/xvdk,/dev/xvdj -volume-group=lvm-double-1540383639 -unix-addr-env=CSI_ENDPOINT -tag=6AbZV6W2DrK4YgcIR3ICVo""},""resources"":[{""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":128.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":10.0},""type"":""SCALAR""}],""services"":[""CONTROLLER_SERVICE"",""NODE_SERVICE""]}],""name"":""plugin_6AbZV6W2DrK4YgcIR3ICVo"",""type"":""io.mesosphere.dcos.storage.csilvm""}},""type"":""org.apache.mesos.rp.local.storage""}
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690474 26685 provider.cpp:546] Received SUBSCRIBED event
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690521 26685 provider.cpp:1492] Subscribed with ID 8326e931-41f2-4f45-9174-13fe35c19300
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690657 26681 status_update_manager_process.hpp:314] Recovering operation status update manager
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: F1116 11:57:24.691496 26682 provider.cpp:3121] Check failed: resource.disk().source().has_profile() != resource.disk().source().has_id() (1 vs. 1)
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: *** Check failure stack trace: ***
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb099e9fd  google::LogMessage::Fail()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09a082d  google::LogMessage::SendToLog()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb099e5ec  google::LogMessage::Flush()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09a1129  google::LogMessageFatal::~LogMessageFatal()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb01654ca  mesos::internal::StorageLocalResourceProviderProcess::applyCreateDisk()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017c683  mesos::internal::StorageLocalResourceProviderProcess::_applyOperation()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017d64a  _ZZN5mesos8internal35StorageLocalResourceProviderProcess26reconcileOperationStatusesEvENKUlRKNS0_26StatusUpdateManagerProcessIN2id4UUIDENS0_27UpdateOperationStatusRecordENS0_28UpdateOperationStatusMessageEE5StateEE_clESA_
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017dd21  _ZNO6lambda12CallableOnceIFN7process6FutureI7NothingEEvEE10CallableFnINS_8internal7PartialIZN5mesos8internal35StorageLocalResourceProviderProcess26reconcileOperationStatusesEvEUlRKNSB_26StatusUpdateManagerProcessIN2id4UUIDENSB_27UpdateOperationStatusRecordENSB_28UpdateOperationStatusMessageEE5StateEE_ISJ_EEEEclEv
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecafa0ce97  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureI7NothingEEEclINS0_IFSD_vEEEEESD_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISC_EESt14default_deleteISP_EEOSH_S3_E_JSS_SH_St12_PlaceholderILi1EEEEEEclEOS3_
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb08eec51  process::ProcessBase::consume()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09056cc  process::ProcessManager::resume()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb090b186  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecad5d8070  (unknown)
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecacdf6e25  start_thread
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecacb20bad  __clone
{noformat}",1,2,MESOS-9395,5.0
UCR container launch stuck at PROVISIONING during image fetching.,"We observed mesos containerizer stuck at PROVISIONING when launching a mesos container using docker image: `kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9`:

The image pulling never finishes. Insufficient image contents are still in image store staging directory /var/lib/mesos/slave/store/docker/staging/egLYqO, forever.
{noformat}
OK-22:50:06-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # ls -alh
total 1.1G
drwx------. 2 root root 4.0K Oct 15 13:02 .
drwxr-xr-x. 3 root root   20 Oct 15 22:40 ..
-rw-r--r--. 1 root root  59K Oct 15 13:02 manifest
-rw-r--r--. 1 root root 2.6K Oct 15 13:02 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 13:02 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 13:02 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 13:02 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 13:02 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 13:02 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 13:02 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 13:02 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 13:02 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 13:02 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 13:02 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 13:02 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 13:02 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 13:02 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 13:02 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 384M Oct 15 13:02 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 13:02 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 13:02 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 13:02 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root 306M Oct 15 13:02 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 13:02 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 13:02 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 13:02 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 13:02 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 13:02 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 13:02 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 13:02 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 13:02 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 13:02 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 13:02 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 13:02 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 13:02 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 13:02 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 13:02 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 13:02 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root 4.1K Oct 15 13:02 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 13:02 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root 165M Oct 15 13:02 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 13:02 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 13:02 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 13:02 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 13:02 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 13:02 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 13:02 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

It is not clear yet why the SHA pulling does not finish, so we use the same image on another empty machine with UCR. The other machine has the container RUNNING correctly, and has the following staging directory before moving to the layers dir:
{noformat}
-rw-r--r--. 1 root root 2.6K Oct 15 18:03 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 18:03 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 18:03 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 18:03 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 18:03 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 18:03 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 18:03 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 18:03 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 18:03 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 18:03 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 18:03 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 18:03 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 18:03 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 18:03 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 18:03 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 122M Oct 15 18:03 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 18:03 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 18:03 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 18:03 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root  92M Oct 15 18:03 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 18:03 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 18:03 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 18:03 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 18:03 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 18:03 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 18:03 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 18:03 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 18:03 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 18:03 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 18:03 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 18:03 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 18:03 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 18:03 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 18:03 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 18:03 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root  44M Oct 15 18:03 sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b
-rw-r--r--. 1 root root 4.1K Oct 15 18:03 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 18:03 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root  82M Oct 15 18:03 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 18:03 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 18:03 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 18:03 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 18:03 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 18:03 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 18:03 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

By comparing two cases, we can see one layer `8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324` is missing on the problematic agent node, and it is the last layer to fetch.

Here is the manifest as a reference:
{noformat}
OK-17:42:20-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # cat manifest 
{
   ""schemaVersion"": 1,
   ""name"": ""kvish/jenkins-dev"",
   ""tag"": ""595c74f713f609fd1d3b05a40d35113fc03227c9"",
   ""architecture"": ""amd64"",
   ""fsLayers"": [
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0""
      },
      {
         ""blobSum"": ""sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215""
      },
      {
         ""blobSum"": ""sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2""
      },
      {
         ""blobSum"": ""sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320""
      },
      {
         ""blobSum"": ""sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5""
      },
      {
         ""blobSum"": ""sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f""
      },
      {
         ""blobSum"": ""sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58""
      },
      {
         ""blobSum"": ""sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d""
      },
      {
         ""blobSum"": ""sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87""
      },
      {
         ""blobSum"": ""sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1""
      },
      {
         ""blobSum"": ""sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa""
      },
      {
         ""blobSum"": ""sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312""
      },
      {
         ""blobSum"": ""sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2""
      },
      {
         ""blobSum"": ""sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1""
      },
      {
         ""blobSum"": ""sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b""
      },
      {
         ""blobSum"": ""sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63""
      },
      {
         ""blobSum"": ""sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728""
      },
      {
         ""blobSum"": ""sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec""
      },
      {
         ""blobSum"": ""sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf""
      },
      {
         ""blobSum"": ""sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10""
      },
      {
         ""blobSum"": ""sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747""
      },
      {
         ""blobSum"": ""sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a""
      },
      {
         ""blobSum"": ""sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352""
      },
      {
         ""blobSum"": ""sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a""
      },
      {
         ""blobSum"": ""sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94""
      },
      {
         ""blobSum"": ""sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01""
      },
      {
         ""blobSum"": ""sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276""
      },
      {
         ""blobSum"": ""sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50""
      },
      {
         ""blobSum"": ""sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac""
      },
      {
         ""blobSum"": ""sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c""
      },
      {
         ""blobSum"": ""sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c""
      },
      {
         ""blobSum"": ""sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b""
      }
   ],
   ""history"": [
      {
         ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""/usr/local/jenkins/bin/run.sh\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":null},\""container\"":\""e4111508e68c304ec5b36009773b41384b96fd887b61177cd42935b9757567fd\"",\""container_config\"":{\""Hostname\"":\""e4111508e68c\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) \"",\""CMD [\\\""/bin/sh\\\"" \\\""-c\\\"" \\\""/usr/local/jenkins/bin/run.sh\\\""]\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":{}},\""created\"":\""2018-09-26T17:33:57.6822239Z\"",\""docker_version\"":\""18.03.0-ce\"",\""id\"":\""fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\"",\""os\"":\""linux\"",\""parent\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""parent\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""created\"":\""2018-09-26T17:33:57.3350528Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 2.0 \\u003e /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""parent\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""created\"":\""2018-09-26T17:33:56.0461597Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""parent\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""created\"":\""2018-09-26T17:33:55.6692099Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c chmod -R ugo+rw \\\""$JENKINS_HOME\\\"" \\\""${JENKINS_FOLDER}\\\""     \\u0026\\u0026 chmod -R ugo+r \\\""${JENKINS_STAGING}\\\""     \\u0026\\u0026 chmod -R ugo+rx /usr/local/jenkins/bin/     \\u0026\\u0026 chmod -R ugo+rw /var/jenkins_home/     \\u0026\\u0026 chmod -R ugo+rw /var/lib/nginx/ /var/nginx/ /var/log/nginx     \\u0026\\u0026 chmod ugo+rx /usr/local/jenkins/bin/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""parent\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""created\"":\""2018-09-26T17:33:49.7534514Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c groupadd -g ${gid} nobody     \\u0026\\u0026 usermod -u ${uid} -g ${gid} ${user}     \\u0026\\u0026 usermod -a -G users nobody     \\u0026\\u0026 echo \\\""nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\\"" \\u003e\\u003e /etc/passwd\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""parent\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""created\"":\""2018-09-26T17:33:48.3150654Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD c84b80e3ceaef7f211a221093369729eeb89e5cfc5f3d0a5cd4917e7b6c7027f in /usr/share/jenkins/ref//plugins/metrics-graphite.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""parent\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""created\"":\""2018-09-26T17:33:47.8920446Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD f4d41c9bf39651b20107d62d85c101014320946e6a33763e5519ec18aee77858 in /usr/share/jenkins/ref//plugins/prometheus.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""parent\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""created\"":\""2018-09-26T17:33:46.775839Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD 652f0ad5e9ad70b4db10957b64265f808b45c63d8ef07b107d3082450084164c in /usr/share/jenkins/ref//plugins/mesos.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""parent\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""created\"":\""2018-09-26T17:33:45.5611867Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c /usr/local/bin/install-plugins.sh         blueocean-bitbucket-pipeline:${BLUEOCEAN_VERSION}      blueocean-commons:${BLUEOCEAN_VERSION}      blueocean-config:${BLUEOCEAN_VERSION}       blueocean-dashboard:${BLUEOCEAN_VERSION}    blueocean-events:${BLUEOCEAN_VERSION}       blueocean-git-pipeline:${BLUEOCEAN_VERSION}            blueocean-github-pipeline:${BLUEOCEAN_VERSION}         blueocean-i18n:${BLUEOCEAN_VERSION}         blueocean-jwt:${BLUEOCEAN_VERSION}          blueocean-jira:${BLUEOCEAN_VERSION}         blueocean-personalization:${BLUEOCEAN_VERSION}          blueocean-pipeline-api-impl:${BLUEOCEAN_VERSION}        blueocean-pipeline-editor:${BLUEOCEAN_VERSION}          blueocean-pipeline-scm-api:${BLUEOCEAN_VERSION}         blueocean-rest-impl:${BLUEOCEAN_VERSION}    blueocean-rest:${BLUEOCEAN_VERSION}         blueocean-web:${BLUEOCEAN_VERSION}          blueocean:${BLUEOCEAN_VERSION}              ant:1.8                          ansicolor:0.5.2                  antisamy-markup-formatter:1.5    artifactory:2.15.1               authentication-tokens:1.3        azure-credentials:1.6.0          azure-vm-agents:0.7.0            branch-api:2.0.19                build-name-setter:1.6.9          build-timeout:1.19               cloudbees-folder:6.4             conditional-buildstep:1.3.6      config-file-provider:2.18        copyartifact:1.39.1              cvs:2.14                         docker-build-publish:1.3.2       docker-workflow:1.15.1           durable-task:1.22                ec2:1.39                         embeddable-build-status:1.9      external-monitor-job:1.7         ghprb:1.40.0                     git:3.8.0                        git-client:2.7.1                 git-server:1.7                   github:1.29.0                    github-api:1.90                  github-branch-source:2.3.3       github-organization-folder:1.6   gitlab-plugin:1.5.5              gradle:1.28                      greenballs:1.15                  handlebars:1.1.1                 ivy:1.28                         jackson2-api:2.8.11.3            job-dsl:1.68                     jobConfigHistory:2.18            jquery:1.12.4-0                  ldap:1.20                        mapdb-api:1.0.9.0                marathon:1.6.0                   matrix-auth:2.2                  matrix-project:1.13              maven-plugin:3.1.2               metrics:3.1.2.11                 monitoring:1.72.0                nant:1.4.3                       node-iterator-api:1.5.0          pam-auth:1.3                     parameterized-trigger:2.35.2     pipeline-build-step:2.7          pipeline-github-lib:1.0          pipeline-input-step:2.8          pipeline-milestone-step:1.3.1    pipeline-model-api:1.2.8         pipeline-model-definition:1.2.8   pipeline-model-extensions:1.2.8   pipeline-rest-api:2.10           pipeline-stage-step:2.3          pipeline-stage-view:2.10         plain-credentials:1.4            prometheus:1.2.0                 rebuild:1.28                     role-strategy:2.7.0              run-condition:1.0                s3:0.11.0                        saferestart:0.3                  saml:1.0.5                       scm-api:2.2.6                    ssh-agent:1.15                   ssh-slaves:1.26                  subversion:2.10.5                timestamper:1.8.9                translation:1.16                 variant:1.1                      windows-slaves:1.3.1             workflow-aggregator:2.5          workflow-api:2.27                workflow-basic-steps:2.6         workflow-cps:2.48                workflow-cps-global-lib:2.9      workflow-durable-task-step:2.19   workflow-job:2.18                workflow-multibranch:2.17        workflow-scm-step:2.6            workflow-step-api:2.14           workflow-support:2.18\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""parent\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""created\"":\""2018-09-26T17:31:24.2544617Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:59ced817d4cd74453e0658c69f937959d2b4d86cfe15d699cd1fdcf2f6867067 in /usr/share/jenkins/ref//init.groovy.d/mesos-auth.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""parent\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""created\"":\""2018-09-26T17:31:23.9384301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8ca0529d27d0fa91b7848e39a5d04e55df01746ab31ca6bae1816f062667f8cc in /usr/share/jenkins/ref//nodeMonitors.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""parent\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""created\"":\""2018-09-26T17:31:23.609004Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:beed7a659bf7217db04b70fa4220df32e07015c6f20edf4d73b5cab69354542e in /usr/share/jenkins/ref//jenkins.model.JenkinsLocationConfiguration.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""parent\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""created\"":\""2018-09-26T17:31:23.3055734Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:46468ed2b6fa66eeea868396b18d952f8cbdd0df6529ec2a4d5782a1acc7ee7a in /usr/share/jenkins/ref//config.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""parent\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""created\"":\""2018-09-26T17:31:23.003904Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:6b54409cf8c3ce4dae538b70b64f8755636613e71806e479c5d8f081224c63e9 in /var/nginx/nginx.conf \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""parent\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""created\"":\""2018-09-26T17:31:22.6859214Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p /var/log/nginx/jenkins /var/nginx/\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""parent\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""created\"":\""2018-09-26T17:31:21.2086534Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:a4cf73ccc8a0e4b1a7acef249766ce76b31bf76d03f97ac157d6eccfab30d4f5 in /usr/local/jenkins/bin/run.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""parent\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""created\"":\""2018-09-26T17:31:20.9064351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:3377f08a63084052efa9902be76b1eb669229849b476b52f448697333457e769 in /usr/local/jenkins/bin/dcos-account.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""parent\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""created\"":\""2018-09-26T17:31:20.5594535Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:5814edade36c8c883f19e868796f1ae1d46d6990af813451101abec8196856d4 in /usr/local/jenkins/bin/export-libssl.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""parent\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""created\"":\""2018-09-26T17:31:20.2349213Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8206c6af7dc8888193958fd9428ba085ae19c8282c26eb05fb9f4c4f46973a4e in /usr/local/jenkins/bin/bootstrap.py \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""parent\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""created\"":\""2018-07-09T20:54:30.984299193Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 'networkaddress.cache.ttl=60' \\u003e\\u003e ${JAVA_HOME}/jre/lib/security/java.security\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""parent\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""created\"":\""2018-07-09T20:54:29.524404063Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p \\\""${JENKINS_HOME}\\\"" \\\""${JENKINS_FOLDER}/war\\\""\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""parent\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""created\"":\""2018-07-09T20:54:28.236876676Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo \\\""deb http://ftp.debian.org/debian testing main\\\"" \\u003e\\u003e /etc/apt/sources.list   \\u0026\\u0026 apt-get update \\u0026\\u0026 apt-get -t testing install -y git\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""parent\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""created\"":\""2018-07-09T20:54:14.100019856Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c curl -fsSL \\\""$LIBMESOS_DOWNLOAD_URL\\\"" -o libmesos-bundle.tar.gz    \\u0026\\u0026 echo \\\""$LIBMESOS_DOWNLOAD_SHA256 libmesos-bundle.tar.gz\\\"" | sha256sum -c -   \\u0026\\u0026 tar -C / -xzf libmesos-bundle.tar.gz    \\u0026\\u0026 rm libmesos-bundle.tar.gz\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""parent\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""created\"":\""2018-07-09T20:54:00.580952612Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y nginx python zip jq\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""parent\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""created\"":\""2018-07-09T20:53:46.425927046Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER root\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""parent\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""created\"":\""2018-07-09T20:53:46.096470837Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""parent\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""created\"":\""2018-07-09T20:53:45.797188526Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""parent\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""created\"":\""2018-07-09T20:53:45.462915577Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""parent\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""created\"":\""2018-07-09T20:53:45.124088811Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""parent\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""created\"":\""2018-07-09T20:53:44.827537014Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""parent\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""created\"":\""2018-07-09T20:53:44.458211965Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""parent\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""created\"":\""2018-07-09T20:53:44.10755361Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_DCOS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""parent\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""created\"":\""2018-07-09T20:53:43.757033301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""parent\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""created\"":\""2018-07-09T20:53:43.442946812Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""parent\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""created\"":\""2018-07-09T20:53:43.116440726Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""parent\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""created\"":\""2018-04-24T20:52:04.5174488Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_STAGING=/usr/share/jenkins/ref/\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""parent\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""created\"":\""2018-04-24T20:52:04.1863586Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG BLUEOCEAN_VERSION=1.5.0\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""parent\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""created\"":\""2018-04-24T20:52:03.8152478Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""parent\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""created\"":\""2018-04-24T20:52:03.4353208Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""parent\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""created\"":\""2018-04-24T20:52:03.0719423Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_FOLDER=/usr/share/jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""parent\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""created\"":\""2018-04-24T20:52:02.73463Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) WORKDIR /tmp\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""parent\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""created\"":\""2018-04-11T10:05:00.283278344Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:2874a36404a19c4075e62bf579a79bf730d317e628e80b03c676af4509481acc in /usr/local/bin/install-plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""parent\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""created\"":\""2018-04-11T10:04:58.564052111Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:39d6085e6ad132734efabf90a5444f3bc74a21e8bf5a79f4d0176ac18bb98217 in /usr/local/bin/plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""parent\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""created\"":\""2018-04-11T10:04:56.647913351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENTRYPOINT [\\\""/sbin/tini\\\"" \\\""--\\\"" \\\""/usr/local/bin/jenkins.sh\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""parent\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""created\"":\""2018-04-11T10:04:54.736575307Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:dc942ca949bb159f81bbc954773b3491e433d2d3e3ef90bac80ecf48a313c9c9 in /bin/tini \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""parent\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""created\"":\""2018-04-11T10:04:51.974150657Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:1a73810a97d134925c37b2276c894e0a9c92125cdd8c750aaf8ef15c3c20aa72 in /usr/local/bin/jenkins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""parent\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""created\"":\""2018-04-11T10:04:50.171056466Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:88dd96a27353c9d476981c3cfc6b39c95983c45083324afa7c8bddb682d91bff in /usr/local/bin/jenkins-support \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""parent\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""created\"":\""2018-04-11T10:04:48.292041295Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""parent\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""created\"":\""2018-04-11T10:04:46.288406797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""parent\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""created\"":\""2018-04-11T10:04:44.37013921Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""parent\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""created\"":\""2018-04-11T10:04:42.447771731Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""parent\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""created\"":\""2018-04-11T10:04:40.453492565Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c chown -R ${user} \\\""$JENKINS_HOME\\\"" /usr/share/jenkins/ref\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""parent\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""created\"":\""2018-04-11T10:04:37.42404848Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""parent\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""created\"":\""2018-04-11T10:04:35.309385797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC=https://updates.jenkins.io\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""parent\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""created\"":\""2018-04-11T10:04:33.341878374Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL ${JENKINS_URL} -o /usr/share/jenkins/jenkins.war   \\u0026\\u0026 echo \\\""${JENKINS_SHA}  /usr/share/jenkins/jenkins.war\\\"" | sha256sum -c -\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""parent\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""created\"":\""2018-04-11T10:04:28.72473862Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""parent\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""created\"":\""2018-04-11T10:04:26.621369421Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_SHA=2d71b8f87c8417f9303a73d52901a59678ee6c0eefcf7325efed6035ff39372a\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""parent\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""created\"":\""2018-04-11T10:04:24.515479866Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_VERSION=2.107.2\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""parent\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""created\"":\""2018-04-11T10:04:22.485876008Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_VERSION\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""parent\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""created\"":\""2018-04-11T10:04:20.518174508Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:c84b91c835048a52bb864c1f4662607c56befe3c4b1520b0ea94633103a4554f in /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""parent\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""created\"":\""2018-04-11T10:04:18.593424219Z\"",\""container_config\"":{\""Cmd\"":[\""|7 TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture) -o /sbin/tini   \\u0026\\u0026 curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture).asc -o /sbin/tini.asc   \\u0026\\u0026 gpg --import /var/jenkins_home/tini_pub.gpg   \\u0026\\u0026 gpg --verify /sbin/tini.asc   \\u0026\\u0026 rm -rf /sbin/tini.asc /root/.gnupg   \\u0026\\u0026 chmod +x /sbin/tini\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""parent\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""created\"":\""2018-04-11T10:04:13.905006564Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:653491cb486e752a4c2b4b407a46ec75646a54eabb597634b25c7c2b82a31424 in /var/jenkins_home/tini_pub.gpg \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""parent\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""created\"":\""2018-04-11T10:04:11.747045116Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG TINI_VERSION=v0.16.1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""parent\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""created\"":\""2018-04-11T10:04:09.646844829Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c mkdir -p /usr/share/jenkins/ref/init.groovy.d\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""parent\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""created\"":\""2018-04-11T10:04:05.986383436Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  VOLUME [/var/jenkins_home]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""parent\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""created\"":\""2018-04-11T10:04:03.98242692Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c groupadd -g ${gid} ${group}     \\u0026\\u0026 useradd -d \\\""$JENKINS_HOME\\\"" -u ${uid} -g ${gid} -m -s /bin/bash ${user}\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""parent\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""created\"":\""2018-04-11T10:04:00.815710832Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_SLAVE_AGENT_PORT=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""parent\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""created\"":\""2018-04-11T10:03:58.893891854Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkins_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""parent\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""created\"":\""2018-04-11T10:03:57.021756845Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG agent_port=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""parent\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""created\"":\""2018-04-11T10:03:55.096596096Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG http_port=8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""parent\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""created\"":\""2018-04-11T10:03:53.140848234Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""parent\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""created\"":\""2018-04-11T10:03:51.085212134Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""parent\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""created\"":\""2018-04-11T10:03:49.08677048Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG group=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""parent\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""created\"":\""2018-04-11T10:03:47.139089021Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""parent\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""created\"":\""2018-04-11T10:03:45.06746326Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y git curl \\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""parent\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""created\"":\""2018-03-19T21:23:43.026367652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c /var/lib/dpkg/info/ca-certificates-java.postinst configure\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""parent\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""created\"":\""2018-03-19T21:23:40.069312316Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\t\\tif [ ! -d /usr/share/man/man1 ]; then \\t\\tmkdir -p /usr/share/man/man1; \\tfi; \\t\\tapt-get update; \\tapt-get install -y \\t\\topenjdk-8-jdk=\\\""$JAVA_DEBIAN_VERSION\\\"" \\t\\tca-certificates-java=\\\""$CA_CERTIFICATES_JAVA_VERSION\\\"" \\t; \\trm -rf /var/lib/apt/lists/*; \\t\\t[ \\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" = \\\""$(docker-java-home)\\\"" ]; \\t\\tupdate-alternatives --get-selections | awk -v home=\\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" 'index($3, home) == 1 { $2 = \\\""manual\\\""; print | \\\""update-alternatives --set-selections\\\"" }'; \\tupdate-alternatives --query java | grep -q 'Status: manual'\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""parent\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""created\"":\""2018-03-19T21:22:53.380702822Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""parent\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""created\"":\""2018-03-19T21:22:53.161529652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""parent\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""created\"":\""2018-03-19T21:22:52.921597489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_VERSION=8u162\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""parent\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""created\"":\""2018-03-14T11:09:02.54085877Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_HOME=/docker-java-home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""parent\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""created\"":\""2018-03-14T11:09:02.292291489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c ln -svT \\\""/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)\\\"" /docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""parent\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""created\"":\""2018-03-14T11:09:01.580163972Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c { \\t\\techo '#!/bin/sh'; \\t\\techo 'set -e'; \\t\\techo; \\t\\techo 'dirname \\\""$(dirname \\\""$(readlink -f \\\""$(which javac || which java)\\\"")\\\"")\\\""'; \\t} \\u003e /usr/local/bin/docker-java-home \\t\\u0026\\u0026 chmod +x /usr/local/bin/docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""parent\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""created\"":\""2018-03-14T11:09:00.816087216Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV LANG=C.UTF-8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""parent\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""created\"":\""2018-03-14T11:09:00.593223495Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzip2 \\t\\tunzip \\t\\txz-utils \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""parent\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""created\"":\""2018-03-13T23:56:55.333999982Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzr \\t\\tgit \\t\\tmercurial \\t\\topenssh-client \\t\\tsubversion \\t\\t\\t\\tprocps \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""parent\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""created\"":\""2018-03-13T23:56:22.934435097Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\tif ! command -v gpg \\u003e /dev/null; then \\t\\tapt-get update; \\t\\tapt-get install -y --no-install-recommends \\t\\t\\tgnupg \\t\\t\\tdirmngr \\t\\t; \\t\\trm -rf /var/lib/apt/lists/*; \\tfi\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""parent\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""created\"":\""2018-03-13T23:56:19.194216172Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tca-certificates \\t\\tcurl \\t\\twget \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""parent\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.547884802Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  CMD [\\\""bash\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.153534342Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:b380df301ccb5ca09f0d7cd5697ed402fa55f3e9bc5df2f4d489ba31f28de58a in / \""]}}""
      }
   ],
   ""signatures"": [
      {
         ""header"": {
            ""jwk"": {
               ""crv"": ""P-256"",
               ""kid"": ""JTGT:L32L:BI2G:TG3A:RLO2:6H6K:OZXC:HFYY:SPZW:QXEZ:XNK3:2KAL"",
               ""kty"": ""EC"",
               ""x"": ""Q3Qr-lNb0qyOiyFBHzF5v4gxgVp_drIszYInemkB464"",
               ""y"": ""oBzQUsRherctDgDVxwOR0zkij_B7GAL9B20PWVtHzfs""
            },
            ""alg"": ""ES256""
         },
         ""signature"": ""X6BvXE9thNyPHIvyH_0GE1blPxznEcPbILpB5HBvI2339gSA5t4HAE7GMalgKLyThJbjrNjiq_PQqreFMBpqzA"",
         ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjU4ODg5LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTgtMTAtMTVUMTM6MDI6MjdaIn0""
      }
   ]
}
{noformat}

This should not be related: when we try to find the extracted layers on the layers dir, we could only find two:
{noformat}
ERROR(130)-22:27:48-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/layers # ls -alh | grep 'fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\|bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\|2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\|36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\|ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\|c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\|2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\|a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\|f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\|0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\|1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\|35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\|12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\|87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\|a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\|39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\|0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\|f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\|2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\|ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\|9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\|e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\|84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\|a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\|bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\|c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\|662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\|5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\|25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\|1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\|fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\|3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\|ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\|b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\|d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\|fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\|f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\|9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\|72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\|9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\|764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\|7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\|35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\|9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\|fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\|afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\|64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\|0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\|d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\|c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\|09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\|3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\|6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\|c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\|9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\|6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\|0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\|14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\|3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\|8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\|14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\|f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\|8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\|1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\|9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\|cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\|8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\|f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\|5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\|b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\|d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\|35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\|d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\|edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\|27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\|558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\|180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\|4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\|ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\|7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\|7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\|a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\|f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\|b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\|800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\|62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\|810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\|e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\|ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\|8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324'
drwxr-xr-x.   3 root root  40 Oct 15 10:23 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324
drwxr-xr-x.   3 root root  40 Oct 15 10:23 ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182
{noformat}

These two are base layers that were downloaded earlier from other images. We still need to figure out why there is one layer fetch not finished. (no curl process and tar process running stuck at background)",1,2,MESOS-9320,5.0
URI disk profile adaptor could deadlock.,"The loop here can be infinit:
https://github.com/apache/mesos/blob/1.7.0/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L61-L80

",1,2,MESOS-9308,1.0
Nested container launch could fail if the agent upgrade with new cgroup subsystems.,"Nested container launch could fail if the agent upgrade with new cgroup subsystems, because the new cgroup subsystems do not exist on parent container's cgroup hierarchy.",1,2,MESOS-9295,5.0
If a framework looses operation information it cannot reconcile to acknowledge updates.,"Normally, frameworks are expected to checkpoint agent ID and resource provider ID before accepting an offer with an OfferOperation. From this expectation comes the requirement in the v1 scheduler API that a framework must provide the agent ID and resource provider ID when acknowledging an offer operation status update. However, this expectation breaks down:

1. the framework might lose its checkpointed data; it no longer remembers the agent ID or the resource provider ID

2. even if the framework checkpoints data, it could be sent a stale update: maybe the original ACK it sent to Mesos was lost, and it needs to re-ACK. If a framework deleted its checkpointed data after sending the ACK (that's dropped) then upon replay of the status update it no longer has the agent ID or resource provider ID for the operation.

An easy remedy would be to add the agent ID and resource provider ID to the OperationStatus message received by the scheduler so that a framework can build a proper ACK for the update, even if it doesn't have access to its previously checkpointed information.

I'm filing this as a BUG because there's no way to reliably use the offer operation status API until this has been fixed.",1,2,MESOS-9293,3.0
SLRP gets a stale checkpoint after system crash.,"SLRP checkpoints a pending operations before issuing the corresponding CSI call through {{slave::state::checkpoint}}, which writes a new checkpoint to a temporary file then do a {{rename}}. However, because we don't do any {{fsync}}, {{rename}} is not atomic w.r.t. system crash. As a result, if the operation is processed during a system crash, it is possible that the CSI call has been executed, but the SLRP gets back a stale checkpoint after reboot and totally doesn't know about the operation.

To address this problem, we need to ensure the followings before issuing the CSI call:
 1. The temp file is synced to the disk.
 2. The rename is committed to the disk.

A possible solution is to do an {{fsync}} after writing the temp file, and do another {{fsync}} on the checkpoint dir after the {{rename}}.",1,3,MESOS-9281,5.0
SLRP does not clean up plugin containers after it is removed.,,1,3,MESOS-9228,5.0
"If some image layers are large, the image pulling may stuck due to the authorized token expired.","The image layer blobs pulling happen asynchronously but in the same libprocess process. There is a chance that one layer get the token then the thread switch to another layer curling which may take long. When the original layer curling resumes, the token already expired (e.g., after 60 seconds).

{noformat}
$ sudo cat /var/lib/mesos/slave/store/docker/staging/0gx64f/sha256\:c75480ad9aafadef6c7faf829ede40cf2fa990c9308d6cd354d53041b01a7cda
{""errors"":[{""code"":""UNAUTHORIZED"",""message"":""authentication required"",""detail"":[{""Type"":""repository"",""Class"":"""",""Name"":""mesosphere/dapis"",""Action"":""pull""}]}]}
{noformat}

The impact is the task launch stuck and all subsequent task using this image would also stuck because it waits for the same image pulling future to become ready.

Please note that this issue is not likely to be reproduced, unless on a busy system using images containing large layers.",1,2,MESOS-9221,5.0
Removing rootfs mounts may fail with EBUSY.,"We observed in production environment that this
{code}
Failed to destroy the provisioned rootfs when destroying container: Collect failed: Failed to destroy overlay-mounted rootfs '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec': Failed to unmount '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec': Device or resource busy
{code}

Consider fixing the issue by using detach unmount when unmounting container rootfs. See MESOS-3349 for details.

The root cause on why ""Device or resource busy"" is received when doing rootfs unmount is still unknown.

_UPDATE_: The production environment has a cronjob that scan filesystems to build index (updatedb for mlocate). This can explain the EBUSY we receive when doing `unmount`.

_UPDATE_: Splunk that's scanning `/var/lib/mesos` could also be a source of triggers.",1,3,MESOS-9196,5.0
Docker command executor may stuck at infinite unkillable loop.,"Due to the change from https://issues.apache.org/jira/browse/MESOS-8574, the behavior of docker command executor to discard the future of docker stop was changed. If there is a new killTask() invoked and there is an existing docker stop in pending state, the old one would call discard and then execute the new one. This is ok for most of cases.

However, docker stop could take long (depends on grace period and whether the application could handle SIGTERM). If the framework retry killTask more frequently than grace period (depends on killpolicy API, env var, or agent flags), then the executor may be stuck forever with unkillable tasks. Because everytime before the docker stop finishes, the future of docker stop is discarded by the new incoming killTask.

We should consider re-use grace period before calling discard() to a pending docker stop future.",1,4,MESOS-9191,3.0
Test `StorageLocalResourceProviderTest.ROOT_CreateDestroyDiskRecovery` is flaky.,"The test is flaky in 1.7.x:
{noformat}
I0824 22:20:01.018494 4208 provider.cpp:1520] Received DESTROY_DISK operation '' (uuid: 7aaadd15-1f6d-4d4e-9000-4c250495f7ba)
W0824 22:20:01.018517 4208 provider.cpp:3008] Dropping operation (uuid: 7aaadd15-1f6d-4d4e-9000-4c250495f7ba): Cannot apply operation when reconciling storage pools
...
I0824 22:20:01.086668 4209 master.cpp:9445] Sending offers [ 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-O4 ] to framework 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-0000 (default) at scheduler-0af22a76-f591-43ba-8470-f4b863292d61@172.16.10.36:35916
../../src/tests/storage_local_resource_provider_tests.cpp:995: Failure
Mock function called more times than expected - returning directly.
Function call: resourceOffers(0x7ffe7ba8c240, @0x7f04a09808c0 { 160-byte object <98-7C 05-AD 04-7F 00-00 00-00 00-00 00-00 00-00 5F-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 05-00 00-00 05-00 00-00 10-A7 03-84 04-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 ... 90-F2 08-84 04-7F 00-00 10-71 00-84 04-7F 00-00 40-71 00-84 04-7F 00-00 00-51 02-84 04-7F 00-00 C0-6A 03-84 04-7F 00-00 00-00 00-00 00-00 00-00 10-F0 00-84 04-7F 00-00 00-00 00-00 00-00 00-00> })
Expected: to be called once
Actual: called twice - over-saturated and active
{noformat}
This is because of `DESTRY_DISK` races with a profile poll. If the poll finishes first, SLRP will start reconciling storage pools, and drop certain incoming operations during reconciliation.",1,1,MESOS-9190,2.0
Add allocator benchmark to allow multiple framework/agent profiles.,We want to add some test harness that allows us to test allocator performance when run with multiple agent and framework profiles.,1,2,MESOS-9187,5.0
Zookeeper doesn't compile with newer gcc due to format error,RR: https://reviews.apache.org/r/68370/,1,1,MESOS-9170,2.0
`UriDiskProfileAdaptor` should not update profiles when a poll returns a non-OK HTTP status.,"Currently if the {{UriDiskProfileAdatpor}} receives an non-OK status, e.g., {{404 Not Found}}, from a URL poll, it would still read the response body (which could be empty or malformed) and update the profile matrix. The expected behavior should be skip this poll and retry later.",1,1,MESOS-9163,1.0
MasterTest.TaskStateMetrics is flaky,"Observed on Ubuntu 16.04, cmake build:
{code}
Mock function called more times than expected - returning directly.
    Function call: offers(0x7fffcf5518d0, @0x7f64d805d440 48-byte object <C0-D3 39-0C 65-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 04-00 00-00 D0-A1 08-D8 64-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{code}

Full log attached.",1,2,MESOS-9154,1.0
Close all file descriptors except whitelist_fds in posix/subprocess.,"Close all file descriptors except whitelist_fds in posix/subprocess (currently whitelist_fds are not honored yet). This would avoid the fd being leaked. Please follow the steps from this commit to make corresponding change:
 https://issues.apache.org/jira/browse/MESOS-8917?focusedCommentId=16522629&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16522629",1,2,MESOS-9152,5.0
Container stuck at ISOLATING due to FD leak,"When containers are launching on a single agent at scale, one container stuck at ISOLATING could occasionally happen. And this container becomes un-destroyable due to containerizer destroy always wait for isolate() finish to continue.

We add more logging to debug this issue:
{noformat}
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.050068  2995 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 2; ready count: 1
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.414436  2998 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 0; ready count: 2
{noformat} 
which shows that the await() in CNI::attach() stuck at the second future (io::read() for stdout).

By looking at the df of this stdout:
{noformat}
Aug 10 17:23:27 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:27.657501  2995 cni.cpp:1287] !!!!: Start to await for plugin '/opt/mesosphere/active/mesos/libexec/mesos/mesos-cni-port-mapper' to finish for container 1c8abf4c-f71a-4704-9a73-1ab0dd709c62 with pid '16644'; stdout fd: 1781; stderr fd: 1800
{noformat}

We found
{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep mesos-agent
core      1674  0.0  0.0   6704   864 pts/0    S+   20:00   0:00 grep --colour=auto mesos-agent
root      2974 16.4  2.5 1211096 414048 ?      Ssl  17:02  29:11 /opt/mesosphere/packages/mesos--61265af3be37861f26b657c1f9800293b86a0374/bin/mesos-agent
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep 1781
l-wx------. 1 root root 64 Aug 10 19:38 1781 -> /var/lib/mesos/slave/meta/slaves/d3089315-8e34-40b4-b1f7-0ac6a624d7db-S0/frameworks/d3089315-8e34-40b4-b1f7-0ac6a624d7db-0000/executors/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/runs/38e9270d-ebda-4758-ad96-40c5b84bffdc/tasks/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/task.updates
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep 27981
core      2201  0.0  0.0   6704   884 pts/0    S+   20:06   0:00 grep --colour=auto 27981
root     27981  0.0  0.0   1516     4 ?        Ss   17:25   0:00 sleep 10000
core@ip-10-0-1-129 ~ $ cat /proc/s^C       
core@ip-10-0-1-129 ~ $ sudo -s
ip-10-0-1-129 core # ls -al /proc/27981/fd | grep 275230
lr-x------. 1 root root 64 Aug 10 20:05 1781 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 20:05 1787 -> pipe:[275230]
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep pipe
lr-x------. 1 root root 64 Aug 10 17:02 11 -> pipe:[49380]
l-wx------. 1 root root 64 Aug 10 17:02 14 -> pipe:[49380]
lr-x------. 1 root root 64 Aug 10 17:02 17 -> pipe:[48909]
lr-x------. 1 root root 64 Aug 10 19:38 1708 -> pipe:[275089]
l-wx------. 1 root root 64 Aug 10 19:38 1755 -> pipe:[275089]
lr-x------. 1 root root 64 Aug 10 19:38 1787 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 17:02 19 -> pipe:[48909]
{noformat}

pipe 275230 is held by the agent process and the sleep process at the same time!

The reason why the leak is possible is because we don't use `pipe2` to create a pipe with `O_CLOEXEC` in subprocess:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp#L61

Although we do set cloexec on those fds later:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp#L366-L373

There is a race where a fork happens after `pipe()` call, but before cloexec is called later. This is more likely on a busy system (this explains why it's not hard to repo the issue when launching a lot of containers on a single box).",1,5,MESOS-9151,8.0
Test `StorageLocalResourceProviderTest.ROOT_ContainerTerminationMetric` is flaky.,"This test is flaky and can fail with the following error:
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:3167
Failed to wait 15secs for pluginRestarted{noformat}
The actual error is the following:
{noformat}
E0802 22:13:37.265038  8216 provider.cpp:1496] Failed to reconcile resource provider b9379982-d990-4f63-8a5b-10edd4f5a1bb: Collect failed: OS Error{noformat}
The root cause is that the SLRP calls {{ListVolumes}} and {{GetCapacity}} when starting up, and if the plugin container is killed when these calls are ongoing, gRPC will return an {{OS Error}} which will lead the SLRP to fail.

This flakiness will be fixed once we finish https://issues.apache.org/jira/browse/MESOS-8400.",1,4,MESOS-9130,1.0
Virtualenv management in support directory is buggy.,"When switching back and forth from Python 2 to 3, the virtualenv does not get correctly reinstalled.",1,3,MESOS-9075,2.0
Tox doesn't run in the support virtualenv when using Python 3 mesos-style.py,,1,1,MESOS-9073,1.0
Default executor should commit suicide if it cannot receive HTTP responses for LAUNCH_NESTED_CONTAINER calls.,"If there is a network problem (e.g., a routing problem), it is possible that the agent has received {{LAUNCH_NESTED_CONTAINER}} calls from the default executor and launched the nested container, but the executor does not get the HTTP response. This would result in tasks stuck at {{TASK_STARTING}} forever. We should consider making the default executor commit suicide if it does not receive the response in a reasonable amount of time. ",1,1,MESOS-9052,3.0
`UPDATE_STATE` can race with `UPDATE_OPERATION_STATUS` for a resource provider.,"Since a resource provider and its operation status update manager run in different actors, for a completed operation, its `UPDATE_OPERATION_STATUS` call may race with an `UPDATE_STATE`. When the `UPDATE_STATE` arrives to the agent earlier, the total resources will be updated, but the terminal status of the completed operation will be ignored since it is known by both the agent and the resource provider. As a result, when the `UPDATE_OPERATION_STATUS` arrives later, the agent will try to apply the operation, but this is incorrect since the total resources has already been updated.",1,2,MESOS-9010,2.0
Wire `UPDATE_QUOTA` call.,"Wire the existing master, auth, registar, and allocator pieces together to complete the `UPDATE_QUOTA` call.

This would enable the master capability `QUOTA_V2`.

This also fixes the ""ignoring zero resource quota"" bug in the old quota implementation, namely:

Currently, Mesos discards resource object with zero scalar value when parsing resources. This means quota set to zero would be ignored and not enforced. For example, role with quota set to ""cpu:10;mem:10;gpu:0"" intends to get no GPU. Due to the above issue, the allocator can only see the quota as ""cpu:10;mem:10"", and no quota GPU means no guarantee and NO limit. Thus GPUs may still be allocated to this role. 

With the completion of `UPDATE_QUOTA` which takes a map of name, scalar values, zero value will no longer be dropped.
",1,4,MESOS-8968,5.0
python3/post-reviews.py errors due to TypeError.,"{code:java}
$ ./support/python3/post-reviews.py 
Running 'rbt post' across all of ...
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API. (2 minutes ago)
Creating diff of:
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API.
Press enter to continue or 'Ctrl-C' to skip.

Traceback (most recent call last):
File ""./support/python3/post-reviews.py"", line 432, in <module>
main()
File ""./support/python3/post-reviews.py"", line 365, in main
sys.stdout.buffer.write(output)
TypeError: a bytes-like object is required, not 'str'{code}
The review still get posted.",1,2,MESOS-8954,1.0
Quota guarantee metric does not handle removal correctly.,"The quota guarantee metric is not removed when the quota gets removed:
https://github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp#L165-L174

The consequence of this is that the metric will hold the initial value that gets set and all subsequent removes / sets will not be exposed via the metric.",1,2,MESOS-8932,2.0
Autotools don't work with newer OpenJDK versions,"There are three distinct issues with modern Java and Linux versions:

1. Mesos configure script expects `libjvm.so` at `$JAVA_HOME/jre/lib/<arch>/server/libjvm.so`, but in the newer openjdk versions, `libjvm.so` is found at `$JAVA_HOME/lib/server/libjvm.so`.

2. On some distros (e.g., Ubuntu 18.04), JAVA_HOME env var might be missing. In such cases, the configure is able to compute it by looking at `java` and `javac` paths and succeeds. However, some maven plugins require JAVA_HOME to be set and could fail if it's not found.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8.1:jar (build-and-attach-javadocs) on project mesos: MavenReportException: Error while creating archive: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. -> [Help 1]
{code}
Because configure scripts generate an automake variable `JAVA_HOME`, we can simply invoke maven in the following way to fix this issue:
{code:java}
JAVA_HOME=$JAVA_HOME mvn ...{code}
 These two behaviors were observed with OpenJDK 1.11 on Ubuntu 18.04 but I suspect that the behavior is present on other distros/OpenJDK versions.

3. `javah` has been removed as of OpenJDK 1.10. Instead `javac -h` is to be used as a replacement. See [http://openjdk.java.net/jeps/313] for more details.",1,3,MESOS-8921,3.0
Docker image fetcher fails with HTTP/2.,"{noformat}
[ RUN      ] ImageAlpine/ProvisionerDockerTest.ROOT_INTERNET_CURL_SimpleCommand/2
...
I0510 20:52:00.209815 25010 registry_puller.cpp:287] Pulling image 'quay.io/coreos/alpine-sh' from 'docker-manifest://quay.iocoreos/alpine-sh?latest#https' to '/tmp/ImageAlpine_ProvisionerDockerTest_ROOT_INTERNET_CURL_SimpleCommand_2_wF7EfM/store/docker/staging/qit1Jn'
E0510 20:52:00.756072 25003 slave.cpp:6176] Container '5eb869c5-555c-4dc9-a6ce-ddc2e7dbd01a' for executor 'ad9aa898-026e-47d8-bac6-0ff993ec5904' of framework 7dbe7cd6-8ffe-4bcf-986a-17ba677b5a69-0000 failed to start: Failed to decode HTTP responses: Decoding failed
HTTP/2 200
server: nginx/1.13.12
date: Fri, 11 May 2018 03:52:00 GMT
content-type: application/vnd.docker.distribution.manifest.v1+prettyjws
content-length: 4486
docker-content-digest: sha256:61bd5317a92c3213cfe70e2b629098c51c50728ef48ff984ce929983889ed663
x-frame-options: DENY
strict-transport-security: max-age=63072000; preload
...
{noformat}

Note that curl is saying the HTTP version is ""HTTP/2"". This happens on modern curl that automatically negotiates HTTP/2, but the docker fetcher isn't prepared to parse that.

{noformat}
$ curl -i --raw -L -s -S -o -  'http://quay.io/coreos/alpine-sh?latest#https'
HTTP/1.1 301 Moved Permanently
Content-Type: text/html
Date: Fri, 11 May 2018 04:07:44 GMT
Location: https://quay.io/coreos/alpine-sh?latest
Server: nginx/1.13.12
Content-Length: 186
Connection: keep-alive

HTTP/2 301
server: nginx/1.13.12
date: Fri, 11 May 2018 04:07:45 GMT
content-type: text/html; charset=utf-8
content-length: 287
location: https://quay.io/coreos/alpine-sh/?latest
x-frame-options: DENY
strict-transport-security: max-age=63072000; preload
{noformat}",1,15,MESOS-8907,3.0
`UriDiskProfileAdaptor` fails to update profile selectors.,"The {{UriDiskProfileAdaptor}} ignores the polled profile matrix if the polled one has the same size as the current one: https://github.com/apache/mesos/blob/1.5.x/src/resource_provider/storage/uri_disk_profile.cpp#L282-L286
{code:cxx}
  // Profiles can only be added, so if the parsed data is the same size,
  // nothing has changed and no notifications need to be sent.
  if (parsed.profile_matrix().size() <= profileMatrix.size()) {
    return;
  }
{code}
However, this prevents the profile selector from being updated, which is not the desired behavior.",1,2,MESOS-8906,2.0
ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider is flaky.,"This test is flaky on CI:
{noformat}
../../src/tests/resource_provider_manager_tests.cpp:1114: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:2972:
    Function call: subscribed(@0x7f881c00aff0 32-byte object <58-04 98-43 88-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 E0-01 01-1C 88-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{noformat}

This is different from https://issues.apache.org/jira/browse/MESOS-8315.",1,2,MESOS-8874,1.0
StorageLocalResourceProviderTest.ROOT_ZeroSizedDisk is flaky.,"This test is flaky on CI:
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:406: Failure
Value of: updateSlave2->has_resource_providers()
  Actual: false
Expected: true
{noformat}",1,6,MESOS-8873,2.0
Agent may fail to recover if the agent dies before image store cache checkpointed.,"{noformat}
E0502 13:51:45.398555 10100 slave.cpp:7305] EXIT with status 1: Failed to perform recovery: Collect failed: Collect failed: Collect failed: Unexpected empty images file '/var/lib/mesos/slave/store/docker/storedImages'
{noformat}

This may happen if the agent dies after the file is created but before the contents are persisted on disk.",1,2,MESOS-8871,3.0
Consider validating that resubscribing resource providers do not change their name or type,"The agent currently uses a resource provider's name and type to construct e.g., paths for persisting resource provider state and their recovery. With that we should likely prevent resource providers from changing that information since we might otherwise be unable to recover them successfully.",1,2,MESOS-8838,2.0
Add a tests of recovery of the resource provider manager registrars.,,1,2,MESOS-8836,2.0
RP-related API should be experimental.,The new offer operations and resource provider API introduced in Mesos 1.5.0 should be marked as experimental.,1,1,MESOS-8787,1.0
OPERATION_DROPPED operation status updates should include the operation/framework IDs,The agent should include the operation/framework IDs in operation status updates sent in response to a reconciliation request from the master. These status updates have the operation status: {{OPERATION_DROPPED}}.,1,2,MESOS-8784,3.0
Transition pending operations to OPERATION_UNREACHABLE when an agent is removed.,"Pending operations on an agent should be transitioned to `OPERATION_UNREACHABLE` when an agent is marked unreachable. We should also make sure that we pro-actively send operation status updates for these operations when the agent becomes unreachable.

We should also make sure that we send new operation updates if/when the agent reconnects - perhaps this is already accomplished with the existing operation update logic in the agent?",1,2,MESOS-8783,3.0
Transition operations to OPERATION_GONE_BY_OPERATOR when marking an agent gone.,"The master should transition operations to the state {{OPERATION_GONE_BY_OPERATOR}} when an agent is marked gone, sending an operation status update to the frameworks that created them.

We should also remove them from {{Master::frameworks}}.",1,2,MESOS-8782,3.0
Mesos master shouldn't silently drop operations,"We should make sure that all call places of {{void Master::drop(Framework*, const Offer::Operation&, const string&)}} send a status update if an operation ID was specified. OR we should make sure that they do NOT send one, and make that method send one.",1,2,MESOS-8781,3.0
Agent resource provider config API calls should be idempotent.,"There are some issues w.r.t. using the current agent resource provider config API calls:

1. {{UPDATE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, there is no way to retry the operation without triggering an RP restart.
2. {{REMOVE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, a retry will return a 404 Not Found. But due to MESOS-7697, there is no way for the caller to know if the 404 is due to a previous successful config removal or not.

To address these issues, we should make these calls idempotent, such that they return 200 OK when the caller retry. It would be nice if {{ADD_RESOURCE_PROVIDER_CONFIG}} is also idempotent for consistency.",1,2,MESOS-8742,2.0
Libprocess: deadlock in process::finalize,"Since we are calling [`libprocess::finalize()`|https://github.com/apache/mesos/blob/02ebf9986ab5ce883a71df72e9e3392a3e37e40e/src/slave/containerizer/mesos/io/switchboard_main.cpp#L157] before returning from the IOSwitchboard's main function, we expect that all http responses are going to be sent back to clients before IOSwitchboard terminates. However, after [adding|https://reviews.apache.org/r/66147/] `libprocess::finalize()` we have seen that IOSwitchboard might get stuck in `libprocess::finalize()`. See attached stacktrace.",1,3,MESOS-8729,5.0
Enable resource provider agent capability by default,"In 1.5.0 we introduced a resource provider agent capability which e.g., enables a modified operation protocol. We should enable this capability by default.

 

If tests explicitly depend on the agent being fully operational, they should be adjusted for the modified protocol. It is e.g., not enough to wait for a {{dispatch}} to the agent's recovery method, but instead one should wait for a dedicated {{UpdateSlaveMessage}} from the agent.",1,2,MESOS-8647,2.0
Terminal task status update will not send if 'docker inspect' is hung,"When the agent processes a terminal status update for a task, it calls {{containerizer->update()}} on the container before it forwards the update: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/slave.cpp#L5509-L5514

In the Docker containerizer, {{update()}} calls {{Docker::inspect()}}, which means that if the inspect call hangs, the terminal update will not be sent: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/containerizer/docker.cpp#L1714",1,3,MESOS-8605,3.0
Allow empty resource provider selector in `UriDiskProfileAdaptor`.,"Currently in {{UriDiskProfileAdaptor}}, it is invalid for a profile to have a resource provider selector with 0 resource providers. However, one can put non-existent provider types and names into the selector to achieve the same effect, and this is semantically inconsistent. We should allow an empty list of resource providers directly.",1,2,MESOS-8598,1.0
Avoid failure for invalid profile in `UriDiskProfileAdaptor`,We should be defensive and not fail the profile module when the user provides an invalid profile in the profile matrix.,0,1,MESOS-8592,1.0
Test UriDiskProfileTest.FetchFromHTTP is flaky.,"The {{UriDiskProfileTest.FetchFromHTTP}} test is flaky on Debian 9:
{noformat}
../../src/tests/disk_profile_tests.cpp:683
Failed to wait 15secs for future
{noformat}

I also run it in repetition and got the following error log (although the test itself is passed):
{noformat}
E0209 18:26:37.030012  7282 uri_disk_profile.cpp:220] Failed to parse result: Failed to parse DiskProfileMapping message: INVALID_ARGUMENT:Unexpected end of string. Expected a value.

^
{noformat}",1,7,MESOS-8567,3.0
Default executor should allow decreasing the escalation grace period of a terminating task,"The command executor supports [decreasing the escalation grace period of a terminating task|https://github.com/apache/mesos/blob/c665dd6c22715fa941200020a8f7209f1f5b1ca1/src/launcher/executor.cpp#L800-L803].

For consistency, this should also be supported by the default executor.",0,1,MESOS-8557,5.0
Test StorageLocalResourceProviderTest.ROOT_Metrics is flaky,"The SLRP Metrics test is flaky because the agent might got two {{SlaveRegisteredMessage}}s due to its retry logic for registration, and thus it would send two {{UpdateSlaveMessage}}s. As a result, the futures waiting for these messages will be ready before the plugin is actually launched. This will lead to a race between the SIGKILL and LAUNCH_CONTAINER in the test, and if the kill happens before SLRP gets connected to the plugin, SLRP will wait for 1 minutes before giving up, which is too long for the test to wait for a second launch.",1,2,MESOS-8548,2.0
The default executor can wrongly indicate that tasks from all task groups are unhealthy,"The default executor sets a ""global"" [unhealthy|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L1286] field to {{true}} once it kills a task due to failed health checks.

When a task from any task group exits, it will [check|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L877-L882] that ""global"" healthy field and set the task status update's {{healthy}} field accordingly. This means that once an unhealthy task belonging to a task group is killed, task status updates for tasks belonging to other task groups, which can contain only healthy tasks, will be sent with their {{healthy}} field set to {{false}}.",0,1,MESOS-8543,3.0
Default executor doesn't wait for status updates to be ack'd before shutting down,"The default executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:

{code}
  void _shutdown()
  {
    const Duration duration = Seconds(1);

    LOG(INFO) << ""Terminating after "" << duration;

    // TODO(qianzhang): Remove this hack since the executor now receives
    // acknowledgements for status updates. The executor can terminate
    // after it receives an ACK for a terminal status update.
    os::sleep(duration);
    terminate(self());
  }
{code}

The event handler should exit if upon receiving a {{Event::ACKNOWLEDGED}} the executor is shutting down, no tasks are running anymore, and all pending status updates have been acknowledged.",1,3,MESOS-8537,3.0
The default executor doesn't retry kills that it initiated,"The default executor might initiate a task kill due to health check failures or to a task in the same task group failing.

If the kill call fails, the executor won't retry it, so the task will get stuck in a killing state.",0,0,MESOS-8532,5.0
Some task status updates sent by the default executor don't contain a REASON.,"The default executor doesn't set a reason when sending {{TASK_KILLING}}, {{TASK_KILLED}},
 and {{TASK_FAILED}} task status update.",0,1,MESOS-8531,3.0
Default executor tasks can get stuck in KILLING state,"The default executor will transition a task to {{TASK_KILLING}} and mark its container as being killed before issuing the {{KILL_NESTED_CONTAINER}} call.

If the kill call fails, the task will get stuck in {{TASK_KILLING}}, and the executor won't allow retrying the kill.
",1,3,MESOS-8530,5.0
"When `UPDATE_SLAVE` messages are received, offers might not be rescinded due to a race ","When an agent with enabled {{RESOURCE_PROVIDER}} capability (re-)registers with the master it sends a {{UPDATE_SLAVE}} after being (re-)registered. In the master, the agent is added (back) to the allocator, as soon as it's (re-)registered, i.e. before {{UPDATE_SLAVE}} is being send. This triggers an allocation and offers might get sent out to frameworks. When {{UPDATE_SLAVE}} is being handled in the master, these offers have to be rescinded, as they're based on an outdated agent state.
Internally, the allocator defers a offer callback in the master ({{Master::offer}}). In rare cases a {{UPDATE_SLAVE}} message might arrive at the same time and its handler in the master called before the offer callback (but after the actual allocation took place). In this case the (outdated) offer is still sent to frameworks and never rescinded.

Here's the relevant log lines, this was discovered while working on https://reviews.apache.org/r/65045/:
{noformat}
I0201 14:17:47.041093 242208768 hierarchical.cpp:1517] Performed allocation for 1 agents in 704915ns
I0201 14:17:47.041738 242745344 master.cpp:7235] Received update of agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 at slave(540)@172.18.8.20:60469 (172.18.8.20) with total oversubscribed resources {}
I0201 14:17:47.042778 242745344 master.cpp:8808] Sending 1 offers to framework 53c557e7-3161-449b-bacc-a4f8c02e78e7-0000 (default) at scheduler-798f476b-b099-443e-bd3b-9e7333f29672@172.18.8.20:60469
I0201 14:17:47.043102 243281920 sched.cpp:921] Scheduler::resourceOffers took 40444ns
I0201 14:17:47.043427 243818496 hierarchical.cpp:712] Grew agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 by disk[MOUNT]:200 (total), {  } (used)
I0201 14:17:47.043643 243818496 hierarchical.cpp:669] Agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 (172.18.8.20) updated with total resources disk[MOUNT]:200; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
{noformat}",1,2,MESOS-8524,2.0
SLRP failed to connect to CSI endpoint.,"After bumping the gRPC bundle to 1.8.3, there are some flakiness in SLRP tests caused by SLRP not being able to connect to a CSI endpoint. The reason is that it seems to take longer for gRPC 1.8 to prepare a domain socket (i.e., the time between the {{bind}} and {{accept}} calls are longer), and as a result, SLRP cannot talk to a CSI plugin immediately after the socket file is created.",1,2,MESOS-8514,2.0
URI disk profile adaptor does not consider plugin type for a profile.,"Currently, the URI disk profile adaptor will fetch an URI, the content of which contains a profile matrix. However, there's no field in the profile matrix for the adaptor to tell which plugin type a profile is for.

We should consider adding a `plugin_type` field in `CSIManifest`.",1,3,MESOS-8510,3.0
Test StorageLocalResourceProviderTest.ROOT_ConvertPreExistingVolume is flaky,"Observed on our internal CI on ubuntu16.04 with SSL and GRPC enabled,
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:1898
      Expected: 2u
      Which is: 2
To be equal to: destroyed.size()
      Which is: 1
{noformat}",1,6,MESOS-8474,2.0
`LAUNCH_GROUP` failure tears down the default executor.,"The following code in the default executor (https://github.com/apache/mesos/blob/12be4ba002f2f5ff314fbc16af51d095b0d90e56/src/launcher/default_executor.cpp#L525-L535) shows that if a `LAUNCH_NESTED_CONTAINER` call is failed (say, due to a fetcher failure), the whole executor will be shut down:
{code:cpp}
// Check if we received a 200 OK response for all the
// `LAUNCH_NESTED_CONTAINER` calls. Shutdown the executor
// if this is not the case.
foreach (const Response& response, responses.get()) {
  if (response.code != process::http::Status::OK) {
    LOG(ERROR) << ""Received '"" << response.status << ""' (""
               << response.body << "") while launching child container"";
    _shutdown();
    return;
  }
}
{code}

This is not expected by a user. Instead, one would expect that a failed `LAUNCH_GROUP` won't affect other task groups launched by the same executor, similar to the case that a task failure only takes down its own task group. We should adjust the semantics to make a failed `LAUNCH_GROUP` not take down the executor and affect other task groups.",1,3,MESOS-8468,5.0
Destroyed executors might be used after `Slave::publishResource()`.,"In the following code from [https://github.com/apache/mesos/blob/7b30b9ccd63dbcd3375e012dae6e2ffb9dc6a79f/src/slave/slave.cpp#L2652:]
{code:cpp}
publishResources()
  .then(defer(self(), [=] {
    return containerizer->update(
        executor->containerId,
        executor->allocatedResources());
  }))
{code}
A destroyed executor might be dereferenced if it has been move to {{Framework.completedExecutors}} and kicked out from this circular buffer. We should refactor {{Slave::publishResources()}} and its uses to make the code less fragile.",1,4,MESOS-8467,2.0
Clean up endpoint socket if the container daemon is destroyed while waiting.,"SLRP uses a post-stop hook to ask the container daemon to clean up the endpoint socket after its plugin container is terminated. However, if the container daemon is destructed while waiting for the container it monitors before the container itself is terminated, the socket file will remain there, making SLRP unable to recover.

There might be two solutions:
1. During SLRP recovery, check if the plugin container is still running.
2. Start the container daemon in the waiting phase.",1,2,MESOS-8429,3.0
Validation for resource provider config agent API calls.,"Currently the API returns 200 OK if the config is put in the resource provider config directory, even if the config is not valid (e.g., don't specify a controller plugin). We should consider validating the config when the call is processed.",1,1,MESOS-8425,2.0
Master's UpdateSlave handler not correctly updating terminated operations,"I created a test that verifies that operation status updates are resent to the master after being dropped en route to it (MESOS-8420).

The test does the following:

# Creates a volume from a RAW disk resource.
# Drops the first `UpdateOperationStatusMessage` message from the agent to the master, so that it isn't acknowledged by the master.
# Restarts the agent.
# Verifies that the agent resends the operation status update.

The good news are that the agent is resending the operation status update, the bad news are that it triggers a CHECK failure that crashes the master.

Here are the relevant sections of the log produced by the test:

{noformat}
[ RUN      ] StorageLocalResourceProviderTest.ROOT_RetryOperationStatusUpdateAfterRecovery
[...]
I0109 16:36:08.515882 24106 master.cpp:4284] Processing ACCEPT call for offers: [ 046b3f21-6e97-4a56-9a13-773f7d481efd-O0 ] on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681
I0109 16:36:08.516487 24106 master.cpp:5260] Processing CREATE_VOLUME operation with source disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096 from framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681 to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.518704 24106 master.cpp:10622] Sending operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.521210 24130 provider.cpp:504] Received APPLY_OPERATION event
I0109 16:36:08.521276 24130 provider.cpp:1368] Received CREATE_VOLUME operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.523131 24432 test_csi_plugin.cpp:305] CreateVolumeRequest '{""version"":{""minor"":1},""name"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""capacityRange"":{""requiredBytes"":""4294967296"",""limitBytes"":""4294967296""},""volumeCapabilities"":[{""mount"":{},""accessMode"":{""mode"":""SINGLE_NODE_WRITER""}}]}'
I0109 16:36:08.525806 24152 provider.cpp:2635] Applying conversion from 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096' to 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' for operation (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.528725 24134 status_update_manager_process.hpp:152] Received operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.529207 24134 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.573177 24150 http.cpp:1185] HTTP POST for /slave(2)/api/v1/resource_provider from 10.0.49.2:53598
I0109 16:36:08.573974 24139 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.574154 24139 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.574785 24139 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
I0109 16:36:08.583748 24084 slave.cpp:931] Agent terminating
I0109 16:36:08.584115 24144 master.cpp:1305] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) disconnected
[...]
I0109 16:36:08.655766 24140 slave.cpp:1378] Re-registered with master master@10.0.49.2:40681
I0109 16:36:08.655936 24117 task_status_update_manager.cpp:188] Resuming sending task status updates
I0109 16:36:08.655995 24149 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0109 16:36:08.656008 24140 slave.cpp:1423] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""icuAKyO6TymMt2Y9vyF6Jg==""},""slave_id"":{""value"":""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""},""update_oversubscribed_resources"":true}
I0109 16:36:08.656121 24149 hierarchical.cpp:754] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 reactivated
W0109 16:36:08.656481 24113 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: true
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
I0109 16:36:08.656637 24113 master.cpp:7320] Received update of agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(3)@10.0.49.2:40681 (core-dev) with total oversubscribed resources {}
W0109 16:36:08.657387 24113 master.cpp:7704] Performing explicit reconciliation with agent for known operation 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 since it was not present in original reconciliation message from agent
I0109 16:36:08.657917 24133 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
W0109 16:36:08.658048 24125 manager.cpp:472] Dropping operation reconciliation message with operation_uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 because resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is not subscribed
I0109 16:36:08.658609 24143 container_daemon.cpp:119] Launching container 'org-apache-mesos-rp-local-storage-test--org-apache-mesos-csi-test-slrp_test--CONTROLLER_SERVICE-NODE_SERVICE'
[...]
I0109 16:36:08.689859 24130 provider.cpp:3066] Sending UPDATE_STATE call with resources 'disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' and 1 operations to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.690449 24130 provider.cpp:1042] Resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is in READY state
I0109 16:36:08.690491 24105 status_update_manager_process.hpp:385] Resuming operation status update manager
I0109 16:36:08.690640 24105 status_update_manager_process.hpp:394] Sending operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.693244 24131 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693912 24140 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693974 24115 manager.cpp:677] Received UPDATE_STATE call with resources '[{""disk"":{""source"":{""id"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""metadata"":{""labels"":[{""key"":""path"",""value"":""\/tmp\/n5thZ3\/test\/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""}]},""mount"":{""root"":"".\/csi\/org.apache.mesos.csi.test\/slrp_test\/mounts\/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""},""profile"":""volume-default"",""type"":""MOUNT""}},""name"":""disk"",""provider_id"":{""value"":""605b22f5-e39d-4d9f-950a-e7f44d202c01""},""reservations"":[{""role"":""storage"",""type"":""DYNAMIC""}],""scalar"":{""value"":4096.0},""type"":""SCALAR""}]' and 1 operations from resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01
I0109 16:36:08.694897 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_STATE: 605b22f5-e39d-4d9f-950a-e7f44d202c01 disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096'
I0109 16:36:08.695184 24144 slave.cpp:7182] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096
I0109 16:36:08.696467 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.696594 24144 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.696666 24144 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
W0109 16:36:08.697093 24142 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: false
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
resource_providers {
  providers {
    info {
      id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      type: ""org.apache.mesos.rp.local.storage""
      name: ""test""
      default_reservations {
        role: ""storage""
        type: DYNAMIC
      }
      storage {
        plugin {
          type: ""org.apache.mesos.csi.test""
          name: ""slrp_test""
          containers {
            [...]
          }
        }
      }
    }
    total_resources {
      name: ""disk""
      type: SCALAR
      scalar {
        value: 4096
      }
      disk {
        source {
          type: MOUNT
          mount {
            root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          }
          id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          metadata {
            labels {
              key: ""path""
              value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
            }
          }
          profile: ""volume-default""
        }
      }
      provider_id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      reservations {
        role: ""storage""
        type: DYNAMIC
      }
    }
    operations {
      operations {
        framework_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-0000""
        }
        slave_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
        }
        info {
          type: CREATE_VOLUME
          create_volume {
            source {
              name: ""disk""
              type: SCALAR
              scalar {
                value: 4096
              }
              disk {
                source {
                  type: RAW
                  profile: ""volume-default""
                }
              }
              allocation_info {
                role: ""storage""
              }
              provider_id {
                value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
              }
              reservations {
                role: ""storage""
                type: DYNAMIC
              }
            }
            target_type: MOUNT
          }
        }
        latest_status {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        statuses {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        uuid {
          value: ""\030\264\304\245\321bM\317\273!\241<n\340\364\010""
        }
      }
    }
    resource_version_uuid {
      value: ""M\250\313j\320\301IG\262\0164e\004\367\304\333""
    }
  }
}
I0109 16:36:08.700137 24142 master.cpp:10411] Updating the state of operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) of framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.700417 24146 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
F0109 16:36:08.700610 24142 master.cpp:11687] CHECK_SOME(resources): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
F0109 16:36:08.700896 24146 hierarchical.cpp:908] CHECK_SOME(updatedTotal): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06b729277  (unknown)
    @     0x55c1c6f3be8a  _ZTSN6lambda12CallableOnceIFvRK6ResultIN5mesos2v117resource_provider5EventEEEE10CallableFnINS_8internal7PartialIZNK7process6FutureIS6_E7onReadyISt5_BindIFSt7_Mem_fnIMSG_FbS8_EESG_St12_PlaceholderILi1EEEEbEERKSG_OT_NSG_6PreferEEUlOSQ_S8_E_ISQ_SO_EEEEE
{noformat}

We can see that once the SLRP reregisters with the agent, the following happens:

# The agent will send an {{UpdateSlave}} message to the master including the converted resources and the {{CREATE_VOLUME}} operation with the status {{OPERATION_FINISHED}}.
# The master will update the agent's resources, including the volume created by the operation.
# The agent will resend the operation status update.
# The master will try to apply the operation and crash, because it already updated the agent's resources on step #2.",1,4,MESOS-8422,5.0
RP manager incorrectly setting framework ID leads to CHECK failure,"The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset.",1,2,MESOS-8419,1.0
CHECK failure if trying to recover nested containers but the framework checkpointing is not enabled.,"{noformat}
I0108 23:05:25.313344 31743 slave.cpp:620] Agent attributes: [  ]
I0108 23:05:25.313832 31743 slave.cpp:629] Agent hostname: vagrant-ubuntu-wily-64
I0108 23:05:25.314916 31763 task_status_update_manager.cpp:181] Pausing sending task status updates
I0108 23:05:25.323496 31766 state.cpp:66] Recovering state from '/var/lib/mesos/slave/meta'
I0108 23:05:25.323639 31766 state.cpp:724] No committed checkpointed resources found at '/var/lib/mesos/slave/meta/resources/resources.info'
I0108 23:05:25.326169 31760 task_status_update_manager.cpp:207] Recovering task status update manager
I0108 23:05:25.326954 31759 containerizer.cpp:674] Recovering containerizer
F0108 23:05:25.331529 31759 containerizer.cpp:919] CHECK_SOME(container->directory): is NONE 
*** Check failure stack trace: ***
    @     0x7f769dbc98bd  google::LogMessage::Fail()
    @     0x7f769dbc8c8e  google::LogMessage::SendToLog()
    @     0x7f769dbc958d  google::LogMessage::Flush()
    @     0x7f769dbcca08  google::LogMessageFatal::~LogMessageFatal()
    @     0x556cb4c2b937  _CheckFatal::~_CheckFatal()
    @     0x7f769c5ac653  mesos::internal::slave::MesosContainerizerProcess::recover()
{noformat}

If the framework does not enable the checkpointing. It means there is no slave state checkpointed. But containers are still checkpointed at the runtime dir, which mean recovering a nested container would cause the CHECK failure due to its parent's sandbox dir is unknown.",1,4,MESOS-8416,5.0
Use unique ID for CSI plugin containers in SLRP.,"If an agent crashed abnormally and the runtime directory is lost, then a standalone container previously launched by SLRP will be considered orphan when the agent restarts. Since the orphan containers are cleaned up asynchronously, it is possible that the cleanup is racing with SLRP launching a new standalone container instance with the same ID. To avoid this race, we should use unique IDs for CSI plugin containers.",1,2,MESOS-8399,3.0
SLRP NewVolumeRecovery and LaunchTaskRecovery tests CHECK failures.,CHECK failures manifested on the two SLRP tests after resource upgrade/downgrade is introduced.,1,1,MESOS-8393,2.0
Resource provider-capable agents not correctly synchronizing checkpointed agent resources on reregistration,"For resource provider-capable agents the master does not re-send checkpointed resources on agent reregistration; instead the checkpointed resources sent as part of the {{ReregisterSlaveMessage}} should be used.

This is not what happens in reality. If e.g., checkpointing of an offer operation fails and the agent fails over the checkpointed resources would, as expected, not be reflected in the agent, but would still be assumed in the master.

A workaround is to fail over the master which would lead to the newly elected master bootstrapping agent state from {{ReregisterSlaveMessage}}.",1,4,MESOS-8350,2.0
"When a resource provider driver is disconnected, it fails to reconnect.","If the resource provider manager closes the HTTP connection of a resource provider, the resource provider should reconnect itself. For that, the resource provider driver will change its state to ""DISCONNECTED"", call a {{disconnected}} callback and use its endpoint detector to reconnect.
This doesn't work in a testing environment where a {{ConstantEndpointDetector}} is used. While the resource provider is notified of the closed HTTP connection (and logs {{End-Of-File received}}), it never disconnects itself and calls the {{disconnected}} callback. Discarding {{HttpConnectionProcess::detection}} in {{HttpConnectionProcess::disconnected}} doesn't trigger the {{onAny}} callback of that future. This might not be a problem in {{HttpConnectionProcess}} but could be related to the test case using a {{ConstantEndpointDetector}}.",1,3,MESOS-8349,2.0
Resubscription of a resource provider will crash the agent if its HTTP connection isn't closed,"A resource provider might resubscribe while its old HTTP connection wasn't properly closed. In that case an agent will crashm with, e.g., the following log:
{noformat}
I1219 13:33:51.937295 128610304 manager.cpp:570] Subscribing resource provider {""id"":{""value"":""8e71beef-796e-4bde-9257-952ed0f230a5""},""name"":""test"",""type"":""org.apache.mesos.rp.test""}
I1219 13:33:51.937443 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5
I1219 13:33:51.937760 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5
E1219 13:33:51.937851 129683456 http_connection.hpp:445] End-Of-File received
I1219 13:33:51.937865 131293184 slave.cpp:7105] Handling resource provider message 'DISCONNECT: resource provider 8e71beef-796e-4bde-9257-952ed0f230a5'
I1219 13:33:51.937968 131293184 slave.cpp:7347] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
F1219 13:33:51.938052 132366336 manager.cpp:606] Check failed: resourceProviders.subscribed.contains(resourceProviderId) 
*** Check failure stack trace: ***
E1219 13:33:51.938583 130756608 http_connection.hpp:445] End-Of-File received
I1219 13:33:51.938987 129683456 hierarchical.cpp:669] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 (172.18.8.13) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
    @        0x1125380ef  google::LogMessageFatal::~LogMessageFatal()
    @        0x112534ae9  google::LogMessageFatal::~LogMessageFatal()
I1219 13:33:51.939131 129683456 hierarchical.cpp:1517] Performed allocation for 1 agents in 61830ns
I1219 13:33:51.945793 2646795072 slave.cpp:927] Agent terminating
I1219 13:33:51.945955 129146880 master.cpp:1305] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13) disconnected
I1219 13:33:51.945979 129146880 master.cpp:3364] Disconnecting agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)
I1219 13:33:51.946022 129146880 master.cpp:3383] Deactivating agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)
I1219 13:33:51.946081 131293184 hierarchical.cpp:766] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 deactivated
    @        0x115f2761d  mesos::internal::ResourceProviderManagerProcess::subscribe()::$_2::operator()()
    @        0x115f2977d  _ZN5cpp176invokeIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS2_14HttpConnectionERKNS1_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSG_DpOSH_
    @        0x115f29740  _ZN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEE13invoke_expandISC_NSt3__15tupleIJSG_EEENSK_IJEEEJLm0EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardIT_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEEOSN_OSO_N5cpp1416integer_sequenceImJXspT2_EEEEOSP_
    @        0x115f296bb  _ZNO6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEclIJEEEDTcl13invoke_expandclL_ZNSt3__14moveIRSC_EEONSJ_16remove_referenceIT_E4typeEOSN_EdtdefpT1fEclL_ZNSK_IRNSJ_5tupleIJSG_EEEEESQ_SR_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOSY_
    @        0x115f2965d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS5_14HttpConnectionERKNS4_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSK_DpOSL_
    @        0x115f29631  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS6_14HttpConnectionERKNS5_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEvOT_DpOT0_
    @        0x115f29526  _ZNO6lambda12CallableOnceIFvvEE10CallableFnINS_8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS7_14HttpConnectionERKNS6_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEEclEv
    @        0x10b6ca690  _ZNO6lambda12CallableOnceIFvvEEclEv
    @        0x10be09295  _ZZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS_4UPIDEOT_ENKUlOS7_PNS_11ProcessBaseEE_clESD_SF_
    @        0x10be09180  _ZN5cpp176invokeIZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS1_4UPIDEOT_EUlOS9_PNS1_11ProcessBaseEE_JS9_SH_EEEDTclclsr3stdE7forwardISD_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESE_DpOSJ_
    @        0x10be0912b  _ZN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEE13invoke_expandISI_NSJ_5tupleIJS9_SM_EEENSP_IJOSH_EEEJLm0ELm1EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardISD_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEESE_OST_N5cpp1416integer_sequenceImJXspT2_EEEEOSU_
    @        0x10be0905f  _ZNO6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEEclIJSH_EEEDTcl13invoke_expandclL_ZNSJ_4moveIRSI_EEONSJ_16remove_referenceISD_E4typeESE_EdtdefpT1fEclL_ZNSP_IRNSJ_5tupleIJS9_SM_EEEEESU_SE_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOS11_
    @        0x10be08f4d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS1_12CallableOnceIFvvEEEEEvRKNS4_4UPIDEOT_EUlOSB_PNS4_11ProcessBaseEE_JSB_NSt3__112placeholders4__phILi1EEEEEEJSJ_EEEDTclclsr3stdE7forwardISF_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESG_DpOSQ_
    @        0x10be08f11  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS5_4UPIDEOT_EUlOSC_PNS5_11ProcessBaseEE_JSC_NSt3__112placeholders4__phILi1EEEEEEJSK_EEEvSH_DpOT0_
    @        0x10be08d36  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchIvEclINS0_IFvvEEEEEvRKNS1_4UPIDEOT_EUlOSE_S3_E_JSE_NSt3__112placeholders4__phILi1EEEEEEEclEOS3_
    @        0x11fd64bc9  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3_
    @        0x11fd64a69  process::ProcessBase::consume()
    @        0x11fe20ac4  _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE
    @        0x113c77819  process::ProcessBase::serve()
    @        0x11fd5b8c9  process::ProcessManager::resume()
    @        0x11fe8260b  process::ProcessManager::init_threads()::$_1::operator()()
    @        0x11fe82190  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_1EEEEEPvSB_
    @     0x7fff64da56c1  _pthread_body
    @     0x7fff64da556d  _pthread_start
    @     0x7fff64da4c5d  thread_start
Abort trap: 6
{noformat}

This is due to a race condition in {{resource_provider/manager.cpp}} when handling closed HTTP connections of resource providers. If a resource provider resubscribes and its old HTTP connection is still open, the resource provider manager will close it. This is unexpected and will trigger closing the new HTTP connection which results in a failed {{CHECK}}.",1,4,MESOS-8346,2.0
Mesos containerizer does not properly handle old running containers,"We were testing an upgrade scenario recently and encountered the following assertion failure:
{code}
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.693977 20810 http.cpp:3116] Processing LAUNCH_NESTED_CONTAINER_SESSION call for container 'a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695179 20807 containerizer.cpp:1169] Trying to chown '/var/lib/mesos/slave/slaves/aaf0a62f-a6eb-4c1d-80db-5fdd26fe8008-S12/frameworks/dcf5f8b5-86a8-44df-ac03-b39404239ad8-0377/executors/kafka__68baefd4-aa8c-4b97-a23e-eb6a73fa91f6/runs/a89b211a-4549-462d-9cc7-0ea2bac2f729/containers/1c262420-7525-4fee-99c1-aff4f66996bd/containers/check-a41362ae-13c6-4750-990e-a1a0b2792b5f' to user 'nobody'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: W1212 16:45:42.695309 20807 containerizer.cpp:1198] Cannot determine executor_info for root container 'a89b211a-4549-462d-9cc7-0ea2bac2f729' which has no config recovered.
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695327 20807 containerizer.cpp:1203] Starting container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695829 20807 containerizer.cpp:2932] Transitioning the state of container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f from PROVISIONING to PREPARING
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.700569 20811 systemd.cpp:98] Assigned child process '20941' to 'mesos_executors.slice'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.702945 20811 systemd.cpp:98] Assigned child process '20942' to 'mesos_executors.slice'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.706069 20806 switchboard.cpp:575] Created I/O switchboard server (pid: 20943) listening on socket file '/tmp/mesos-io-switchboard-74af71bb-2385-4dde-9762-94d0196124d3' for container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: mesos-agent: /pkg/src/mesos/3rdparty/stout/include/stout/option.hpp:115: T& Option<T>::get() & [with T = mesos::slave::ContainerConfig]: Assertion `isSome()' failed.
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** Aborted at 1513097142 (unix time) try ""date -d @1513097142"" if you are using GNU date ***
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: PC: @     0x7f472f2851f7 __GI_raise
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** SIGABRT (@0x5134) received by PID 20788 (TID 0x7f472a2bf700) from PID 20788; stack trace: ***
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f6225e0 (unknown)
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2851f7 __GI_raise
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2868e8 __GI_abort
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e266 __assert_fail_base
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e312 __GI___assert_fail
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c481e3 _ZNR6OptionIN5mesos5slave15ContainerConfigEE3getEv.part.170
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c61c2d mesos::internal::slave::MesosContainerizerProcess::_launch()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f403 _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENS5_25MesosContainerizerProcessERKNS3_11ContainerIDERK6OptionINS3_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSC_ISsESB_SH_SR_SU_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMSZ_FSX_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseIS7_EESt14default_deleteIS1J_EEOS9_OSF_OSP_OSS_PNS1_11ProcessBaseEE_IS1M_S9_SF_SP_SS_S1S_EEEDTclcl7forwardISW_Efp_Espcl7forwardIT0_Efp0_EEEOSW_DpOS1U_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f4f1 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENSC_25MesosContainerizerProcessERKNSA_11ContainerIDERK6OptionINSA_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSJ_ISsESI_SO_SY_S11_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMS16_FS14_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseISE_EESt14default_deleteIS1Q_EEOSG_OSM_OSW_OSZ_S3_E_IS1T_SG_SM_SW_SZ_St12_PlaceholderILi1EEEEEEclEOS3_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325dbb31 process::ProcessBase::consume()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325ea882 process::ProcessManager::resume()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325efcf6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472fafa230 (unknown)
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f61ae25 start_thread
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f34834d __clone
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service: main process exited, code=killed, status=6/ABRT
Dec 12 16:45:42 agent.hostname systemd[1]: Unit dcos-mesos-slave.service entered failed state.
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service failed.
{code}

Looking into {{Slave::_launch}}, indeed we find an unguarded access to the parent container's {{ContainerConfig}} [here|https://github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp#L1716].

We recently [added checkpointing|https://github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545] of {{ContainerConfig}} to the Mesos containerizer. It seems that we are not appropriately handling upgrades, when there may be old containers running for which we do not expect to recover a {{ContainerConfig}}.",1,2,MESOS-8325,2.0
Pass resource provider information to master as part of UpdateSlaveMessage,"We extended {{UpdateSlaveMessage}} so updates to an agent's total resources from resource providers are possible. We realized that will need to explicitly pass resource provider details (here for now: {{ResourceProviderInfo}}) to the master so it can be queried for the providers present on certain agents. This should happen as part of {{UpdateSlaveMessage}} so a single synchronization channel is used for this kind of information.

We need to adjust {{UpdateSlaveMessage}} for these requirements. This should happen before 1.5.0 gets released so we do not need to deprecate a never really used message format.",1,2,MESOS-8312,5.0
Mesos Containerizer GC should set 'layers' after checkpointing layer ids in provisioner.,"{noformat}
11111
222222
333333
444444
11111
222222
333333
444444
I1129 23:24:45.469543  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e/rootfs.overlay'
I1129 23:24:45.473287  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:b56ae66c29370df48e7377c8f9baa744a3958058a766793f821dadcb144a4647 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3/rootfs.overlay'
I1129 23:24:45.582002  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs.overlay'
I1129 23:24:45.589404  6595 metadata_manager.cpp:167] Successfully cached image 'alpine'
I1129 23:24:45.590204  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs.overlay'
I1129 23:24:45.595190  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs.overlay'
I1129 23:24:45.599500  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs.overlay'
I1129 23:24:45.602047  6597 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 using overlay backend
I1129 23:24:45.602751  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs.overlay'
I1129 23:24:45.603054  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links' -> '/tmp/xAWQ8y'
I1129 23:24:45.604398  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/xAWQ8y/1:/tmp/xAWQ8y/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/workdir'
I1129 23:24:45.607802  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs.overlay'
I1129 23:24:45.612139  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs.overlay'
I1129 23:24:45.612253  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/config'
I1129 23:24:45.612298  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PROVISIONING to PREPARING
I1129 23:24:45.625658  6596 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 1""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""12"" --pipe_write=""15"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5"" --unshare_namespace_mnt=""false""'
I1129 23:24:45.626317  6598 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 and cloning with namespaces CLONE_NEWNS
I1129 23:24:45.633211  6598 systemd.cpp:96] Assigned child process '6745' to 'mesos_executors.slice'
I1129 23:24:45.636270  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PREPARING to ISOLATING
I1129 23:24:45.691830  6597 metadata_manager.cpp:167] Successfully cached image 'mesosphere/inky'
I1129 23:24:45.694399  6594 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/rootfses/1187cc83-a23a-4390-9c28-092a7b7690b5' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 using overlay backend
I1129 23:24:45.694919  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/links' -> '/tmp/GXhXiT'
I1129 23:24:45.695103  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/GXhXiT/6:/tmp/GXhXiT/5:/tmp/GXhXiT/4:/tmp/GXhXiT/3:/tmp/GXhXiT/2:/tmp/GXhXiT/1:/tmp/GXhXiT/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/workdir'
I1129 23:24:45.696255  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.696349  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.696449  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/config'
I1129 23:24:45.696506  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PROVISIONING to PREPARING
I1129 23:24:45.697865  6595 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.697918  6595 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.697968  6595 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.697999  6595 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.698025  6595 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.698050  6595 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.698076  6595 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.698104  6595 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.698129  6595 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.698894  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.698966  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.700333  6596 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.700394  6596 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.700412  6596 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.700428  6596 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.700441  6596 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.700454  6596 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.700467  6596 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.700480  6596 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.700495  6596 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.702491  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.702554  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.703707  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.703783  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.703812  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.703816  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.703816  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.704164  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.704208  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.704255  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.704285  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.704814  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.704861  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.708112  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.708204  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.708238  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.708264  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.708375  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.708407  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.708472  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.708514  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.708545  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.709048  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.709161  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.710321  6594 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.710382  6594 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.710412  6594 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.710436  6594 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.710458  6594 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.710480  6594 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.710522  6594 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.710551  6594 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.710590  6594 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.713176  6594 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 100000""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""HOME"",""type"":""VALUE"",""value"":""\/""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""13"" --pipe_write=""14"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3"" --unshare_namespace_mnt=""false""'
I1129 23:24:45.713954  6597 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 and cloning with namespaces CLONE_NEWNS
I1129 23:24:45.721781  6597 systemd.cpp:96] Assigned child process '6775' to 'mesos_executors.slice'
I1129 23:24:45.725494  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PREPARING to ISOLATING
I1129 23:24:45.791635  6595 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from ISOLATING to FETCHING
I1129 23:24:45.791880  6591 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:45.792626  6591 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from FETCHING to RUNNING
11111
222222
333333
444444
I1129 23:24:45.807262  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.807375  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.808658  6591 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.808843  6591 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.808869  6591 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.808897  6591 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.808962  6591 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.808990  6591 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.809012  6591 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.809036  6591 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.809057  6591 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.893280  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from ISOLATING to FETCHING
I1129 23:24:45.893523  6596 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3
I1129 23:24:45.894335  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from FETCHING to RUNNING
I1129 23:24:45.902606  6598 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:45.903908  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I1129 23:24:45.904618  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I1129 23:24:45.908681  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57620
I1129 23:24:45.909113  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57622
I1129 23:24:45.909708  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5'
I1129 23:24:45.910148  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3'
I1129 23:24:45.938350  6596 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:45.938781  6596 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:45.939141  6596 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.940809  6591 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:45.941130  6591 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.942906  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943076  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:45.943322  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:45.943332  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943506  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:45.943717  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943905  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:45.943905  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.992866  6591 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.993261  6595 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.993968  6593 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.994295  6598 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
11111
222222
333333
444444
I1129 23:24:46.808684  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:46.808876  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:46.810683  6593 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:46.810751  6593 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:46.810781  6593 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:46.810808  6593 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:46.810834  6593 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:46.810860  6593 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:46.810885  6593 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:46.810911  6593 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:46.810937  6593 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:47.057590  6596 containerizer.cpp:2775] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has exited
I1129 23:24:47.057667  6596 containerizer.cpp:2324] Destroying container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 in RUNNING state
I1129 23:24:47.057695  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from RUNNING to DESTROYING
I1129 23:24:47.058082  6596 linux_launcher.cpp:514] Asked to destroy container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.059027  6596 linux_launcher.cpp:560] Using freezer to destroy cgroup mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.060667  6596 cgroups.cpp:3058] Freezing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.062700  6597 cgroups.cpp:1413] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.838336ms
I1129 23:24:47.064627  6592 cgroups.cpp:3076] Thawing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.066498  6598 cgroups.cpp:1442] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.642752ms
I1129 23:24:47.071521  6592 provisioner.cpp:648] Destroying container rootfs at '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.098203  6596 overlay.cpp:296] Removed temporary directory '/tmp/xAWQ8y' pointed by '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links'
I1129 23:24:47.100265  6591 containerizer.cpp:2613] Checkpointing termination state to nested container's runtime directory '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/termination'
I1129 23:24:47.107206  6594 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:47.151911  6594 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:47.152243  6594 slave.cpp:4584] Handling status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.154391  6594 task_status_update_manager.cpp:328] Received task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.154578  6594 task_status_update_manager.cpp:383] Forwarding task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:47.154810  6593 slave.cpp:5067] Forwarding the update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:47.157249  6593 slave.cpp:4960] Task status update manager successfully handled status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.385977  6592 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.386334  6592 task_status_update_manager.cpp:538] Cleaning up status update stream for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.387459  6592 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.387568  6592 slave.cpp:8457] Completing task 1
11111
222222
333333
444444
I1129 23:24:47.818768  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:47.824591  6598 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:47.824724  6598 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:47.824753  6598 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:47.824767  6598 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:47.824782  6598 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:47.824861  6598 store.cpp:550] Marking layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' to gc by renaming '/tmp/mesos/store/docker/layers/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' to '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
I1129 23:24:47.824918  6598 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:47.824960  6598 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:47.825088  6598 store.cpp:550] Marking layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' to gc by renaming '/tmp/mesos/store/docker/layers/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' to '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.825389  6598 store.cpp:577] Deleting path '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.829903  6598 store.cpp:584] Deleted '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.829980  6598 store.cpp:577] Deleting path '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
I1129 23:24:47.830047  6598 store.cpp:584] Deleted '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
11111
222222
333333
444444
I1129 23:24:48.829519  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:48.831161  6598 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:48.831225  6598 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:48.831248  6598 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:48.831266  6598 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:48.831284  6598 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:48.831302  6598 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:48.831321  6598 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
11111
222222
333333
444444
I1129 23:24:49.830904  6597 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:49.832487  6597 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:49.832584  6597 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:49.833329  6597 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:49.833367  6597 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:49.833387  6597 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:49.833406  6597 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:49.833425  6597 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
{noformat}

Please neglect the debugging logs like '111111'. To reproduce this issue, just continuously trigger image gc. The log above was from a scenario that we launch two nested containers. One sleeps 1 second, another sleep forever.

This is related to this patch: https://github.com/apache/mesos/commit/e273efe6976434858edb85bbcf367a02e963a467#diff-a3593ed0ebd2b205775f7f04d9b5afe7

The root cause is that we did not set the 'layers' after we checkpoint the layer ids in provisioner. The log below is the prove:
{noformat}
I1129 23:24:45.698894  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.698966  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
{noformat}",1,3,MESOS-8280,3.0
Support image prune in mesos containerizer and provisioner.,"Implement image prune in containerizer and the provisioner, by using mark and sweep to garbage collect unused layers.",1,1,MESOS-8249,13.0
Unified Containerizer Auto backend should check xfs ftype for overlayfs backend.,"when using xfs as the backing filesystem in unified containerizer, the `ftype` has to be equal to 1 if we are using the overlay fs backend. we should add the detection in auto backend logic because some OS (like centos 7.2) has xfs ftype=0 by default.

https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/",1,3,MESOS-8121,3.0
v1 role-related endpoints need to reflect hierarchical accounting.,"With the introduction of hierarchical roles, the role-related endpoints need to be updated to provide aggregated accounting information.

For example, information about how many resources are allocated to ""/eng"" should include the resources allocated to ""/eng/frontend"" and ""/eng/backend"", since quota guarantees and limits are also applied on the aggregation.

This also affects the UI display, for example the 'Roles' tab.",1,4,MESOS-8069,8.0
Agent and master can race when updating agent state.,"In {{2af9a5b07dc80151154264e974d03f56a1c25838}} we introduce the use of {{UpdateSlaveMessage}} for the agent to inform the master about its current total resources. Currently we trigger this message only on agent registration and reregistration.

This can race with operations applied in the master and communicated via {{CheckpointResourcesMessage}}.

Example:

1. Agent ({{cpus:4(\*)}} registers.
2. Master is triggered to apply an operation to the agent's resources, e.g., a reservation: {{cpus:4(\*) -> cpus:4(A)}}. The master applies the operation to its current view of the agent's resources and sends the agent a {{CheckpointResourcesMessage}} so the agent can persist the result.
3. The agent sends the master an {{UpdateSlaveMessage}}, e.g., {{cpus:4(\*)}} since it hasn't received the {{CheckpointResourcesMessage}} yet.
4. The master processes the {{UpdateSlaveMessage}} and updates its view of the agent's resources to be {{cpus:4(\*)}}.
5. The agent processes the {{CheckpointResourcesMessage}} and updates its view of its resources to be {{cpus:4(A)}}.
6. The agent and the master have an inconsistent view of the agent's resources.",1,5,MESOS-8058,2.0
OOM due to LibeventSSLSocket send incorrectly returning 0 after shutdown.,"LibeventSSLSocket can return 0 from send incorrectly, which leads the caller to send the data twice!

See here: https://github.com/apache/mesos/blob/1.3.1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L396-L398

In some particular cases, it's possible that the caller keeps getting back 0 and loops infinitely, blowing up the memory and OOMing the process.

One example is when a send occurs after a shutdown:

{code}
TEST_F(SSLTest, ShutdownThenSend)
{
  Clock::pause();

  Try<Socket> server = setup_server({
      {""LIBPROCESS_SSL_ENABLED"", ""true""},
      {""LIBPROCESS_SSL_KEY_FILE"", key_path().string()},
      {""LIBPROCESS_SSL_CERT_FILE"", certificate_path().string()}});

  ASSERT_SOME(server);
  ASSERT_SOME(server.get().address());
  ASSERT_SOME(server.get().address().get().hostname());

  Future<Socket> socket = server.get().accept();

  Clock::settle();
  EXPECT_TRUE(socket.isPending());

  Try<Socket> client = Socket::create(SocketImpl::Kind::SSL);
  ASSERT_SOME(client);
  AWAIT_ASSERT_READY(client->connect(server->address().get()));

  AWAIT_ASSERT_READY(socket);

  EXPECT_SOME(Socket(socket.get()).shutdown());

  // This loops forever!
  AWAIT_FAILED(Socket(socket.get()).send(""Hello World""));
}
{code}",1,2,MESOS-7934,5.0
Fix communication between old masters and new agents.,"For re-registration, agents currently send the resources in tasks
and executors to the master in the ""post-reservation-refinement"" format,
which is incompatible for pre-1.4 masters. We should change the agent
such that it always downgrades the resources to
the ""pre-reservation-refinement"" format, and the master unconditionally
upgrade the resources to ""post-reservation-refinement"" format.",1,2,MESOS-7922,2.0
Non-checkpointing framework's tasks should not be marked LOST when agent disconnects.,"Currently, when framework with checkpointing disabled has tasks running on an agent and that agent disconnects from the master, the master will mark those tasks LOST and remove them from its memory. The assumption is that the agent is disconnecting because it terminated.

However, it's possible that this disconnection occurred due to a transient loss of connectivity and the agent re-connects while never having terminated. This case violates our assumption of there being no unknown tasks to the master:

```
 void Master::reconcileKnownSlave(
 Slave* slave,
 const vector<ExecutorInfo>& executors,
 const vector<Task>& tasks)
 {
 ...

// TODO(bmahler): There's an implicit assumption here the slave
 // cannot have tasks unknown to the master. This _should_ be the
 // case since the causal relationship is:
 // slave removes task -> master removes task
 // Add error logging for any violations of this assumption!
 ```

As a result, the tasks would remain on the agent but the master would not know about them!

A more appropriate action here would be:

# When an agent disconnects, mark the tasks as unreachable.
## If the framework is not partition aware, only show it the last known task state.
## If the framework is partition aware, let it know that it's now unreachable.
# If the agent re-connects:
## And the agent had restarted, let the non-checkpointing framework know its tasks are GONE/LOST.
## If the agent still holds the tasks, the tasks are restored as reachable.
# If the agent gets removed:
## For partition aware non-checkpointing frameworks, let them know the tasks are unreachable.
## For non partition aware non-checkpointing frameworks, let them know the tasks are lost and kill them if the agent comes back.",0,0,MESOS-7911,5.0
Mesos master rescinds all the in-flight offers from all the registered agents when a new maintenance schedule is posted for a subset of slaves,"We are running mesos 1.1.0 in production. We use a custom autoscaler for scaling our mesos  cluster up and down. While scaling down the cluster, autoscaler makes a POST request to mesos master /maintenance/schedule endpoint with a set of slaves to move to maintenance mode. This forces mesos master to rescind all the in-flight offers from *all the slaves* in the cluster. If our scheduler accepts one of these offers, then we get a TASK_LOST status update back for that task. We also see such (https://gist.github.com/sagar8192/8858e7cb59a23e8e1762a27571824118) log lines in mesos master logs.

After reading the code(refs: https://github.com/apache/mesos/blob/master/src/master/master.cpp#L6772), it appears that offers are getting rescinded for all the slaves. I am not sure what is the expected behavior here, but it makes more sense if only resources from slaves marked for maintenance are reclaimed.

*Experiment:*
To verify that it is actually happening, I checked out the master branch(sha: a31dd52ab71d2a529b55cd9111ec54acf7550ded ) and added some log lines(https://gist.github.com/sagar8192/42ca055720549c5ff3067b1e6c7c68b3). Built the binary and started a mesos master and 2 agent processes. Used a basic python framework that launches docker containers on these slaves. Verified that there is no existing schedule for any slaves using `curl 10.40.19.239:5050/maintenance/status`. Posted maintenance schedule for one of the slaves(https://gist.github.com/sagar8192/fb65170240dd32a53f27e6985c549df0) after starting the mesos framework.

*Logs:*
mesos-master: https://gist.github.com/sagar8192/91888419fdf8284e33ebd58351131203
mesos-slave1: https://gist.github.com/sagar8192/3a83364b1f5ffc63902a80c728647f31
mesos-slave2: https://gist.github.com/sagar8192/1b341ef2271dde11d276974a27109426
Mesos framework: https://gist.github.com/sagar8192/bcd4b37dba03bde0a942b5b972004e8a

I think mesos should rescind offers and inverse offers only for those slaves that are marked for maintenance(draining mode).",1,3,MESOS-7882,3.0
Master stores old resource format in the registry,"We intend for the master to store all internal resource representations in the new, post-reservation-refinement format. However, [when persisting registered agents to the registrar|https://github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp#L5861-L5876], the master does not convert the resources; agents provide resources in the pre-reservation-refinement format, and these resources are stored as-is. This means that after recovery, any agents in the master's {{slaves.recovered}} map will have {{SlaveInfo.resources}} in the pre-reservation-refinement format.

We should update the master to convert these resources before persisting them to the registry.",1,4,MESOS-7851,3.0
Sandbox_path volume does not have ownership set correctly.,"This issue was exposed when using sandbox_path volume to support shared volume for nested containers under one task group. Here is a scenario:

The agent process runs as 'root' user, while the framework user is set as 'nobody'. No matter the commandinfo user is set or not, any non-root user cannot access the sandbox_path volume (e.g., a PARENT sandbox_path volume is not writable from a nested container). This is because the source path at the parent sandbox level is created by the agent process (aka root in this case). 

While the operator is responsible for guaranteeing a nested container should have permission to write to its sandbox path volume at its parent's sandbox, we should guarantee the source path created at parent's sandbox should be set as the same ownership as this sandbox's ownership.",1,2,MESOS-7830,3.0
Current approach to parse protobuf enum from JSON does not support upgrades,"To use protobuf enum in a backwards compatible way, [the suggestion on the protobuf mailing list|https://groups.google.com/forum/#!msg/protobuf/NhUjBfDyGmY/pf294zMi2bIJ] is to use optional enum fields and include an UNKNOWN value as the first entry in the enum list (and/or explicitly specifying it as the default). This can handle the case of parsing protobuf message from a serialized string, but it can not handle the case of parsing protobuf message from JSON.

E.g., when I access master endpoint with an inexistent enum {{xxx}}, I will get an error:
{code}
$ curl -X POST -H ""Content-Type: application/json"" -d '{""type"": ""xxx""}' 127.0.0.1:5050/api/v1
Failed to convert JSON into Call protobuf: Failed to find enum for 'xxx'% 
{code}

In the {{Call}} protobuf message, the enum {{Type}} already has a default value {{UNKNOWN}} (see [here|https://github.com/apache/mesos/blob/1.3.0/include/mesos/v1/master/master.proto#L45] for details) and the field {{Call.type}} is optional, but the above curl command will still fail. The root cause is, in the code [here|https://github.com/apache/mesos/blob/1.3.0/3rdparty/stout/include/stout/protobuf.hpp#L449:L454] when we try to get the enum value for the string ""xxx"", it will fail since there is no any enum value corresponding to ""xxx"".",1,7,MESOS-7828,3.0
mesos-execute has incorrect example TaskInfo in help string,"{{mesos-execute}} documents that a task can be defined via JSON as
{noformat}
{
  ""name"": ""Name of the task"",
  ""task_id"": {""value"" : ""Id of the task""},
  ""agent_id"": {""value"" : """"},
  ""resources"": [
    {
      ""name"": ""cpus"",
      ""type"": ""SCALAR"",
      ""scalar"": {
        ""value"": 0.1
      },
      ""role"": ""*""
    },
    {
      ""name"": ""mem"",
      ""type"": ""SCALAR"",
      ""scalar"": {
        ""value"": 32
      },
      ""role"": ""*""
    }
  ],
  ""command"": {
    ""value"": ""sleep 1000""
  }
}
{noformat}

If one actually uses that example task definition one gets
{noformat}
% ./build/src/mesos-execute --master=127.0.0.1:5050 --task=task.json
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0719 17:08:17.909696 3291313088 parse.hpp:114] Specifying an absolute filename to read a command line option out of without using 'file:// is deprecated and will be removed in a future release. Simply adding 'file://' to the beginning of the path should eliminate this warning.
[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0
I0719 17:08:17.919190 119246848 scheduler.cpp:184] Version: 1.4.0
I0719 17:08:17.923991 119783424 scheduler.cpp:470] New master detected at master@127.0.0.1:5050
Subscribed with ID bb0d36b4-fee0-4412-9cd9-1fa4e330355c-0000
F0719 17:08:18.137984 119783424 resources.cpp:1081] Check failed: !resource.has_role()
*** Check failure stack trace: ***
    @        0x101d65f5f  google::LogMessageFatal::~LogMessageFatal()
    @        0x101d62609  google::LogMessageFatal::~LogMessageFatal()
    @        0x1016ef3a3  mesos::v1::Resources::isEmpty()
    @        0x1016ed267  mesos::v1::Resources::add()
    @        0x1016f05af  mesos::v1::Resources::operator+=()
    @        0x1016f08fb  mesos::v1::Resources::Resources()
    @        0x100c0d89f  CommandScheduler::offers()
    @        0x100c085e4  CommandScheduler::received()
    @        0x100c0ae06  _ZZN7process8dispatchI16CommandSchedulerNSt3__15queueIN5mesos2v19scheduler5EventENS2_5dequeIS7_NS2_9allocatorIS7_EEEEEESC_EEvRKNS_3PIDIT_EEMSE_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESN_
    @        0x101ce5a21  process::ProcessBase::visit()
    @        0x101ce3747  process::ProcessManager::resume()
    @        0x101d0e243  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_0EEEEEPvSB_
    @     0x7fffbb5d693b  _pthread_body
    @     0x7fffbb5d6887  _pthread_start
    @     0x7fffbb5d608d  thread_start
[1]    73521 abort      ./build/src/mesos-execute --master=127.0.0.1:5050 --task=task.json
{noformat}

Removing the resource role field allows the task to execute.",1,2,MESOS-7805,1.0
fs::list drops path components on Windows,"fs::list(/foo/bar/*.txt) returns a.txt, b.txt, not /foo/bar/a.txt, /foo/bar/b.txt

This breaks a ZooKeeper test on Windows.",1,2,MESOS-7803,2.0
Copy-n-paste error in slave/main.cpp,"Coverity diagnosed a copy-n-paste error in {{slave/main.cpp}} (https://scan5.coverity.com/reports.htm#v10074/p10429/fileInstanceId=120155401&defectInstanceId=33592186&mergedDefectId=1414687+1+Comment),

{noformat}
323  } else if (flags.ip6.isSome()) {
CID 1414687 (#1 of 1): Copy-paste error (COPY_PASTE_ERROR)
copy_paste_error: ip in flags.ip looks like a copy-paste error.
   	Should it say ip6 instead?
324    os::setenv(""LIBPROCESS_IP6"", flags.ip.get());
325  }
{noformat}

We check the incorrect IP for some value here (check on {{ip6}}, but use of {{ip}}), and it seems extremely likely we intended to use {{flags.ip6}}.",1,2,MESOS-7772,1.0
Persistent volume might not be mounted if there is a sandbox volume whose source is the same as the target of the persistent volume.,"This issue is only for Mesos Containerizer.

If the source of a sandbox volume is a relative path, we'll create the directory in the sandbox in Isolator::prepare method:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L480-L485

And then, we'll try to mount persistent volumes. However, because of this TODO in the code:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L726-L739

We'll skip mounting the persistent volume. That will cause a silent failure.

This is important because the workaround we suggest folks to solve MESOS-4016 is to use an additional sandbox volume.",1,2,MESOS-7770,3.0
libprocess initializes to bind to random port if --ip is not specified,"When running current [HEAD|https://github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25],

{noformat:title=without --ip}
./mesos-master.sh --work_dir=/tmp/mesos-test1
...
I0707 14:14:05.927870  5820 master.cpp:438] Master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 (<host>) started on <addr>:36839
{noformat}

{noformat:title=with --ip}
./mesos-master.sh --ip=<addr> --work_dir=/tmp/mesos-test1
I0707 14:09:56.851483  5729 master.cpp:438] Master 963e0f42-9767-4629-8e3d-02c6ab6ad225 (<host>) started on <addr>:5050
{noformat}

It would be great this is caught by tests/CI.",1,3,MESOS-7769,1.0
MasterTest.KillUnknownTask is failling due to a bug in `net::IPv4::ANY()`,"Seeing the following failure when running `MasterTest.KillUnknownTask`:
```
I0706 14:08:20.724071 25596 sched.cpp:1041] Scheduler::statusUpdate took 19411ns
[libprotobuf FATAL google/protobuf/message_lite.cc:294] CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.value
libprocess: scheduler-5cca230e-e4c9-466e-b2cd-bde7b7d7ed71@127.0.0.1:44650 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.valueI0706 14:08:20.724196 25570 sched.cpp:2021] Asked to stop the driver
```

Looks we introduced a bug when we create the `net::IPv4` class. The `ANY` method of this class returns `INADDR_LOOPBACK` instead of `INADDR_ANY`. This ends up causing weird issues in terms of connectivity. We need to fix `net::IPv4::ANY` to return `INADDR_ANY`.",1,1,MESOS-7765,1.0
Master's agent removal rate limit also applies to agent unreachability.,"Currently, the implementation of partition awareness re-uses the {{--agent_removal_rate_limit}} when marking agents as unreachable. This means that partition aware frameworks are exposed to the agent removal rate limit, when they rather would like to see the information immediately and impose their own rate limiting.

Rather than waiting for non-partition-aware support to be removed (that may not occur for a long time) per MESOS-5948, we should instead fix the implementation so that unreachability does not get gated behind the agent removal rate limiting.

Marking this as a bug since from the user's perspective it doesn't behave as expected, there should be a separate flag for rate limiting unreachability marking, but likely unreachability marking does not need rate limiting, since the intention was for frameworks to impose their own rate limiting for replacing tasks.",0,0,MESOS-7721,3.0
Test that master rejects requests to create refined reservations on a non-capable agent.,"This test was done manually for now, but we should write a test for it. Similar to {{CreateOperationValidationTest.AgentHierarchicalRoleCapability}}.",0,0,MESOS-7715,2.0
Fix agent downgrade for reservation refinement,"The agent code only partially supports downgrading of an agent correctly.
The checkpointed resources are done correctly, but the resources within
the {{SlaveInfo}} message as well as tasks and executors also need to be downgraded
correctly and converted back on recovery.",1,13,MESOS-7714,8.0
Prevent non-RESERVATION_REFINEMENT frameworks from refining reservations.,"We output the ""endpoint"" format through the endpoints
for backward compatibility of external tooling. A framework should be
able to use the result of an endpoint and pass it back to Mesos,
since the result was produced by Mesos. This is especially applicable
to the V1 API. We also allow the ""pre-reservation-refinement"" format
because existing ""resources files"" are written in that format, and
they should still be usable without modification.

This is probably too flexible however, since a framework without
a RESERVATION_REFINEMENT capability could make refined reservations
using the ""post-reservation-refinement"" format, although they wouldn't be
offered such resources. It still seems undesirable if anyone were to
run into it, and we should consider adding sensible restrictions.",0,6,MESOS-7705,3.0
Docker image with universal containerizer does not work if WORKDIR is missing in the rootfs.,"hello,
used the following docker image recently

quay.io/spinnaker/front50:master
https://quay.io/repository/spinnaker/front50

Here the link to the Dockerfile
https://github.com/spinnaker/front50/blob/master/Dockerfile

and here the source
{color:blue}FROM java:8

MAINTAINER delivery-engineering@netflix.com

COPY . workdir/

WORKDIR workdir

RUN GRADLE_USER_HOME=cache ./gradlew buildDeb -x test && \
  dpkg -i ./front50-web/build/distributions/*.deb && \
  cd .. && \
  rm -rf workdir

CMD [""/opt/front50/bin/front50""]{color}


The image works fine with the docker containerizer, but the universal containerizer shows the following in stderr.

""Failed to chdir into current working directory '/workdir': No such file or directory""

The problem comes from the fact that the Dockerfile creates a workdir but then later removes the created dir as part of a RUN. The docker containerizer has no problem with it if you do

docker run -ti --rm quay.io/spinnaker/front50:master bash

you get into the working dir, but the universal containerizer fails with the error.

thanks for your help,
Michael",1,6,MESOS-7652,3.0
Introduce a heartbeat mechanism for v1 HTTP executor <-> agent communication.,"Currently, we do not have heartbeats for executor <-> agent communication. This is especially problematic in scenarios when IPFilters are enabled since the default conntrack keep alive timeout is 5 days. When that timeout elapses, the executor doesn't get notified via a socket disconnection when the agent process restarts. The executor would then get killed if it doesn't re-register when the agent recovery process is completed.

Enabling application level heartbeats or TCP KeepAlive's can be a possible way for fixing this issue.

We should also update executor API documentation to explain the new behavior.",1,6,MESOS-7564,5.0
Command checks via agent lead to flaky tests.,Tests that rely on command checks via agent are flaky on Apache CI. Here is an example from one of the failed run: https://pastebin.com/g2mPgYzu,1,7,MESOS-7500,8.0
Provisioner recover should not always assume 'rootfses' dir exists.,"The mesos agent would restart due to many reasons (e.g., disk full). Always assume the provisioner 'rootfses' dir exists would block the agent to recover.

{noformat}
Failed to perform recovery: Collect failed: Unable to list rootfses belonged to container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847: Unable to list the backend directory: Failed to opendir '/var/lib/mesos/slave/provisioner/containers/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/backends/overlay/rootfses': No such file or directory
{noformat}

This issue may occur due to the race between removing the provisioner container dir and the agent restarts:
{noformat}
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.058349 11441 linux_launcher.cpp:429] Launching container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.072191 11441 systemd.cpp:96] Assigned child process '11577' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.075932 11439 containerizer.cpp:1592] Checkpointing container's forked pid 11577 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008/executors/node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05/runs/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.081516 11438 linux_launcher.cpp:429] Launching container 03a57a37-eede-46ec-8420-dda3cc54e2e0 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.083516 11438 systemd.cpp:96] Assigned child process '11579' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.087345 11444 containerizer.cpp:1592] Checkpointing container's forked pid 11579 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/36a25adb-4ea2-49d3-a195-448cff1dc146-0002/executors/66897/runs/03a57a37-eede-46ec-8420-dda3cc54e2e0/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: W0505 02:14:32.213049 11440 fetcher.cpp:896] Begin fetcher log (stderr in sandbox) for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac from running command: /opt/mesosphere/packages/mesos--aaedd03eee0d57f5c0d49c74ff1e5721862cad98/libexec/mesos/mesos-fetcher
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.006201 11561 fetcher.cpp:531] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/36a25adb-4ea2-49d3-a195-448cff1dc146-S34\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""https:\/\/downloads.mesosphere.com\/libmesos-bundle\/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz""}},{""action"":""BYPASS_CACHE"",
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009678 11561 fetcher.cpp:442] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009693 11561 fetcher.cpp:283] Fetching directly into the sandbox directory
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009711 11561 fetcher.cpp:220] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009723 11561 fetcher.cpp:163] Downloading resource from 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz' to '/var/lib/mesos/slave/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011/executors/hello__91922a16-889e-4e94-9dab-9f6754f091de/
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: Failed to fetch 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz': Error downloading resource: Failed writing received data to disk/application
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: End fetcher log for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213114 11440 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213351 11444 slave.cpp:4642] Container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' for executor 'hello__91922a16-889e-4e94-9dab-9f6754f091de' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011 failed to start: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213614 11443 containerizer.cpp:2071] Destroying container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213977 11443 linux_launcher.cpp:505] Asked to destroy container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.214757 11443 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.216047 11444 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.218407 11443 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 2.326016ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.220391 11445 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.222124 11445 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 1.693952ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239018 11441 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239162 11442 slave.cpp:4642] Container 'a30b74d5-53ac-4fbf-b8f3-5cfba58ea847' for executor 'node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008 failed to start: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239284 11445 containerizer.cpp:2071] Destroying container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239390 11444 linux_launcher.cpp:505] Asked to destroy container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.240103 11444 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.241353 11440 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.243120 11444 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.726976ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.245045 11440 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.246800 11440 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.715968ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.285477 11438 slave.cpp:1625] Got assigned task 'dse-1-agent__720d6f09-9d60-4667-b224-abcd495e0e58' for framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0009
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: F0505 02:14:32.296481 11438 slave.cpp:6381] CHECK_SOME(state::checkpoint(path, info)): Failed to create temporary file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: *** Check failure stack trace: ***
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be857d  google::LogMessage::Fail()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856bea3ad  google::LogMessage::SendToLog()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be816c  google::LogMessage::Flush()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856beaca9  google::LogMessageFatal::~LogMessageFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5855e4b5e9  _CheckFatal::~_CheckFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314082 11445 containerizer.cpp:2434] Container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314826 11440 containerizer.cpp:2434] Container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316660 11439 container_assigner.cpp:101] Unregistering container_id[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316761 11474 container_assigner_strategy.cpp:202] Closing ephemeral-port reader for container[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""] at endpoint[198.51.100.1:34273].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316804 11474 container_reader_impl.cpp:38] Triggering ContainerReader shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316833 11474 sync_util.hpp:39] Dispatching and waiting <=5s for ticket 7: ~ContainerReaderImpl:shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316769 11439 container_assigner.cpp:101] Unregistering container_id[value: ""a30b74d5-53ac-4fbf-b8f3-5cfba58ea847""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316864 11474 container_reader
{noformat}

In provisioner recover, when listing the container rootfses, it is possible that the 'rootfses' dir does not exist. Because a possible race between the provisioner destroy and the agent restart. For instance, while the provisioner is destroying the container dir the agent restarts. Due to os::rmdir() is recursive by traversing the FTS tree, it is possible that 'rootfses' dir is removed but the others (e.g., scratch dir) are not.

Currently, we are returning an error if the 'rootfses' dir does not exist, which blocks the agent from recovery. We should skip it if 'rootfses' does not exist.",1,2,MESOS-7471,2.0
Double free or corruption when using parallel test runner,"I observed the following when using the parallel test runner:

{noformat}
/home/bmahler/git/mesos/build/../support/mesos-gtest-runner.py --sequential=*ROOT_* ./mesos-tests
..
*** Error in `/home/bmahler/git/mesos/build/src/.libs/mesos-tests': double free or corruption (out): 0x00007fa818001310 ***
======= Backtrace: =========
/usr/lib64/libc.so.6(+0x7c503)[0x7fa87f27e503]
/usr/lib64/libsasl2.so.3(+0x866d)[0x7fa880f0d66d]
/usr/lib64/libsasl2.so.3(sasl_dispose+0x3b)[0x7fa880f1075b]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD1Ev+0x5d)[0x7fa88708f67d]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD0Ev+0x18)[0x7fa88708f734]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD1Ev+0xfb)[0x7fa88708a065]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD0Ev+0x18)[0x7fa88708a0b4]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal5slave5Slave13_authenticateEv+0x67)[0x7fa8879ff579]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZZN7process8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS_3PIDIT_EEMS6_FvvEENKUlPNS_11ProcessBaseEE_clESD_+0xe2)[0x7fa887a60b7a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS0_3PIDIT_EEMSA_FvvEEUlS2_E_E9_M_invokeERKSt9_Any_dataS2_+0x37)[0x7fa887aa0efe]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNKSt8functionIFvPN7process11ProcessBaseEEEclES2_+0x49)[0x7fa8888d1177]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process11ProcessBase5visitERKNS_13DispatchEventE+0x2f)[0x7fa8888b5063]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNK7process13DispatchEvent5visitEPNS_12EventVisitorE+0x2e)[0x7fa8888c0422]
/home/bmahler/git/mesos/build/src/.libs/mesos-tests(_ZN7process11ProcessBase5serveERKNS_5EventE+0x2e)[0xb088c8]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process14ProcessManager6resumeEPNS_11ProcessBaseE+0x525)[0x7fa8888b10d5]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f1a880)[0x7fa8888ad880]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2ca8a)[0x7fa8888bfa8a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c9ce)[0x7fa8888bf9ce]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c958)[0x7fa8888bf958]
/usr/lib64/libstdc++.so.6(+0xb5230)[0x7fa87fb90230]
/usr/lib64/libpthread.so.0(+0x7dc5)[0x7fa88040ddc5]
/usr/lib64/libc.so.6(clone+0x6d)[0x7fa87f2f973d]
{noformat}

Not sure how reproducible this is, appears to occur in the authentication path of the agent.",0,1,MESOS-7438,2.0
Registry puller cannot fetch manifests from Google GCR: 403 Forbidden.,"When the registry puller is pulling a repository from Google's GCE Container Registry, a '403 Forbidden' error occurs instead of 401 when fetching manifests.",1,4,MESOS-7431,5.0
Running DOCKER images in Mesos Container Runtime without `linux/filesystem` isolation enabled renders host unusable,"If I run the pod below (using Marathon 1.4.2) against a mesos agent that has the flags (also below), then the overlay filesystem replaces the system root mount, effectively rendering the host unusable until reboot.

flags:

- {{--containerizers mesos,docker}}
- {{--image_providers APPC,DOCKER}}
- {{--isolation cgroups/cpu,cgroups/mem,docker/runtime}}

pod definition for Marathon:
{code:java}
{
  ""id"": ""/simplepod"",
  ""scaling"": { ""kind"": ""fixed"", ""instances"": 1 },
  ""containers"": [
    {
      ""name"": ""sleep1"",
      ""exec"": { ""command"": { ""shell"": ""sleep 1000"" } },
      ""resources"": { ""cpus"": 0.1, ""mem"": 32 },
      ""image"": {
        ""id"": ""alpine"",
        ""kind"": ""DOCKER""
      }
    }
  ],
  ""networks"": [ {""mode"": ""host""} ]
}
{code}

Mesos should probably check for this and avoid replacing the system root mount point at startup or launch time.",1,7,MESOS-7374,3.0
Failed to pull image from Nexus Registry due to signature missing.,"I’m trying to launch docker container with universal containerizer, mesos 1.2.0. But getting error “Failed to parse the image manifest: Docker v2 image manifest validation failed: ‘signatures’ field size must be at least one”. And if I switch to docker containerizer, app is starting normally. 

We are working with private docker registry v2 backed by nexus repository manager  3.1.0
{code}
cat /etc/mesos-slave/docker_registry 
https://docker.company.ru

cat /etc/mesos-slave/docker_config 
{
	""auths"": {
		""docker.company.ru"": {
			""auth"": ""........""
		}
	}
}
{code}

Here agent's log:

{code}
I0405 22:00:49.860234 44856 slave.cpp:4346] Received ping from slave-observer(7)@10.34.1.31:5050
I0405 22:00:50.327030 44865 slave.cpp:1625] Got assigned task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.327785 44865 slave.cpp:1785] Launching task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.329324 44865 paths.cpp:547] Trying to chown '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff' to user 'dockdata'
I0405 22:00:50.329607 44865 slave.cpp:6896] Checkpointing ExecutorInfo to '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/executor.info'
I0405 22:00:50.330531 44865 slave.cpp:6472] Launching executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 with resources cpus(*)(allocated: general_marathon_service_role):0.1; mem(*)(allocated: general_marathon_service_role):32 in work directory '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff'
I0405 22:00:50.331244 44865 slave.cpp:6919] Checkpointing TaskInfo to '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff/tasks/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/task.info'
I0405 22:00:50.331568 44862 docker.cpp:1106] Skipping non-docker container
I0405 22:00:50.331822 44865 slave.cpp:2118] Queued task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.331966 44865 slave.cpp:884] Successfully attached file '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff'
I0405 22:00:50.332582 44861 containerizer.cpp:993] Starting container f82f5f69-87a3-4586-b4cc-b91d285dcaff for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.333286 44862 metadata_manager.cpp:168] Looking for image 'docker.company.ru/company-infra/kafka:0.10.2.0-16'
I0405 22:00:50.333627 44879 registry_puller.cpp:247] Pulling image 'docker.company.ru/company-infra/kafka:0.10.2.0-16' from 'docker-manifest://docker.company.rucompany-infra/kafka?0.10.2.0-16#https' to '/export/intssd/mesos-slave/docker-store/staging/aV2yko'
E0405 22:00:50.834630 44872 slave.cpp:4642] Container 'f82f5f69-87a3-4586-b4cc-b91d285dcaff' for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 failed to start: Failed to parse the image manifest: Docker v2 image manifest validation failed: 'signatures' field size must be at least one
I0405 22:00:50.835008 44853 containerizer.cpp:2069] Destroying container f82f5f69-87a3-4586-b4cc-b91d285dcaff in PROVISIONING state
I0405 22:00:50.835127 44853 containerizer.cpp:2124] Waiting for the provisioner to complete provisioning before destroying container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.835273 44844 provisioner.cpp:484] Ignoring destroy request for unknown container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.836199 44837 slave.cpp:4754] Executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 has terminated with unknown status
I0405 22:00:50.837193 44837 slave.cpp:3816] Handling status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 from @0.0.0.0:0
E0405 22:00:50.837766 44846 slave.cpp:4097] Failed to update resources for container f82f5f69-87a3-4586-b4cc-b91d285dcaff of executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' running task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 on status update for terminal task, destroying container: Container not found
W0405 22:00:50.837962 44865 composing.cpp:630] Attempted to destroy unknown container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.838018 44877 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838081 44877 status_update_manager.cpp:500] Creating StatusUpdate stream for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838560 44877 status_update_manager.cpp:832] Checkpointing UPDATE for status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838708 44877 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 to the agent
I0405 22:00:50.838860 44878 slave.cpp:4256] Forwarding the update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 to master@10.34.1.31:5050
I0405 22:00:50.839059 44878 slave.cpp:4150] Status update manager successfully handled status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848595 44866 status_update_manager.cpp:395] Received status update acknowledgement (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848696 44866 status_update_manager.cpp:832] Checkpointing ACK for status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848801 44866 status_update_manager.cpp:531] Cleaning up status update stream for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.849365 44850 slave.cpp:3105] Status update manager successfully handled status update acknowledgement (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.849431 44850 slave.cpp:6875] Completing task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14
{code}",1,4,MESOS-7350,2.0
Update Resource proto for storage resource providers.,"Storage resource provider support requires a number of changes to the {{Resource}} proto:

* support for {{RAW}} and {{BLOCK}} type {{Resource::DiskInfo::Source}}
* {{ResourceProviderID}} in Resource
* {{Resource::DiskInfo::Source::Path}} should be {{optional}}.",1,8,MESOS-7312,3.0
Unified containerizer provisions docker image error with COPY backend,"Error occurs on some specific docker images with COPY backend, both 1.0.2 and 1.2.0. It works well with OVERLAY backend on 1.2.0.

{quote}
I0321 09:36:07.308830 27613 paths.cpp:528] Trying to chown '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' to user 'root'
I0321 09:36:07.319628 27613 slave.cpp:5703] Launching executor ct:Transcoding_Test_114489497_1490060156172:3 of framework 20151223-150303-2677017098-5050-30032-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7'
I0321 09:36:07.321436 27615 containerizer.cpp:781] Starting container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework '20151223-150303-2677017098-5050-30032-0000'
I0321 09:36:37.902195 27600 provisioner.cpp:294] Provisioning image rootfs '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7
*E0321 09:36:58.707718 27606 slave.cpp:4000] Container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework 20151223-150303-2677017098-5050-30032-0000 failed to start: Collect failed: Failed to copy layer: cp: cannot create regular file ‘/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9/usr/bin/python’: Text file busy*
I0321 09:36:58.707991 27608 containerizer.cpp:1622] Destroying container '7e518538-7b56-4b14-a3c9-bee43c669bd7'
I0321 09:36:58.708468 27607 provisioner.cpp:434] Destroying container rootfs at '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7
{quote}

Docker image is a private one, so that i have to try to reproduce this bug with some sample Dockerfile as possible.",1,14,MESOS-7280,2.0
Unified containerizer does not support docker registry version < 2.3.,"in file `src/uri/fetchers/docker.cpp`

```
    Option<string> contentType = response.headers.get(""Content-Type"");  
        if (contentType.isSome() &&  
            !strings::startsWith(  
                contentType.get(),  
                ""application/vnd.docker.distribution.manifest.v1"")) {  
          return Failure(  
              ""Unsupported manifest MIME type: "" + contentType.get());  
        }  
```

Docker fetcher check the contentType strictly, while docker registry with version < 2.3 returns manifests with contentType `application/json`, that leading failure like `E0321 13:27:27.572402 40370 slave.cpp:4650] Container 'xxx' for executor 'xxx' of framework xxx failed to start: Unsupported manifest MIME type: application/json; charset=utf-8`.",1,5,MESOS-7272,2.0
Support pulling images from AliCloud private registry.,"The image puller via curl doesn't work when I'm specifying the image name as:
registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75
400 BAD REQUEST

But the docker pulls it successfully 
bq. docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75",1,1,MESOS-7251,2.0
Tasks launched via the default executor cannot access disk resource volumes.,"Currently, when a task in a task group tries to access a volume specified in disk resources (e.g., persistent volumes), it doesn't have access to them since they are mounted in the root container (executor). This happens due to there being no mechanism to specify resources for child containers yet. Hence, by default any resources (e.g., disk) are added to the root container.

A possible solution can be to set up the mapping manually by the default executor using the {{SANDBOX_PATH}} volume source type giving child containers access to the volume mounted in the parent container. This is at best a workaround and the ideal solution would be tackled as part of MESOS-7207.",1,2,MESOS-7225,3.0
HTTP health check doesn't work when mesos runs with --docker_mesos_image,"When running mesos-slave with option ""docker_mesos_image"" like:
{code}
--master=zk://standalone:2181/mesos  --containerizers=docker,mesos  --executor_registration_timeout=5mins  --hostname=standalone  --ip=0.0.0.0  --docker_stop_timeout=5secs  --gc_delay=1days  --docker_socket=/var/run/docker.sock  --no-systemd_enable_support  --work_dir=/tmp/mesos  --docker_mesos_image=panteras/paas-in-a-box:0.4.0
{code}

from the container that was started with option ""pid: host"" like:
{code}
  net:        host
  privileged: true
  pid:        host
{code}

and example marathon job, that use MESOS_HTTP checks like:
{code}
{
 ""id"": ""python-example-stable"",
 ""cmd"": ""python3 -m http.server 8080"",
 ""mem"": 16,
 ""cpus"": 0.1,
 ""instances"": 2,
 ""container"": {
   ""type"": ""DOCKER"",
   ""docker"": {
     ""image"": ""python:alpine"",
     ""network"": ""BRIDGE"",
     ""portMappings"": [
        { ""containerPort"": 8080, ""hostPort"": 0, ""protocol"": ""tcp"" }
     ]
   }
 },
 ""env"": {
   ""SERVICE_NAME"" : ""python""
 },
 ""healthChecks"": [
   {
     ""path"": ""/"",
     ""portIndex"": 0,
     ""protocol"": ""MESOS_HTTP"",
     ""gracePeriodSeconds"": 30,
     ""intervalSeconds"": 10,
     ""timeoutSeconds"": 30,
     ""maxConsecutiveFailures"": 3
   }
 ]
}
{code}

I see the errors like:
{code}
F0306 07:41:58.844293    35 health_checker.cpp:94] Failed to enter the net namespace of task (pid: '13527'): Pid 13527 does not exist
*** Check failure stack trace: ***
    @     0x7f51770b0c1d  google::LogMessage::Fail()
    @     0x7f51770b29d0  google::LogMessage::SendToLog()
    @     0x7f51770b0803  google::LogMessage::Flush()
    @     0x7f51770b33f9  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f517647ce46  _ZNSt17_Function_handlerIFivEZN5mesos8internal6health14cloneWithSetnsERKSt8functionIS0_E6OptionIiERKSt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISG_EEEUlvE_E9_M_invokeERKSt9_Any_data
    @     0x7f517647bf2b  mesos::internal::health::cloneWithSetns()
    @     0x7f517648374b  std::_Function_handler<>::_M_invoke()
    @     0x7f5177068167  process::internal::cloneChild()
    @     0x7f5177065c32  process::subprocess()
    @     0x7f5176481a9d  mesos::internal::health::HealthCheckerProcess::_httpHealthCheck()
    @     0x7f51764831f7  mesos::internal::health::HealthCheckerProcess::_healthCheck()
    @     0x7f517701f38c  process::ProcessBase::visit()
    @     0x7f517702c8b3  process::ProcessManager::resume()
    @     0x7f517702fb77  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
    @     0x7f51754ddc80  (unknown)
    @     0x7f5174cf06ba  start_thread
    @     0x7f5174a2682d  (unknown)
I0306 07:41:59.077986     9 health_checker.cpp:199] Ignoring failure as health check still in grace period
{code}

Looks like option docker_mesos_image makes, that newly started mesos job is not using ""pid host"" option same as mother container was started, but has his own PID namespace (so it doesn't matter if mother container was started with ""pid host"" or not it will never be able to find PID)",1,17,MESOS-7210,3.0
Persistent volume ownership is set to root when task is running with non-root user,"I’m running docker container in universal containerizer, mesos 1.1.0. switch_user=true, isolator=filesystem/linux,docker/runtime.  Container is launched with marathon, “user”:”someappuser”. I’d want to use persistent volume, but it’s exposed to container with root user permissions even if root folder is created with someppuser ownership (looks like mesos do chown to this folder). 

here logs for my container:
{code}
I0305 22:51:36.414655 10175 slave.cpp:1701] Launching task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.415118 10175 paths.cpp:536] Trying to chown '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a' to user 'root'
I0305 22:51:36.422992 10175 slave.cpp:6179] Launching executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a'
I0305 22:51:36.424278 10175 slave.cpp:1987] Queued task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.424347 10158 docker.cpp:1000] Skipping non-docker container
I0305 22:51:36.425639 10142 containerizer.cpp:938] Starting container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.428725 10166 provisioner.cpp:294] Provisioning image rootfs '/export/intssd/mesos-slave/workdir/provisioner/containers/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/backends/copy/rootfses/0e2181e9-1bf2-42d4-8cb0-ee70e466c3ae' for container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a
I0305 22:51:42.981240 10149 linux.cpp:695] Changing the ownership of the persistent volume at '/export/intssd/mesos-slave/data/volumes/roles/general_marathon_service_role/md_hdfs_journal#data#23f813aa-01dd-11e7-a012-0242ce94d92a' with uid 0 and gid 0
I0305 22:51:42.986593 10136 linux_launcher.cpp:421] Launching container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a and cloning with namespaces CLONE_NEWNS
{code}

{code}
ls -la /export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/
drwxr-xr-x 3 someappuser someappgroup   4096 22:51 .
drwxr-xr-x 3 root     root            4096 22:51 ..
drwxr-xr-x 2 root     root            4096 22:51 data
-rw-r--r-- 1 root     root             169 22:51 stderr
-rw-r--r-- 1 root     root          183012 23:00 stdout
{code}",1,2,MESOS-7208,3.0
Add documentation for Debug APIs to Operator API doc,,1,2,MESOS-7188,1.0
Agent should validate that the nested container ID does not exceed certain length.,"This is related to MESOS-691.

Since nested container ID is generated by the executor, the agent should verify that the length of it does not exceed certain length.",1,3,MESOS-7168,3.0
The agent may be flapping after the machine reboots due to provisioner recover.,"After the agent machine reboots, if the agent work dir survives (e.g., /var/lib/mesos) and the container runtime directory is gone (an empty SlaveState as well), the provisioner recover() would get into segfault because that case break the semantic that a child container should always be cleaned up before it parent container.

This is a particular case which only happens if the machine reboots and the provisioner directory survives.

{noformat}
F0217 01:10:18.423238 30099 provisioner.cpp:504] Check failed: entry.parent() != containerId Failed to destroy container 1 since its nested container 1.2 has not been destroyed yet
*** Check failure stack trace: ***
    @     0x7fceb444121d  google::LogMessage::Fail()
    @     0x7fceb44405ee  google::LogMessage::SendToLog()
    @     0x7fceb4440eed  google::LogMessage::Flush()
    @     0x7fceb4444368  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fceb36137f9  mesos::internal::slave::ProvisionerProcess::destroy()
    @     0x7fceb36126f0  mesos::internal::slave::ProvisionerProcess::recover()
    @     0x7fceb3637fc6  _ZZN7process8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS2_11ContainerIDESt4hashIS7_ESt8equal_toIS7_EESC_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSJ_FSH_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESS_
    @     0x7fceb3637bc2  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS6_11ContainerIDESt4hashISB_ESt8equal_toISB_EESG_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSN_FSL_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataOS2_
    @     0x7fceb43848e4  std::function<>::operator()()
    @     0x7fceb436baf4  process::ProcessBase::visit()
    @     0x7fceb43e5fde  process::DispatchEvent::visit()
    @           0x9e4101  process::ProcessBase::serve()
    @     0x7fceb4369007  process::ProcessManager::resume()
    @     0x7fceb4377a8c  process::ProcessManager::init_threads()::$_2::operator()()
    @     0x7fceb4377995  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_2vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7fceb4377965  std::_Bind_simple<>::operator()()
    @     0x7fceb437793c  std::thread::_Impl<>::_M_run()
    @     0x7fceadefa030  (unknown)
    @     0x7fcead70b6aa  start_thread
    @     0x7fcead440e9d  (unknown)
{noformat}

The provisioner directory is supposed to be under the container runtime directory. However, this is not backward compatible. We can only change it after a deprecation cycle.

For now, we have to three options:
1. make provisioner::destroy() recursive.
2. sort the container during recovery to guarantee `child before parent` semantic.
3. remove the check-failure since the while provisioner dir will be removed eventually at the end (not recommended).

Recommend (1).",1,4,MESOS-7152,5.0
Wrap IOSwitchboard.connect() in a dispatch,"Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most
of its API calls are automatically dispatched onto its underlying
process by an Isolator wrapper. However, the IOSwitchboard also
includes an additional connect() call which is not accessed through
the Isolator wrapper. As such, we need to wrap it in a dispatch call
manually.",1,3,MESOS-7144,1.0
Quota can be exceeded due to coarse-grained offer technique.,"The current implementation of quota allocation allocates the entire available resources on an agent when trying to satisfy the quota. What this means is that quota can be exceeded by the size of an agent.

This is especially problematic for large machines, consider a 48 core, 512 GB memory server where a role is given 4 cores and 4GB of memory. Given our current approach, we will send an offer for the entire 48 cores and 512 GB of memory!

This ticket is to perform fine grained offers when the allocation will exceed the quota.",1,2,MESOS-7099,5.0
libprocess tests fail when using libevent 2.1.8,"Running {{libprocess-tests}} on Mesos compiled with {{--enable-libevent --enable-ssl}} on an operating system using libevent 2.1.8, SSL related tests fail like
{noformat}
[ RUN      ] SSLTest.SSLSocket
I0207 15:20:46.017881 2528580544 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0207 15:20:46.017904 2528580544 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0207 15:20:46.017918 2528580544 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0207 15:20:46.017923 2528580544 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0207 15:20:46.033001 2528580544 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0207 15:20:46.033179 2528580544 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0207 15:20:46.033196 2528580544 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0207 15:20:46.033201 2528580544 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
../../../3rdparty/libprocess/src/tests/ssl_tests.cpp:257: Failure
Failed to wait 15secs for Socket(socket.get()).recv()
[  FAILED  ] SSLTest.SSLSocket (15196 ms)
{noformat}

Tests failing are

{noformat}
SSLTest.SSLSocket
SSLTest.NoVerifyBadCA
SSLTest.VerifyCertificate
SSLTest.ProtocolMismatch
SSLTest.ECDHESupport
SSLTest.PeerAddress
SSLTest.HTTPSGet
SSLTest.HTTPSPost
SSLTest.SilentSocket
SSLTest.ShutdownThenSend
SSLVerifyIPAdd/SSLTest.BasicSameProcess/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.BasicSameProcess/1, where GetParam() = ""true""
SSLVerifyIPAdd/SSLTest.BasicSameProcessUnix/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.BasicSameProcessUnix/1, where GetParam() = ""true""
SSLVerifyIPAdd/SSLTest.RequireCertificate/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.RequireCertificate/1, where GetParam() = ""true""
{noformat}",1,20,MESOS-7076,8.0
The linux filesystem isolator should set mode and ownership for host volumes.,"If the host path is a relative path, the linux filesystem isolator should set the mode and ownership for this host volume since it allows non-root user to write to the volume. Note that this is the case of sharing the host fileysystem (without rootfs).",1,9,MESOS-7069,2.0
IOSwitchboard FDs leaked when containerizer launch fails -- leads to deadlock,"If the containizer launch path fails before actually
launching the container, the FDs allocated to the container by the
IOSwitchboard isolator are leaked. This leads to deadlock in
the destroy path because the IOSwitchboard does not shutdown until the
FDs it allocates to the container have been closed. Since the
switchboard doesn't shutdown, the future returned by its 'cleanup()'
function is never satisfied. 

We need a general purpose method for closing the IOSwitchboard FDs when failing in the launch path.",1,13,MESOS-7050,2.0
Send SIGKILL after SIGTERM to IOSwitchboard after container termination.,This is follow up for MESOS-6664,1,2,MESOS-7042,2.0
Update framework authorization to support multiple roles,"Currently the master assumes that a framework is only in a single role, see {{Master::authorizeFramework}}. This code should be updated to support frameworks with multiple roles. In particular it should get authorization of the framework's principal to register in each of the framework's roles.",1,2,MESOS-7022,3.0
Change `Environment.Variable.Value` from required to optional,"To prepare for future work which will enable the modular fetching of secrets, we should change the {{Environment.Variable.Value}} field from {{required}} to {{optional}}. This way, the field can be left empty and filled in by a secret fetching module.",1,2,MESOS-6991,2.0
Fix BOOST random generator initialization on Windows,"seed_rng::seed_rng does not produced the expected result in Windows since is using `/dev/urandom` file.  

0:005> k
 # Child-SP          RetAddr           Call Site
00 00000049`22dfc108 00007ff6`5193822f kernel32!CreateFileW
...
0e 00000049`22dfc660 00007ff6`502228fd mesos_agent!boost::uuids::detail::seed_rng::seed_rng+0x3d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80]
0f 00000049`22dfc690 00007ff6`502591e3 mesos_agent!boost::uuids::detail::seed<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0x4d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246]
10 00000049`22dfc790 00007ff6`50395518 mesos_agent!boost::uuids::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0xd3 [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50]
11 00000049`22dfc800 00007ff6`500ad140 mesos_agent!id::UUID::random+0x78 [d:\repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49]
12 00000049`22dfc870 00007ff6`5007ff55 mesos_agent!mesos::internal::slave::Framework::launchExecutor+0x70 [d:\repositories\mesoswin\src\slave\slave.cpp @ 6301]
13 00000049`22dfd520 00007ff6`502a0a35 mesos_agent!mesos::internal::slave::Slave::_run+0x2455 [d:\repositories\mesoswin\src\slave\slave.cpp @ 1990]
...
0:005> du @rcx
000001d7`cc55fb60  ""/dev/urandom""
",1,5,MESOS-6973,1.0
Launching two tasks with the same Docker image simultaneously may cause a staging dir never cleaned up,"If user launches two tasks with the same Docker image simultaneously (e.g., run {{mesos-executor}} twice with the same Docker image), there will be a staging directory which is for the second task never cleaned up, like this:
{code}
└── store
    └── docker
        ├── layers
        │    ...
        ├── staging
        │   └── a6rXWC
        └── storedImages
{code}",1,6,MESOS-6950,2.0
Improve health checks validation.,"The ""general""  fields should also be validated (i.e., `timeout_seconds`), similar to what's done in https://reviews.apache.org/r/55458/",1,2,MESOS-6916,2.0
Zero health check timeout is interpreted literally.,"Currently zero health check timeout is interpreted literally, which is not very helpful since a health check does not even get a chance to finish. We suggest to fixe this behaviour by interpreting zero as {{Duration::max()}} effectively rendering the timeout infinite.",1,2,MESOS-6908,1.0
Add test for framework upgrading to multi-role capability.,"Frameworks can upgrade to multi-role capability as long as the framework's role remains the same.

We consider the framework roles unchanged if 
* a framework previously didn't specify a {{role}} now has {{roles=()}}, or
* a framework which previously had {{role=A}} and now has {{roles=(A)}}.",1,2,MESOS-6900,2.0
Reconsider process creation primitives on Windows,"Windows does not have the same notions of process hierarchies as Unix, and so killing groups of processes requires us to make sure all processes are contained in a job object, which acts something like a cgroup. This is particularly important when we decide to kill a task, as there is no way to reliably do this unless all the processes you'd like to kill are in the job object.

This causes us a number of issues; it is a big reason we needed to fork the command executor, and it is the reason tasks are currently unkillable in the default executor.

As we clean this issue up, we need to think carefully about the process governance semantics of Mesos, and how we can map them to a reliable, simple Windows implementation.",1,3,MESOS-6892,5.0
Transition Windows away from `os::killtree`.,"Windows does not have as robust a notion of a process hierarchy as Unix, and thus functions like `os::killtree` will always have critical limitations and semantic mismatches between Unix and Windows.

We should transition away from this function when we can, and replace it with something similar to how we kill a cgroup.",1,3,MESOS-6868,3.0
Container Exec should be possible with tasks belonging to a task group,"{{LaunchNestedContainerSession}} currently requires the parent container to be an Executor (https://github.com/apache/mesos/blob/f89f28724f5837ff414dc6cc84e1afb63f3306e5/src/slave/http.cpp#L2189-L2211).

This works for command tasks, because the task container id is the same as the executor container id.

But it won't work for pod tasks whose container id is different from executor’s container id.

In order to resolve this ticket, we need to allow launching a child container at an arbitrary level.",1,2,MESOS-6864,5.0
FaultToleranceTest.FrameworkReregister is flaky,"Observed on internal CI:

{noformat}
[21:27:38] :     [Step 11/11] /mnt/teamcity/work/4240ba9ddd0997c3/src/tests/fault_tolerance_tests.cpp:892: Failure
[21:27:38] :     [Step 11/11] Value of: framework.values[""registered_time""].as().as()
[21:27:38] :     [Step 11/11]   Actual: 1482442093
[21:27:38] :     [Step 11/11] Expected: static_cast(registerTime.secs())
[21:27:38] :     [Step 11/11] Which is: 1482442094
{noformat}

Looks like another instance of MESOS-4695.",1,2,MESOS-6837,3.0
consecutive_failures 0 == 1 in HealthCheck.,"When defining a HealthCheck with consecutive_failures=0 one would expect Mesos to never kill the task and only notify about the failure.

What seems to happen instead is Mesos handles consecutive_failures=0 as consecutive_failures=1 and kills the task after 1 failure.

Since 0 isn't the same as 1 this seems to be a bug and results in unexpected behaviour.
",0,1,MESOS-6833,3.0
mesos-this-capture clang-tidy check has false positives,"The {{mesos-this-capture}} clang-tidy checks incorrectly triggers on the code here,

  https://github.com/apache/mesos/blob/d2117362349ab4c383045720f77d42b2d9fd6871/src/slave/containerizer/mesos/io/switchboard.cpp#L1487

We should tighten the matcher to avoid triggering on such constructs.",1,3,MESOS-6824,2.0
Enable glog stack traces when we call things like `ABORT` on Windows,"Currently in the Windows builds, if we call `ABORT` (etc.) we will simply bail out, with no stack traces.

This is highly undesirable. Stack traces are important for operating clusters in production. We should work to enable this behavior, including possibly working with glog to add this support if they currently they do not natively support it.",1,7,MESOS-6815,5.0
Check unreachable task cache for task ID collisions on launch,"As discussed in MESOS-6785, it is possible to crash the master by launching a task that reuses the ID of an unreachable/partitioned task. A complete solution to this problem will be quite involved, but an incremental improvement is easy: when we see a task launch operation, reject the launch attempt if the task ID collides with an ID in the per-framework {{unreachableTasks}} cache. This doesn't catch all situations in which IDs are reused, but it is better than nothing.",1,2,MESOS-6805,2.0
SSL socket can lose bytes in the case of EOF,"During recent work on SSL-enabled tests in libprocess (MESOS-5966), we discovered a bug in {{LibeventSSLSocketImpl}}, wherein the socket can either fail to receive an EOF, or lose data when an EOF is received.

The {{LibeventSSLSocketImpl::event_callback(short events)}} method immediately sets any pending {{RecvRequest}}'s promise to zero upon receipt of an EOF. However, at the time the promise is set, there may actually be data waiting to be read by libevent. Upon receipt of an EOF, we should attempt to read the socket's bufferevent first to ensure that we aren't losing any data previously received by the socket.",1,3,MESOS-6802,3.0
SSL socket's 'shutdown()' method is broken,"We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method:
* The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future.
* The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown.",1,2,MESOS-6789,2.0
IOSwitchboardTest.KillSwitchboardContainerDestroyed is flaky,"{noformat}
[ RUN      ] IOSwitchboardTest.KillSwitchboardContainerDestroyed
I1212 13:57:02.641043  2211 containerizer.cpp:220] Using isolation: posix/cpu,filesystem/posix,network/cni
W1212 13:57:02.641438  2211 backend.cpp:76] Failed to create 'overlay' backend: OverlayBackend requires root privileges, but is running as user nrc
W1212 13:57:02.641559  2211 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1212 13:57:02.642822  2268 containerizer.cpp:594] Recovering containerizer
I1212 13:57:02.643975  2253 provisioner.cpp:253] Provisioner recovery complete
I1212 13:57:02.644953  2255 containerizer.cpp:986] Starting container 09e87380-00ab-4987-83c9-fa1c5d86717f for executor 'executor' of framework
I1212 13:57:02.647004  2245 switchboard.cpp:430] Allocated pseudo terminal '/dev/pts/54' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.652305  2245 switchboard.cpp:596] Created I/O switchboard server (pid: 2705) listening on socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.655513  2267 launcher.cpp:133] Forked child with pid '2706' for container '09e87380-00ab-4987-83c9-fa1c5d86717f'
I1212 13:57:02.655732  2267 containerizer.cpp:1621] Checkpointing container's forked pid 2706 to '/tmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_Me5CRx/meta/slaves/frameworks/executors/executor/runs/09e87380-00ab-4987-83c9-fa1c5d86717f/pids/forked.pid'
I1212 13:57:02.726306  2265 containerizer.cpp:2463] Container 09e87380-00ab-4987-83c9-fa1c5d86717f has exited
I1212 13:57:02.726352  2265 containerizer.cpp:2100] Destroying container 09e87380-00ab-4987-83c9-fa1c5d86717f in RUNNING state
E1212 13:57:02.726495  2243 switchboard.cpp:861] Unexpected termination of I/O switchboard server: 'IOSwitchboard' exited with signal: Killed for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.726563  2265 launcher.cpp:149] Asked to destroy container 09e87380-00ab-4987-83c9-fa1c5d86717f
E1212 13:57:02.783607  2228 switchboard.cpp:799] Failed to remove unix domain socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container '09e87380-00ab-4987-83c9-fa1c5d86717f': No such file or directory
../../mesos/src/tests/containerizer/io_switchboard_tests.cpp:661: Failure
Value of: wait.get()->reasons().size() == 1
  Actual: false
Expected: true
*** Aborted at 1481579822 (unix time) try ""date -d @1481579822"" if you are using GNU date ***
PC: @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 2211 (TID 0x7faed7d078c0) from PID 0; stack trace: ***
    @     0x7faecf855100 (unknown)
    @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
    @          0x1be6247 testing::internal::AssertHelper::operator=()
    @          0x19ed751 mesos::internal::tests::IOSwitchboardTest_KillSwitchboardContainerDestroyed_Test::TestBody()
    @          0x1c0ed8c testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c09e74 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1beb505 testing::Test::Run()
    @          0x1bebc88 testing::TestInfo::Run()
    @          0x1bec2ce testing::TestCase::Run()
    @          0x1bf2ba8 testing::internal::UnitTestImpl::RunAllTests()
    @          0x1c0f9b1 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c0a9f2 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1bf18ee testing::UnitTest::Run()
    @          0x11bc9e3 RUN_ALL_TESTS()
    @          0x11bc599 main
    @     0x7faece663b15 __libc_start_main
    @           0xa9c219 (unknown)
{noformat}
",0,7,MESOS-6784,1.0
The 'http::connect(address)' always uses the DEFAULT_KIND() of socket even if SSL is undesired.,"    The 'http::connect(address)' variant of 'http::connect()' doesn't
    currently support SSL. However, when SSL is enabled, the default for
    all 'Socket::create()' calls is to use the 'DEFAULT_KIND()' of socket
    which is set to SSL. This causes problems with 'connect()' becuuse it
    will create a socket of 'kind' SSL without a way to override it.
",1,2,MESOS-6775,1.0
Reached unreachable statement at <path>/mesos/src/slave/containerizer/mesos/launch.cpp:766,"This error message can pop up in unexpected places (e.g. when running a LAUNCH_NESTED_CONTAINER_SESSION and an invalid command is passed to it).

We should likely just remove the UNREACHABLE() statement here as it's obviously reachable in cases where the command we are trying to launch is not found.",1,1,MESOS-6767,1.0
IOSwitchboardServerTest.AttachOutput has CHECK failure if run it multiple times.,"I can easily repo this issue on my dev centos7 box with the following command:
{noformat}
GLOG_v=1 bin/mesos-tests.sh --gtest_filter=IOSwitchboardServerTest.AttachOutput --verbose --gtest_repeat=2
....
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from IOSwitchboardServerTest
[ RUN      ] IOSwitchboardServerTest.AttachOutput
I1208 10:46:31.574084 41813 poll_socket.cpp:209] Socket error while sending: Broken pipe
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:265: Failure
(response).failure(): Disconnected
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:266: Failure
(response).failure(): Disconnected
F1208 10:46:31.574919 41751 future.hpp:1137] Check failed: !isFailed() Future::get() but state == FAILED: Disconnected
*** Check failure stack trace: ***
    @     0x7fc3f35a633a  google::LogMessage::Fail()
    @     0x7fc3f35a6299  google::LogMessage::SendToLog()
    @     0x7fc3f35a5caa  google::LogMessage::Flush()
    @     0x7fc3f35a89de  google::LogMessageFatal::~LogMessageFatal()
    @           0xb6a352  process::Future<>::get()
    @          0x1a050fe  mesos::internal::tests::IOSwitchboardServerTest_AttachOutput_Test::TestBody()
    @          0x1c54ce2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c4fe00  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1c31491  testing::Test::Run()
    @          0x1c31c14  testing::TestInfo::Run()
    @          0x1c3225a  testing::TestCase::Run()
    @          0x1c38b34  testing::internal::UnitTestImpl::RunAllTests()
    @          0x1c55907  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c50948  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1c3787a  testing::UnitTest::Run()
    @          0x11cc653  RUN_ALL_TESTS()
    @          0x11cc209  main
    @     0x7fc3ecb61b15  __libc_start_main
    @           0xab5e89  (unknown)
Aborted (core dumped)
{noformat}",1,8,MESOS-6759,3.0
I/O switchboard should deal with the case when reaping of the server failed.,"Currently, we don't deal with the reaping failure, which we should.",1,1,MESOS-6756,3.0
I/O switchboard should inherit agent environment variables.,"Since it is a libexec binary that owned by Mesos. Agent might have some environment variables (e.g., LD_LIBRARY_PATH) that are needed by the io switchboard server process.",1,1,MESOS-6748,1.0
IOSwitchboard doesn't properly flush data on ATTACH_CONTAINER_OUTPUT,"Currently we are doing a close on the write end of all connection pipes when we exit the switchboard, but we don't wait until the read is flushed before exiting. This can cause some data to get dropped since the process may exit before the reader is flushed.  The current code is:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}

We should change it to:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();
    connection.closed().await();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}",1,2,MESOS-6746,1.0
The agent should synchronize with the IOSwitchboard to determine when it is ready to accept incoming connections.,"Currently, the agent has no way of knowing when the IOSwitchboard has started up and is ready to listen for incoming connections. We should add support to synchronize between them so the agent can figure this out.

The implementation should not block the launch path of the container, but rather incoming connections through the IOSwitchboards {{connect()}} call.",1,2,MESOS-6737,2.0
"Create a test filter for stout tests that use `symlink` on Windows, as they will fail if not run as admin",,1,2,MESOS-6731,3.0
Reserve operation should validate reserved resource role against resource allocationInfo role,"When doing dynamic reservation validation, the current logic is make sure the reserved resources role is same as the framework role (see [src/master/validation.cpp|https://github.com/apache/mesos/blob/0228fa74c25f450478a6a5a42e1ca384c26db8bd/src/master/validation.cpp#L1539-L1544]):

{code}
  if (frameworkRole.isSome() && resource.role() != frameworkRole.get()) {
      return Error(
          ""A reserve operation was attempted for a resource with role""
          "" '"" + resource.role() + ""', but the framework can only reserve""
          "" resources with role '"" + frameworkRole.get() + ""'"");
    }
{code}

With multi-role framework, we should validate reserved resource role same as resource allocation role.

Please make sure distinguish dynamic reservation with framework and http endpoint. If dynamic reservation was triggered by a framework, then we need to do such validation. If done by the http endpoint, then no need to validate the roles.",1,2,MESOS-6730,3.0
Check that `PreferredToolArchitecture` is set to `x64` on Windows before building,"If this variable is not set before we build, it will cause the linker to occasionally hang forever, due to a MSVC toolchain bug in the linker.

We should make this easy on developers and check for them. If the variable is not set, we should display an error message explaining.",1,2,MESOS-6720,2.0
Should destroy DEBUG containers on agent recovery.,We need to add support to destroy DEBUG containers on agent recovery. Right now these containers will stick around forever (or until they run to completion).,1,1,MESOS-6718,3.0
Port `slave_recovery_tests.cpp`,https://reviews.apache.org/r/65408/,1,1,MESOS-6713,3.0
Remove of unix domain socket path in IOSwitchboard::cleanup,We currently leak all of the unix domain socket files created by the switchboard in the `/tmp` directory. We need to clean them up properly.,1,2,MESOS-6689,1.0
IOSwitchboard should recover spawned server pid on agent restarts,"We need to do proper recovery of the io switchboard server pid across agent restarts. As of now, if the agent restarts there is now way to recover this pid.",1,4,MESOS-6688,2.0
Duplicate image layer ids may make the backend failed to mount rootfs.,"Some images (e.g., 'mesosphere/inky') may contain duplicate layer ids in manifest, which may cause some backends unable to mount the rootfs (e.g., 'aufs' backend). We should make sure that each layer path returned in 'ImageInfo' is unique.

Here is an example manifest from 'mesosphere/inky':
{noformat}
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
{noformat}

These two layer ids are totally identical:
{noformat}
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
{noformat}

It would make the backend (e.g., aufs) failed to mount the rootfs due to invalid arguments.
{noformat}
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
{noformat}

We should make sure the vector of layer paths that is passed to the backend contains only unique layer path.",1,2,MESOS-6654,3.0
Overlayfs backend may fail to mount the rootfs if both container image and image volume are specified.,"Depending on MESOS-6000, we use symlink to shorten the overlayfs mounting arguments. However, if more than one image need to be provisioned (e.g., a container image is specified while image volumes are specified for the same container), the symlink .../backends/overlay/links would fail to be created since it exists already.

Here is a simple log when we hard code overlayfs as our default backend:
{noformat}
[07:02:45] :	 [Step 10/10] [ RUN      ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0
[07:02:46] :	 [Step 10/10] I1127 07:02:46.416021  2919 containerizer.cpp:207] Using isolation: filesystem/linux,volume/image,docker/runtime,network/cni
[07:02:46] :	 [Step 10/10] I1127 07:02:46.419312  2919 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[07:02:46] :	 [Step 10/10] E1127 07:02:46.425336  2919 shell.hpp:107] Command 'hadoop version 2>&1' failed; this is the output:
[07:02:46] :	 [Step 10/10] sh: 1: hadoop: not found
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425379  2919 fetcher.cpp:69] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425452  2919 local_puller.cpp:94] Creating local puller with docker registry '/tmp/R6OUei/registry'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427258  2934 containerizer.cpp:956] Starting container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 for executor 'test_executor' of framework 
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427592  2938 metadata_manager.cpp:167] Looking for image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427774  2936 local_puller.cpp:147] Untarring image 'test_image_rootfs' from '/tmp/R6OUei/registry/test_image_rootfs.tar' to '/tmp/R6OUei/store/staging/9krDz2'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512070  2933 local_puller.cpp:167] The repositories JSON file for image 'test_image_rootfs' is '{""test_image_rootfs"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512279  2933 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617442  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617908  2938 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617925  2938 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617949  2938 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617959  2938 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617967  2938 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617974  2938 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618408  2936 overlay.cpp:175] Created symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/DQ3blT'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618472  2936 overlay.cpp:203] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/DQ3blT/0,upperdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/upperdir,workdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/workdir'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619098  2933 linux.cpp:451] Ignored an image volume for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619745  2938 metadata_manager.cpp:167] Looking for image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619925  2937 local_puller.cpp:147] Untarring image 'test_image_volume' from '/tmp/R6OUei/registry/test_image_volume.tar' to '/tmp/R6OUei/store/staging/2GNlJO'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713526  2935 local_puller.cpp:167] The repositories JSON file for image 'test_image_volume' is '{""test_image_volume"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713726  2935 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.818696  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819169  2934 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819188  2934 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819221  2934 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819232  2934 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819236  2934 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819241  2934 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/baf632b3-29c5-45e4-9d2e-6f3a2bdd9759' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] ../../src/tests/containerizer/volume_image_isolator_tests.cpp:214: Failure
[07:02:46] :	 [Step 10/10] (launch).failure(): Failed to create symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/6dj9IG'
[07:02:46] :	 [Step 10/10] [  FAILED  ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0, where GetParam() = false (919 ms)
{noformat}

We should differenciate the links for different provisioned images.",1,4,MESOS-6653,3.0
Expose container id in ContainerStatus in DockerContainerizer.,"Currently, the container id is only exposed for MesosContainerizer. We should make it consistent in DockerContainerizer.",1,3,MESOS-6625,2.0
SSL downgrade path will CHECK-fail when using both temporary and persistent sockets,"The code path for downgrading sockets from SSL to non-SSL includes this code:
{code}
    // If this address is a temporary link.
    if (temps.count(addresses[to_fd]) > 0) {
      temps[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }

    // If this address is a persistent link.
    if (persists.count(addresses[to_fd]) > 0) {
      persists[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321

It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.

If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later:
* {code}
    bool persist = persists.count(address) > 0;
    bool temp = temps.count(address) > 0;
    if (persist || temp) {
      int s = persist ? persists[address] : temps[address];
      CHECK(sockets.count(s) > 0);
socket = sockets.at(s);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942
* {code}
        if (dispose.count(s) > 0) {
          // This is either a temporary socket we created or it's a
          // socket that we were receiving data from and possibly
          // sending HTTP responses back on. Clean up either way.
          if (addresses.count(s) > 0) {
            const Address& address = addresses[s];
            CHECK(temps.count(address) > 0 && temps[address] == s);
temps.erase(address);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044",1,4,MESOS-6621,3.0
Improve task management for unreachable tasks,"Scenario:

# Framework starts non-partition-aware task T on agent A
# Agent A is partitioned. Task T is marked as a ""completed task"" in the {{Framework}} struct of the master, as part of {{Framework::removeTask}}.
# Agent A re-registers with the master. The tasks running on A are re-added to their respective frameworks on the master as running tasks.
# In {{Master::\_reregisterSlave}}, the master sends a {{ShutdownFrameworkMessage}} for all non-partition-aware frameworks running on the agent. The master then does {{removeTask}} for each task managed by one of these frameworks, which results in calling {{Framework::removeTask}}, which adds _another_ task to {{completed_tasks}}. Note that {{completed_tasks}} does not attempt to detect/suppress duplicates, so this results in two elements in the {{completed_tasks}} collection.

Similar problems occur when a partition-aware task is running on a partitioned agent that re-registers: the result is a task in the {{tasks}} list _and_ a task in the {{completed_tasks}} list.

Possible fixes/changes:

* Adding a task to the {{completed_tasks}} list when an agent becomes partitioned is debatable; certainly for partition-aware tasks, the task is not ""completed"". We might consider adding an ""{{unreachable_tasks}}"" list to the HTTP endpoints.
* Regardless of whether we continue to use {{completed_tasks}} or add a new collection, we should ensure the consistency of that data structure after agent re-registration.",1,2,MESOS-6619,8.0
Some tests use hardcoded port numbers.,"DockerContainerizerTest.ROOT_DOCKER_NoTransitionFromKillingToRunning and many HealthCheckTests use hardcoded port numbers. This can create false failures if these tests are run in parallel on the same machine.

It appears instead we should use random port numbers.",1,2,MESOS-6618,3.0
Shutdown completed frameworks when unreachable agent re-registers,"We currently shutdown completed frameworks when an agent re-registers with a master that it is already registered with (MESOS-633). We should also shutdown completed frameworks when an unreachable agent re-registers.

This is distinct from the more difficult problem of shutting down completed frameworks after master failover (MESOS-4659).",1,2,MESOS-6602,5.0
MesosContainerizer/DefaultExecutorTest.KillTask/0 failing on ASF CI,"{noformat:title=}
[ RUN      ] MesosContainerizer/DefaultExecutorTest.KillTask/0
I1110 01:20:11.482097 29700 cluster.cpp:158] Creating default 'local' authorizer
I1110 01:20:11.485241 29700 leveldb.cpp:174] Opened db in 2.774513ms
I1110 01:20:11.486237 29700 leveldb.cpp:181] Compacted db in 953614ns
I1110 01:20:11.486299 29700 leveldb.cpp:196] Created db iterator in 24739ns
I1110 01:20:11.486325 29700 leveldb.cpp:202] Seeked to beginning of db in 2300ns
I1110 01:20:11.486344 29700 leveldb.cpp:271] Iterated through 0 keys in the db in 378ns
I1110 01:20:11.486399 29700 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1110 01:20:11.486933 29733 recover.cpp:451] Starting replica recovery
I1110 01:20:11.487289 29733 recover.cpp:477] Replica is in EMPTY status
I1110 01:20:11.488503 29721 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(7318)@172.17.0.3:52462
I1110 01:20:11.488855 29727 recover.cpp:197] Received a recover response from a replica in EMPTY status
I1110 01:20:11.489398 29729 recover.cpp:568] Updating replica status to STARTING
I1110 01:20:11.490223 29723 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 575135ns
I1110 01:20:11.490284 29732 master.cpp:380] Master d28fbae1-c3dc-45fa-8384-32ab9395a975 (3a31be8bf679) started on 172.17.0.3:52462
I1110 01:20:11.490317 29732 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/k50x7x/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.2.0/_inst/share/mesos/webui"" --work_dir=""/tmp/k50x7x/master"" --zk_session_timeout=""10secs""
I1110 01:20:11.490696 29732 master.cpp:432] Master only allowing authenticated frameworks to register
I1110 01:20:11.490712 29732 master.cpp:446] Master only allowing authenticated agents to register
I1110 01:20:11.490720 29732 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1110 01:20:11.490730 29732 credentials.hpp:37] Loading credentials for authentication from '/tmp/k50x7x/credentials'
I1110 01:20:11.490281 29723 replica.cpp:320] Persisted replica status to STARTING
I1110 01:20:11.491210 29732 master.cpp:504] Using default 'crammd5' authenticator
I1110 01:20:11.491225 29720 recover.cpp:477] Replica is in STARTING status
I1110 01:20:11.491394 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1110 01:20:11.491621 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1110 01:20:11.491770 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1110 01:20:11.491937 29732 master.cpp:584] Authorization enabled
I1110 01:20:11.492276 29725 whitelist_watcher.cpp:77] No whitelist given
I1110 01:20:11.492310 29723 hierarchical.cpp:149] Initialized hierarchical allocator process
I1110 01:20:11.492569 29721 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(7319)@172.17.0.3:52462
I1110 01:20:11.492830 29719 recover.cpp:197] Received a recover response from a replica in STARTING status
I1110 01:20:11.493371 29720 recover.cpp:568] Updating replica status to VOTING
I1110 01:20:11.494002 29721 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 367673ns
I1110 01:20:11.494032 29721 replica.cpp:320] Persisted replica status to VOTING
I1110 01:20:11.494218 29734 recover.cpp:582] Successfully joined the Paxos group
I1110 01:20:11.494469 29734 recover.cpp:466] Recover process terminated
I1110 01:20:11.495633 29733 master.cpp:2033] Elected as the leading master!
I1110 01:20:11.495685 29733 master.cpp:1560] Recovering from registrar
I1110 01:20:11.495880 29720 registrar.cpp:329] Recovering registrar
I1110 01:20:11.496842 29730 log.cpp:553] Attempting to start the writer
I1110 01:20:11.498610 29725 replica.cpp:493] Replica received implicit promise request from __req_res__(7320)@172.17.0.3:52462 with proposal 1
I1110 01:20:11.499179 29725 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 524192ns
I1110 01:20:11.499213 29725 replica.cpp:342] Persisted promised to 1
I1110 01:20:11.500258 29726 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1110 01:20:11.501874 29731 replica.cpp:388] Replica received explicit promise request from __req_res__(7321)@172.17.0.3:52462 for position 0 with proposal 2
I1110 01:20:11.502413 29731 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 484138ns
I1110 01:20:11.502457 29731 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.503885 29720 replica.cpp:537] Replica received write request for position 0 from __req_res__(7322)@172.17.0.3:52462
I1110 01:20:11.503985 29720 leveldb.cpp:436] Reading position from leveldb took 56800ns
I1110 01:20:11.504534 29720 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 467426ns
I1110 01:20:11.504566 29720 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.505470 29721 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1110 01:20:11.505988 29721 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 479078ns
I1110 01:20:11.506021 29721 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.506706 29732 log.cpp:569] Writer started with ending position 0
I1110 01:20:11.508041 29734 leveldb.cpp:436] Reading position from leveldb took 50010ns
I1110 01:20:11.509210 29733 registrar.cpp:362] Successfully fetched the registry (0B) in 13.068032ms
I1110 01:20:11.509356 29733 registrar.cpp:461] Applied 1 operations in 27124ns; attempting to update the registry
I1110 01:20:11.510251 29732 log.cpp:577] Attempting to append 168 bytes to the log
I1110 01:20:11.510457 29724 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1110 01:20:11.511355 29728 replica.cpp:537] Replica received write request for position 1 from __req_res__(7323)@172.17.0.3:52462
I1110 01:20:11.511828 29728 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 423890ns
I1110 01:20:11.511859 29728 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.512572 29734 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1110 01:20:11.513051 29734 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 368122ns
I1110 01:20:11.513087 29734 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.514302 29726 registrar.cpp:506] Successfully updated the registry in 4.862976ms
I1110 01:20:11.514503 29726 registrar.cpp:392] Successfully recovered registrar
I1110 01:20:11.514593 29728 log.cpp:596] Attempting to truncate the log to 1
I1110 01:20:11.514760 29730 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1110 01:20:11.515249 29723 master.cpp:1676] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I1110 01:20:11.515534 29722 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I1110 01:20:11.516068 29722 replica.cpp:537] Replica received write request for position 2 from __req_res__(7324)@172.17.0.3:52462
I1110 01:20:11.516619 29722 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 497823ns
I1110 01:20:11.516652 29722 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.517526 29734 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1110 01:20:11.518040 29734 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 384129ns
I1110 01:20:11.518111 29734 leveldb.cpp:399] Deleting ~1 keys from leveldb took 39398ns
I1110 01:20:11.518138 29734 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.525027 29700 containerizer.cpp:201] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W1110 01:20:11.525806 29700 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W1110 01:20:11.526018 29700 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1110 01:20:11.527331 29700 cluster.cpp:435] Creating default 'local' authorizer
I1110 01:20:11.528741 29725 slave.cpp:208] Mesos agent started on (571)@172.17.0.3:52462
I1110 01:20:11.528789 29725 slave.cpp:209] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.2.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD""
I1110 01:20:11.529228 29725 credentials.hpp:86] Loading credential for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential'
I1110 01:20:11.529305 29700 scheduler.cpp:176] Version: 1.2.0
I1110 01:20:11.529436 29725 slave.cpp:346] Agent using credential for: test-principal
I1110 01:20:11.529464 29725 credentials.hpp:37] Loading credentials for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials'
I1110 01:20:11.529747 29725 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1110 01:20:11.529855 29729 scheduler.cpp:469] New master detected at master@172.17.0.3:52462
I1110 01:20:11.529884 29729 scheduler.cpp:478] Waiting for 0ns before initiating a re-(connection) attempt with the master
I1110 01:20:11.531039 29725 slave.cpp:533] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.531113 29725 slave.cpp:541] Agent attributes: [  ]
I1110 01:20:11.531126 29725 slave.cpp:546] Agent hostname: 3a31be8bf679
I1110 01:20:11.532897 29723 state.cpp:57] Recovering state from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta'
I1110 01:20:11.533222 29727 status_update_manager.cpp:203] Recovering status update manager
I1110 01:20:11.533269 29721 scheduler.cpp:353] Connected with the master at http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.533627 29734 containerizer.cpp:557] Recovering containerizer
I1110 01:20:11.534519 29725 scheduler.cpp:235] Sending SUBSCRIBE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.535482 29732 provisioner.cpp:253] Provisioner recovery complete
I1110 01:20:11.535652 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.535815 29724 slave.cpp:5411] Finished recovery
I1110 01:20:11.536440 29724 slave.cpp:5585] Querying resource estimator for oversubscribable resources
I1110 01:20:11.536898 29721 slave.cpp:915] New master detected at master@172.17.0.3:52462
I1110 01:20:11.536906 29731 status_update_manager.cpp:177] Pausing sending status updates
I1110 01:20:11.536941 29721 slave.cpp:974] Authenticating with master master@172.17.0.3:52462
I1110 01:20:11.537076 29721 slave.cpp:985] Using default CRAM-MD5 authenticatee
I1110 01:20:11.537214 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54635
I1110 01:20:11.537353 29719 authenticatee.cpp:121] Creating new client SASL connection
I1110 01:20:11.537256 29721 slave.cpp:947] Detecting new master
I1110 01:20:11.537591 29733 master.cpp:2329] Received subscription request for HTTP framework 'default'
I1110 01:20:11.537611 29721 slave.cpp:5599] Received oversubscribable resources {} from the resource estimator
I1110 01:20:11.537701 29733 master.cpp:2069] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1110 01:20:11.538077 29733 master.cpp:6745] Authenticating slave(571)@172.17.0.3:52462
I1110 01:20:11.538208 29732 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.538291 29733 master.cpp:2427] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1110 01:20:11.538508 29731 authenticator.cpp:98] Creating new server SASL connection
I1110 01:20:11.538782 29720 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1110 01:20:11.538823 29720 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1110 01:20:11.539227 29730 hierarchical.cpp:275] Added framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539317 29722 master.hpp:2161] Sending heartbeat to d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539331 29730 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.539696 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.539818 29730 hierarchical.cpp:1286] Performed allocation for 0 agents in 554795ns
I1110 01:20:11.540354 29720 authenticator.cpp:204] Received SASL authentication start
I1110 01:20:11.540361 29719 scheduler.cpp:675] Enqueuing event SUBSCRIBED received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.540438 29720 authenticator.cpp:326] Authentication requires more steps
I1110 01:20:11.540750 29720 authenticatee.cpp:259] Received SASL authentication step
I1110 01:20:11.541038 29721 authenticator.cpp:232] Received SASL authentication step
I1110 01:20:11.541081 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1110 01:20:11.541112 29721 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1110 01:20:11.541147 29719 scheduler.cpp:675] Enqueuing event HEARTBEAT received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.541178 29721 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1110 01:20:11.541260 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1110 01:20:11.541285 29721 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541307 29721 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541342 29721 authenticator.cpp:318] Authentication success
I1110 01:20:11.541517 29733 authenticatee.cpp:299] Authentication success
I1110 01:20:11.541586 29720 master.cpp:6775] Successfully authenticated principal 'test-principal' at slave(571)@172.17.0.3:52462
I1110 01:20:11.541826 29721 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.542129 29730 slave.cpp:1069] Successfully authenticated with master master@172.17.0.3:52462
I1110 01:20:11.542362 29730 slave.cpp:1483] Will retry registration in 9.532818ms if necessary
I1110 01:20:11.542577 29733 master.cpp:5154] Registering agent at slave(571)@172.17.0.3:52462 (3a31be8bf679) with id d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.543083 29731 registrar.cpp:461] Applied 1 operations in 60476ns; attempting to update the registry
I1110 01:20:11.543926 29729 log.cpp:577] Attempting to append 337 bytes to the log
I1110 01:20:11.544077 29723 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1110 01:20:11.545238 29731 replica.cpp:537] Replica received write request for position 3 from __req_res__(7325)@172.17.0.3:52462
I1110 01:20:11.546116 29731 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 825474ns
I1110 01:20:11.546169 29731 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.547427 29725 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1110 01:20:11.547969 29725 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 483290ns
I1110 01:20:11.548005 29725 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.550129 29732 registrar.cpp:506] Successfully updated the registry in 6.962944ms
I1110 01:20:11.550396 29726 log.cpp:596] Attempting to truncate the log to 3
I1110 01:20:11.550614 29720 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1110 01:20:11.551375 29723 slave.cpp:4263] Received ping from slave-observer(531)@172.17.0.3:52462
I1110 01:20:11.551326 29734 master.cpp:5225] Registered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.551720 29730 replica.cpp:537] Replica received write request for position 4 from __req_res__(7326)@172.17.0.3:52462
I1110 01:20:11.551892 29723 slave.cpp:1115] Registered with master master@172.17.0.3:52462; given agent ID d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.551975 29723 fetcher.cpp:86] Clearing fetcher cache
I1110 01:20:11.552170 29732 hierarchical.cpp:485] Added agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1110 01:20:11.552338 29721 status_update_manager.cpp:184] Resuming sending status updates
I1110 01:20:11.552486 29730 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 709727ns
I1110 01:20:11.552655 29723 slave.cpp:1138] Checkpointing SlaveInfo to '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/slave.info'
I1110 01:20:11.552609 29730 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.553383 29731 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1110 01:20:11.553409 29723 slave.cpp:1175] Forwarding total oversubscribed resources {}
I1110 01:20:11.553653 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.553671 29727 master.cpp:5624] Received update of agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with total oversubscribed resources {}
I1110 01:20:11.553755 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 1.528714ms
I1110 01:20:11.553975 29731 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 525057ns
I1110 01:20:11.554072 29731 leveldb.cpp:399] Deleting ~2 keys from leveldb took 59750ns
I1110 01:20:11.554065 29732 hierarchical.cpp:555] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1110 01:20:11.554105 29731 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.554260 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.554314 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.554345 29727 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.554379 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 239597ns
I1110 01:20:11.556370 29724 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.559177 29730 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.560282 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.561323 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.562417 29733 master.cpp:3581] Processing ACCEPT call for offers: [ d28fbae1-c3dc-45fa-8384-32ab9395a975-O0 ] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.562584 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task c96eb523-0365-49b2-8b3b-78976ff28797
I1110 01:20:11.563097 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.567248 29733 master.cpp:8337] Adding task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567651 29733 master.cpp:8337] Adding task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567845 29733 master.cpp:4438] Launching task group { 08848440-4c0e-4ad6-a0a9-b5947c5d21ba, c96eb523-0365-49b2-8b3b-78976ff28797 } of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) with resources cpus(*):0.2; mem(*):64; disk(*):64 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.568495 29724 slave.cpp:1547] Got assigned task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569128 29729 hierarchical.cpp:1018] Recovered cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569211 29729 hierarchical.cpp:1055] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 filtered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for 5secs
I1110 01:20:11.570461 29724 slave.cpp:1709] Launching task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.571297 29724 paths.cpp:530] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' to user 'mesos'
I1110 01:20:11.580168 29724 slave.cpp:6319] Launching executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 with resources cpus(*):0.1; mem(*):32; disk(*):32 in work directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.580930 29734 containerizer.cpp:940] Starting container a283035b-25d3-4b48-b59a-964e5a4dfa06 for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581110 29724 slave.cpp:2031] Queued task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581214 29724 slave.cpp:868] Successfully attached file '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.585572 29722 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-default-executor"",""--launcher_dir=\/mesos\/mesos-1.2.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.2.0\/_build\/src\/mesos-default-executor""}"" --help=""false"" --pipe_read=""60"" --pipe_write=""61"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06""'
I1110 01:20:11.588587 29722 launcher.cpp:127] Forked child with pid '10191' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.592996 29734 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:11.777189 10229 executor.cpp:189] Version: 1.2.0
I1110 01:20:11.786099 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.787382 29722 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54638
I1110 01:20:11.787693 29722 slave.cpp:3086] Received Subscribe request for HTTP executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.790436 29730 slave.cpp:2276] Sending queued task group task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] to executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:11.795663 10221 default_executor.cpp:130] Received SUBSCRIBED event
I1110 01:20:11.797111 10221 default_executor.cpp:134] Subscribed executor on 3a31be8bf679
I1110 01:20:11.797611 10221 default_executor.cpp:130] Received LAUNCH_GROUP event
I1110 01:20:11.801981 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.802435 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.803306 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803452 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.803827 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.803865 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803978 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.804236 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d' to user 'mesos'
I1110 01:20:11.814858 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.815129 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611' to user 'mesos'
I1110 01:20:11.824666 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""63"" --pipe_write=""64"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d""'
I1110 01:20:11.826855 29727 launcher.cpp:127] Forked child with pid '10240' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d'
I1110 01:20:11.828918 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""65"" --pipe_write=""66"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611""'
I1110 01:20:11.831428 29727 launcher.cpp:127] Forked child with pid '10241' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611'
I1110 01:20:11.834421 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.837882 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.847651 10227 default_executor.cpp:470] Successfully launched child containers [ a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 ] for tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ]
I1110 01:20:11.849225 29728 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850085 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.850145 29734 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850405 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba'
I1110 01:20:11.850746 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851114 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.851552 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851727 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.852295 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.852826 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.853938 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854076 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854460 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54641
I1110 01:20:11.854559 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.854610 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855126 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855190 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855200 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54642
I1110 01:20:11.855409 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.855608 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855803 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856199 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856346 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.856439 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856598 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856828 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856998 29725 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.857322 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.857386 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.857787 29725 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.858124 10226 default_executor.cpp:130] Received ACKNOWLEDGED event
I1110 01:20:11.858530 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859519 29732 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859676 10233 default_executor.cpp:130] Received ACKNOWLEDGED event
../../src/tests/default_executor_tests.cpp:338: Failure
Value of: runningUpdate1->status().task_id()
  Actual: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
Expected: taskInfo1.task_id()
Which is: c96eb523-0365-49b2-8b3b-78976ff28797
../../src/tests/default_executor_tests.cpp:342: Failure
Value of: runningUpdate2->status().task_id()
  Actual: c96eb523-0365-49b2-8b3b-78976ff28797
Expected: taskInfo2.task_id()
Which is: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.861587 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.861948 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862280 29733 scheduler.cpp:235] Sending KILL call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862632 29721 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.863528 29719 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.863664 29719 master.cpp:4870] Processing ACKNOWLEDGE call 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.864003 29732 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:11.864294 29732 status_update_manager.cpp:769] Unexpected status update acknowledgement (received 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87, expecting d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.864575 29726 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.864804 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.865231 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.866297 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.866420 29722 master.cpp:4870] Processing ACKNOWLEDGE call d91c7deb-4646-4b4e-ba1a-5650a256e8d2 for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.867036 29726 status_update_manager.cpp:395] Received status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867076 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.867291 29722 master.cpp:4762] Telling agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:11.867297 29726 status_update_manager.cpp:769] Unexpected status update acknowledgement (received d91c7deb-4646-4b4e-ba1a-5650a256e8d2, expecting 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867449 29733 slave.cpp:2344] Asked to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.867710 29733 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.869391 10228 default_executor.cpp:130] Received KILL event
I1110 01:20:11.869498 10228 default_executor.cpp:810] Received kill for task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.869544 10228 default_executor.cpp:694] Shutting down
I1110 01:20:11.870112 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.870338 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.870965 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871399 29730 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871984 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872088 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872284 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d in RUNNING state
I1110 01:20:11.872340 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872416 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872597 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.877090 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 in RUNNING state
I1110 01:20:11.877320 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.962539 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d has exited
I1110 01:20:11.963811 29733 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.963851 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 has exited
I1110 01:20:11.964437 29729 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d/termination'
I1110 01:20:11.965940 29728 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.966202 29732 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611/termination'
I1110 01:20:11.970046 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797' in state TASK_KILLED
I1110 01:20:11.970501 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' in state TASK_KILLED
I1110 01:20:11.970559 10231 default_executor.cpp:768] Terminating after 1secs
I1110 01:20:11.971288 29723 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.972218 29728 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.972488 29728 slave.cpp:3740] Handling status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.974179 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.975229 29726 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.975278 29729 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.975517 29729 slave.cpp:3740] Handling status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.976048 29729 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978132 29720 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978482 29725 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494274 29725 hierarchical.cpp:1880] Filtered offer with cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494357 29725 hierarchical.cpp:1694] No allocations performed
I1110 01:20:12.494402 29725 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:12.494491 29725 hierarchical.cpp:1286] Performed allocation for 1 agents in 915780ns
I1110 01:20:13.071280 29734 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06 has exited
I1110 01:20:13.071339 29734 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06 in RUNNING state
I1110 01:20:13.071746 29734 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.076637 29723 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.077929 29721 slave.cpp:4672] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 exited with status 0
I1110 01:20:13.078433 29732 master.cpp:5884] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679): exited with status 0
I1110 01:20:13.078538 29732 master.cpp:7840] Removing executor 'default' with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:13.079448 29730 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:13.081049 29723 scheduler.cpp:675] Enqueuing event FAILURE received from http://172.17.0.3:52462/master/api/v1/scheduler

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: failure(0x7fff1aa9a950, @0x2ab91c02dd10 48-byte object <90-62 27-EC B8-2A 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-CE 01-1C B9-2A 00-00 70-CE 02-1C B9-2A 00-00 00-00 00-00 B8-2A 00-00>)
Stack trace:
I1110 01:20:13.496551 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:13.496680 29730 hierarchical.cpp:1286] Performed allocation for 1 agents in 1.498625ms
I1110 01:20:13.497339 29729 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:13.499797 29721 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:14.497707 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:14.497795 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:14.497895 29732 hierarchical.cpp:1286] Performed allocation for 1 agents in 410313ns
I1110 01:20:15.499423 29728 hierarchical.cpp:1694] No allocations performed
I1110 01:20:15.499526 29728 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:15.499658 29728 hierarchical.cpp:1286] Performed allocation for 1 agents in 547651ns
I1110 01:20:16.500463 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:16.500581 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:16.500699 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 505442ns
I1110 01:20:17.502176 29727 hierarchical.cpp:1694] No allocations performed
I1110 01:20:17.502262 29727 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:17.502367 29727 hierarchical.cpp:1286] Performed allocation for 1 agents in 464526ns
I1110 01:20:18.503680 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:18.503762 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:18.503851 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 425163ns
I1110 01:20:19.505476 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:19.505586 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:19.505705 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 590762ns
I1110 01:20:20.507310 29724 hierarchical.cpp:1694] No allocations performed
I1110 01:20:20.507390 29724 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:20.507477 29724 hierarchical.cpp:1286] Performed allocation for 1 agents in 411721ns
I1110 01:20:21.508368 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:21.508458 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:21.508564 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 432440ns
W1110 01:20:21.855908 29728 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.856066 29728 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.856652 29734 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
W1110 01:20:21.857002 29727 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857069 29727 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.857378 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.857475 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857662 29722 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:21.858206 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.858988 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.859259 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859647 29725 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.9; mem(*):992; disk(*):992; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859845 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:21.859979 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.860970 29729 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.861687 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
../../src/tests/default_executor_tests.cpp:400: Failure
Value of: killedUpdate1->status().state()
  Actual: TASK_RUNNING
Expected: TASK_KILLED
I1110 01:20:21.864666 29721 master.cpp:1297] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) disconnected
I1110 01:20:21.864765 29721 master.cpp:2918] Disconnecting framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.864820 29721 master.cpp:2942] Deactivating framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.865016 29732 hierarchical.cpp:386] Deactivated framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:21.865586 29721 master.hpp:2264] Master attempted to send message to disconnected framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:21.865691 29721 master.hpp:2270] Unable to send event to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default): connection closed
I1110 01:20:21.865777 29721 master.cpp:1310] Giving framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) 0ns to failover
I1110 01:20:21.866277 29728 hierarchical.cpp:1018] Recovered cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867295 29733 master.cpp:6426] Framework failover timeout, removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867328 29733 master.cpp:7170] Removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867539 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.867559 29731 slave.cpp:2575] Asked to shut down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 by master@172.17.0.3:52462
I1110 01:20:21.867617 29731 slave.cpp:2600] Shutting down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867585 29733 master.cpp:7811] Removing task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.867707 29731 slave.cpp:4776] Cleaning up executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:21.867904 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.868042 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' for gc 6.99998999732444days in the future
I1110 01:20:21.867939 29733 master.cpp:7811] Removing task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.868232 29731 slave.cpp:4864] Cleaning up framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868252 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default' for gc 6.99998999732444days in the future
I1110 01:20:21.868422 29725 status_update_manager.cpp:285] Closing status update streams for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868484 29725 status_update_manager.cpp:531] Cleaning up status update stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868777 29721 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000' for gc 6.99998999732444days in the future
I1110 01:20:21.868926 29720 hierarchical.cpp:337] Removed framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.869235 29725 status_update_manager.cpp:531] Cleaning up status update stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.870568 29730 slave.cpp:787] Agent terminating
I1110 01:20:21.870795 29732 master.cpp:1258] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) disconnected
I1110 01:20:21.870825 29732 master.cpp:2977] Disconnecting agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.870930 29732 master.cpp:2996] Deactivating agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.871158 29726 hierarchical.cpp:584] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 deactivated
I1110 01:20:21.876986 29733 master.cpp:1097] Master terminating
I1110 01:20:21.877754 29724 hierarchical.cpp:517] Removed agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
[  FAILED  ] MesosContainerizer/DefaultExecutorTest.KillTask/0, where GetParam() = ""mesos"" (10406 ms)
{noformat}",1,4,MESOS-6569,2.0
Parallel test running does not respect GTEST_FILTER,"Normally, you can use {{GTEST_FILTER}} to control which tests will be run by {{make check}}. However, this doesn't currently work if Mesos is configured with {{--enable-parallel-test-execution}}.",1,2,MESOS-6516,2.0
Use 'geteuid()' for the root privileges check.,"Currently, parts of code in Mesos check the root privileges using os::user() to compare to ""root"", which is not sufficient, since it compares the real user. When people change the mesos binary by 'setuid root', the process may not have the right permission to execute.

We should check the effective user id instead in our code. ",1,3,MESOS-6504,3.0
PosixRLimitsIsolatorTest.TaskExceedingLimit fails on OS X,"This test consistently fails on OS X:

{code}
31-7e9c-4248-acfd-21634150a657@172.28.128.1:64864 on agent 52cc4957-1a39-4d66-ace6-5622fac3b85e-S0
../../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:120: Failure
Value of: statusFailed->state()
  Actual: TASK_FINISHED
Expected: TASK_FAILED
{code}",1,4,MESOS-6459,2.0
"Roles with quota assigned can ""game"" the system to receive excessive resources.","The current implementation of quota allocation attempts to satisfy each resource quota for a role, but in doing so can far exceed the quota assigned to the role.

For example, if a role has quota for {{\[30,20,10\]}}, it can consume up to: {{\[∞, ∞, 10\]}} or {{\[∞, 20, ∞\]}} or {{\[30, ∞, ∞\]}} as only once each resource in the quota vector is satisfied do we stop allocating agent's resources to the role!

As a first step for preventing gaming, we could consider quota satisfied once any of the resources in the vector has quota satisfied. This approach works reasonably well for resources that are required and are present on every agent (cpus, mem, disk). However, it doesn't work well for resources that are optional / only present on some agents (e.g. gpus) (a.k.a. non-ubiquitous / scarce resources). For this we would need to determine which agents have resources that can satisfy the quota prior to performing the allocation.",1,6,MESOS-6432,5.0
"The python linter doesn't rebuild the virtual environment before linting when ""pip-requirements.txt"" has changed","We need to detect if ""pip-requirements.txt"" changes and rebuild the virtual environment if it has.",1,1,MESOS-6430,2.0
Report new PARTITION_AWARE task statuses in HTTP endpoints,"At a minimum, the {{/state-summary}} endpoint needs to be updated.",1,2,MESOS-6388,1.0
'mesos-containerizer launch' should inherit agent environment variables.,"If some dynamic libraries that agent depends on are stored in a non standard location, and the operator starts the agent using LD_LIBRARY_PATH. When we actually fork/exec the 'mesos-containerizer launch' helper, we need to make sure it inherits agent's environment variables. Otherwise, it might throw linking errors. This makes sense because it's a Mesos controlled process.

However, the the helper actually fork/exec the user container (or executor), we need to make sure to strip the agent environment variables.

The tricky case is for default executor and command executor. These two are controlled by Mesos as well, we also want them to have agent environment variables. We need to somehow distinguish this from custom executor case.",1,3,MESOS-6323,5.0
Agent fails to kill empty parent container,"I launched a pod using Marathon, which led to the launching of a task group on a Mesos agent. The pod spec was flawed, so this led to Marathon repeatedly re-launching multiple instances of the task group. After this went on for a few minutes, I told Marathon to scale the app to 0 instances, which sends {{TASK_KILLED}} for one task in each task group. After this, the Mesos agent reports {{TASK_KILLED}} status updates for all 3 tasks in the pod, but hitting the {{/containers}} endpoint on the agent reveals that the executor container for this task group is still running.

Here is the task group launching on the agent:
{code}
slave.cpp:1696] Launching task group containing tasks [ test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask ] for framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the executor container starting:
{code}
mesos-agent[2994]: I1006 20:23:27.407563  3094 containerizer.cpp:965] Starting container bf38ff09-3da1-487a-8926-1f4cc88bce32 for executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the output showing the {{TASK_KILLED}} updates for one task group:
{code}
mesos-agent[2994]: I1006 20:23:28.728224  3097 slave.cpp:2283] Asked to kill task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
mesos-agent[2994]: W1006 20:23:28.728304  3097 slave.cpp:2364] Transitioning the state of task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 to TASK_KILLED because the executor is not registered
mesos-agent[2994]: I1006 20:23:28.728659  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 1cb8197a-7829-4a05-9cb1-14ba97519c42) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728817  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: e377e9fb-6466-4ce5-b32a-37d840b9f87c) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728912  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 24d44b25-ea52-43a1-afdb-6c04389879d2) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
{code}
however, if we grep the log for the executor's ID, the last line mentioning it is:
{code}
slave.cpp:3080] Creating a marker file for HTTP based executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 (via HTTP) at path '/var/lib/mesos/slave/meta/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32/http.marker'
{code}
so it seems the executor never exited. If we hit the agent's {{/containers}} endpoint, we get output which includes this executor container:
{code}
{
    ""container_id"": ""bf38ff09-3da1-487a-8926-1f4cc88bce32"",
    ""executor_id"": ""instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601"",
    ""executor_name"": """",
    ""framework_id"": ""42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000"",
    ""source"": """",
    ""statistics"": {
      ""cpus_limit"": 0.1,
      ""cpus_nr_periods"": 17,
      ""cpus_nr_throttled"": 11,
      ""cpus_system_time_secs"": 0.02,
      ""cpus_throttled_time_secs"": 0.784142448,
      ""cpus_user_time_secs"": 0.09,
      ""disk_limit_bytes"": 10485760,
      ""disk_used_bytes"": 20480,
      ""mem_anon_bytes"": 11337728,
      ""mem_cache_bytes"": 0,
      ""mem_critical_pressure_counter"": 0,
      ""mem_file_bytes"": 0,
      ""mem_limit_bytes"": 33554432,
      ""mem_low_pressure_counter"": 0,
      ""mem_mapped_file_bytes"": 0,
      ""mem_medium_pressure_counter"": 0,
      ""mem_rss_bytes"": 11337728,
      ""mem_swap_bytes"": 0,
      ""mem_total_bytes"": 12013568,
      ""mem_unevictable_bytes"": 0,
      ""timestamp"": 1475792290.12373
    },
    ""status"": {
      ""executor_pid"": 9068,
      ""network_infos"": [
        {
          ""ip_addresses"": [
            {
              ""ip_address"": ""9.0.1.34"",
              ""protocol"": ""IPv4""
            }
          ],
          ""labels"": {},
          ""name"": ""dcos"",
          ""port_mappings"": [
            {
              ""container_port"": 8080,
              ""host_port"": 24758,
              ""protocol"": ""tcp""
            },
            {
              ""container_port"": 8081,
              ""host_port"": 24759,
              ""protocol"": ""tcp""
            }
          ]
        }
      ]
    }
  },
{code}
and looking through the output of {{ps}} on the agent, indeed we can locate the executor process:
{code}
$ ps aux | grep 9068
root      9068  0.0  0.1  96076 25380 ?        Ss   20:23   0:00 mesos-containerizer launch --command={""arguments"":[""mesos-default-executor""],""shell"":false,""user"":""root"",""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-default-executor""} --help=false --pipe_read=49 --pipe_write=50 --pre_exec_commands=[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""},{""shell"":true,""value"":""ifconfig lo up""}] --runtime_directory=/var/run/mesos/containers/bf38ff09-3da1-487a-8926-1f4cc88bce32 --unshare_namespace_mnt=false --user=root --working_directory=/var/lib/mesos/slave/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32
core     20406  0.0  0.0   6764  1020 pts/1    S+   23:05   0:00 grep --colour=auto 9068
$ sudo cat /proc/9068/task/9068/children
9330
$ ps aux | grep 9330
root      9330  0.0  0.2 498040 32944 ?        Sl   20:23   0:00 mesos-default-executor
root     19330  0.0  0.0      0     0 ?        S    22:49   0:00 [kworker/0:2]
core     20573  0.0  0.0   6764   888 pts/1    S+   23:07   0:00 grep --colour=auto 9330
{code}
Looking at the executor's logs, we find stdout is:
{code}
Executing pre-exec command '{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""}'
Executing pre-exec command '{""shell"":true,""value"":""ifconfig lo up""}'
{code}
and stderr is:
{code}
I1006 20:23:28.817401  9330 executor.cpp:189] Version: 1.1.0
I1006 20:23:28.906349  9352 default_executor.cpp:123] Received SUBSCRIBED event
I1006 20:23:28.908797  9352 default_executor.cpp:127] Subscribed executor on 10.0.0.133
{code}
With this short executor log, it seems possible that the agent received the {{TASK_KILLED}} before any tasks were sent to the executor, and the agent removed those tasks from its data structures without terminating the parent container. ",1,3,MESOS-6322,3.0
Implement clang-tidy check to catch incorrect flags hierarchies,"Classes need to always use {{virtual}} inheritance when being derived from {{FlagsBase}}. Also, in order to compose such derived flags they should be inherited virtually again.

Some examples:
{code}
struct A : virtual FlagsBase {}; // OK
struct B : FlagsBase {}; // ERROR
struct C : A {}; // ERROR
{code}


We should implement a clang-tidy checker to catch such wrong inheritance issues.",1,2,MESOS-6320,3.0
Agent recovery can fail after nested containers are launched,"After launching a nested container which used a Docker image, I restarted the agent which ran that task group and saw the following in the agent logs during recovery:
{code}
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.813596  4640 status_update_manager.cpp:203] Recovering status update manager
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.813622  4640 status_update_manager.cpp:211] Recovering executor 'instance-testvolume.02c26bce-8778-11e6-9ff3-7a3cd7c1568e' of framework 118ca38d-daee-4b2d-b584-b5581738a3dd-0000
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.814249  4639 docker.cpp:745] Recovering Docker containers
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.815294  4642 containerizer.cpp:581] Recovering containerizer
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Failed to perform recovery: Collect failed: Unable to list rootfses belonged to container a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53: Unable to list the container directory: Failed to opendir '/var/lib/mesos/slave/provisioner/containers/a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53/backends': No such file or directory
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: To remedy this do as follows:
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Step 1: rm -f /var/lib/mesos/slave/meta/slaves/latest
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]:         This ensures agent doesn't recover old live executors.
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Step 2: Restart the agent.
{code}
and the agent continues to restart in this fashion. Attached is the Marathon app definition that I used to launch the task group.",1,3,MESOS-6302,3.0
Recursive destroy in MesosContainerizer is problematic.,"    When doing recursive destroy, we should return the collected future of
    nested container destroys. Intead, we should fail the corresponding
    termination and return that termination if nested container destroys
    failed.
    
    In addition, we cannot remove 'Container' struct from the internal map
    when the destroy of a nested container failed. This is to ensure that
    the top level container do not proceed with destroy if any of its
    nested container failed to destroy.",1,1,MESOS-6301,3.0
A destroyed nested container is not reflected in the parent container's children map.,We should update parent container's children map if it's nested container is terminated.,1,2,MESOS-6300,2.0
HealthCheckTest.HealthyTaskViaHTTPWithoutType fails on some distros.,"I see consistent failures of this test in the internal CI in *some* distros, specifically CentOS 6, Ubuntu 14, 15, 16. The source of the health check failure is always the same: {{curl}} cannot connect to the target:
{noformat}
Received task health update, healthy: false
W0929 17:22:05.270992  2730 health_checker.cpp:204] Health check failed 1 times consecutively: HTTP health check failed: curl returned exited with status 7: curl: (7) couldn't connect to host
I0929 17:22:05.273634 26850 slave.cpp:3609] Handling status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from executor(1)@172.30.2.20:58660
I0929 17:22:05.274178 26844 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274226 26844 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to the agent
I0929 17:22:05.274314 26845 slave.cpp:4026] Forwarding the update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to master@172.30.2.20:38955
I0929 17:22:05.274415 26845 slave.cpp:3920] Status update manager successfully handled status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274436 26845 slave.cpp:3936] Sending acknowledgement for status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to executor(1)@172.30.2.20:58660
I0929 17:22:05.274534 26849 master.cpp:5661] Status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from agent 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-S0 at slave(77)@172.30.2.20:38955 (ip-172-30-2-20.mesosphere.io)
../../src/tests/health_check_tests.cpp:1398: Failure
I0929 17:22:05.274567 26849 master.cpp:5723] Forwarding status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
Value of: statusHealth.get().healthy()
  Actual: false
  Expected: true
I0929 17:22:05.274636 26849 master.cpp:7560] Updating the state of task aa0792d3-8d85-4c32-bd04-56a9b552ebda of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0929 17:22:05.274829 26844 sched.cpp:1025] Scheduler::statusUpdate took 43297ns
Received SHUTDOWN event
{noformat}",1,6,MESOS-6293,3.0
Support nested containers for logger in Mesos Containerizer.,"Currently, there are two issues in mesos containerizer using logger for nested contaienrs:

1. An empty executorinfo is passed to logger when launching a nested container, it would potentially break some logger modules if any module tries to access the required proto field (e.g., executorId).

2. The logger does not reocver the nested containers yet in MesosContainerizer::recover.",1,5,MESOS-6290,2.0
The default executor should maintain launcher_dir.,"Both command and docker executors require {{launcher_dir}} is provided in a flag. This directory contains mesos binaries, e.g. a tcp checker necessary for TCP health check. The default executor should obtain somehow (a flag, env var) and maintain this directory for health checker to use.",1,2,MESOS-6288,3.0
Mesos containerizer should figure out the correct sandbox directory for nested launch.,"Currently the mesos containerizer take the sandbox directory from the agent. Ideally, a nested sandbox dir can be figured out by the containerizer. And there is no need to pass it from the agent. We should remove the `directory` parameter in nested launch interface.",1,2,MESOS-6263,3.0
Default executor should kill all other tasks in a task group if any task exits with a non-zero exit status.,The default restart policy for a task group is to kill all active containers if any of the tasks terminates with a non-zero exit status code for now. The default executor needs to honor this default policy.,1,2,MESOS-6262,3.0
Composing containerizer needs to properly handle nested container launch,"Right now if the agent is started with --containerizers=""docker,mesos"" , nested container launches will fail because the composing containerizer doesn't implement the nested `launch` method. This results in it using the default `launch` method defined in the based class, which just returns an Error.",1,4,MESOS-6260,3.0
CNI isolator should not `CHECK` for `resolv.conf` under `rootContainerDir`,,1,1,MESOS-6259,1.0
Driver based schedulers performing explicit acknowledgements cannot acknowledge updates from HTTP based executors.,"It seems that the agent code sets {{StatusUpdate}}->{{slave_id}} but does not set the {{TaskStatus}}->{{slave_id}} if it's not already set. On the driver, when we receive such a status update and if it has explicit ACK enabled, it would pass the {{TaskStatus}} to the scheduler. But, the scheduler has no way of acking this update due to {{slave_id}} not being present. Note that, implicit acknowledgements still work since they use the {{slave_id}} from {{StatusUpdate}}. Hence, we never noticed this in our tests as all of them use implicit acknowledgements on the driver.",1,3,MESOS-6245,3.0
PAGE_SIZE was not declared in PPC64LE,"When compile Mesos in PPC64LE, get this error

{code}
../../src/slave/containerizer/mesos/isolators/gpu/isolator.cpp  -fPIC -DPIC -o slave/containerizer/mesos/isolators/gpu/.libs/libmesos_no_3rdparty_la-isolator.o
../../src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp: In member function 'virtual process::Future<Nothing> mesos::internal::slave::MemorySubsystem::update(const mesos::ContainerID&, const mesos::Resources&)':
../../src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp:230:55: error: 'PAGE_SIZE' was not declared in this scope
   Bytes initialLimit(static_cast<uint64_t>(LONG_MAX / PAGE_SIZE * PAGE_SIZE));
                                                       ^
{code}

",1,1,MESOS-6217,1.0
Health check grace period covers failures happening after first success.,"Currently, the health check library [ignores *all* failures|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/health-check/health_checker.cpp#L192-L197] from the task’s start (technically from the health check library initialization) [until after the grace period ends|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/include/mesos/v1/mesos.proto#L403].

This behaviour is misleading. Once the health check succeeds for the first time, grace period rule for failures should not be applied any more.

For example, if the grace period is set to 10 minutes, the task becomes healthy after 1 minute and fails after 2 minutes, the failure should be treated as a normal failure with all the consequences.",1,2,MESOS-6170,5.0
Remove stout's Set type,"stout provides a {{Set}} type which wraps a {{std::set}}. As only addition it provides new constructors,
{code}
Set(const T& t1);
Set(const T& t1, const T& t2);
Set(const T& t1, const T& t2, const T& t3);
Set(const T& t1, const T& t2, const T& t3, const T& t4);
{code}
which simplified creation of a {{Set}} from (up to four) known elements.

C++11 brought {{std::initializer_list}} which can be used to create a {{std::set}} from an arbitrary number of elements, so it appears that it should be possible to retire {{Set}}.",1,2,MESOS-6159,1.0
Clean up queued tasks if a task group is killed before launch.,"We are not properly cleaning up a queued task group when one of its task is killed i.e. https://github.com/apache/mesos/blob/aaa353acc515c0435a859113c9ee236247b51169/src/slave/slave.cpp#L6554 , we clean up the queued task but don't go around cleaning up the queued task group. Also, it would be great to add a test similar to we did for exercising the {{pending}} tasks workflow i.e. {{SlaveTest.KillTaskGroupBetweenRunTaskParts}} for {{queuedTasks}}.",1,2,MESOS-6154,2.0
Resource leak in slave.cpp.,"Coverity detected the following resource leak:
{code}
1. Condition this->queuedTasks.contains(taskId), taking true branch.
6547  if (queuedTasks.contains(taskId)) {
    	2. Condition terminal, taking true branch.
6548    if (terminal) {
    	3. alloc_fn: Storage is returned from allocation function operator new. [Note: The source code implementation of the function has been overridden by a builtin model.]
    	4. var_assign: Assigning: task = storage returned from new mesos::Task(mesos::internal::protobuf::createTask(this->queuedTasks.at(taskId), mesos::TaskState const(status->state()), this->frameworkId)).
6549      Task* task = new Task(protobuf::createTask(
6550          queuedTasks.at(taskId),
6551          status.state(),
6552          frameworkId));
6553
6554      queuedTasks.erase(taskId);
6555
6556      // This might be a queued task belonging to a task group.
6557      // If so, we need to update the other tasks belonging to this task group.
6558      Option<TaskGroupInfo> taskGroup = getQueuedTaskGroup(taskId);
6559
    	5. Condition taskGroup.isSome(), taking true branch.
6560      if (taskGroup.isSome()) {
    	6. No elements left in taskGroup->tasks(), leaving loop.
6561        foreach (const TaskInfo& task_, taskGroup->tasks()) {
6562          Task* task = new Task(
6563              protobuf::createTask(task_, status.state(), frameworkId));
6564
6565          tasks.push_back(task);
6566        }
    	7. Falling through to end of if statement.
6567      } else {
6568        tasks.push_back(task);
6569      }
    	
CID 1372871 (#1 of 1): Resource leak (RESOURCE_LEAK)
8. leaked_storage: Variable task going out of scope leaks the storage it points to.
6570    } else {
6571      return Error(""Cannot send non-terminal update for queued task"");
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881751&defectInstanceId=28450463",1,1,MESOS-6153,1.0
Resource leak in libevent_ssl_socket.cpp.,"Coverity detected the following resource leak.
IMO {code} if (fd == -1) {code} should be  {code} if (owned_fd == -1) {code}.


{code}
 // Duplicate the file descriptor because Libevent will take ownership
754  // and control the lifecycle separately.
755  //
756  // TODO(josephw): We can avoid duplicating the file descriptor in
757  // future versions of Libevent. In Libevent versions 2.1.2 and later,
758  // we may use `evbuffer_file_segment_new` and `evbuffer_add_file_segment`
759  // instead of `evbuffer_add_file`.
   	3. open_fn: Returning handle opened by dup.
   	4. var_assign: Assigning: owned_fd = handle returned from dup(fd).
760  int owned_fd = dup(fd);
   	CID 1372873: Argument cannot be negative (REVERSE_NEGATIVE) [select issue]
   	5. Condition fd == -1, taking true branch.
761  if (fd == -1) {
   	
CID 1372872 (#1 of 1): Resource leak (RESOURCE_LEAK)
6. leaked_handle: Handle variable owned_fd going out of scope leaks the handle.
762    return Failure(ErrnoError(""Failed to duplicate file descriptor""));
763  }
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881747&defectInstanceId=28450468",1,2,MESOS-6152,2.0
Frameworks may RESERVE for an arbitrary role.,"The master does not validate that resources from a reservation request have the same role the framework is registered with. As a result, frameworks may reserve resources for arbitrary roles.

I've modified the role in [the {{ReserveThenUnreserve}} test|https://github.com/apache/mesos/blob/bca600cf5602ed8227d91af9f73d689da14ad786/src/tests/reservation_tests.cpp#L117] to ""yoyo"" and observed the following in the test's log:
{noformat}
I0908 18:35:43.379122 2138112 master.cpp:3362] Processing ACCEPT call for offers: [ dfaf67e6-7c1c-4988-b427-c49842cb7bb7-O0 ] on agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train) for framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 (default) at scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da@10.200.181.237:60116
I0908 18:35:43.379170 2138112 master.cpp:3022] Authorizing principal 'test-principal' to reserve resources 'cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512'
I0908 18:35:43.379678 2138112 master.cpp:3642] Applying RESERVE operation for resources cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512 from framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 (default) at scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da@10.200.181.237:60116 to agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train)
I0908 18:35:43.379767 2138112 master.cpp:7341] Sending checkpointed resources cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512 to agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train)
I0908 18:35:43.380273 3211264 slave.cpp:2497] Updated checkpointed resources from  to cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512
I0908 18:35:43.380574 2674688 hierarchical.cpp:760] Updated allocation of framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 on agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 from cpus(*):1; mem(*):512; disk(*):470841; ports(*):[31000-32000] to ports(*):[31000-32000]; cpus(yoyo, test-principal):1; disk(*):470841; mem(yoyo, test-principal):512 with RESERVE operation
{noformat}",1,3,MESOS-6142,3.0
TCP health checks are not portable.,"MESOS-3567 introduced a dependency on ""bash"" for TCP health checks, which is undesirable. We should implement a portable solution for TCP health checks.",1,2,MESOS-6119,3.0
Consider supporting TCP half-open in checks.,A TCP half-open check does not complete the TCP handshake and hence the tested task is not notified that the someone is connecting. This is usually more performant than doing a complete TCP connection.,0,0,MESOS-6116,8.0
Deprecate using health checks without setting the type,"When sending a task launch using the 1.0.x protos and the legacy (non-http) API, tasks with a healthcheck defined are rejected (TASK_ERROR) because the 'type' field is not set.

This field is marked optional in the proto and is not available before 1.1.0, so it should not be required in order to keep the mesos v1 api compatibility promise.

For backwards compatibility temporarily allow the use case when command health check is set without a type.",1,5,MESOS-6110,3.0
Potential FD double close in libevent's implementation of `sendfile`.,"Repro copied from: https://reviews.apache.org/r/51509/

It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:

1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.

2) Start the master with SSL enabled:
{code}
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_KEY_FILE=key.pem LIBPROCESS_SSL_CERT_FILE=cert.pem bin/mesos-master.sh --work_dir=/tmp/master
{code}

3) Run two instances of this python script repeatedly:
{code}
import socket
import ssl

s = ssl.wrap_socket(socket.socket())
s.connect((""localhost"", 5050))

s.sendall(""""""GET /static/home.html HTTP/1.1
User-Agent: foobar
Host: localhost:5050
Accept: */*
Connection: Keep-Alive

"""""")

# The HTTP part of the response
print s.recv(1000)
{code}

i.e. 
{code}
while python test.py; do :; done & while python test.py; do :; done
{code}",1,5,MESOS-6104,3.0
Unable to launch containers on CNI networks on CoreOS,"CoreOS does not have an `/etc/hosts`. Currently, in the `network/cni` isolator, if we don't see a `/etc/hosts` on the host filesystem we don't bind mount the containers `hosts` file to this target for the `command executor`. On distros such as CoreOS this fails the container launch since the `libprocess` initialization of the `command executor` fails cause it can't resolve its `hostname`.

We should be creating the `/etc/hosts` and `/etc/hostname` files when they are absent on the host filesystem since creating these files should not affect name resolution on the host network namespace, and it will allow the `/etc/hosts` file to be bind mounted correctly and allow name resolution in the containers network namespace as well. ",1,3,MESOS-6052,1.0
Aufs backend cannot support the image with numerous layers.,"This issue was exposed in this unit test `ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller` by manually specifying the `bind` backend. Most likely mounting the aufs with specific options is limited by string length.

{noformat}
[20:13:07] :	 [Step 10/10] [ RUN      ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.615844 23416 cluster.cpp:155] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.624106 23416 leveldb.cpp:174] Opened db in 8.148813ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627252 23416 leveldb.cpp:181] Compacted db in 3.126629ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627275 23416 leveldb.cpp:196] Created db iterator in 4410ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627282 23416 leveldb.cpp:202] Seeked to beginning of db in 763ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627287 23416 leveldb.cpp:271] Iterated through 0 keys in the db in 491ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627301 23416 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627563 23434 recover.cpp:451] Starting replica recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627800 23437 recover.cpp:477] Replica is in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628113 23431 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(5852)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628243 23430 recover.cpp:197] Received a recover response from a replica in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628365 23437 recover.cpp:568] Updating replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628744 23432 master.cpp:375] Master dd755a55-0dd1-4d2d-9a49-812a666015cb (ip-172-30-2-138.mesosphere.io) started on 172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628758 23432 master.cpp:377] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/OZHDIQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/OZHDIQ/master"" --zk_session_timeout=""10secs""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628893 23432 master.cpp:427] Master only allowing authenticated frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628900 23432 master.cpp:441] Master only allowing authenticated agents to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628902 23432 master.cpp:454] Master only allowing authenticated HTTP frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628906 23432 credentials.hpp:37] Loading credentials for authentication from '/tmp/OZHDIQ/credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628999 23432 master.cpp:499] Using default 'crammd5' authenticator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629041 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629114 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629166 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629231 23432 master.cpp:579] Authorization enabled
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629290 23434 whitelist_watcher.cpp:77] No whitelist given
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629302 23430 hierarchical.cpp:151] Initialized hierarchical allocator process
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629921 23433 master.cpp:1851] Elected as the leading master!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629933 23433 master.cpp:1547] Recovering from registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629992 23436 registrar.cpp:332] Recovering registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630861 23435 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.358536ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630877 23435 replica.cpp:320] Persisted replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630924 23435 recover.cpp:477] Replica is in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631178 23435 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(5853)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631285 23435 recover.cpp:197] Received a recover response from a replica in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631433 23436 recover.cpp:568] Updating replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633391 23433 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.912156ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633409 23433 replica.cpp:320] Persisted replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633438 23433 recover.cpp:582] Successfully joined the Paxos group
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633479 23433 recover.cpp:466] Recover process terminated
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633635 23435 log.cpp:553] Attempting to start the writer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.634021 23432 replica.cpp:493] Replica received implicit promise request from __req_res__(5854)@172.30.2.138:44256 with proposal 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636034 23432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.995908ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636049 23432 replica.cpp:342] Persisted promised to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636239 23432 coordinator.cpp:238] Coordinator attempting to fill missing positions
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636672 23432 replica.cpp:388] Replica received explicit promise request from __req_res__(5855)@172.30.2.138:44256 for position 0 with proposal 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637307 23432 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 614745ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637318 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637668 23432 replica.cpp:537] Replica received write request for position 0 from __req_res__(5856)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637692 23432 leveldb.cpp:436] Reading position from leveldb took 10680ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638314 23432 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 610038ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638325 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638569 23436 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640446 23436 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.856131ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640461 23436 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640645 23437 log.cpp:569] Writer started with ending position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640940 23430 leveldb.cpp:436] Reading position from leveldb took 11341ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641152 23430 registrar.cpp:365] Successfully fetched the registry (0B) in 11.14496ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641185 23430 registrar.cpp:464] Applied 1 operations in 5010ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641381 23434 log.cpp:577] Attempting to append 209 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641425 23430 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641706 23434 replica.cpp:537] Replica received write request for position 1 from __req_res__(5857)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642320 23434 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 596016ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642333 23434 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642608 23435 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644492 23435 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.868216ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644507 23435 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644716 23432 registrar.cpp:509] Successfully updated the registry in 3.512064ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644759 23432 registrar.cpp:395] Successfully recovered registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644811 23431 log.cpp:596] Attempting to truncate the log to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644879 23433 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644949 23430 master.cpp:1655] Recovered 0 agents from the registry (170B); allowing 10mins for agents to re-register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644959 23437 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645247 23431 replica.cpp:537] Replica received write request for position 2 from __req_res__(5858)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645884 23431 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 618643ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645896 23431 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.646080 23437 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648093 23437 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.995217ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648118 23437 leveldb.cpp:399] Deleting ~1 keys from leveldb took 10026ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648125 23437 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.649564 23416 containerizer.cpp:200] Using isolation: docker/runtime,filesystem/linux,network/cni
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.652878 23416 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[20:13:07]W:	 [Step 10/10] E0805 20:13:07.656265 23416 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[20:13:07]W:	 [Step 10/10] sh: 1: hadoop: not found
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656286 23416 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656338 23416 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.657330 23416 linux.cpp:148] Bind mounting '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn' and making it a shared mount
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663147 23416 cluster.cpp:434] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663566 23436 slave.cpp:198] Mesos agent started on (506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663583 23436 slave.cpp:199] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/OZHDIQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials"" --image_providers=""docker"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663796 23436 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663868 23436 slave.cpp:336] Agent using credential for: test-principal
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663882 23436 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663969 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664010 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664225 23416 sched.cpp:226] Version: 1.1.0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664423 23435 sched.cpp:330] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664451 23435 sched.cpp:396] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664428 23436 slave.cpp:519] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664458 23435 sched.cpp:403] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664463 23436 slave.cpp:527] Agent attributes: [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664470 23436 slave.cpp:532] Agent hostname: ip-172-30-2-138.mesosphere.io
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664588 23437 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664810 23437 master.cpp:5900] Authenticating scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664873 23437 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664939 23432 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665006 23431 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665024 23435 status_update_manager.cpp:203] Recovering status update manager
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665174 23434 containerizer.cpp:527] Recovering containerizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665201 23431 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665221 23431 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665266 23431 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665303 23431 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665347 23431 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665436 23431 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665457 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665465 23431 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665482 23431 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665494 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665503 23431 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665510 23431 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665524 23431 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665575 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665596 23435 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665624 23431 master.cpp:5930] Successfully authenticated principal 'test-principal' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665705 23436 sched.cpp:502] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665715 23436 sched.cpp:820] Sending SUBSCRIBE call to master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665751 23436 sched.cpp:853] Will retry registration in 188.601026ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665796 23437 master.cpp:2425] Received SUBSCRIBE call for framework 'default' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665817 23437 master.cpp:1887] Authorizing framework principal 'test-principal' to receive offers for role '*'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665998 23430 master.cpp:2501] Subscribing framework default with checkpointing disabled and capabilities [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666132 23432 hierarchical.cpp:271] Added framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666148 23434 sched.cpp:743] Framework registered with dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666154 23432 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666173 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666177 23434 sched.cpp:757] Scheduler::registered took 11084ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666189 23432 hierarchical.cpp:1192] Performed allocation for 0 agents in 43102ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666486 23431 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/OZHDIQ/store/storedImages' does not exist
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666558 23436 provisioner.cpp:255] Provisioner recovery complete
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666677 23435 slave.cpp:4872] Finished recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666831 23435 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666919 23435 slave.cpp:895] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666929 23435 slave.cpp:954] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666931 23436 status_update_manager.cpp:177] Pausing sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666944 23435 slave.cpp:965] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666982 23435 slave.cpp:927] Detecting new master
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667006 23431 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667014 23435 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667162 23431 master.cpp:5900] Authenticating slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667225 23434 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667275 23434 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667418 23434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667436 23434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667492 23436 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667515 23436 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667546 23436 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667592 23436 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667603 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667610 23436 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667619 23436 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667630 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667639 23436 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667642 23436 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667652 23436 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667688 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667713 23432 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667733 23434 master.cpp:5930] Successfully authenticated principal 'test-principal' at slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667783 23437 slave.cpp:1049] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667836 23437 slave.cpp:1455] Will retry registration in 4.197236ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667901 23436 master.cpp:4554] Registering agent at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with id dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668021 23430 registrar.cpp:464] Applied 1 operations in 13306ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668269 23433 log.cpp:577] Attempting to append 395 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668329 23434 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668622 23433 replica.cpp:537] Replica received write request for position 3 from __req_res__(5859)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669297 23433 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 658552ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669309 23433 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669589 23432 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672566 23432 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 2.962622ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672580 23432 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672866 23435 registrar.cpp:509] Successfully updated the registry in 4.822784ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672936 23434 log.cpp:596] Attempting to truncate the log to 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673001 23437 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673110 23436 master.cpp:4623] Registered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673152 23432 hierarchical.cpp:478] Added agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673174 23430 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673254 23430 slave.cpp:1095] Registered with master master@172.30.2.138:44256; given agent ID dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673266 23430 fetcher.cpp:86] Clearing fetcher cache
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673288 23433 replica.cpp:537] Replica received write request for position 4 from __req_res__(5860)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673317 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673333 23432 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 160981ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673358 23432 status_update_manager.cpp:184] Resuming sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673435 23437 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673467 23430 slave.cpp:1118] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/slave.info'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673566 23437 sched.cpp:917] Scheduler::resourceOffers took 40919ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673607 23430 slave.cpp:1155] Forwarding total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673710 23437 master.cpp:5006] Received update of agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673781 23437 hierarchical.cpp:542] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673823 23437 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673830 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673838 23437 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 31940ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674163 23435 master.cpp:3346] Processing ACCEPT call for offers: [ dd755a55-0dd1-4d2d-9a49-812a666015cb-O0 ] on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674186 23435 master.cpp:2981] Authorizing framework principal 'test-principal' to launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674538 23437 master.cpp:7451] Adding task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674564 23437 master.cpp:3835] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674665 23430 slave.cpp:1495] Got assigned task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674713 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674736 23436 hierarchical.cpp:961] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 filtered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 for 5secs
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674866 23430 slave.cpp:1614] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.675107 23430 paths.cpp:536] Trying to chown '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' to user 'root'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678246 23433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 4.916164ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678267 23433 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678629 23436 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679050 23430 slave.cpp:5764] Launching executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679200 23430 slave.cpp:1840] Queuing task 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679219 23437 containerizer.cpp:786] Starting container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679234 23430 slave.cpp:848] Successfully attached file '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679435 23430 metadata_manager.cpp:167] Looking for image 'mesosphere/inky'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679572 23430 registry_puller.cpp:236] Pulling image 'mesosphere/inky' from 'docker-manifest://registry-1.docker.io:443mesosphere/inky?latest#https' to '/tmp/OZHDIQ/store/staging/HbsybX'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.680943 23436 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.14361ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681073 23436 leveldb.cpp:399] Deleting ~2 keys from leveldb took 60273ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681112 23436 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104004 23431 registry_puller.cpp:259] The manifest for image 'mesosphere/inky' is '{
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104116 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104130 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104138 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104146 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104151 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104158 23431 registry_puller.cpp:369] Fetching blob 'sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66' for layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104164 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104171 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.504564 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.507129 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.508962 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.510915 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.512848 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.515400 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.517390 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.519486 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.606955 23434 metadata_manager.cpp:155] Successfully cached image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607501 23436 provisioner.cpp:312] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451 using the 'aufs' backend
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607787 23434 aufs.cpp:152] Provisioning image rootfs with aufs: 'dirs=/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/scratch/427b7851-bf82-4553-80f3-da2d42cede77/workdir:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs:/tmp/OZHDIQ/store/layers/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs:/tmp/OZHDIQ/store/layers/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs:/tmp/OZHDIQ/store/layers/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs:/tmp/OZHDIQ/store/layers/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs:/tmp/OZHDIQ/store/layers/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615058 23436 containerizer.cpp:1637] Destroying container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615072 23436 containerizer.cpp:1640] Waiting for the provisioner to complete for container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615279 23435 provisioner.cpp:455] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616097 23430 slave.cpp:4135] Executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 has terminated with unknown status
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616173 23430 slave.cpp:3264] Handling status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from @0.0.0.0:0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616320 23435 slave.cpp:6104] Terminating task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] W0805 20:13:08.616402 23432 containerizer.cpp:1466] Ignoring update for unknown container: f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616528 23433 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616545 23433 status_update_manager.cpp:500] Creating StatusUpdate stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616750 23433 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to the agent
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616827 23431 slave.cpp:3657] Forwarding the update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to master@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616936 23431 slave.cpp:3551] Status update manager successfully handled status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617010 23433 master.cpp:5141] Status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617032 23433 master.cpp:5203] Forwarding status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617079 23433 master.cpp:6845] Updating the state of task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617187 23435 sched.cpp:1025] Scheduler::statusUpdate took 57204ns
[20:13:08] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:309: Failure
[20:13:08] :	 [Step 10/10] Value of: statusRunning->state()
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617234 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08] :	 [Step 10/10]   Actual: TASK_FAILED
[20:13:08] :	 [Step 10/10] Expected: TASK_RUNNING
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617281 23432 master.cpp:4266] Processing ACKNOWLEDGE call 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617311 23432 master.cpp:6911] Removing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617450 23430 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617480 23430 status_update_manager.cpp:531] Cleaning up status update stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617545 23430 slave.cpp:2650] Status update manager successfully handled status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617561 23430 slave.cpp:6145] Completing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617575 23430 slave.cpp:4246] Cleaning up executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617660 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' for gc 6.99999285160889days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617688 23430 slave.cpp:4334] Cleaning up framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617708 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for gc 6.9999928509363days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617748 23434 status_update_manager.cpp:285] Closing status update streams for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617772 23434 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000' for gc 6.99999285021926days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630481 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630504 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 155186ns
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630609 23430 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630728 23430 sched.cpp:917] Scheduler::resourceOffers took 13371ns
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631413 23437 hierarchical.cpp:1548] No allocations performed
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631450 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631465 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 202676ns
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631609 23435 hierarchical.cpp:1548] No allocations performed
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631640 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631655 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 102058ns
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632261 23431 hierarchical.cpp:1548] No allocations performed
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632294 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632308 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 112653ns
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632477 23433 hierarchical.cpp:1548] No allocations performed
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632510 23433 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632525 23433 hierarchical.cpp:1192] Performed allocation for 1 agents in 144467ns
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633517 23430 hierarchical.cpp:1548] No allocations performed
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633549 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633563 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 111395ns
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.633985 23436 hierarchical.cpp:1548] No allocations performed
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634018 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634048 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 132707ns
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634266 23430 hierarchical.cpp:1548] No allocations performed
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634299 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634313 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 103933ns
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635295 23431 hierarchical.cpp:1548] No allocations performed
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635330 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635346 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 115517ns
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635922 23436 hierarchical.cpp:1548] No allocations performed
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635958 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635973 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 109700ns
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636693 23437 hierarchical.cpp:1548] No allocations performed
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636728 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636744 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 123133ns
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637589 23432 hierarchical.cpp:1548] No allocations performed
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637624 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637639 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 118581ns
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638517 23431 hierarchical.cpp:1548] No allocations performed
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638550 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638566 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 107979ns
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639577 23435 hierarchical.cpp:1548] No allocations performed
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639612 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639628 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 126299ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640533 23430 hierarchical.cpp:1548] No allocations performed
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640566 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640581 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 106384ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.667985 23437 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.668124 23434 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.674278 23433 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:311: Failure
[20:13:23] :	 [Step 10/10] Failed to wait 15secs for statusFinished
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:301: Failure
[20:13:23] :	 [Step 10/10] Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
[20:13:23] :	 [Step 10/10]          Expected: to be called twice
[20:13:23] :	 [Step 10/10]            Actual: called once - unsatisfied and active
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618680 23433 master.cpp:1284] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618721 23433 master.cpp:2726] Disconnecting framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618737 23433 master.cpp:2750] Deactivating framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618883 23434 hierarchical.cpp:382] Deactivated framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] W0805 20:13:23.618918 23433 master.hpp:2131] Master attempted to send message to disconnected framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618963 23433 master.cpp:1297] Giving framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 0ns to failover
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619046 23434 hierarchical.cpp:924] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619258 23416 slave.cpp:767] Agent terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619321 23432 master.cpp:1245] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619336 23432 master.cpp:2785] Disconnecting agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619371 23432 master.cpp:2804] Deactivating agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619431 23432 hierarchical.cpp:571] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 deactivated
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620216 23435 master.cpp:5581] Framework failover timeout, removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620232 23435 master.cpp:6316] Removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620357 23433 hierarchical.cpp:333] Removed framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621464 23416 master.cpp:1092] Master terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621561 23433 hierarchical.cpp:510] Removed agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:23] :	 [Step 10/10] [  FAILED  ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller (16012 ms)
{noformat}",1,4,MESOS-6001,3.0
Re-evaluate libevent SSL socket EOF semantics in libprocess,"While debugging some issues related to libprocess finalization/reinitialization, [~bmahler] pointed out that libprocess doesn't strictly adhere to the expected behavior of Unix sockets after an EOF is received. If a socket receives EOF, this means only that the writer on the other end has closed the write end of its socket. However, the other end may still be interested in reading. Libprocess currently treats a received EOF as if {{shutdown()}} has been called on the socket, and both ends have been closed for both reading and writing (see [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L349-L360] and [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L692-L697]).

We should consider changing the EOF semantics of the {{Socket}} object to more closely match those of Unix sockets.",0,0,MESOS-5999,3.0
SSL Socket CHECK can fail after socket receives EOF,"While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:
{code}
I0804 08:32:33.263211 273793024 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.263209 273256448 process.cpp:2970] Cleaning up __limiter__(3)@127.0.0.1:55688
I0804 08:32:33.263263 275939328 libevent_ssl_socket.cpp:152] *** in initialize(): 14
I0804 08:32:33.263206 272719872 process.cpp:2865] Resuming (61)@127.0.0.1:55688 at 2016-08-04 15:32:33.263261952+00:00
I0804 08:32:33.263327 275939328 libevent_ssl_socket.cpp:584] *** in recv()14
I0804 08:32:33.263337 272719872 hierarchical.cpp:571] Agent e2a49340-34ec-403f-a5a4-15e29c4a2434-S0 deactivated
I0804 08:32:33.263322 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263343104+00:00
I0804 08:32:33.263510 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263536 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 19
I0804 08:32:33.263592 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 19
I0804 08:32:33.263622 1985901312 process.cpp:3170] Donating thread to (87)@127.0.0.1:55688 while waiting
I0804 08:32:33.263639 274329600 process.cpp:2865] Resuming __http__(12)@127.0.0.1:55688 at 2016-08-04 15:32:33.263653888+00:00
I0804 08:32:33.263659 1985901312 process.cpp:2865] Resuming (87)@127.0.0.1:55688 at 2016-08-04 15:32:33.263671040+00:00
I0804 08:32:33.263730 1985901312 process.cpp:2970] Cleaning up (87)@127.0.0.1:55688
I0804 08:32:33.263741 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263736 274329600 process.cpp:2970] Cleaning up __http__(12)@127.0.0.1:55688
I0804 08:32:33.263778 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 17
I0804 08:32:33.263818 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 17
I0804 08:32:33.263839 272183296 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263857920+00:00
I0804 08:32:33.263933 273793024 process.cpp:2865] Resuming __gc__@127.0.0.1:55688 at 2016-08-04 15:32:33.263951104+00:00
I0804 08:32:33.264034 275939328 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.264020 272719872 process.cpp:2865] Resuming __http__(11)@127.0.0.1:55688 at 2016-08-04 15:32:33.264041984+00:00
I0804 08:32:33.264036 274329600 process.cpp:2865] Resuming status-update-manager(3)@127.0.0.1:55688 at 2016-08-04 15:32:33.264056064+00:00
I0804 08:32:33.264071 272719872 process.cpp:2970] Cleaning up __http__(11)@127.0.0.1:55688
I0804 08:32:33.264088 274329600 process.cpp:2970] Cleaning up status-update-manager(3)@127.0.0.1:55688
I0804 08:32:33.264086 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.264112 272183296 process.cpp:2865] Resuming (89)@127.0.0.1:55688 at 2016-08-04 15:32:33.264126976+00:00
I0804 08:32:33.264118 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.264144896+00:00
I0804 08:32:33.264149 272183296 process.cpp:2970] Cleaning up (89)@127.0.0.1:55688
I0804 08:32:33.264202 275939328 libevent_ssl_socket.cpp:281] *** in send_callback(bev)
I0804 08:32:33.264400 273793024 process.cpp:3170] Donating thread to (86)@127.0.0.1:55688 while waiting
I0804 08:32:33.264413 273256448 process.cpp:2865] Resuming (76)@127.0.0.1:55688 at 2016-08-04 15:32:33.264428032+00:00
I0804 08:32:33.296268 275939328 libevent_ssl_socket.cpp:300] *** in send_callback(): 17
I0804 08:32:33.296419 273256448 process.cpp:2970] Cleaning up (76)@127.0.0.1:55688
I0804 08:32:33.296357 273793024 process.cpp:2865] Resuming (86)@127.0.0.1:55688 at 2016-08-04 15:32:33.296414976+00:00
I0804 08:32:33.296464 273793024 process.cpp:2970] Cleaning up (86)@127.0.0.1:55688
I0804 08:32:33.296497 275939328 libevent_ssl_socket.cpp:104] *** releasing SSL socket
I0804 08:32:33.296517 275939328 libevent_ssl_socket.cpp:106] *** released SSL socket: 19
I0804 08:32:33.296515 274329600 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.296532992+00:00
I0804 08:32:33.296550 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.296583 273793024 process.cpp:2865] Resuming (77)@127.0.0.1:55688 at 2016-08-04 15:32:33.296616960+00:00
F0804 08:32:33.296623 275939328 libevent_ssl_socket.cpp:723] Check failed: 'self->send_request.get()' Must be non NULL
*** Check failure stack trace: ***
{code}

The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive.",1,3,MESOS-5986,3.0
NvidiaVolume errors out if any binary is missing,"We currently error out if a binary we were trying to add to the volume is not found on the host filesystem. However, these are not the semantics that we want. By design, we list all the binaries that *may* exist on the filesystem that we want to put in the volume, not all of the binaries that *must* exist. We should simply skip any unfound binaries and move on to the next one instead of erroring out.",1,1,MESOS-5982,2.0
HealthChecker should not decide when to kill tasks and when to stop performing health checks.,"Currently, {{HealthChecker}} library decides when a task should be killed based on its health status. Moreover, it stops checking it health after that. This seems unfortunate, because it's up to the executor and / or framework to decide both when to kill tasks and when to health check them. ",1,2,MESOS-5963,5.0
All non-root tests fail on GPU machine,"A recent addition to ensure that {{NvidiaVolume::create()}} ran as root broke all non-root tests on GPU machines. The reason is that we unconditionally create this volume so long as we detect {{nvml.isAvailable()}} which will fail now that we are only allowed to create this volume if we have root permissions.

We should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of {{\-\-containerizer}} and {{\-\-isolation}} flags.",1,2,MESOS-5959,2.0
NvidiaVolume::create() should check for root before creating volume,"Without root, we cannot create the nvidia volume in {{/var/run/mesos}} or mount a {{tmpfs}} in cases where we need to override the {{noexec}} on the current file system.",1,1,MESOS-5945,2.0
"Unable to run ""scratch"" Dockerfiles with Unified Containerizer.","It is not possible to run Docker containers that are based upon the ""scratch"" container.

Setup: Mesos 1.0.0 with the following Mesos settings:

{code:none}
echo 'docker' | sudo tee /etc/mesos-slave/image_providers
echo 'filesystem/linux,docker/runtime' | sudo tee /etc/mesos-slave/isolation
{code}

Recreate: From a Master or Slave, run:

{code:none}
mesos-execute --command='echo ok' --docker_image=hello-seattle --master=localhost:5050 --name=test
{code}

Effect: The container will crash with messages from Mesos reporting it can't mount folder x/y/z. E.g. can't mount /tmp. This means you can't run any container that is not a ""fat"" container (i.e. one with a full OS). E.g. error: 
bq. Failed to enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd': /tmp in chroot does not existI0729 07:49:56.753474  4362 exec.cpp:413] Executor asked to shutdown

Expected: Run without issues.

Use case: We use scratch based containers with static binaries to keep the image size down. This is a common practice.",1,2,MESOS-5927,3.0
"Ubuntu 14.04 LTS GPU Isolator ""/run"" directory is noexec","In Ubuntu 14.04 LTS the mount for /run directory is noexec.  It affect the {{/var/run/mesos/isolators/gpu/nvidia_352.63/bin}} directory which mesos GPU isolators depended on.

{{bill@billz:/var/run$ mount | grep noexec
proc on /proc type proc (rw,noexec,nosuid,nodev)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)
tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)}}

The /var/run is link to /run:
{{bill@billz:/var$ ll
total 52
drwxr-xr-x 13 root root     4096 May  5 20:00 ./
drwxr-xr-x 27 root root     4096 Jul 14 17:29 ../
lrwxrwxrwx  1 root root        9 May  5 19:50 lock -> /run/lock/
drwxrwxr-x 19 root syslog   4096 Jul 28 08:00 log/
drwxr-xr-x  2 root root     4096 Aug  4  2015 opt/
lrwxrwxrwx  1 root root        4 May  5 19:50 run -> /run/}}

Current the work around is mount without noexec:
{{sudo mount -o remount,exec /run}}",1,2,MESOS-5923,3.0
Strict/RegistrarTest.UpdateQuota/0 is flaky,"Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}.",1,1,MESOS-5878,3.0
Document MESOS_SANDBOX executor env variable.,And we should document the difference with MESOS_DIRECTORY.,1,2,MESOS-5864,2.0
Logrotate ContainerLogger module does not rotate logs when run as root with `--switch_user`.,"The logrotate ContainerLogger module runs as the agent's user.  In most cases, this is {{root}}.

When {{logrotate}} is run as root, there is an additional check the configuration files must pass (because a root {{logrotate}} needs to be secured against non-root modifications to the configuration):
https://github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#L807-L815

Log rotation will fail under the following scenario:
1) The agent is run with {{--switch_user}} (default: true)
2) A task is launched with a non-root user specified
3) The logrotate module spawns a few companion processes (as root) and this creates the {{stdout}}, {{stderr}}, {{stdout.logrotate.conf}}, and {{stderr.logrotate.conf}} files (as root).  This step races with the next step.
4) The Mesos containerizer and Fetcher will {{chown}} the task's sandbox to the non-root user.  Including the files just created.
5) When {{logrotate}} is run, it will skip any non-root configuration files.  This means the files are not rotated.

----

Fix: The logrotate module's companion processes should call {{setuid}} and {{setgid}}.",1,4,MESOS-5856,3.0
CMake build needs to generate protobufs before building libmesos,"The existing CMake lists place protobufs at the same level as other Mesos sources:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415

This is incorrect, as protobuf changes need to be regenerated before we can build against them.

Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305",1,2,MESOS-5852,2.0
Modularize Network in replicated_log,Currently replicated_log relies on Zookeeper for coordinator election. This is done through network abstraction _ZookeeperNetwork_. We need to modularize this part in order to enable replicated_log when using Master contender/detector modules.,1,4,MESOS-5828,8.0
MasterAPITest.Subscribe is flaky,"This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.

On Mac OS X:
{noformat}[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 11:42:48.474665 1927435008 cluster.cpp:155] Creating default 'local' authorizer
I0708 11:42:48.480677 1927435008 leveldb.cpp:174] Opened db in 5727us
I0708 11:42:48.481494 1927435008 leveldb.cpp:181] Compacted db in 722us
I0708 11:42:48.481541 1927435008 leveldb.cpp:196] Created db iterator in 19us
I0708 11:42:48.481572 1927435008 leveldb.cpp:202] Seeked to beginning of db in 9us
I0708 11:42:48.481587 1927435008 leveldb.cpp:271] Iterated through 0 keys in the db in 7us
I0708 11:42:48.481617 1927435008 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 11:42:48.482030 350982144 recover.cpp:451] Starting replica recovery
I0708 11:42:48.482203 350982144 recover.cpp:477] Replica is in EMPTY status
I0708 11:42:48.484107 348299264 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3780)@127.0.0.1:50325
I0708 11:42:48.484318 350982144 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 11:42:48.484750 348835840 master.cpp:382] Master e055d60c-05ff-487e-82da-d0a43e52605c (localhost) started on 127.0.0.1:50325
I0708 11:42:48.484850 349908992 recover.cpp:568] Updating replica status to STARTING
I0708 11:42:48.484788 348835840 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/Sn2Kf4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/Sn2Kf4/master"" --zk_session_timeout=""10secs""
W0708 11:42:48.485263 348835840 master.cpp:387] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or agents. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.485291 348835840 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 11:42:48.485314 348835840 master.cpp:448] Master only allowing authenticated agents to register
I0708 11:42:48.485335 348835840 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 11:42:48.485347 348835840 credentials.hpp:37] Loading credentials for authentication from '/private/tmp/Sn2Kf4/credentials'
I0708 11:42:48.485373 349372416 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 397us
I0708 11:42:48.485414 349372416 replica.cpp:320] Persisted replica status to STARTING
I0708 11:42:48.485608 350982144 recover.cpp:477] Replica is in STARTING status
I0708 11:42:48.485749 348835840 master.cpp:506] Using default 'crammd5' authenticator
I0708 11:42:48.485852 348835840 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 11:42:48.486018 348835840 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 11:42:48.486140 348835840 master.cpp:705] Authorization enabled
I0708 11:42:48.486486 350982144 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (3783)@127.0.0.1:50325
I0708 11:42:48.486758 352055296 recover.cpp:197] Received a recover response from a replica in STARTING status
I0708 11:42:48.487176 350982144 recover.cpp:568] Updating replica status to VOTING
I0708 11:42:48.487576 352055296 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 300us
I0708 11:42:48.487658 352055296 replica.cpp:320] Persisted replica status to VOTING
I0708 11:42:48.487736 350982144 recover.cpp:582] Successfully joined the Paxos group
I0708 11:42:48.487951 350982144 recover.cpp:466] Recover process terminated
I0708 11:42:48.489441 348835840 master.cpp:1973] The newly elected leader is master@127.0.0.1:50325 with id e055d60c-05ff-487e-82da-d0a43e52605c
I0708 11:42:48.489518 348835840 master.cpp:1986] Elected as the leading master!
I0708 11:42:48.489545 348835840 master.cpp:1673] Recovering from registrar
I0708 11:42:48.489637 350982144 registrar.cpp:332] Recovering registrar
I0708 11:42:48.490120 351518720 log.cpp:553] Attempting to start the writer
I0708 11:42:48.491161 350445568 replica.cpp:493] Replica received implicit promise request from (3784)@127.0.0.1:50325 with proposal 1
I0708 11:42:48.491461 350445568 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 252us
I0708 11:42:48.491528 350445568 replica.cpp:342] Persisted promised to 1
I0708 11:42:48.492337 348299264 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0708 11:42:48.493482 349372416 replica.cpp:388] Replica received explicit promise request from (3785)@127.0.0.1:50325 for position 0 with proposal 2
I0708 11:42:48.493854 349372416 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 283us
I0708 11:42:48.493904 349372416 replica.cpp:712] Persisted action at 0
I0708 11:42:48.495302 348299264 replica.cpp:537] Replica received write request for position 0 from (3786)@127.0.0.1:50325
I0708 11:42:48.495455 348299264 leveldb.cpp:436] Reading position from leveldb took 45us
I0708 11:42:48.495761 348299264 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 261us
I0708 11:42:48.495803 348299264 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496484 350445568 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0708 11:42:48.496795 350445568 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 255us
I0708 11:42:48.496857 350445568 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496896 350445568 replica.cpp:697] Replica learned NOP action at position 0
I0708 11:42:48.497445 350982144 log.cpp:569] Writer started with ending position 0
I0708 11:42:48.498523 350982144 leveldb.cpp:436] Reading position from leveldb took 80us
I0708 11:42:48.499307 349908992 registrar.cpp:365] Successfully fetched the registry (0B) in 9.63712ms
I0708 11:42:48.499464 349908992 registrar.cpp:464] Applied 1 operations in 36us; attempting to update the 'registry'
I0708 11:42:48.499953 351518720 log.cpp:577] Attempting to append 159 bytes to the log
I0708 11:42:48.500088 350982144 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0708 11:42:48.500880 348299264 replica.cpp:537] Replica received write request for position 1 from (3787)@127.0.0.1:50325
I0708 11:42:48.501186 348299264 leveldb.cpp:341] Persisting action (178 bytes) to leveldb took 259us
I0708 11:42:48.501231 348299264 replica.cpp:712] Persisted action at 1
I0708 11:42:48.501786 351518720 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0708 11:42:48.502118 351518720 leveldb.cpp:341] Persisting action (180 bytes) to leveldb took 311us
I0708 11:42:48.502260 351518720 replica.cpp:712] Persisted action at 1
I0708 11:42:48.502305 351518720 replica.cpp:697] Replica learned APPEND action at position 1
I0708 11:42:48.503475 349908992 registrar.cpp:509] Successfully updated the 'registry' in 3.944192ms
I0708 11:42:48.503909 349908992 registrar.cpp:395] Successfully recovered registrar
I0708 11:42:48.504003 350982144 log.cpp:596] Attempting to truncate the log to 1
I0708 11:42:48.504250 349372416 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0708 11:42:48.504546 350445568 master.cpp:1781] Recovered 0 agents from the Registry (121B) ; allowing 10mins for agents to re-register
I0708 11:42:48.506022 352055296 replica.cpp:537] Replica received write request for position 2 from (3788)@127.0.0.1:50325
I0708 11:42:48.506479 352055296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 320us
I0708 11:42:48.506513 352055296 replica.cpp:712] Persisted action at 2
I0708 11:42:48.506978 351518720 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0708 11:42:48.507155 351518720 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 169us
I0708 11:42:48.507237 351518720 leveldb.cpp:399] Deleting ~1 keys from leveldb took 37us
I0708 11:42:48.507264 351518720 replica.cpp:712] Persisted action at 2
I0708 11:42:48.507285 351518720 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0708 11:42:48.521363 1927435008 cluster.cpp:432] Creating default 'local' authorizer
I0708 11:42:48.522498 350982144 slave.cpp:205] Agent started on 119)@127.0.0.1:50325
I0708 11:42:48.522538 350982144 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/zhitao/Uber/sync/zhitao-mesos1.dev.uber.com/home/uber/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX""
W0708 11:42:48.522903 350982144 slave.cpp:209] 
**************************************************
Agent bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.522922 350982144 credentials.hpp:86] Loading credential for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential'
W0708 11:42:48.522965 1927435008 scheduler.cpp:157] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0708 11:42:48.522992 1927435008 scheduler.cpp:172] Version: 1.0.0
I0708 11:42:48.523066 350982144 slave.cpp:343] Agent using credential for: test-principal
I0708 11:42:48.523092 350982144 credentials.hpp:37] Loading credentials for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials'
I0708 11:42:48.523334 350982144 slave.cpp:395] Using default 'basic' HTTP authenticator
I0708 11:42:48.523973 352055296 scheduler.cpp:461] New master detected at master@127.0.0.1:50325
I0708 11:42:48.524050 350982144 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.524196 350982144 slave.cpp:602] Agent attributes: [  ]
I0708 11:42:48.524224 350982144 slave.cpp:607] Agent hostname: localhost
I0708 11:42:48.525522 350445568 state.cpp:57] Recovering state from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/meta'
I0708 11:42:48.525853 350445568 status_update_manager.cpp:200] Recovering status update manager
I0708 11:42:48.526165 350445568 slave.cpp:4856] Finished recovery
I0708 11:42:48.527223 349372416 status_update_manager.cpp:174] Pausing sending status updates
I0708 11:42:48.527231 352055296 slave.cpp:969] New master detected at master@127.0.0.1:50325
I0708 11:42:48.527276 352055296 slave.cpp:1028] Authenticating with master master@127.0.0.1:50325
I0708 11:42:48.527328 352055296 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0708 11:42:48.527561 352055296 slave.cpp:1001] Detecting new master
I0708 11:42:48.527582 348299264 authenticatee.cpp:121] Creating new client SASL connection
I0708 11:42:48.528666 349908992 master.cpp:6006] Authenticating slave(119)@127.0.0.1:50325
I0708 11:42:48.528880 352055296 authenticator.cpp:98] Creating new server SASL connection
I0708 11:42:48.529089 350445568 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50918
I0708 11:42:48.529233 350445568 master.cpp:2272] Received subscription request for HTTP framework 'default'
I0708 11:42:48.529261 350445568 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0708 11:42:48.529323 352055296 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0708 11:42:48.529357 352055296 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0708 11:42:48.529417 352055296 authenticator.cpp:204] Received SASL authentication start
I0708 11:42:48.529503 352055296 authenticator.cpp:326] Authentication requires more steps
I0708 11:42:48.529561 352055296 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0708 11:42:48.529721 349908992 authenticatee.cpp:259] Received SASL authentication step
I0708 11:42:48.530005 348835840 authenticator.cpp:232] Received SASL authentication step
I0708 11:42:48.530241 348835840 authenticator.cpp:318] Authentication success
I0708 11:42:48.530254 350445568 hierarchical.cpp:271] Added framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.530900 349908992 authenticatee.cpp:299] Authentication success
I0708 11:42:48.531186 350982144 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(119)@127.0.0.1:50325
I0708 11:42:48.531657 348299264 slave.cpp:1123] Successfully authenticated with master master@127.0.0.1:50325
I0708 11:42:48.531935 349372416 master.cpp:4676] Registering agent at slave(119)@127.0.0.1:50325 (localhost) with id e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.532304 349908992 registrar.cpp:464] Applied 1 operations in 55us; attempting to update the 'registry'
I0708 11:42:48.532908 348835840 log.cpp:577] Attempting to append 326 bytes to the log
I0708 11:42:48.533015 352055296 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0708 11:42:48.533641 349372416 replica.cpp:537] Replica received write request for position 3 from (3798)@127.0.0.1:50325
I0708 11:42:48.533867 349372416 leveldb.cpp:341] Persisting action (345 bytes) to leveldb took 186us
I0708 11:42:48.533917 349372416 replica.cpp:712] Persisted action at 3
I0708 11:42:48.537066 349908992 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0708 11:42:48.538169 349908992 leveldb.cpp:341] Persisting action (347 bytes) to leveldb took 914us
I0708 11:42:48.538226 349908992 replica.cpp:712] Persisted action at 3
I0708 11:42:48.538255 349908992 replica.cpp:697] Replica learned APPEND action at position 3
I0708 11:42:48.539247 352055296 registrar.cpp:509] Successfully updated the 'registry' in 6.895104ms
I0708 11:42:48.539302 348299264 log.cpp:596] Attempting to truncate the log to 3
I0708 11:42:48.539393 348299264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0708 11:42:48.539798 348835840 master.cpp:4745] Registered agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.539881 348299264 hierarchical.cpp:478] Added agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0708 11:42:48.539901 349908992 slave.cpp:1169] Registered with master master@127.0.0.1:50325; given agent ID e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.540287 350445568 status_update_manager.cpp:181] Resuming sending status updates
I0708 11:42:48.540501 351518720 replica.cpp:537] Replica received write request for position 4 from (3799)@127.0.0.1:50325
I0708 11:42:48.540583 352055296 master.cpp:5835] Sending 1 offers to framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.540798 351518720 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 247us
I0708 11:42:48.540868 351518720 replica.cpp:712] Persisted action at 4
I0708 11:42:48.540895 349908992 slave.cpp:1229] Forwarding total oversubscribed resources 
I0708 11:42:48.541035 352055296 master.cpp:5128] Received update of agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with total oversubscribed resources 
I0708 11:42:48.541291 349908992 hierarchical.cpp:542] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0708 11:42:48.541630 350982144 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0708 11:42:48.541911 350982144 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 189us
I0708 11:42:48.541965 350982144 leveldb.cpp:399] Deleting ~2 keys from leveldb took 28us
I0708 11:42:48.541987 350982144 replica.cpp:712] Persisted action at 4
I0708 11:42:48.542006 350982144 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0708 11:42:48.544836 352055296 http.cpp:381] HTTP POST for /master/api/v1 from 127.0.0.1:50920
I0708 11:42:48.544884 352055296 http.cpp:484] Processing call SUBSCRIBE
I0708 11:42:48.545382 352055296 master.cpp:7599] Added subscriber: a85e7341-ac15-4f18-9021-1a2efa326442 to the list of active subscribers
I0708 11:42:48.550048 348835840 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50919
I0708 11:42:48.550339 348835840 master.cpp:3468] Processing ACCEPT call for offers: [ e055d60c-05ff-487e-82da-d0a43e52605c-O0 ] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.550390 348835840 master.cpp:3106] Authorizing framework principal 'test-principal' to launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1
W0708 11:42:48.551434 348835840 validation.cpp:650] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0708 11:42:48.551477 348835840 validation.cpp:662] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0708 11:42:48.551803 348835840 master.cpp:7565] Adding task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost)
I0708 11:42:48.551949 348835840 master.cpp:3957] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:42:48.552151 352055296 slave.cpp:1569] Got assigned task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.552592 352055296 slave.cpp:1688] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.553282 352055296 paths.cpp:528] Trying to chown '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' to user 'zhitao'
I0708 11:42:48.566201 352055296 slave.cpp:5748] Launching executor default of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 with resources  in work directory '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26'
I0708 11:42:48.567876 352055296 executor.cpp:188] Version: 1.0.0
I0708 11:42:48.568428 352055296 slave.cpp:1914] Queuing task 'd94e54c0-8c89-43bd-be2f-adeb8cf70cb1' for executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
E0708 11:42:48.571115 352591872 process.cpp:2104] Failed to shutdown socket with fd 254: Socket is not connected
W0708 11:42:48.570768 352055296 executor.cpp:739] Dropping SUBSCRIBE: Executor is in state DISCONNECTED

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: disconnected(0x7fad21fcebf0)
Stack trace:
../../src/tests/api_tests.cpp:1537: Failure
Failed to wait 15secs for event
E0708 11:43:03.556205 352591872 process.cpp:2104] Failed to shutdown socket with fd 235: Socket is not connected
E0708 11:43:03.556584 352591872 process.cpp:2104] Failed to shutdown socket with fd 223: Socket is not connected
I0708 11:43:03.557134 349908992 master.cpp:1410] Framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) disconnected
I0708 11:43:03.557176 349908992 master.cpp:2851] Disconnecting framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557209 349908992 master.cpp:2875] Deactivating framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557415 349908992 master.cpp:1423] Giving framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) 0ns to failover
I0708 11:43:03.557456 348835840 hierarchical.cpp:382] Deactivated framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.557878 350445568 master.cpp:5687] Framework failover timeout, removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557945 350445568 master.cpp:6422] Removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.558076 352055296 slave.cpp:2292] Asked to shut down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 by master@127.0.0.1:50325
I0708 11:43:03.558106 352055296 slave.cpp:2317] Shutting down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.558131 352055296 slave.cpp:4481] Shutting down executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.558147 352055296 slave.hpp:768] Unable to send event to executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000: unknown connection type
I0708 11:43:03.558188 350445568 master.cpp:6959] Updating the state of task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0708 11:43:03.558507 350445568 master.cpp:7025] Removing task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.558709 350445568 master.cpp:7054] Removing executor 'default' with resources  of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.559051 349372416 hierarchical.cpp:333] Removed framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.567955 350982144 slave.cpp:4163] Executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 exited with status 0
I0708 11:43:03.568176 350982144 slave.cpp:4267] Cleaning up executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.568258 348299264 master.cpp:5369] Ignoring unknown exited executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.568584 348299264 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' for gc 6.99999342143407days in the future
I0708 11:43:03.568864 350982144 slave.cpp:4355] Cleaning up framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.568879 352055296 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default' for gc 6.99999341739556days in the future
I0708 11:43:03.569056 350445568 status_update_manager.cpp:282] Closing status update streams for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.569247 350982144 slave.cpp:841] Agent terminating
I0708 11:43:03.569239 348835840 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000' for gc 6.99999341315852days in the future
I0708 11:43:03.569524 350982144 master.cpp:1371] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) disconnected
I0708 11:43:03.569577 350982144 master.cpp:2910] Disconnecting agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.569767 350982144 master.cpp:2929] Deactivating agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.570020 349372416 hierarchical.cpp:571] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 deactivated
../../src/tests/api_tests.cpp:1509: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, acknowledged(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1505: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, launch(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1503: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, subscribed(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1496: Failure
Actual function call count doesn't match EXPECT_CALL(*scheduler, update(_, _))...
         Expected: to be called twice
           Actual: never called - unsatisfied and active
I0708 11:43:03.572598 1927435008 master.cpp:1218] Master terminating
I0708 11:43:03.572844 352055296 hierarchical.cpp:510] Removed agent e055d60c-05ff-487e-82da-d0a43e52605c-S0
[  FAILED  ] ContentType/MasterAPITest.Subscribe/0, where GetParam() = application/x-protobuf (15105 ms)
{noformat}

On CentOS 7
{noformat}
[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 15:42:16.042171 29138 cluster.cpp:155] Creating default 'local' authorizer
I0708 15:42:16.154358 29138 leveldb.cpp:174] Opened db in 111.818825ms
I0708 15:42:16.197175 29138 leveldb.cpp:181] Compacted db in 42.714984ms
I0708 15:42:16.197293 29138 leveldb.cpp:196] Created db iterator in 32582ns
I0708 15:42:16.197324 29138 leveldb.cpp:202] Seeked to beginning of db in 4050ns
I0708 15:42:16.197343 29138 leveldb.cpp:271] Iterated through 0 keys in the db in 538ns
I0708 15:42:16.197417 29138 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 15:42:16.198655 29157 recover.cpp:451] Starting replica recovery
I0708 15:42:16.199364 29161 recover.cpp:477] Replica is in EMPTY status
I0708 15:42:16.200865 29161 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (16431)@172.17.0.3:34502
I0708 15:42:16.201282 29158 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 15:42:16.203222 29160 recover.cpp:568] Updating replica status to STARTING
I0708 15:42:16.204633 29158 master.cpp:382] Master 2aea5b7f-ec9f-4fda-8f34-877d8adf064f (0382d073a49a) started on 172.17.0.3:34502
I0708 15:42:16.204675 29158 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/Lu916I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.1.0/_inst/share/mesos/webui"" --work_dir=""/tmp/Lu916I/master"" --zk_session_timeout=""10secs""
I0708 15:42:16.205265 29158 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 15:42:16.205283 29158 master.cpp:448] Master only allowing authenticated agents to register
I0708 15:42:16.205294 29158 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 15:42:16.205307 29158 credentials.hpp:37] Loading credentials for authentication from '/tmp/Lu916I/credentials'
I0708 15:42:16.205705 29158 master.cpp:506] Using default 'crammd5' authenticator
I0708 15:42:16.205940 29158 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 15:42:16.206192 29158 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 15:42:16.206374 29158 master.cpp:705] Authorization enabled
I0708 15:42:16.206866 29172 hierarchical.cpp:151] Initialized hierarchical allocator process
I0708 15:42:16.207018 29172 whitelist_watcher.cpp:77] No whitelist given
I0708 15:42:16.210026 29165 master.cpp:1973] The newly elected leader is master@172.17.0.3:34502 with id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f
I0708 15:42:16.210187 29165 master.cpp:1986] Elected as the leading master!
I0708 15:42:16.210330 29165 master.cpp:1673] Recovering from registrar
I0708 15:42:16.210577 29171 registrar.cpp:332] Recovering registrar
I0708 15:42:16.239378 29160 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 35.540287ms
I0708 15:42:16.239485 29160 replica.cpp:320] Persisted replica status to STARTING
I0708 15:42:16.239938 29161 recover.cpp:477] Replica is in STARTING status
I0708 15:42:16.242017 29165 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (16434)@172.17.0.3:34502
I0708 15:42:16.242527 29167 recover.cpp:197] Received a recover response from a replica in STARTING status
I0708 15:42:16.243140 29167 recover.cpp:568] Updating replica status to VOTING
I0708 15:42:16.281746 29167 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.318978ms
I0708 15:42:16.281828 29167 replica.cpp:320] Persisted replica status to VOTING
I0708 15:42:16.282094 29170 recover.cpp:582] Successfully joined the Paxos group
I0708 15:42:16.282440 29170 recover.cpp:466] Recover process terminated
I0708 15:42:16.283365 29170 log.cpp:553] Attempting to start the writer
I0708 15:42:16.285605 29167 replica.cpp:493] Replica received implicit promise request from (16435)@172.17.0.3:34502 with proposal 1
I0708 15:42:16.315435 29167 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.761608ms
I0708 15:42:16.315528 29167 replica.cpp:342] Persisted promised to 1
I0708 15:42:16.317147 29159 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0708 15:42:16.318914 29160 replica.cpp:388] Replica received explicit promise request from (16436)@172.17.0.3:34502 for position 0 with proposal 2
I0708 15:42:16.348886 29160 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 29.896283ms
I0708 15:42:16.349161 29160 replica.cpp:712] Persisted action at 0
I0708 15:42:16.350939 29170 replica.cpp:537] Replica received write request for position 0 from (16437)@172.17.0.3:34502
I0708 15:42:16.351029 29170 leveldb.cpp:436] Reading position from leveldb took 42967ns
I0708 15:42:16.382378 29170 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 31.28917ms
I0708 15:42:16.382464 29170 replica.cpp:712] Persisted action at 0
I0708 15:42:16.383646 29169 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0708 15:42:16.415894 29169 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 32.189511ms
I0708 15:42:16.416015 29169 replica.cpp:712] Persisted action at 0
I0708 15:42:16.416056 29169 replica.cpp:697] Replica learned NOP action at position 0
I0708 15:42:16.417312 29168 log.cpp:569] Writer started with ending position 0
I0708 15:42:16.418628 29167 leveldb.cpp:436] Reading position from leveldb took 56748ns
I0708 15:42:16.420019 29165 registrar.cpp:365] Successfully fetched the registry (0B) in 209.31712ms
I0708 15:42:16.420155 29165 registrar.cpp:464] Applied 1 operations in 30566ns; attempting to update the 'registry'
I0708 15:42:16.420994 29172 log.cpp:577] Attempting to append 168 bytes to the log
I0708 15:42:16.421149 29157 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0708 15:42:16.422169 29162 replica.cpp:537] Replica received write request for position 1 from (16438)@172.17.0.3:34502
I0708 15:42:16.457743 29162 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 35.505294ms
I0708 15:42:16.457844 29162 replica.cpp:712] Persisted action at 1
I0708 15:42:16.459228 29172 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0708 15:42:16.495947 29172 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 36.653391ms
I0708 15:42:16.496048 29172 replica.cpp:712] Persisted action at 1
I0708 15:42:16.496091 29172 replica.cpp:697] Replica learned APPEND action at position 1
I0708 15:42:16.497947 29172 registrar.cpp:509] Successfully updated the 'registry' in 77.703936ms
I0708 15:42:16.498132 29172 registrar.cpp:395] Successfully recovered registrar
I0708 15:42:16.498169 29171 log.cpp:596] Attempting to truncate the log to 1
I0708 15:42:16.498294 29162 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0708 15:42:16.498668 29171 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register
I0708 15:42:16.498919 29162 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
I0708 15:42:16.499577 29171 replica.cpp:537] Replica received write request for position 2 from (16439)@172.17.0.3:34502
I0708 15:42:16.521065 29171 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 21.423468ms
I0708 15:42:16.521160 29171 replica.cpp:712] Persisted action at 2
I0708 15:42:16.522766 29171 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0708 15:42:16.546223 29171 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.402601ms
I0708 15:42:16.546380 29171 leveldb.cpp:399] Deleting ~1 keys from leveldb took 70830ns
I0708 15:42:16.546411 29171 replica.cpp:712] Persisted action at 2
I0708 15:42:16.546445 29171 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0708 15:42:16.560467 29138 cluster.cpp:432] Creating default 'local' authorizer
I0708 15:42:16.565003 29162 slave.cpp:205] Agent started on 449)@172.17.0.3:34502
I0708 15:42:16.565520 29138 scheduler.cpp:172] Version: 1.1.0
I0708 15:42:16.565150 29162 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-1.1.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M""
I0708 15:42:16.566128 29162 credentials.hpp:86] Loading credential for authentication from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/credential'
I0708 15:42:16.566423 29162 slave.cpp:343] Agent using credential for: test-principal
I0708 15:42:16.566520 29171 scheduler.cpp:461] New master detected at master@172.17.0.3:34502
I0708 15:42:16.566543 29162 credentials.hpp:37] Loading credentials for authentication from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/http_credentials'
I0708 15:42:16.566557 29171 scheduler.cpp:470] Waiting for 0ns before initiating a re-(connection) attempt with the master
I0708 15:42:16.566838 29162 slave.cpp:395] Using default 'basic' HTTP authenticator
I0708 15:42:16.568023 29162 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0708 15:42:16.568527 29162 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0708 15:42:16.569443 29162 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 15:42:16.569535 29162 slave.cpp:602] Agent attributes: [  ]
I0708 15:42:16.569552 29162 slave.cpp:607] Agent hostname: 0382d073a49a
I0708 15:42:16.571897 29165 state.cpp:57] Recovering state from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/meta'
I0708 15:42:16.572376 29165 status_update_manager.cpp:200] Recovering status update manager
I0708 15:42:16.572638 29165 slave.cpp:4856] Finished recovery
I0708 15:42:16.573194 29165 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0708 15:42:16.574082 29165 slave.cpp:969] New master detected at master@172.17.0.3:34502
I0708 15:42:16.574111 29165 slave.cpp:1028] Authenticating with master master@172.17.0.3:34502
I0708 15:42:16.574174 29165 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0708 15:42:16.574213 29162 status_update_manager.cpp:174] Pausing sending status updates
I0708 15:42:16.574323 29165 slave.cpp:1001] Detecting new master
I0708 15:42:16.574525 29165 authenticatee.cpp:121] Creating new client SASL connection
I0708 15:42:16.574851 29160 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0708 15:42:16.575621 29164 scheduler.cpp:349] Connected with the master at http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.577546 29164 scheduler.cpp:231] Sending SUBSCRIBE call to http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.579020 29168 master.cpp:6006] Authenticating slave(449)@172.17.0.3:34502
I0708 15:42:16.579133 29165 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(926)@172.17.0.3:34502
I0708 15:42:16.579236 29168 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0708 15:42:16.579448 29157 authenticator.cpp:98] Creating new server SASL connection
I0708 15:42:16.579684 29165 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0708 15:42:16.579722 29165 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0708 15:42:16.579831 29165 authenticator.cpp:204] Received SASL authentication start
I0708 15:42:16.579910 29165 authenticator.cpp:326] Authentication requires more steps
I0708 15:42:16.580013 29165 authenticatee.cpp:259] Received SASL authentication step
I0708 15:42:16.580111 29165 authenticator.cpp:232] Received SASL authentication step
I0708 15:42:16.580143 29165 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0382d073a49a' server FQDN: '0382d073a49a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0708 15:42:16.580157 29165 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0708 15:42:16.580196 29165 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0708 15:42:16.580227 29165 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0382d073a49a' server FQDN: '0382d073a49a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0708 15:42:16.580240 29165 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0708 15:42:16.580251 29165 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0708 15:42:16.580271 29165 authenticator.cpp:318] Authentication success
I0708 15:42:16.580420 29165 authenticatee.cpp:299] Authentication success
I0708 15:42:16.580525 29165 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(926)@172.17.0.3:34502
I0708 15:42:16.580840 29165 slave.cpp:1123] Successfully authenticated with master master@172.17.0.3:34502
I0708 15:42:16.581131 29165 slave.cpp:1529] Will retry registration in 814473ns if necessary
I0708 15:42:16.581560 29168 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(449)@172.17.0.3:34502
I0708 15:42:16.581795 29168 master.cpp:4676] Registering agent at slave(449)@172.17.0.3:34502 (0382d073a49a) with id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
I0708 15:42:16.583050 29162 registrar.cpp:464] Applied 1 operations in 131284ns; attempting to update the 'registry'
I0708 15:42:16.584233 29170 slave.cpp:1529] Will retry registration in 27.411836ms if necessary
I0708 15:42:16.584384 29158 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.585019 29168 log.cpp:577] Attempting to append 337 bytes to the log
I0708 15:42:16.585113 29162 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:33142
I0708 15:42:16.585156 29159 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0708 15:42:16.585417 29162 master.cpp:2272] Received subscription request for HTTP framework 'default'
I0708 15:42:16.585486 29162 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0708 15:42:16.586509 29159 replica.cpp:537] Replica received write request for position 3 from (16448)@172.17.0.3:34502
I0708 15:42:16.587302 29168 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0708 15:42:16.588059 29170 master.hpp:2010] Sending heartbeat to 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.588745 29168 hierarchical.cpp:271] Added framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.588819 29168 hierarchical.cpp:1537] No allocations performed
I0708 15:42:16.588851 29168 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.588910 29168 hierarchical.cpp:1172] Performed allocation for 0 agents in 138375ns
I0708 15:42:16.593391 29162 scheduler.cpp:662] Enqueuing event SUBSCRIBED received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.594115 29162 scheduler.cpp:662] Enqueuing event HEARTBEAT received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.612622 29162 slave.cpp:1529] Will retry registration in 35.186867ms if necessary
I0708 15:42:16.613113 29169 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.621047 29159 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 34.409256ms
I0708 15:42:16.621134 29159 replica.cpp:712] Persisted action at 3
I0708 15:42:16.622661 29159 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0708 15:42:16.646806 29159 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 24.085822ms
I0708 15:42:16.646906 29159 replica.cpp:712] Persisted action at 3
I0708 15:42:16.646986 29159 replica.cpp:697] Replica learned APPEND action at position 3
I0708 15:42:16.649273 29157 registrar.cpp:509] Successfully updated the 'registry' in 66.121984ms
I0708 15:42:16.649538 29167 slave.cpp:1529] Will retry registration in 111.475397ms if necessary
I0708 15:42:16.649603 29158 log.cpp:596] Attempting to truncate the log to 3
I0708 15:42:16.649811 29157 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.650069 29160 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0708 15:42:16.650713 29160 slave.cpp:3760] Received ping from slave-observer(404)@172.17.0.3:34502
I0708 15:42:16.650879 29160 slave.cpp:1169] Registered with master master@172.17.0.3:34502; given agent ID 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
I0708 15:42:16.651007 29160 fetcher.cpp:86] Clearing fetcher cache
I0708 15:42:16.651065 29158 hierarchical.cpp:478] Added agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0708 15:42:16.651480 29160 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/meta/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/slave.info'
I0708 15:42:16.651499 29166 status_update_manager.cpp:181] Resuming sending status updates
I0708 15:42:16.650825 29157 master.cpp:4745] Registered agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 15:42:16.651847 29158 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.652433 29158 hierarchical.cpp:1195] Performed allocation for agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 in 1.317746ms
I0708 15:42:16.651897 29172 replica.cpp:537] Replica received write request for position 4 from (16450)@172.17.0.3:34502
I0708 15:42:16.653264 29157 master.cpp:5835] Sending 1 offers to framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.654682 29160 slave.cpp:1229] Forwarding total oversubscribed resources 
I0708 15:42:16.656188 29165 master.cpp:5128] Received update of agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) with total oversubscribed resources 
I0708 15:42:16.656200 29164 scheduler.cpp:662] Enqueuing event OFFERS received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.656708 29159 hierarchical.cpp:542] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0708 15:42:16.657385 29159 hierarchical.cpp:1537] No allocations performed
I0708 15:42:16.657438 29159 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.657519 29159 hierarchical.cpp:1195] Performed allocation for agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 in 516462ns
I0708 15:42:16.660909 29163 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1'
I0708 15:42:16.661958 29163 http.cpp:381] HTTP POST for /master/api/v1 from 172.17.0.3:33143
I0708 15:42:16.662125 29163 http.cpp:484] Processing call SUBSCRIBE
I0708 15:42:16.663280 29164 master.cpp:7599] Added subscriber: 726edf8d-ad3d-4d08-9243-de3dc2df5f4a to the list of active subscribers
I0708 15:42:16.671409 29161 scheduler.cpp:231] Sending ACCEPT call to http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.672615 29165 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0708 15:42:16.676375 29169 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:33141
I0708 15:42:16.677199 29169 master.cpp:3468] Processing ACCEPT call for offers: [ 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-O0 ] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.677291 29169 master.cpp:3106] Authorizing framework principal 'test-principal' to launch task d8bd1ba3-055a-4420-820c-8e85fdde7c08
W0708 15:42:16.679435 29169 validation.cpp:650] Executor default for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0708 15:42:16.679492 29169 validation.cpp:662] Executor default for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0708 15:42:16.680003 29169 master.cpp:7573] Notifying all active subscribers about TASK_ADDED event
I0708 15:42:16.680454 29172 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 27.707387ms
I0708 15:42:16.680685 29172 replica.cpp:712] Persisted action at 4
I0708 15:42:16.681685 29168 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0708 15:42:16.680449 29169 master.cpp:7565] Adding task d8bd1ba3-055a-4420-820c-8e85fdde7c08 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a)
I0708 15:42:16.682688 29169 master.cpp:3957] Launching task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.683289 29171 slave.cpp:1569] Got assigned task d8bd1ba3-055a-4420-820c-8e85fdde7c08 for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.683903 29171 slave.cpp:1688] Launching task d8bd1ba3-055a-4420-820c-8e85fdde7c08 for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.684563 29171 paths.cpp:528] Trying to chown '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' to user 'mesos'
I0708 15:42:16.699834 29171 slave.cpp:5748] Launching executor default of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 with resources  in work directory '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3'
I0708 15:42:16.702018 29171 executor.cpp:188] Version: 1.1.0
I0708 15:42:16.702541 29171 slave.cpp:1914] Queuing task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08' for executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.702777 29171 slave.cpp:922] Successfully attached file '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3'
I0708 15:42:16.704201 29169 executor.cpp:389] Connected with the agent
I0708 15:42:16.704911 29159 executor.cpp:290] Sending SUBSCRIBE call to http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.706003 29170 process.cpp:3322] Handling HTTP event for process 'slave(449)' with path: '/slave(449)/api/v1/executor'
I0708 15:42:16.706897 29157 http.cpp:270] HTTP POST for /slave(449)/api/v1/executor from 172.17.0.3:33144
I0708 15:42:16.707108 29157 slave.cpp:2735] Received Subscribe request for HTTP executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.707819 29168 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 26.083403ms
I0708 15:42:16.707986 29168 leveldb.cpp:399] Deleting ~2 keys from leveldb took 117548ns
I0708 15:42:16.708031 29168 replica.cpp:712] Persisted action at 4
I0708 15:42:16.708076 29168 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0708 15:42:16.708082 29157 slave.cpp:2079] Sending queued task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08' to executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.710268 29163 executor.cpp:707] Enqueuing event SUBSCRIBED received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.712131 29169 executor.cpp:707] Enqueuing event LAUNCH received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.713174 29172 executor.cpp:290] Sending UPDATE call to http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.713984 29170 process.cpp:3322] Handling HTTP event for process 'slave(449)' with path: '/slave(449)/api/v1/executor'
I0708 15:42:16.714614 29170 http.cpp:270] HTTP POST for /slave(449)/api/v1/executor from 172.17.0.3:33145
I0708 15:42:16.714753 29170 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715451 29172 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715498 29172 status_update_manager.cpp:497] Creating StatusUpdate stream for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715996 29172 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to the agent
I0708 15:42:16.716584 29172 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to master@172.17.0.3:34502
I0708 15:42:16.716956 29172 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.717159 29171 master.cpp:5273] Status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 from agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.717265 29171 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.718080 29168 executor.cpp:707] Enqueuing event ACKNOWLEDGED received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.718299 29171 master.cpp:7573] Notifying all active subscribers about TASK_UPDATED event
I0708 15:42:16.719683 29171 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0708 15:42:16.720386 29171 scheduler.cpp:662] Enqueuing event UPDATE received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.726471 29164 hierarchical.cpp:1537] No allocations performed
W0708 15:42:16.726742 29163 status_update_manager.cpp:475] Resending status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.726840 29163 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to the agent
I0708 15:42:16.727401 29163 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to master@172.17.0.3:34502
I0708 15:42:16.727880 29163 master.cpp:5273] Status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 from agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.728035 29163 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.728570 29163 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0708 15:42:16.728003 29164 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.730080 29164 hierarchical.cpp:1172] Performed allocation for 1 agents in 3.858387ms
I0708 15:42:16.751055 29160 master.cpp:1410] Framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) disconnected
I0708 15:42:16.751116 29160 master.cpp:2851] Disconnecting framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.751149 29160 master.cpp:2875] Deactivating framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.751242 29160 master.cpp:1423] Giving framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) 0ns to failover
I0708 15:42:16.751602 29160 hierarchical.cpp:382] Deactivated framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.755091 29157 master.cpp:5687] Framework failover timeout, removing framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.755164 29157 master.cpp:6422] Removing framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.755425 29157 master.cpp:7573] Notifying all active subscribers about TASK_UPDATED event
I0708 15:42:16.755795 29157 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0708 15:42:16.756032 29166 slave.cpp:2292] Asked to shut down framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 by master@172.17.0.3:34502
I0708 15:42:16.756093 29166 slave.cpp:2317] Shutting down framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.756172 29166 slave.cpp:4481] Shutting down executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.757699 29157 master.cpp:7025] Removing task d8bd1ba3-055a-4420-820c-8e85fdde7c08 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.758779 29161 hierarchical.cpp:924] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 from framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.759784 29161 executor.cpp:707] Enqueuing event SHUTDOWN received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.761289 29157 master.cpp:7054] Removing executor 'default' with resources  of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.763124 29157 hierarchical.cpp:333] Removed framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.777029 29163 slave.cpp:4163] Executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 exited with status 0
I0708 15:42:16.777218 29163 slave.cpp:4267] Cleaning up executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.777710 29163 slave.cpp:4355] Cleaning up framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778026 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' for gc 6.99999163714074days in the future
W0708 15:42:16.778028 29167 master.cpp:5369] Ignoring unknown exited executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.778195 29157 status_update_manager.cpp:282] Closing status update streams for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778239 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default' for gc 6.99999163714074days in the future
I0708 15:42:16.778257 29157 status_update_manager.cpp:528] Cleaning up status update stream for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778327 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000' for gc 6.99999163714074days in the future
I0708 15:42:16.797328 29138 slave.cpp:841] Agent terminating
I0708 15:42:16.799114 29165 master.cpp:1371] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) disconnected
I0708 15:42:16.800149 29165 master.cpp:2910] Disconnecting agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.800727 29165 master.cpp:2929] Deactivating agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.801389 29165 hierarchical.cpp:571] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 deactivated
../../src/tests/api_tests.cpp:1496: Failure
Actual function call count doesn't match EXPECT_CALL(*scheduler, update(_, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
I0708 15:42:16.806820 29138 master.cpp:1218] Master terminating
I0708 15:42:16.807718 29160 hierarchical.cpp:510] Removed agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
[  FAILED  ] ContentType/MasterAPITest.Subscribe/0, where GetParam() = application/x-protobuf (780 ms)
{noformat}",1,5,MESOS-5812,3.0
CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.,"Currently, the CNI isolator will just ignore those containers that want to join the host network (i.e., not specifying NetworkInfo). However, if the container specifies a container image, we need to make sure that it has access to host /etc/* files. We should perform the bind mount for the container. This is also what docker does when a container is running in host mode.",1,6,MESOS-5806,5.0
Missing License Information for Bundled NVML headers,See Summary,1,2,MESOS-5766,1.0
"When start an agent with `--resources`, the GPU resource can be fractional","So far, the GPU resource is not fractional, only integer values are allowed. But when starting agents with {{\-\-resources='gpu:1.2'}}, it can also work without any warning or error. And in the webui the GPU resource is `1.2`.",1,3,MESOS-5742,1.0
Command executor health check does not work when the task specifies container image.,"Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.

One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor. This statement is *arguable*, see the comment below.",1,3,MESOS-5727,5.0
SSL downgrade support will leak sockets in CLOSE_WAIT status,"Repro steps:
1) Start a master:
{code}
bin/mesos-master.sh --work_dir=/tmp/master
{code}

2) Start an agent with SSL and downgrade enabled:
{code}
# Taken from http://mesos.apache.org/documentation/latest/ssl/
openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096
openssl req -new -x509 -passin pass:some_password -days 365 -key key.pem -out cert.pem

SSL_KEY_FILE=key.pem SSL_CERT_FILE=cert.pem SSL_ENABLED=true SSL_SUPPORT_DOWNGRADE=true sudo -E bin/mesos-agent.sh --master=localhost:5050 --work_dir=/tmp/agent
{code}

3) Start a framework that launches lots of executors, one after another:
{code}
sudo src/balloon-framework --master=localhost:5050 --task_memory=64mb --task_memory_usage_limit=256mb --long_running
{code}

4) Check FDs, repeatedly
{code}
sudo lsof -i | grep mesos | grep CLOSE_WAIT | wc -l
{code}

The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors.",1,2,MESOS-5691,5.0
Port mapping isolator may fail in 'isolate' method.,"Port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that ContainerId already existed. We should overwrite the symlink if it exist.

This affects a couple test failures:
{noformat}
PortMappingIsolatorTest.ROOT_TooManyContainers
PortMappingIsolatorTest.ROOT_ContainerARPExternal
PortMappingIsolatorTest.ROOT_ContainerCMPInternal
PortMappingIsolatorTest.ROOT_NC_HostToContainerTCP
{noformat}

Here is an example failure test log:
{noformat}
[00:28:37] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_TooManyContainers
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.046444 24846 port_mapping_tests.cpp:229] Using eth0 as the public interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.046728 24846 port_mapping_tests.cpp:237] Using lo as the loopback interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.058758 24846 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]
[00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.059711 24846 port_mapping.cpp:1557] Using eth0 as the public interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.059998 24846 port_mapping.cpp:1582] Using lo as the loopback interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061126 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061172 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061206 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061256 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061297 24846 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061331 24846 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061360 24846 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061390 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061419 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061450 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061480 24846 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061511 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061540 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061569 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061599 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.069964 24846 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.070144 24846 resources.cpp:572] Parsing resources as JSON failed: ports:[31000-31499]
[00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.070677 24867 port_mapping.cpp:2512] Using non-ephemeral ports {[31000,31500)} and ephemeral ports [30208,30720) for container container1 of executor ''
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.071688 24846 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS | CLONE_NEWNET
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.084079 24863 port_mapping.cpp:2576] Bind mounted '/proc/11997/ns/net' to '/run/netns/11997' for container container1
[00:28:37] :	 [Step 10/10] ../../src/tests/containerizer/port_mapping_tests.cpp:1438: Failure
[00:28:37] :	 [Step 10/10] (isolator.get()->isolate(containerId1, pid.get())).failure(): Failed to symlink the network namespace handle '/var/run/mesos/netns/container1' -> '/run/netns/11997': File exists
[00:28:37] :	 [Step 10/10] [  FAILED  ] PortMappingIsolatorTest.ROOT_TooManyContainers (57 ms)
{noformat}",1,1,MESOS-5674,3.0
Port mapping isolator may cause segfault if it bind mount root does not exist.,"A check is needed for port mapping isolator for its bind mount root. Otherwise, non-existed port-mapping bind mount root may cause segmentation fault for some cases. Here is the test log:

{noformat}
[00:57:42] :	 [Step 10/10] [----------] 11 tests from PortMappingIsolatorTest
[00:57:42] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_NC_ContainerToContainerTCP
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.723029 24841 port_mapping_tests.cpp:229] Using eth0 as the public interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.723348 24841 port_mapping_tests.cpp:237] Using lo as the loopback interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.735090 24841 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]
[00:57:42]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.736006 24841 port_mapping.cpp:1557] Using eth0 as the public interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.736331 24841 port_mapping.cpp:1582] Using lo as the loopback interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737501 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737545 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737578 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737608 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737637 24841 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737666 24841 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737694 24841 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737720 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737746 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737772 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737798 24841 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737828 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737854 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737879 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737905 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'
[00:57:42]W:	 [Step 10/10] F0604 00:57:42.737968 24841 port_mapping_tests.cpp:448] CHECK_SOME(isolator): Failed to get realpath for bind mount root '/var/run/netns': Not found 
[00:57:42]W:	 [Step 10/10] *** Check failure stack trace: ***
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd52583d2  google::LogMessage::Fail()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd525832b  google::LogMessage::SendToLog()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd5257d21  google::LogMessage::Flush()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd525ab92  google::LogMessageFatal::~LogMessageFatal()
[00:57:42]W:	 [Step 10/10]     @           0xa62171  _CheckFatal::~_CheckFatal()
[00:57:42]W:	 [Step 10/10]     @          0x1931b17  mesos::internal::tests::PortMappingIsolatorTest_ROOT_NC_ContainerToContainerTCP_Test::TestBody()
[00:57:42]W:	 [Step 10/10]     @          0x19e17b6  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19dc864  testing::internal::HandleExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19bd2ae  testing::Test::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19bda66  testing::TestInfo::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19be0b7  testing::TestCase::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19c4bf5  testing::internal::UnitTestImpl::RunAllTests()
[00:57:42]W:	 [Step 10/10]     @          0x19e247d  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19dd3a4  testing::internal::HandleExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19c38d1  testing::UnitTest::Run()
[00:57:42]W:	 [Step 10/10]     @           0xfd28cb  RUN_ALL_TESTS()
[00:57:42]W:	 [Step 10/10]     @           0xfd24b1  main
[00:57:42]W:	 [Step 10/10]     @     0x7f8bceb89580  __libc_start_main
[00:57:42]W:	 [Step 10/10]     @           0xa607c9  _start
[00:57:43]W:	 [Step 10/10] /mnt/teamcity/temp/agentTmp/custom_script659125926639545396: line 3: 24841 Aborted                 (core dumped) GLOG_v=1 ./bin/mesos-tests.sh --verbose --gtest_filter=""$GTEST_FILTER""
[00:57:43]W:	 [Step 10/10] Process exited with code 134
{noformat}",1,1,MESOS-5673,3.0
MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.,"{noformat}
[00:48:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
[00:48:29]W:	 [Step 10/10] 1+0 records in
[00:48:29]W:	 [Step 10/10] 1+0 records out
[00:48:29]W:	 [Step 10/10] 1048576 bytes (1.0 MB) copied, 0.000517638 s, 2.0 GB/s
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.000998 25413 cluster.cpp:155] Creating default 'local' authorizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.020459 25413 leveldb.cpp:174] Opened db in 19.338463ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022897 25413 leveldb.cpp:181] Compacted db in 2.416906ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022919 25413 leveldb.cpp:196] Created db iterator in 4037ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022927 25413 leveldb.cpp:202] Seeked to beginning of db in 769ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022932 25413 leveldb.cpp:271] Iterated through 0 keys in the db in 390ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022944 25413 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023272 25432 recover.cpp:451] Starting replica recovery
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023425 25434 recover.cpp:477] Replica is in EMPTY status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023748 25434 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19361)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023849 25429 recover.cpp:197] Received a recover response from a replica in EMPTY status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024019 25435 recover.cpp:568] Updating replica status to STARTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024338 25432 master.cpp:382] Master 0e92ffa4-4f26-4cea-84d3-9c67612de1bd (ip-172-30-2-56.mesosphere.io) started on 172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024348 25432 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/jBjY5p/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/jBjY5p/master"" --zk_session_timeout=""10secs""
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024502 25432 master.cpp:434] Master only allowing authenticated frameworks to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024508 25432 master.cpp:448] Master only allowing authenticated agents to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024513 25432 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024516 25432 credentials.hpp:37] Loading credentials for authentication from '/tmp/jBjY5p/credentials'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024603 25432 master.cpp:506] Using default 'crammd5' authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024644 25432 master.cpp:578] Using default 'basic' HTTP authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024701 25432 master.cpp:658] Using default 'basic' HTTP framework authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024770 25432 master.cpp:705] Authorization enabled
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024883 25435 whitelist_watcher.cpp:77] No whitelist given
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024885 25434 hierarchical.cpp:142] Initialized hierarchical allocator process
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025539 25433 master.cpp:1969] The newly elected leader is master@172.30.2.56:53790 with id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025555 25433 master.cpp:1982] Elected as the leading master!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025560 25433 master.cpp:1669] Recovering from registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025611 25432 registrar.cpp:332] Recovering registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026397 25431 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.288187ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026438 25431 replica.cpp:320] Persisted replica status to STARTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026486 25431 recover.cpp:477] Replica is in STARTING status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026793 25432 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19364)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026897 25429 recover.cpp:197] Received a recover response from a replica in STARTING status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.027031 25428 recover.cpp:568] Updating replica status to VOTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.028960 25432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.874668ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.028975 25432 replica.cpp:320] Persisted replica status to VOTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029007 25432 recover.cpp:582] Successfully joined the Paxos group
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029047 25432 recover.cpp:466] Recover process terminated
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029209 25430 log.cpp:553] Attempting to start the writer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029614 25429 replica.cpp:493] Replica received implicit promise request from (19365)@172.30.2.56:53790 with proposal 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031486 25429 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.850474ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031502 25429 replica.cpp:342] Persisted promised to 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031726 25431 coordinator.cpp:238] Coordinator attempting to fill missing positions
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.032245 25428 replica.cpp:388] Replica received explicit promise request from (19366)@172.30.2.56:53790 for position 0 with proposal 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034101 25428 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.831441ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034117 25428 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034561 25433 replica.cpp:537] Replica received write request for position 0 from (19367)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034589 25433 leveldb.cpp:436] Reading position from leveldb took 10586ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036419 25433 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.817267ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036434 25433 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036679 25429 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038661 25429 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.96521ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038677 25429 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038682 25429 replica.cpp:697] Replica learned NOP action at position 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038839 25435 log.cpp:569] Writer started with ending position 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039198 25433 leveldb.cpp:436] Reading position from leveldb took 10572ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039412 25433 registrar.cpp:365] Successfully fetched the registry (0B) in 13.778944ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039448 25433 registrar.cpp:464] Applied 1 operations in 4778ns; attempting to update the 'registry'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039643 25428 log.cpp:577] Attempting to append 205 bytes to the log
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039696 25432 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039945 25430 replica.cpp:537] Replica received write request for position 1 from (19368)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041738 25430 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 1.771112ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041754 25430 replica.cpp:712] Persisted action at 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041977 25432 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043805 25432 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 1.810425ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043820 25432 replica.cpp:712] Persisted action at 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043825 25432 replica.cpp:697] Replica learned APPEND action at position 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044040 25430 registrar.cpp:509] Successfully updated the 'registry' in 4.556032ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044100 25430 registrar.cpp:395] Successfully recovered registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044124 25428 log.cpp:596] Attempting to truncate the log to 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044215 25431 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044244 25430 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044317 25433 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044497 25433 replica.cpp:537] Replica received write request for position 2 from (19369)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046368 25433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.851883ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046383 25433 replica.cpp:712] Persisted action at 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046583 25430 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048426 25430 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.821628ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048455 25430 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14283ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048463 25430 replica.cpp:712] Persisted action at 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048468 25430 replica.cpp:697] Replica learned TRUNCATE action at position 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.055145 25413 containerizer.cpp:203] Using isolation: cgroups/mem,filesystem/posix,network/cni
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.058349 25413 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069301 25413 cluster.cpp:432] Creating default 'local' authorizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069707 25431 slave.cpp:203] Agent started on 485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069718 25431 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p""
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069916 25431 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069967 25431 slave.cpp:341] Agent using credential for: test-principal
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069984 25431 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070050 25431 slave.cpp:393] Using default 'basic' HTTP authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070127 25431 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070282 25431 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070309 25431 slave.cpp:600] Agent attributes: [  ]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070314 25431 slave.cpp:605] Agent hostname: ip-172-30-2-56.mesosphere.io
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070484 25413 sched.cpp:224] Version: 1.0.0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070667 25433 sched.cpp:328] New master detected at master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070711 25429 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/meta'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070749 25433 sched.cpp:394] Authenticating with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070758 25433 sched.cpp:401] Using default CRAM-MD5 authenticatee
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070793 25430 status_update_manager.cpp:200] Recovering status update manager
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070904 25432 authenticatee.cpp:121] Creating new client SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070914 25430 containerizer.cpp:518] Recovering containerizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071049 25432 master.cpp:5943] Authenticating scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071105 25428 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(984)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071164 25434 authenticator.cpp:98] Creating new server SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071241 25434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071254 25434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071292 25434 authenticator.cpp:204] Received SASL authentication start
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071336 25434 authenticator.cpp:326] Authentication requires more steps
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071374 25434 authenticatee.cpp:259] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071553 25434 authenticator.cpp:232] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071574 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071586 25434 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071594 25434 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071604 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071615 25434 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071619 25434 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071630 25434 authenticator.cpp:318] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071684 25428 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(984)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071687 25431 authenticatee.cpp:299] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071704 25434 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071826 25431 sched.cpp:484] Successfully authenticated with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071841 25431 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071954 25431 sched.cpp:833] Will retry registration in 731.385085ms if necessary
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071996 25434 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072013 25434 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072180 25430 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072305 25429 hierarchical.cpp:264] Added framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072326 25429 hierarchical.cpp:1488] No allocations performed
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072335 25429 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072347 25429 hierarchical.cpp:1139] Performed allocation for 0 agents in 26673ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072351 25431 provisioner.cpp:253] Provisioner recovery complete
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072371 25430 sched.cpp:723] Framework registered with 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072403 25430 sched.cpp:737] Scheduler::registered took 11852ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072587 25433 slave.cpp:4840] Finished recovery
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072760 25433 slave.cpp:5012] Querying resource estimator for oversubscribable resources
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072865 25431 status_update_manager.cpp:174] Pausing sending status updates
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072893 25432 slave.cpp:962] New master detected at master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072906 25432 slave.cpp:1024] Authenticating with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072917 25432 slave.cpp:1035] Using default CRAM-MD5 authenticatee
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072948 25432 slave.cpp:997] Detecting new master
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072976 25432 slave.cpp:5026] Received oversubscribable resources  from the resource estimator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072974 25435 authenticatee.cpp:121] Creating new client SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073099 25434 master.cpp:5943] Authenticating slave(485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073142 25434 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(985)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073213 25431 authenticator.cpp:98] Creating new server SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073268 25431 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073287 25431 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073320 25431 authenticator.cpp:204] Received SASL authentication start
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073353 25431 authenticator.cpp:326] Authentication requires more steps
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073390 25431 authenticatee.cpp:259] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073444 25435 authenticator.cpp:232] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073460 25435 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073465 25435 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073472 25435 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073477 25435 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073480 25435 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073484 25435 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073493 25435 authenticator.cpp:318] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073534 25431 authenticatee.cpp:299] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073561 25435 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073590 25433 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(985)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073698 25431 slave.cpp:1103] Successfully authenticated with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073742 25431 slave.cpp:1506] Will retry registration in 17.704164ms if necessary
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073786 25434 master.cpp:4653] Registering agent at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073874 25434 registrar.cpp:464] Applied 1 operations in 9493ns; attempting to update the 'registry'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074077 25430 log.cpp:577] Attempting to append 390 bytes to the log
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074152 25432 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074385 25431 replica.cpp:537] Replica received write request for position 3 from (19391)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076269 25431 leveldb.cpp:341] Persisting action (409 bytes) to leveldb took 1.86243ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076284 25431 replica.cpp:712] Persisted action at 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076551 25434 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078383 25434 leveldb.cpp:341] Persisting action (411 bytes) to leveldb took 1.815955ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078398 25434 replica.cpp:712] Persisted action at 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078404 25434 replica.cpp:697] Replica learned APPEND action at position 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078703 25432 registrar.cpp:509] Successfully updated the 'registry' in 4.813056ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078745 25429 log.cpp:596] Attempting to truncate the log to 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078806 25433 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078909 25431 master.cpp:4721] Registered agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078928 25428 slave.cpp:3742] Received ping from slave-observer(439)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078991 25430 hierarchical.cpp:473] Added agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079001 25428 slave.cpp:1147] Registered with master master@172.30.2.56:53790; given agent ID 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079020 25428 fetcher.cpp:86] Clearing fetcher cache
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079093 25430 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079111 25430 hierarchical.cpp:1162] Performed allocation for agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 in 100093ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079150 25435 replica.cpp:537] Replica received write request for position 4 from (19392)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079233 25429 master.cpp:5772] Sending 1 offers to framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079259 25434 status_update_manager.cpp:181] Resuming sending status updates
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079263 25428 slave.cpp:1170] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/meta/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/slave.info'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079396 25428 slave.cpp:1207] Forwarding total oversubscribed resources 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079427 25429 sched.cpp:897] Scheduler::resourceOffers took 25735ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079448 25428 master.cpp:5066] Received update of agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with total oversubscribed resources 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079608 25413 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:256;disk:1024
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079612 25434 hierarchical.cpp:531] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079645 25434 hierarchical.cpp:1488] No allocations performed
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079651 25434 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079660 25434 hierarchical.cpp:1162] Performed allocation for agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 in 26873ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079957 25428 master.cpp:3457] Processing ACCEPT call for offers: [ 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-O0 ] on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079979 25428 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080334 25432 master.hpp:178] Adding task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080365 25432 master.cpp:3946] Launching task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080495 25429 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 from framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080507 25428 slave.cpp:1546] Got assigned task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080528 25429 hierarchical.cpp:928] Framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 filtered agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 for 5secs
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080602 25428 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080718 25428 slave.cpp:1665] Launching task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080747 25428 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.081048 25428 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' to user 'root'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.082818 25435 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.508394ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.082859 25435 replica.cpp:712] Persisted action at 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.083400 25435 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085247 25435 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.827229ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085294 25435 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30113ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085304 25435 replica.cpp:712] Persisted action at 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085310 25435 replica.cpp:697] Replica learned TRUNCATE action at position 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085690 25428 slave.cpp:5729] Launching executor 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085846 25428 slave.cpp:1891] Queuing task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085849 25429 containerizer.cpp:777] Starting container '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework '0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085898 25428 slave.cpp:915] Successfully attached file '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.087308 25428 mem.cpp:602] Started listening for OOM events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.087671 25428 mem.cpp:722] Started listening on low memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088007 25428 mem.cpp:722] Started listening on medium memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088412 25428 mem.cpp:722] Started listening on critical memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088750 25428 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089221 25428 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089759 25430 containerizer.cpp:1271] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""110"" --pipe_write=""111"" --sandbox=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8"" --user=""root""'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089825 25430 linux_launcher.cpp:281] Cloning child process with flags = 
[00:48:30]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.153952 10096 process.cpp:1060] libprocess is initialized on 172.30.2.56:34658 with 8 worker threads
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.156230 10096 logging.cpp:199] Logging to STDERR
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157129 10096 exec.cpp:161] Version: 1.0.0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157197 10125 exec.cpp:211] Executor started at: executor(1)@172.30.2.56:34658 with pid 10096
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157687 25431 slave.cpp:2879] Got registration for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.158280 10129 exec.cpp:236] Executor registered on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.158689 25433 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159274 25435 slave.cpp:2056] Sending queued task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' to executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 at executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159399 10129 exec.cpp:248] Executor::registered took 64598ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159651 10128 exec.cpp:323] Executor asked to run task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159704 10128 exec.cpp:332] Executor::launchTask took 30558ns
[00:48:30] :	 [Step 10/10] Received SUBSCRIBED event
[00:48:30] :	 [Step 10/10] Subscribed executor on ip-172-30-2-56.mesosphere.io
[00:48:30] :	 [Step 10/10] Received LAUNCH event
[00:48:30] :	 [Step 10/10] Starting task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb
[00:48:30] :	 [Step 10/10] sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
[00:48:30] :	 [Step 10/10] Forked command at 10134
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.163949 10126 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164324 25431 slave.cpp:3262] Handling status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164824 25428 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164849 25428 status_update_manager.cpp:497] Creating StatusUpdate stream for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165026 25428 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to the agent
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165132 25433 slave.cpp:3660] Forwarding the update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165230 25433 slave.cpp:3554] Status update manager successfully handled status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165251 25433 slave.cpp:3570] Sending acknowledgement for status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165329 25430 master.cpp:5211] Status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165349 25430 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165410 25430 master.cpp:6871] Updating the state of task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165560 10128 exec.cpp:369] Executor received status update acknowledgement 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165628 25432 sched.cpp:1005] Scheduler::statusUpdate took 78385ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165765 25432 master.cpp:4365] Processing ACKNOWLEDGE call 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165927 25428 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.166052 25428 slave.cpp:2648] Status update manager successfully handled status update acknowledgement (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.583686 25428 master.cpp:4269] Telling agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) to kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.583760 25428 slave.cpp:2086] Asked to kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.584074 10125 exec.cpp:343] Executor asked to kill task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.584121 10125 exec.cpp:352] Executor::killTask took 27333ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.959868 25430 mem.cpp:625] OOM notifier is triggered for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.959900 25430 mem.cpp:644] OOM detected for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.960912 25430 mem.cpp:685] Memory limit exceeded: Requested: 288MB Maximum Used: 288MB
[00:48:30]W:	 [Step 10/10] 
[00:48:30]W:	 [Step 10/10] MEMORY STATISTICS: 
[00:48:30]W:	 [Step 10/10] cache 297218048
[00:48:30]W:	 [Step 10/10] rss 4771840
[00:48:30]W:	 [Step 10/10] rss_huge 0
[00:48:30]W:	 [Step 10/10] mapped_file 0
[00:48:30]W:	 [Step 10/10] pgpgin 75849
[00:48:30]W:	 [Step 10/10] pgpgout 2121
[00:48:30]W:	 [Step 10/10] pgfault 19539
[00:48:30]W:	 [Step 10/10] pgmajfault 0
[00:48:30]W:	 [Step 10/10] inactive_anon 0
[00:48:30]W:	 [Step 10/10] active_anon 4771840
[00:48:30]W:	 [Step 10/10] inactive_file 296955904
[00:48:30]W:	 [Step 10/10] active_file 253952
[00:48:30]W:	 [Step 10/10] unevictable 0
[00:48:30]W:	 [Step 10/10] hierarchical_memory_limit 301989888
[00:48:30]W:	 [Step 10/10] total_cache 297218048
[00:48:30]W:	 [Step 10/10] total_rss 4771840
[00:48:30]W:	 [Step 10/10] total_rss_huge 0
[00:48:30]W:	 [Step 10/10] total_mapped_file 0
[00:48:30]W:	 [Step 10/10] total_pgpgin 75849
[00:48:30]W:	 [Step 10/10] total_pgpgout 2121
[00:48:30]W:	 [Step 10/10] total_pgfault 19539
[00:48:30]W:	 [Step 10/10] total_pgmajfault 0
[00:48:30]W:	 [Step 10/10] total_inactive_anon 0
[00:48:30]W:	 [Step 10/10] total_active_anon 4771840
[00:48:30]W:	 [Step 10/10] total_inactive_file 296873984
[00:48:30]W:	 [Step 10/10] total_active_file 253952
[00:48:30]W:	 [Step 10/10] total_unevictable 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.961012 25430 containerizer.cpp:1833] Container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 has reached its limit for resource mem(*):288 and will be terminated
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.961050 25430 containerizer.cpp:1580] Destroying container '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.962447 25431 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:45] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:172: Failure
[00:48:45] :	 [Step 10/10] Failed to wait 15secs for killed
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585013 25429 master.cpp:1406] Framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 disconnected
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585052 25429 master.cpp:2840] Disconnecting framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585072 25429 master.cpp:2864] Deactivating framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585110 25429 master.cpp:1419] Giving framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 0ns to failover
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585193 25432 hierarchical.cpp:375] Deactivated framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585247 25431 master.cpp:5624] Framework failover timeout, removing framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585269 25431 master.cpp:6354] Removing framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585325 25431 master.cpp:6871] Updating the state of task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585352 25434 slave.cpp:2269] Asked to shut down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 by master@172.30.2.56:53790
[00:48:45] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:128: Failure
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585373 25434 slave.cpp:2294] Shutting down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45] :	 [Step 10/10] Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
[00:48:45] :	 [Step 10/10]          Expected: to be called at least twice
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585387 25434 slave.cpp:4465] Shutting down executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 at executor(1)@172.30.2.56:34658
[00:48:45] :	 [Step 10/10]            Actual: called once - unsatisfied and active
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585476 25429 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):256; disk(*):1024 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 from framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585492 25431 master.cpp:6937] Removing task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585698 25431 hierarchical.cpp:326] Removed framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:49:00] :	 [Step 10/10] ../../src/tests/cluster.cpp:551: Failure
[00:49:00] :	 [Step 10/10] Failed to wait 15secs for wait
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596581 25413 slave.cpp:834] Agent terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596611 25413 slave.cpp:2269] Asked to shut down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 by @0.0.0.0:0
[00:49:00]W:	 [Step 10/10] W0617 00:49:00.596624 25413 slave.cpp:2290] Ignoring shutdown framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 because it is terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596742 25428 master.cpp:1367] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) disconnected
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596761 25428 master.cpp:2899] Disconnecting agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596807 25428 master.cpp:2918] Deactivating agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596863 25428 hierarchical.cpp:560] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 deactivated
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.598708 25413 master.cpp:1214] Master terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.598848 25431 hierarchical.cpp:505] Removed agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.601809 25433 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.602758 25434 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 after 0ns
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.603759 25431 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.604717 25433 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 after 0ns
[00:49:00]W:	 [Step 10/10] E0617 00:49:00.605662 25436 process.cpp:2050] Failed to shutdown socket with fd 111: Transport endpoint is not connected
[00:49:15] :	 [Step 10/10] ../../src/tests/mesos.cpp:937: Failure
[00:49:15] :	 [Step 10/10] Failed to wait 15secs for cgroups::destroy(hierarchy, cgroup)
[00:49:15] :	 [Step 10/10] [  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (45618 ms)
{noformat}",1,1,MESOS-5671,2.0
MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.,"{noformat}
[03:36:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.461802  2797 cluster.cpp:155] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.469468  2797 leveldb.cpp:174] Opened db in 7.527163ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470188  2797 leveldb.cpp:181] Compacted db in 699544ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470206  2797 leveldb.cpp:196] Created db iterator in 4293ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470211  2797 leveldb.cpp:202] Seeked to beginning of db in 535ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470216  2797 leveldb.cpp:271] Iterated through 0 keys in the db in 321ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470230  2797 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470510  2815 recover.cpp:451] Starting replica recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470592  2817 recover.cpp:477] Replica is in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471029  2813 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19800)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471139  2816 recover.cpp:197] Received a recover response from a replica in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471271  2818 recover.cpp:568] Updating replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471606  2811 master.cpp:382] Master 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 (ip-172-30-2-29.mesosphere.io) started on 172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471619  2811 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/baXWq5/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/baXWq5/master"" --zk_session_timeout=""10secs""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471745  2811 master.cpp:434] Master only allowing authenticated frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471753  2811 master.cpp:448] Master only allowing authenticated agents to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471757  2811 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471761  2811 credentials.hpp:37] Loading credentials for authentication from '/tmp/baXWq5/credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471829  2811 master.cpp:506] Using default 'crammd5' authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471868  2811 master.cpp:578] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471941  2811 master.cpp:658] Using default 'basic' HTTP framework authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471977  2811 master.cpp:705] Authorization enabled
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472034  2817 hierarchical.cpp:142] Initialized hierarchical allocator process
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472038  2814 whitelist_watcher.cpp:77] No whitelist given
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472506  2811 master.cpp:1969] The newly elected leader is master@172.30.2.29:37328 with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472522  2811 master.cpp:1982] Elected as the leading master!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472527  2811 master.cpp:1669] Recovering from registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472573  2812 registrar.cpp:332] Recovering registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473511  2816 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.195002ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473527  2816 replica.cpp:320] Persisted replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473578  2816 recover.cpp:477] Replica is in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473877  2815 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19803)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473989  2814 recover.cpp:197] Received a recover response from a replica in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474126  2817 recover.cpp:568] Updating replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474735  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547332ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474748  2811 replica.cpp:320] Persisted replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474783  2811 recover.cpp:582] Successfully joined the Paxos group
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474829  2811 recover.cpp:466] Recover process terminated
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474969  2818 log.cpp:553] Attempting to start the writer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475361  2811 replica.cpp:493] Replica received implicit promise request from (19804)@172.30.2.29:37328 with proposal 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475944  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 559444ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475956  2811 replica.cpp:342] Persisted promised to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476215  2815 coordinator.cpp:238] Coordinator attempting to fill missing positions
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476660  2816 replica.cpp:388] Replica received explicit promise request from (19805)@172.30.2.29:37328 for position 0 with proposal 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477262  2816 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 584333ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477273  2816 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477699  2815 replica.cpp:537] Replica received write request for position 0 from (19806)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477726  2815 leveldb.cpp:436] Reading position from leveldb took 8842ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478277  2815 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 537361ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478291  2815 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478569  2811 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479132  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 545208ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479146  2811 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479152  2811 replica.cpp:697] Replica learned NOP action at position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479317  2814 log.cpp:569] Writer started with ending position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479568  2811 leveldb.cpp:436] Reading position from leveldb took 8325ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479786  2814 registrar.cpp:365] Successfully fetched the registry (0B) in 7.192064ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479822  2814 registrar.cpp:464] Applied 1 operations in 3018ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479995  2818 log.cpp:577] Attempting to append 205 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480044  2818 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480309  2811 replica.cpp:537] Replica received write request for position 1 from (19807)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480928  2811 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 596433ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480942  2811 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481148  2815 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481710  2815 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 545656ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481722  2815 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481727  2815 replica.cpp:697] Replica learned APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481958  2816 registrar.cpp:509] Successfully updated the 'registry' in 2.119168ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482014  2816 registrar.cpp:395] Successfully recovered registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482045  2817 log.cpp:596] Attempting to truncate the log to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482117  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482166  2816 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482177  2817 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482404  2817 replica.cpp:537] Replica received write request for position 2 from (19808)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482975  2817 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 552763ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482986  2817 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483301  2813 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483870  2813 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 547529ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483896  2813 leveldb.cpp:399] Deleting ~1 keys from leveldb took 12161ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483904  2813 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483911  2813 replica.cpp:697] Replica learned TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.492995  2797 containerizer.cpp:201] Using isolation: cgroups/mem,filesystem/posix,network/cni
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.496548  2797 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503572  2797 cluster.cpp:432] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503936  2817 slave.cpp:203] Agent started on 488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503952  2817 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504148  2817 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504189  2817 slave.cpp:341] Agent using credential for: test-principal
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504199  2817 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504245  2817 slave.cpp:393] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504410  2797 sched.cpp:224] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504416  2817 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504580  2818 sched.cpp:328] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504613  2818 sched.cpp:394] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504622  2818 sched.cpp:401] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504649  2817 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504673  2817 slave.cpp:600] Agent attributes: [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504678  2817 slave.cpp:605] Agent hostname: ip-172-30-2-29.mesosphere.io
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504703  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504830  2818 master.cpp:5943] Authenticating scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504887  2816 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504982  2811 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505004  2816 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505105  2813 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505131  2813 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505138  2818 status_update_manager.cpp:200] Recovering status update manager
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505167  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2814 containerizer.cpp:514] Recovering containerizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505241  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505300  2812 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505317  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505323  2812 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505331  2812 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505337  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505342  2812 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505347  2812 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505355  2812 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505399  2813 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505421  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505436  2812 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505534  2816 sched.cpp:484] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505553  2816 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505591  2816 sched.cpp:833] Will retry registration in 11.319315ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505672  2815 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505702  2815 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505854  2818 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506031  2818 sched.cpp:723] Framework registered with 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506050  2816 hierarchical.cpp:264] Added framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506072  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506073  2818 sched.cpp:737] Scheduler::registered took 28711ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506093  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506126  2816 hierarchical.cpp:1139] Performed allocation for 0 agents in 59667ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506428  2818 provisioner.cpp:253] Provisioner recovery complete
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506570  2815 slave.cpp:4845] Finished recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506747  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506878  2813 slave.cpp:967] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506886  2814 status_update_manager.cpp:174] Pausing sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506903  2813 slave.cpp:1029] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506924  2813 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506976  2813 slave.cpp:1002] Detecting new master
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506989  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507069  2813 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507145  2815 master.cpp:5943] Authenticating slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507202  2811 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507264  2817 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507374  2817 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507387  2817 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507433  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507467  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507511  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507578  2811 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507597  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507606  2811 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507617  2811 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507629  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507640  2811 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507648  2811 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507686  2811 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507750  2817 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507766  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507786  2813 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507863  2817 slave.cpp:1108] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507910  2817 slave.cpp:1511] Will retry registration in 10.588836ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507966  2812 master.cpp:4653] Registering agent at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508059  2817 registrar.cpp:464] Applied 1 operations in 13429ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508244  2812 log.cpp:577] Attempting to append 390 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508296  2817 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508546  2815 replica.cpp:537] Replica received write request for position 3 from (19831)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509158  2815 leveldb.cpp:341] Persisting action (409 bytes) to leveldb took 589901ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509171  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509403  2815 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509980  2815 leveldb.cpp:341] Persisting action (411 bytes) to leveldb took 558737ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509992  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509999  2815 replica.cpp:697] Replica learned APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510262  2818 registrar.cpp:509] Successfully updated the 'registry' in 2.178048ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510313  2811 log.cpp:596] Attempting to truncate the log to 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510375  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510486  2818 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510519  2816 master.cpp:4721] Registered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510540  2818 slave.cpp:1152] Registered with master master@172.30.2.29:37328; given agent ID 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2818 fetcher.cpp:86] Clearing fetcher cache
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2815 hierarchical.cpp:473] Added agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510639  2811 replica.cpp:537] Replica received write request for position 4 from (19832)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510658  2816 status_update_manager.cpp:181] Resuming sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510730  2815 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510747  2815 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 127305ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510766  2818 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/slave.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510848  2816 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510892  2818 slave.cpp:1212] Forwarding total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510956  2818 master.cpp:5066] Received update of agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510987  2817 sched.cpp:897] Scheduler::resourceOffers took 30391ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511080  2816 hierarchical.cpp:531] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511124  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511132  2797 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:256;disk:1024
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511133  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511167  2816 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 57933ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511201  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 542938ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511214  2811 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511431  2818 master.cpp:3457] Processing ACCEPT call for offers: [ 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-O0 ] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511461  2818 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511560  2816 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511827  2811 master.hpp:177] Adding task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511859  2811 master.cpp:3946] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511968  2814 slave.cpp:1551] Got assigned task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511984  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512009  2815 hierarchical.cpp:928] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 filtered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for 5secs
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512022  2814 slave.cpp:5654] Checkpointing FrameworkInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512127  2816 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 544409ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512138  2814 slave.cpp:5665] Checkpointing framework pid 'scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512153  2816 leveldb.cpp:399] Deleting ~2 keys from leveldb took 13134ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512162  2816 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512167  2816 replica.cpp:697] Replica learned TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512245  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512377  2814 slave.cpp:1670] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512408  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512596  2814 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' to user 'root'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517411  2814 slave.cpp:6136] Checkpointing ExecutorInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/executor.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517659  2814 slave.cpp:5734] Launching executor e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517853  2814 slave.cpp:6159] Checkpointing TaskInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/tasks/e9fcbad2-73bf-409e-9f71-023b826b5286/task.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517861  2818 containerizer.cpp:773] Starting container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework '6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518013  2814 slave.cpp:1896] Queuing task 'e9fcbad2-73bf-409e-9f71-023b826b5286' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518056  2814 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519455  2817 mem.cpp:602] Started listening for OOM events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519815  2817 mem.cpp:722] Started listening on low memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520133  2817 mem.cpp:722] Started listening on medium memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520447  2817 mem.cpp:722] Started listening on critical memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520769  2817 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521339  2817 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521926  2816 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""119"" --pipe_write=""120"" --sandbox=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82"" --user=""root""'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521984  2816 linux_launcher.cpp:281] Cloning child process with flags = 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.544052  2816 containerizer.cpp:1302] Checkpointing executor's forked pid 20673 to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/forked.pid'
[03:36:29]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.603862 20687 process.cpp:1060] libprocess is initialized on 172.30.2.29:44617 with 8 worker threads
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.605692 20687 logging.cpp:199] Logging to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606240 20687 exec.cpp:161] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606302 20704 exec.cpp:211] Executor started at: executor(1)@172.30.2.29:44617 with pid 20687
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606724  2814 slave.cpp:2884] Got registration for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606885  2814 slave.cpp:2970] Checkpointing executor pid 'executor(1)@172.30.2.29:44617' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/libprocess.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607306 20703 exec.cpp:236] Executor registered on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607925  2815 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608141 20703 exec.cpp:248] Executor::registered took 89576ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608538  2816 slave.cpp:2061] Sending queued task 'e9fcbad2-73bf-409e-9f71-023b826b5286' to executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608767 20705 exec.cpp:323] Executor asked to run task 'e9fcbad2-73bf-409e-9f71-023b826b5286'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608811 20705 exec.cpp:332] Executor::launchTask took 26475ns
[03:36:29] :	 [Step 10/10] Received SUBSCRIBED event
[03:36:29] :	 [Step 10/10] Subscribed executor on ip-172-30-2-29.mesosphere.io
[03:36:29] :	 [Step 10/10] Received LAUNCH event
[03:36:29] :	 [Step 10/10] Starting task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29] :	 [Step 10/10] Forked command at 20710
[03:36:29] :	 [Step 10/10] sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611716 20705 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611974  2815 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612499  2818 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612527  2818 status_update_manager.cpp:497] Creating StatusUpdate stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612751  2818 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725725  2818 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to the agent
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725908  2817 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725999  2817 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726016  2817 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726124  2813 master.cpp:5211] Status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726157  2813 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726238  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726300 20701 exec.cpp:369] Executor received status update acknowledgement bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726363  2818 sched.cpp:1005] Scheduler::statusUpdate took 77055ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726517  2814 master.cpp:4365] Processing ACKNOWLEDGE call bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726757  2816 status_update_manager.cpp:392] Received status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726812  2816 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472790  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472841  2817 hierarchical.cpp:1488] No allocations performed
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472847  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472864  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 181038ns
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474026  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474076  2814 hierarchical.cpp:1488] No allocations performed
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474083  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474097  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 180187ns
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475332  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475383  2817 hierarchical.cpp:1488] No allocations performed
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475389  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475402  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 176560ns
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476011  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476059  2814 hierarchical.cpp:1488] No allocations performed
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476066  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476080  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 194002ns
[03:36:33]W:	 [Step 10/10] 512+0 records in
[03:36:33]W:	 [Step 10/10] 512+0 records out
[03:36:33]W:	 [Step 10/10] 536870912 bytes (537 MB, 512 MiB) copied, 4.23412 s, 127 MB/s
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477355  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477406  2814 hierarchical.cpp:1488] No allocations performed
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477413  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477427  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 184403ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477726  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477774  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 202326ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477824  2818 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477948  2818 sched.cpp:897] Scheduler::resourceOffers took 9712ns
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478219  2814 hierarchical.cpp:1488] No allocations performed
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478235  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478245  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 47187ns
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478663  2811 hierarchical.cpp:1488] No allocations performed
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478678  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478693  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 45629ns
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479481  2817 hierarchical.cpp:1488] No allocations performed
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479516  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479532  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 98966ns
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480494  2813 hierarchical.cpp:1488] No allocations performed
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480526  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480543  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 87017ns
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481472  2812 hierarchical.cpp:1488] No allocations performed
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481504  2812 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481519  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 122806ns
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482342  2813 hierarchical.cpp:1488] No allocations performed
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482378  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482393  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 98739ns
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483055  2817 hierarchical.cpp:1488] No allocations performed
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483083  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483095  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 73620ns
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483800  2811 hierarchical.cpp:1488] No allocations performed
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483837  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483853  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 103486ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484480  2818 hierarchical.cpp:1488] No allocations performed
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484508  2818 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484522  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 76447ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507843  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507937  2815 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.511128  2812 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:44] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:263: Failure
[03:36:44] :	 [Step 10/10] Failed to wait 15secs for _statusUpdateAcknowledgement
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727337  2815 master.cpp:1406] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 disconnected
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727363  2815 master.cpp:2840] Disconnecting framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727396  2815 master.cpp:2864] Deactivating framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727478  2814 hierarchical.cpp:375] Deactivated framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] W0618 03:36:44.727489  2815 master.hpp:1967] Master attempted to send message to disconnected framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727519  2815 master.cpp:1419] Giving framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 0ns to failover
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727556  2814 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727741  2814 containerizer.cpp:1576] Destroying container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728740  2813 master.cpp:5624] Framework failover timeout, removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728765  2813 master.cpp:6354] Removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728817  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728827  2817 slave.cpp:2274] Asked to shut down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 by master@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728853  2817 slave.cpp:2299] Shutting down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728869  2817 slave.cpp:4470] Shutting down executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728896  2811 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728937  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):256; disk(*):1024 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44] :	 [Step 10/10] Received SHUTDOWN event
[03:36:44] :	 [Step 10/10] Shutting down
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728950  2813 master.cpp:6937] Removing task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:44] :	 [Step 10/10] Sending SIGTERM to process tree at pid 20710
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729131 20707 exec.cpp:410] Executor asked to shutdown
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729141  2815 hierarchical.cpp:326] Removed framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729179 20707 exec.cpp:425] Executor::shutdown took 6153ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729199 20707 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485015  2818 hierarchical.cpp:1488] No allocations performed
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485038  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 47043ns
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485332  2811 hierarchical.cpp:1488] No allocations performed
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485350  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 33542ns
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486548  2817 hierarchical.cpp:1488] No allocations performed
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486588  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 84621ns
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487707  2813 hierarchical.cpp:1488] No allocations performed
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487751  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 83039ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488706  2812 hierarchical.cpp:1488] No allocations performed
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488745  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 78192ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.729018  2811 slave.cpp:4543] Killing executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489168  2817 hierarchical.cpp:1488] No allocations performed
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489207  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 87236ns
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.369570  2818 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.430644  2813 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 6.70171904secs
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.431812  2818 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.432981  2817 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 1.140992ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.433709  2816 slave.cpp:3793] executor(1)@172.30.2.29:44617 exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.443989  2813 containerizer.cpp:1812] Executor for container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' has exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446597  2818 provisioner.cpp:411] Ignoring destroy request for unknown container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446734  2813 slave.cpp:4152] Executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 terminated with signal Killed
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446758  2813 slave.cpp:4256] Cleaning up executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446943  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482767407days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447018  2813 slave.cpp:4344] Cleaning up framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447038  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.9999948270963days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447082  2816 status_update_manager.cpp:282] Closing status update streams for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447098  2816 status_update_manager.cpp:528] Cleaning up status update stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447100  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482669037days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447103  2813 slave.cpp:839] Agent terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447149  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.99999482630815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447190  2816 master.cpp:1367] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) disconnected
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447209  2816 master.cpp:2899] Disconnecting agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447211  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482555556days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447237  2816 master.cpp:2918] Deactivating agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447254  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482534815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447300  2816 hierarchical.cpp:560] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 deactivated
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448766  2797 master.cpp:1214] Master terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448875  2814 hierarchical.cpp:505] Removed agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.460062  2813 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.562192  2816 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 102.104064ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.563100  2816 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.564021  2815 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 901888ns
[03:36:51] :	 [Step 10/10] [  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (22119 ms)
{noformat}",1,1,MESOS-5670,2.0
CNI isolator should not return failure if /etc/hostname does not exist on host.,"/etc/hostname may not necessarily exist on every system (e.g., CentOS 6). Currently CNI isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. This is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.

This issue relates to 3 failure tests:
{noformat}
[22:45:21] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.647611 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.655230 24647 leveldb.cpp:174] Opened db in 7.510408ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657680 24647 leveldb.cpp:181] Compacted db in 2.427309ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657702 24647 leveldb.cpp:196] Created db iterator in 6209ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657709 24647 leveldb.cpp:202] Seeked to beginning of db in 692ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657713 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 431ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657727 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657888 24662 recover.cpp:451] Starting replica recovery
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658051 24668 recover.cpp:477] Replica is in EMPTY status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658495 24664 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18401)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658583 24662 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658687 24664 recover.cpp:568] Updating replica status to STARTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659111 24664 master.cpp:382] Master 9a4a353b-91c5-43b9-8c37-19245c37758c (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659126 24664 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/l8346Z/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/l8346Z/master"" --zk_session_timeout=""10secs""
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659267 24664 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659276 24664 master.cpp:448] Master only allowing authenticated agents to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659278 24664 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659282 24664 credentials.hpp:37] Loading credentials for authentication from '/tmp/l8346Z/credentials'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659375 24664 master.cpp:506] Using default 'crammd5' authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659415 24664 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659495 24664 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659569 24664 master.cpp:705] Authorization enabled
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659684 24666 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659696 24665 whitelist_watcher.cpp:77] No whitelist given
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660269 24666 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 9a4a353b-91c5-43b9-8c37-19245c37758c
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660281 24666 master.cpp:1982] Elected as the leading master!
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660290 24666 master.cpp:1669] Recovering from registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660342 24662 registrar.cpp:332] Recovering registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661232 24669 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.48585ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661254 24669 replica.cpp:320] Persisted replica status to STARTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661326 24669 recover.cpp:477] Replica is in STARTING status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661667 24668 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18404)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661758 24665 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661893 24664 recover.cpp:568] Updating replica status to VOTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663851 24664 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.915617ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663866 24664 replica.cpp:320] Persisted replica status to VOTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663899 24664 recover.cpp:582] Successfully joined the Paxos group
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663944 24664 recover.cpp:466] Recover process terminated
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.664088 24668 log.cpp:553] Attempting to start the writer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.664556 24668 replica.cpp:493] Replica received implicit promise request from (18405)@172.30.2.247:42024 with proposal 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666551 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.971938ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666566 24668 replica.cpp:342] Persisted promised to 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666767 24667 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.667230 24668 replica.cpp:388] Replica received explicit promise request from (18406)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669271 24668 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 2.02399ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669287 24668 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669656 24669 replica.cpp:537] Replica received write request for position 0 from (18407)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669680 24669 leveldb.cpp:436] Reading position from leveldb took 10808ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671674 24669 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.977316ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671689 24669 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671907 24665 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673920 24665 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.991274ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673935 24665 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673941 24665 replica.cpp:697] Replica learned NOP action at position 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674190 24665 log.cpp:569] Writer started with ending position 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674489 24663 leveldb.cpp:436] Reading position from leveldb took 9059ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674718 24663 registrar.cpp:365] Successfully fetched the registry (0B) in 14.355968ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674747 24663 registrar.cpp:464] Applied 1 operations in 3070ns; attempting to update the 'registry'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674935 24665 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674978 24665 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.675242 24666 replica.cpp:537] Replica received write request for position 1 from (18408)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677088 24666 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.823904ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677103 24666 replica.cpp:712] Persisted action at 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677299 24667 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679270 24667 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.952303ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679286 24667 replica.cpp:712] Persisted action at 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679291 24667 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679481 24663 registrar.cpp:509] Successfully updated the 'registry' in 4.715264ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679503 24666 log.cpp:596] Attempting to truncate the log to 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679560 24667 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679581 24663 registrar.cpp:395] Successfully recovered registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679745 24664 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679774 24662 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679986 24662 replica.cpp:537] Replica received write request for position 2 from (18409)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.681895 24662 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.891877ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.681910 24662 replica.cpp:712] Persisted action at 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.682160 24666 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684331 24666 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.153217ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684375 24666 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26973ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684383 24666 replica.cpp:712] Persisted action at 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684389 24666 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.691529 24647 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.694491 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:21]W:	 [Step 10/10] E0619 22:45:21.699741 24647 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[22:45:21]W:	 [Step 10/10] sh: hadoop: command not found
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.699769 24647 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.699823 24647 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.700865 24647 linux.cpp:146] Bind mounting '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG' and making it a shared mount
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.707801 24647 cni.cpp:286] Bind mounting '/var/run/mesos/isolators/network/cni' and making it a shared mount
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714337 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714825 24668 slave.cpp:203] Agent started on 468)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714839 24668 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/l8346Z/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/l8346Z/configs"" --network_cni_plugins_dir=""/tmp/l8346Z/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG""
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715116 24668 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715195 24668 slave.cpp:341] Agent using credential for: test-principal
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715214 24668 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715296 24668 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715400 24668 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
{noformat}

{noformat}
[22:45:38] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_VerifyCheckpointedInfo
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.459836 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.470319 24647 leveldb.cpp:174] Opened db in 10.34226ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472771 24647 leveldb.cpp:181] Compacted db in 2.403554ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472795 24647 leveldb.cpp:196] Created db iterator in 4446ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472801 24647 leveldb.cpp:202] Seeked to beginning of db in 810ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472806 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 393ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472822 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473093 24665 recover.cpp:451] Starting replica recovery
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473260 24663 recover.cpp:477] Replica is in EMPTY status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473647 24663 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18464)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473752 24665 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473896 24667 recover.cpp:568] Updating replica status to STARTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474319 24663 master.cpp:382] Master 64f1f7ac-e810-4fb1-b549-6e29fc62622b (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474329 24663 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/qJWqSY/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/qJWqSY/master"" --zk_session_timeout=""10secs""
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474452 24663 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474457 24663 master.cpp:448] Master only allowing authenticated agents to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474459 24663 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474463 24663 credentials.hpp:37] Loading credentials for authentication from '/tmp/qJWqSY/credentials'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474551 24663 master.cpp:506] Using default 'crammd5' authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474598 24663 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474643 24663 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474674 24663 master.cpp:705] Authorization enabled
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474771 24668 whitelist_watcher.cpp:77] No whitelist given
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474798 24664 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475177 24663 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 64f1f7ac-e810-4fb1-b549-6e29fc62622b
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475188 24663 master.cpp:1982] Elected as the leading master!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475191 24663 master.cpp:1669] Recovering from registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475244 24662 registrar.cpp:332] Recovering registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476292 24669 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.312046ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476308 24669 replica.cpp:320] Persisted replica status to STARTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476368 24669 recover.cpp:477] Replica is in STARTING status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476687 24668 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18467)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476824 24666 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476953 24668 recover.cpp:568] Updating replica status to VOTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478798 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.793996ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478813 24668 replica.cpp:320] Persisted replica status to VOTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478844 24668 recover.cpp:582] Successfully joined the Paxos group
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478889 24668 recover.cpp:466] Recover process terminated
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.479060 24665 log.cpp:553] Attempting to start the writer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.479547 24667 replica.cpp:493] Replica received implicit promise request from (18468)@172.30.2.247:42024 with proposal 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481433 24667 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.8684ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481449 24667 replica.cpp:342] Persisted promised to 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481667 24662 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.482067 24668 replica.cpp:388] Replica received explicit promise request from (18469)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.483842 24668 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.754044ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.483858 24668 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.484235 24665 replica.cpp:537] Replica received write request for position 0 from (18470)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.484261 24665 leveldb.cpp:436] Reading position from leveldb took 10298ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486331 24665 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.057217ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486346 24665 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486574 24669 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488533 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.941228ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488548 24669 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488553 24669 replica.cpp:697] Replica learned NOP action at position 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488690 24666 log.cpp:569] Writer started with ending position 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489006 24662 leveldb.cpp:436] Reading position from leveldb took 11082ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489244 24667 registrar.cpp:365] Successfully fetched the registry (0B) in 13.976832ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489276 24667 registrar.cpp:464] Applied 1 operations in 3438ns; attempting to update the 'registry'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489450 24662 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489514 24665 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489785 24662 replica.cpp:537] Replica received write request for position 1 from (18471)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491642 24662 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.838371ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491657 24662 replica.cpp:712] Persisted action at 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491885 24665 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493649 24665 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.743495ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493665 24665 replica.cpp:712] Persisted action at 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493670 24665 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493930 24669 registrar.cpp:509] Successfully updated the 'registry' in 4.638976ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493983 24667 log.cpp:596] Attempting to truncate the log to 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493994 24669 registrar.cpp:395] Successfully recovered registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494034 24668 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494197 24662 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494210 24666 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494396 24662 replica.cpp:537] Replica received write request for position 2 from (18472)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496301 24662 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.884992ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496315 24662 replica.cpp:712] Persisted action at 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496574 24666 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498500 24666 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.906093ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498529 24666 leveldb.cpp:399] Deleting ~1 keys from leveldb took 13787ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498538 24666 replica.cpp:712] Persisted action at 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498543 24666 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.505269 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.508313 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.509832 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510205 24666 slave.cpp:203] Agent started on 469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510213 24666 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/qJWqSY/configs"" --network_cni_plugins_dir=""/tmp/qJWqSY/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru""
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510442 24666 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/credential'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510510 24666 slave.cpp:341] Agent using credential for: test-principal
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510521 24666 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/http_credentials'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510604 24666 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510696 24666 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510915 24647 sched.cpp:224] Version: 1.0.0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510962 24666 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510984 24666 slave.cpp:600] Agent attributes: [  ]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510989 24666 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511077 24669 sched.cpp:328] New master detected at master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511162 24669 sched.cpp:394] Authenticating with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511173 24669 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511294 24662 authenticatee.cpp:121] Creating new client SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511371 24667 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/meta'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511494 24665 master.cpp:5943] Authenticating scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511523 24668 status_update_manager.cpp:200] Recovering status update manager
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511566 24662 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(957)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511612 24664 containerizer.cpp:514] Recovering containerizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511706 24667 authenticator.cpp:98] Creating new server SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511800 24667 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511816 24667 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511865 24667 authenticator.cpp:204] Received SASL authentication start
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511934 24667 authenticator.cpp:326] Authentication requires more steps
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511977 24667 authenticatee.cpp:259] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512080 24668 authenticator.cpp:232] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512102 24668 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512112 24668 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512125 24668 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512136 24668 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512145 24668 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512152 24668 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512166 24668 authenticator.cpp:318] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512228 24665 authenticatee.cpp:299] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512233 24668 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512253 24667 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(957)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512434 24664 sched.cpp:484] Successfully authenticated with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512445 24664 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512490 24667 provisioner.cpp:253] Provisioner recovery complete
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512609 24664 sched.cpp:833] Will retry registration in 550.501359ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512648 24663 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512665 24663 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512678 24667 slave.cpp:4845] Finished recovery
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512763 24663 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512876 24664 hierarchical.cpp:264] Added framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512893 24664 hierarchical.cpp:1488] No allocations performed
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512905 24664 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512922 24664 hierarchical.cpp:1139] Performed allocation for 0 agents in 33065ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512940 24667 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513025 24666 sched.cpp:723] Framework registered with 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513056 24666 sched.cpp:737] Scheduler::registered took 18725ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513074 24669 status_update_manager.cpp:174] Pausing sending status updates
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513089 24667 slave.cpp:967] New master detected at master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513105 24667 slave.cpp:1029] Authenticating with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513120 24667 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513169 24667 slave.cpp:1002] Detecting new master
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513192 24663 authenticatee.cpp:121] Creating new client SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513260 24667 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513324 24666 master.cpp:5943] Authenticating slave(469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513365 24666 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(958)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513423 24665 authenticator.cpp:98] Creating new server SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513484 24665 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513494 24665 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513525 24665 authenticator.cpp:204] Received SASL authentication start
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513563 24665 authenticator.cpp:326] Authentication requires more steps
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513594 24665 authenticatee.cpp:259] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513635 24665 authenticator.cpp:232] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513653 24665 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513661 24665 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513667 24665 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513674 24665 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513677 24665 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513680 24665 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513689 24665 authenticator.cpp:318] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513727 24665 authenticatee.cpp:299] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513737 24664 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(958)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513754 24666 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513859 24669 slave.cpp:1108] Successfully authenticated with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513921 24669 slave.cpp:1511] Will retry registration in 834760ns if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513974 24666 master.cpp:4653] Registering agent at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with id 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514077 24668 registrar.cpp:464] Applied 1 operations in 12262ns; attempting to update the 'registry'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514245 24666 log.cpp:577] Attempting to append 395 bytes to the log
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514282 24666 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514566 24662 replica.cpp:537] Replica received write request for position 3 from (18487)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.515151 24665 slave.cpp:1511] Will retry registration in 1.465145ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.515202 24667 master.cpp:4641] Ignoring register agent message from slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) as admission is already in progress
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.517513 24663 slave.cpp:1511] Will retry registration in 70.844019ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.517555 24664 master.cpp:4641] Ignoring register agent message from slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) as admission is already in progress
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518628 24662 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 4.043654ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518643 24662 replica.cpp:712] Persisted action at 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518877 24665 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520764 24665 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.869511ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520779 24665 replica.cpp:712] Persisted action at 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520784 24665 replica.cpp:697] Replica learned APPEND action at position 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521023 24663 registrar.cpp:509] Successfully updated the 'registry' in 6.930176ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521083 24668 log.cpp:596] Attempting to truncate the log to 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521152 24665 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521239 24667 master.cpp:4721] Registered agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521272 24665 slave.cpp:3747] Received ping from slave-observer(424)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521280 24664 hierarchical.cpp:473] Added agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521340 24665 slave.cpp:1152] Registered with master master@172.30.2.247:42024; given agent ID 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521354 24665 fetcher.cpp:86] Clearing fetcher cache
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521428 24664 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521455 24664 hierarchical.cpp:1162] Performed allocation for agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 in 131318ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521443 24669 replica.cpp:537] Replica received write request for position 4 from (18488)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521466 24662 status_update_manager.cpp:181] Resuming sending status updates
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521502 24668 master.cpp:5772] Sending 1 offers to framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521553 24665 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/meta/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/slave.info'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521667 24665 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521709 24665 master.cpp:5066] Received update of agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with total oversubscribed resources 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521725 24668 sched.cpp:897] Scheduler::resourceOffers took 35814ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521827 24665 hierarchical.cpp:531] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521860 24665 hierarchical.cpp:1488] No allocations performed
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521870 24665 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521884 24665 hierarchical.cpp:1162] Performed allocation for agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 in 36469ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521885 24647 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522244 24666 master.cpp:3457] Processing ACCEPT call for offers: [ 64f1f7ac-e810-4fb1-b549-6e29fc62622b-O0 ] on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522267 24666 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522642 24666 master.hpp:177] Adding task 123bdde2-b542-4206-9554-249c053f63d2 with resources cpus(*):1; mem(*):128 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522666 24666 master.cpp:3946] Launching task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 with resources cpus(*):1; mem(*):128 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522780 24662 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 from framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522799 24667 slave.cpp:1551] Got assigned task 123bdde2-b542-4206-9554-249c053f63d2 for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522804 24662 hierarchical.cpp:928] Framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 filtered agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 for 5secs
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522893 24667 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523059 24667 slave.cpp:1670] Launching task 123bdde2-b542-4206-9554-249c053f63d2 for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523108 24667 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523439 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.965378ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523454 24669 replica.cpp:712] Persisted action at 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523521 24667 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' to user 'root'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.526239 24665 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528328 24665 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.028744ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528360 24665 leveldb.cpp:399] Deleting ~2 keys from leveldb took 16691ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528368 24665 replica.cpp:712] Persisted action at 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528374 24665 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528923 24667 slave.cpp:5734] Launching executor 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529093 24667 slave.cpp:1896] Queuing task '123bdde2-b542-4206-9554-249c053f63d2' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529100 24669 containerizer.cpp:773] Starting container 'e533d091-9fc2-4161-b6b3-4c99a88be466' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework '64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529126 24667 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529799 24663 containerizer.cpp:1120] Overwriting environment variable 'LIBPROCESS_IP', original: '172.30.2.247', new: '0.0.0.0', for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.530079 24666 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""96"" --pipe_write=""106"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466"" --user=""root""'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.530154 24666 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.533272 24662 cni.cpp:683] Bind mounted '/proc/7922/ns/net' to '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/ns' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.533452 24662 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.606812 24663 cni.cpp:1066] Got assigned IPv4 address '172.17.42.1/16' from CNI network '__MESOS_TEST__' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.607293 24666 cni.cpp:808] DNS nameservers for container e533d091-9fc2-4161-b6b3-4c99a88be466 are:
[22:45:38]W:	 [Step 10/10] nameserver 172.30.0.2
[22:45:38]W:	 [Step 10/10] Failed to synchronize with agent (it's probably exited)
[22:45:38]W:	 [Step 10/10] E0619 22:45:38.707609 24662 slave.cpp:4039] Container 'e533d091-9fc2-4161-b6b3-4c99a88be466' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 failed to start: Collect failed: Failed to setup hostname and network files: WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.645313  7938 cni.cpp:1449] Set hostname to 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] Mount point '/etc/hostname' does not exist on the host filesystem
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.707772 24669 containerizer.cpp:1576] Destroying container 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.707787 24669 containerizer.cpp:1624] Waiting for the isolators to complete for container 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.708878 24667 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.807951 24664 containerizer.cpp:1812] Executor for container 'e533d091-9fc2-4161-b6b3-4c99a88be466' has exited
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.810672 24666 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 after 101.766144ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.811637 24668 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.812523 24664 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 after 864us
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.908664 24668 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/ns' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.908843 24668 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909222 24669 provisioner.cpp:411] Ignoring destroy request for unknown container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909346 24664 slave.cpp:4152] Executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 exited with status 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909437 24664 slave.cpp:3267] Handling status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909620 24664 slave.cpp:6074] Terminating task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] W0619 22:45:38.909713 24665 containerizer.cpp:1418] Ignoring update for unknown container: e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909871 24666 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909888 24666 status_update_manager.cpp:497] Creating StatusUpdate stream for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910080 24666 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 to the agent
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910163 24665 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 to master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910253 24665 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910490 24667 master.cpp:5211] Status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 from agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910512 24667 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910560 24667 master.cpp:6871] Updating the state of task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:45:38] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:292: Failure
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910698 24668 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 from framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38] :	 [Step 10/10] Value of: statusRunning->state()
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910755 24666 sched.cpp:1005] Scheduler::statusUpdate took 50939ns
[22:45:38] :	 [Step 10/10]   Actual: TASK_FAILED
[22:45:38] :	 [Step 10/10] Expected: TASK_RUNNING
[22:45:38] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:296: Failure
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910995 24662 master.cpp:4365] Processing ACKNOWLEDGE call 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38] :	 [Step 10/10] Value of: containers.get().size()
[22:45:38] :	 [Step 10/10]   Actual: 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911026 24662 master.cpp:6937] Removing task 123bdde2-b542-4206-9554-249c053f63d2 with resources cpus(*):1; mem(*):128 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38] :	 [Step 10/10] Expected: 1u
[22:45:38] :	 [Step 10/10] Which is: 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911234 24665 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911278 24665 status_update_manager.cpp:528] Cleaning up status update stream for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911402 24669 master.cpp:1406] Framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 disconnected
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911418 24669 master.cpp:2840] Disconnecting framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911414 24665 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911433 24669 master.cpp:2864] Deactivating framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911453 24665 slave.cpp:6115] Completing task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911470 24665 slave.cpp:4256] Cleaning up executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911548 24669 master.cpp:1419] Giving framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 0ns to failover
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911583 24662 hierarchical.cpp:375] Deactivated framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911640 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' for gc 6.99998944950222days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911680 24665 slave.cpp:4344] Cleaning up framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911689 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2' for gc 6.99998944832296days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911738 24664 status_update_manager.cpp:282] Closing status update streams for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911805 24662 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000' for gc 6.99998944754667days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911918 24647 slave.cpp:839] Agent terminating
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911991 24662 master.cpp:1367] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) disconnected
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912009 24662 master.cpp:2899] Disconnecting agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912029 24662 master.cpp:2918] Deactivating agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912135 24665 hierarchical.cpp:560] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 deactivated
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912824 24669 master.cpp:5624] Framework failover timeout, removing framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912842 24669 master.cpp:6354] Removing framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.913030 24669 hierarchical.cpp:326] Removed framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.913905 24647 master.cpp:1214] Master terminating
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.914031 24664 hierarchical.cpp:505] Removed agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_VerifyCheckpointedInfo (457 ms)
{noformat}

{noformat}
[22:45:39] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_SlaveRecovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.224643 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.232614 24647 leveldb.cpp:174] Opened db in 7.839626ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235198 24647 leveldb.cpp:181] Compacted db in 2.563679ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235219 24647 leveldb.cpp:196] Created db iterator in 4353ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235224 24647 leveldb.cpp:202] Seeked to beginning of db in 668ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235231 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 399ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235246 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235555 24662 recover.cpp:451] Starting replica recovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235777 24663 recover.cpp:477] Replica is in EMPTY status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236134 24669 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18550)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236197 24663 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236351 24667 recover.cpp:568] Updating replica status to STARTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236580 24668 master.cpp:382] Master 032cd99a-1cdc-42d4-b94a-f7b00f37fb52 (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236594 24668 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ghfuib/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/ghfuib/master"" --zk_session_timeout=""10secs""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236723 24668 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236729 24668 master.cpp:448] Master only allowing authenticated agents to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236732 24668 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236737 24668 credentials.hpp:37] Loading credentials for authentication from '/tmp/ghfuib/credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236829 24668 master.cpp:506] Using default 'crammd5' authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236871 24668 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236946 24668 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236991 24668 master.cpp:705] Authorization enabled
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237077 24663 whitelist_watcher.cpp:77] No whitelist given
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237159 24665 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237638 24667 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237650 24667 master.cpp:1982] Elected as the leading master!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237655 24667 master.cpp:1669] Recovering from registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237700 24669 registrar.cpp:332] Recovering registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239017 24662 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.616259ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239032 24662 replica.cpp:320] Persisted replica status to STARTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239084 24662 recover.cpp:477] Replica is in STARTING status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239437 24669 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18553)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239538 24662 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239672 24663 recover.cpp:568] Updating replica status to VOTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241654 24662 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.871972ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241670 24662 replica.cpp:320] Persisted replica status to VOTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241703 24662 recover.cpp:582] Successfully joined the Paxos group
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241745 24662 recover.cpp:466] Recover process terminated
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241880 24662 log.cpp:553] Attempting to start the writer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.242295 24668 replica.cpp:493] Replica received implicit promise request from (18554)@172.30.2.247:42024 with proposal 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244303 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.98443ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244318 24668 replica.cpp:342] Persisted promised to 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244529 24663 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.245007 24664 replica.cpp:388] Replica received explicit promise request from (18555)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.246898 24664 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.869865ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.246915 24664 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.247295 24666 replica.cpp:537] Replica received write request for position 0 from (18556)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.247320 24666 leveldb.cpp:436] Reading position from leveldb took 10783ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249264 24666 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.93015ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249279 24666 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249492 24663 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251349 24663 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.840655ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251364 24663 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251369 24663 replica.cpp:697] Replica learned NOP action at position 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251634 24668 log.cpp:569] Writer started with ending position 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251905 24666 leveldb.cpp:436] Reading position from leveldb took 11014ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252132 24664 registrar.cpp:365] Successfully fetched the registry (0B) in 14.413312ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252176 24664 registrar.cpp:464] Applied 1 operations in 5009ns; attempting to update the 'registry'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252378 24663 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252437 24669 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252768 24666 replica.cpp:537] Replica received write request for position 1 from (18557)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.254878 24666 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 2.087874ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.254894 24666 replica.cpp:712] Persisted action at 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.255100 24664 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.256983 24664 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.863178ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.256999 24664 replica.cpp:712] Persisted action at 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257004 24664 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257231 24663 registrar.cpp:509] Successfully updated the 'registry' in 5.034752ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257283 24663 registrar.cpp:395] Successfully recovered registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257304 24665 log.cpp:596] Attempting to truncate the log to 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257431 24666 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257462 24668 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257484 24662 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257690 24668 replica.cpp:537] Replica received write request for position 2 from (18558)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259577 24668 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.867119ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259593 24668 replica.cpp:712] Persisted action at 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259788 24667 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261890 24667 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.084656ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261920 24667 leveldb.cpp:399] Deleting ~1 keys from leveldb took 13997ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261929 24667 replica.cpp:712] Persisted action at 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261934 24667 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.269104 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.272172 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273219 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273654 24662 slave.cpp:203] Agent started on 471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273664 24662 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/ghfuib/configs"" --network_cni_plugins_dir=""/tmp/ghfuib/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273874 24662 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273952 24662 slave.cpp:341] Agent using credential for: test-principal
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273967 24662 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274041 24662 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274193 24662 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274448 24647 sched.cpp:224] Version: 1.0.0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274459 24662 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274492 24662 slave.cpp:600] Agent attributes: [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274500 24662 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274618 24669 sched.cpp:328] New master detected at master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274714 24669 sched.cpp:394] Authenticating with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274724 24669 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274826 24667 authenticatee.cpp:121] Creating new client SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274855 24662 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274950 24667 master.cpp:5943] Authenticating scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275002 24669 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(961)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275041 24667 status_update_manager.cpp:200] Recovering status update manager
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275116 24668 authenticator.cpp:98] Creating new server SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275132 24662 containerizer.cpp:514] Recovering containerizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275185 24668 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275197 24668 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275246 24666 authenticator.cpp:204] Received SASL authentication start
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275322 24666 authenticator.cpp:326] Authentication requires more steps
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275370 24666 authenticatee.cpp:259] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275445 24667 authenticator.cpp:232] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275462 24667 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275468 24667 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275485 24667 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275492 24667 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275497 24667 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275501 24667 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275511 24667 authenticator.cpp:318] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275563 24667 authenticatee.cpp:299] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275574 24664 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(961)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275586 24665 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275640 24666 sched.cpp:484] Successfully authenticated with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275653 24666 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275758 24666 sched.cpp:833] Will retry registration in 1.75141928secs if necessary
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275781 24664 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275804 24664 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.275826 24665 cni.cpp:503] The checkpointed CNI plugin output '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/__MESOS_TEST__/eth0/network.info' for container 422a6a27-4327-4dc1-9a4c-7de578226eab does not exist
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275856 24665 cni.cpp:407] Removing unknown orphaned container 422a6a27-4327-4dc1-9a4c-7de578226eab
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275918 24662 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278825 24667 hierarchical.cpp:264] Added framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278898 24667 hierarchical.cpp:1488] No allocations performed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278906 24667 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278916 24667 hierarchical.cpp:1139] Performed allocation for 0 agents in 28334ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278957 24662 sched.cpp:723] Framework registered with 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279021 24662 sched.cpp:737] Scheduler::registered took 46037ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279216 24669 provisioner.cpp:253] Provisioner recovery complete
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279381 24663 slave.cpp:4845] Finished recovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279583 24663 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279724 24667 status_update_manager.cpp:174] Pausing sending status updates
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279791 24668 slave.cpp:967] New master detected at master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279808 24668 slave.cpp:1029] Authenticating with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279826 24668 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279878 24668 slave.cpp:1002] Detecting new master
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279916 24666 authenticatee.cpp:121] Creating new client SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279953 24668 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280045 24666 master.cpp:5943] Authenticating slave(471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280129 24665 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(962)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280186 24665 authenticator.cpp:98] Creating new server SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280266 24665 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280279 24665 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280308 24665 authenticator.cpp:204] Received SASL authentication start
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280345 24665 authenticator.cpp:326] Authentication requires more steps
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280383 24665 authenticatee.cpp:259] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280447 24669 authenticator.cpp:232] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280468 24669 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280474 24669 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280483 24669 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280488 24669 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280493 24669 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280496 24669 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280504 24669 authenticator.cpp:318] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280544 24669 authenticatee.cpp:299] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280568 24668 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(962)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280596 24665 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280673 24669 slave.cpp:1108] Successfully authenticated with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280725 24669 slave.cpp:1511] Will retry registration in 8.06966ms if necessary
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280796 24667 master.cpp:4653] Registering agent at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280905 24663 registrar.cpp:464] Applied 1 operations in 11081ns; attempting to update the 'registry'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281116 24669 log.cpp:577] Attempting to append 395 bytes to the log
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281182 24664 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281452 24663 replica.cpp:537] Replica received write request for position 3 from (18575)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283769 24663 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 2.297945ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283785 24663 replica.cpp:712] Persisted action at 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283993 24663 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285805 24663 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.793213ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285820 24663 replica.cpp:712] Persisted action at 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285826 24663 replica.cpp:697] Replica learned APPEND action at position 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286088 24668 registrar.cpp:509] Successfully updated the 'registry' in 5.161984ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286118 24666 log.cpp:596] Attempting to truncate the log to 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286172 24666 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286332 24667 slave.cpp:3747] Received ping from slave-observer(426)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286325 24665 master.cpp:4721] Registered agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286382 24668 hierarchical.cpp:473] Added agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286411 24667 slave.cpp:1152] Registered with master master@172.30.2.247:42024; given agent ID 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286424 24667 fetcher.cpp:86] Clearing fetcher cache
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286480 24665 status_update_manager.cpp:181] Resuming sending status updates
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286545 24669 replica.cpp:537] Replica received write request for position 4 from (18576)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286556 24668 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286579 24668 hierarchical.cpp:1162] Performed allocation for agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 in 176726ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286625 24667 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/slave.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286660 24663 master.cpp:5772] Sending 1 offers to framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286782 24667 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286842 24667 master.cpp:5066] Received update of agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with total oversubscribed resources 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286912 24663 sched.cpp:897] Scheduler::resourceOffers took 41729ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286981 24667 hierarchical.cpp:531] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287026 24667 hierarchical.cpp:1488] No allocations performed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287036 24667 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287051 24667 hierarchical.cpp:1162] Performed allocation for agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 in 40073ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287082 24647 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287443 24668 master.cpp:3457] Processing ACCEPT call for offers: [ 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-O0 ] on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287467 24668 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287804 24667 master.hpp:177] Adding task 44eba68b-0f7c-437f-935f-26a52ac3f64b with resources cpus(*):1; mem(*):128 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287829 24667 master.cpp:3946] Launching task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 with resources cpus(*):1; mem(*):128 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287947 24664 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 from framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287981 24665 slave.cpp:1551] Got assigned task 44eba68b-0f7c-437f-935f-26a52ac3f64b for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287981 24664 hierarchical.cpp:928] Framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 filtered agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 for 5secs
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288043 24665 slave.cpp:5654] Checkpointing FrameworkInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288200 24665 slave.cpp:5665] Checkpointing framework pid 'scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024' to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.pid'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288331 24665 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288467 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.901433ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288480 24669 replica.cpp:712] Persisted action at 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288480 24665 slave.cpp:1670] Launching task 44eba68b-0f7c-437f-935f-26a52ac3f64b for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288516 24665 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288709 24669 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288784 24665 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' to user 'root'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290657 24669 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.915037ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290714 24669 leveldb.cpp:399] Deleting ~2 keys from leveldb took 27510ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290726 24669 replica.cpp:712] Persisted action at 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290736 24669 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.292919 24665 slave.cpp:6136] Checkpointing ExecutorInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/executor.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293200 24665 slave.cpp:5734] Launching executor 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293373 24669 containerizer.cpp:773] Starting container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293403 24665 slave.cpp:6159] Checkpointing TaskInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/tasks/44eba68b-0f7c-437f-935f-26a52ac3f64b/task.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293581 24665 slave.cpp:1896] Queuing task '44eba68b-0f7c-437f-935f-26a52ac3f64b' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293622 24665 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294059 24662 containerizer.cpp:1120] Overwriting environment variable 'LIBPROCESS_IP', original: '172.30.2.247', new: '0.0.0.0', for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294361 24664 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""96"" --pipe_write=""107"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d"" --user=""root""'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294427 24664 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.296911 24664 containerizer.cpp:1302] Checkpointing executor's forked pid 7982 to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/pids/forked.pid'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.297456 24663 cni.cpp:683] Bind mounted '/proc/7982/ns/net' to '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/ns' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.297610 24663 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311471 24667 cni.cpp:1066] Got assigned IPv4 address '172.17.42.1/16' from CNI network '__MESOS_TEST__' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311671 24667 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/ns' for container 422a6a27-4327-4dc1-9a4c-7de578226eab
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311774 24667 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311911 24667 cni.cpp:808] DNS nameservers for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d are:
[22:45:39]W:	 [Step 10/10] nameserver 172.30.0.2
[22:45:39]W:	 [Step 10/10] EFailed to synchronize with agent (it's probably exited)0619 22:45:39.412294 24666 slave.cpp:4039] Container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 failed to start: Collect failed: Failed to setup hostname and network files: WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.352311  7998 cni.cpp:1449] Set hostname to 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] Mount point '/etc/hostname' does not exist on the host filesystem
[22:45:39]W:	 [Step 10/10] 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.412427 24664 containerizer.cpp:1576] Destroying container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.412444 24664 containerizer.cpp:1624] Waiting for the isolators to complete for container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.413815 24662 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.512704 24665 containerizer.cpp:1812] Executor for container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' has exited
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.516521 24662 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d after 102.685184ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.517462 24664 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.518301 24662 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d after 813824ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614039 24664 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/ns' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614209 24664 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614532 24662 provisioner.cpp:411] Ignoring destroy request for unknown container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614665 24668 slave.cpp:4152] Executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 exited with status 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614742 24668 slave.cpp:3267] Handling status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614964 24669 slave.cpp:6074] Terminating task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.615054 24669 containerizer.cpp:1418] Ignoring update for unknown container: ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615197 24668 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615211 24668 status_update_manager.cpp:497] Creating StatusUpdate stream for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615552 24668 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623800 24668 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 to the agent
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623888 24662 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 to master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623988 24662 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624162 24668 master.cpp:5211] Status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 from agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624181 24668 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624243 24668 master.cpp:6871] Updating the state of task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624356 24666 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 from framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624442 24669 sched.cpp:1005] Scheduler::statusUpdate took 38999ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624614 24664 master.cpp:4365] Processing ACKNOWLEDGE call addda641-2dda-4d21-bc97-f2d8f9e28fe7 for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:487: Failure
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624634 24664 master.cpp:6937] Removing task 44eba68b-0f7c-437f-935f-26a52ac3f64b with resources cpus(*):1; mem(*):128 of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39] :	 [Step 10/10] Value of: statusRunning->state()
[22:45:39] :	 [Step 10/10]   Actual: TASK_FAILED
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624804 24665 status_update_manager.cpp:392] Received status update acknowledgement (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39] :	 [Step 10/10] Expected: TASK_RUNNING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624847 24665 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628197 24665 status_update_manager.cpp:528] Cleaning up status update stream for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628276 24665 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628291 24665 slave.cpp:6115] Completing task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628304 24665 slave.cpp:4256] Cleaning up executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628487 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999272627556days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628530 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999272575111days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628592 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999272505778days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628623 24665 slave.cpp:4344] Cleaning up framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628635 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.9999927244563days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628679 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999272379556days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628686 24667 status_update_manager.cpp:282] Closing status update streams for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628693 24665 slave.cpp:839] Agent terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628718 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.9999927235763days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628746 24665 master.cpp:1367] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) disconnected
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628760 24665 master.cpp:2899] Disconnecting agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628783 24665 master.cpp:2918] Deactivating agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628832 24665 hierarchical.cpp:560] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 deactivated
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.629724 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.633214 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634099 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634585 24664 slave.cpp:203] Agent started on 472)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634599 24664 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/ghfuib/configs"" --network_cni_plugins_dir=""/tmp/ghfuib/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634822 24664 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634886 24664 slave.cpp:341] Agent using credential for: test-principal
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634897 24664 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634953 24664 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635114 24664 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635422 24664 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635444 24664 slave.cpp:600] Agent attributes: [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635449 24664 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635941 24668 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635964 24668 state.cpp:697] No checkpointed resources found at '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/resources/resources.info'
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.636117 24669 master.cpp:4232] Cannot kill task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 because it is unknown; performing reconciliation
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636142 24669 master.cpp:5510] Performing explicit task state reconciliation for 1 tasks of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:504: Failure
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636162 24669 master.cpp:5600] Sending explicit reconciliation state TASK_LOST for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39] :	 [Step 10/10] Value of: statusKilled->state()
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636317 24669 sched.cpp:1005] Scheduler::statusUpdate took 28596ns
[22:45:39] :	 [Step 10/10]   Actual: TASK_LOST
[22:45:39] :	 [Step 10/10] Expected: TASK_KILLED
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636548 24647 sched.cpp:1964] Asked to stop the driver
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636605 24669 sched.cpp:1167] Stopping framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636728 24662 master.cpp:6342] Processing TEARDOWN call for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.636739 24668 state.cpp:544] Failed to find executor libprocess pid/http marker file
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636747 24662 master.cpp:6354] Removing framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636823 24664 hierarchical.cpp:375] Deactivated framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636973 24667 hierarchical.cpp:326] Removed framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637154 24663 fetcher.cpp:86] Clearing fetcher cache
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637192 24663 slave.cpp:4933] Recovering framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637208 24663 slave.cpp:5858] Recovering executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637316 24663 slave.cpp:6074] Terminating task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637331 24663 slave.cpp:6115] Completing task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637421 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999262296296days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637454 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999262252444days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637460 24663 slave.cpp:4344] Cleaning up framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637477 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999262231407days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637514 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999262212741days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637534 24667 status_update_manager.cpp:282] Closing status update streams for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637547 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999262155259days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637574 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999262134222days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637648 24668 status_update_manager.cpp:200] Recovering status update manager
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637660 24668 status_update_manager.cpp:208] Recovering executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637676 24668 status_update_manager.cpp:233] Skipping recovering updates of executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 because its latest run ef0b6221-1073-42f0-adfc-cfe75ddb3a5d is completed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637729 24663 slave.cpp:839] Agent terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.639163 24647 master.cpp:1214] Master terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.639279 24667 hierarchical.cpp:505] Removed agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_SlaveRecovery (418 ms)
{noformat}",1,1,MESOS-5669,3.0
Add CGROUP namespace to linux ns helper.,"Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.

This also relates to two test failures on Ubuntu 16:
{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_setns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:75: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_setns (1 ms)
{noformat}

{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_getns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:160: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_getns (0 ms)
{noformat}",1,1,MESOS-5668,3.0
CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.,"{noformat}
[22:41:54] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.348641 30896 cluster.cpp:155] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.353384 30896 leveldb.cpp:174] Opened db in 4.634552ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354763 30896 leveldb.cpp:181] Compacted db in 1.360201ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354784 30896 leveldb.cpp:196] Created db iterator in 3421ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354790 30896 leveldb.cpp:202] Seeked to beginning of db in 633ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354797 30896 leveldb.cpp:271] Iterated through 0 keys in the db in 401ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354811 30896 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354990 30913 recover.cpp:451] Starting replica recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355123 30915 recover.cpp:477] Replica is in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355391 30915 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18695)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355479 30912 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355581 30914 recover.cpp:568] Updating replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356091 30910 master.cpp:382] Master 27c796db-6f98-4d61-96c0-f583f22787ff (ip-172-30-2-105.mesosphere.io) started on 172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356104 30910 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/KhgYrQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/KhgYrQ/master"" --zk_session_timeout=""10secs""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356237 30910 master.cpp:434] Master only allowing authenticated frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356245 30910 master.cpp:448] Master only allowing authenticated agents to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356247 30910 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356251 30910 credentials.hpp:37] Loading credentials for authentication from '/tmp/KhgYrQ/credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356351 30910 master.cpp:506] Using default 'crammd5' authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356389 30910 master.cpp:578] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356439 30910 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356467 30910 master.cpp:705] Authorization enabled
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356531 30913 whitelist_watcher.cpp:77] No whitelist given
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356549 30912 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356868 30916 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.232816ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356884 30916 replica.cpp:320] Persisted replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356945 30916 recover.cpp:477] Replica is in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357100 30917 master.cpp:1969] The newly elected leader is master@172.30.2.105:40724 with id 27c796db-6f98-4d61-96c0-f583f22787ff
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357115 30917 master.cpp:1982] Elected as the leading master!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357122 30917 master.cpp:1669] Recovering from registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357213 30910 registrar.cpp:332] Recovering registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357429 30913 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18698)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357549 30914 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357728 30913 recover.cpp:568] Updating replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358937 30913 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.14792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358952 30913 replica.cpp:320] Persisted replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358986 30913 recover.cpp:582] Successfully joined the Paxos group
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359041 30913 recover.cpp:466] Recover process terminated
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359180 30916 log.cpp:553] Attempting to start the writer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359578 30917 replica.cpp:493] Replica received implicit promise request from (18699)@172.30.2.105:40724 with proposal 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360752 30917 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.157449ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360767 30917 replica.cpp:342] Persisted promised to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360982 30914 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.361426 30910 replica.cpp:388] Replica received explicit promise request from (18700)@172.30.2.105:40724 for position 0 with proposal 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362571 30910 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.124969ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362587 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362999 30911 replica.cpp:537] Replica received write request for position 0 from (18701)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.363030 30911 leveldb.cpp:436] Reading position from leveldb took 14967ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364264 30911 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.214497ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364279 30911 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364470 30910 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365622 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.131398ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365636 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365643 30910 replica.cpp:697] Replica learned NOP action at position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365769 30915 log.cpp:569] Writer started with ending position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366080 30913 leveldb.cpp:436] Reading position from leveldb took 8794ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366284 30915 registrar.cpp:365] Successfully fetched the registry (0B) in 9.053952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366315 30915 registrar.cpp:464] Applied 1 operations in 3436ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366487 30911 log.cpp:577] Attempting to append 209 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366539 30917 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366839 30917 replica.cpp:537] Replica received write request for position 1 from (18702)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367966 30917 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.106053ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367982 30917 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.368201 30915 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371786 30915 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 3.566076ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371803 30915 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371809 30915 replica.cpp:697] Replica learned APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372032 30910 registrar.cpp:509] Successfully updated the 'registry' in 5.693952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372097 30910 registrar.cpp:395] Successfully recovered registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372107 30911 log.cpp:596] Attempting to truncate the log to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372151 30910 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372218 30911 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372242 30915 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372467 30914 replica.cpp:537] Replica received write request for position 2 from (18703)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373693 30914 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.207676ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373708 30914 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373920 30913 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375115 30913 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.17978ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375145 30913 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14216ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375154 30913 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375159 30913 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.383839 30896 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.388789 30896 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:41:54]W:	 [Step 10/10] E0619 22:41:54.393234 30896 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[22:41:54]W:	 [Step 10/10] sh: hadoop: command not found
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393265 30896 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393316 30896 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.395668 30896 cluster.cpp:432] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396100 30914 slave.cpp:203] Agent started on 469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396116 30914 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/KhgYrQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/KhgYrQ/configs"" --network_cni_plugins_dir=""/tmp/KhgYrQ/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396380 30914 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396495 30914 slave.cpp:341] Agent using credential for: test-principal
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396509 30914 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396586 30914 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396698 30914 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396780 30896 sched.cpp:224] Version: 1.0.0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396991 30914 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397020 30914 slave.cpp:600] Agent attributes: [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397029 30914 slave.cpp:605] Agent hostname: ip-172-30-2-105.mesosphere.io
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397040 30916 sched.cpp:328] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397068 30916 sched.cpp:394] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397078 30916 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397188 30916 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397467 30914 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397476 30912 master.cpp:5943] Authenticating scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397544 30913 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397614 30915 status_update_manager.cpp:200] Recovering status update manager
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397668 30912 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397709 30915 containerizer.cpp:514] Recovering containerizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397869 30912 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397886 30912 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397927 30912 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397964 30912 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398000 30912 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398052 30912 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398066 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398073 30912 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398087 30912 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398098 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398103 30912 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398108 30912 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398116 30912 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398162 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398181 30913 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398200 30912 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398270 30914 sched.cpp:484] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398280 30914 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398342 30914 sched.cpp:833] Will retry registration in 869.123866ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398381 30916 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398398 30916 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398483 30916 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398679 30916 sched.cpp:723] Framework registered with 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398701 30916 sched.cpp:737] Scheduler::registered took 10291ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398784 30910 hierarchical.cpp:264] Added framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398802 30910 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398808 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398818 30910 hierarchical.cpp:1139] Performed allocation for 0 agents in 22451ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399222 30916 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/KhgYrQ/store/storedImages' does not exist
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399318 30910 provisioner.cpp:253] Provisioner recovery complete
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399453 30913 slave.cpp:4845] Finished recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399690 30913 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399796 30911 slave.cpp:967] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399811 30911 slave.cpp:1029] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399801 30914 status_update_manager.cpp:174] Pausing sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399821 30911 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399855 30911 slave.cpp:1002] Detecting new master
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399879 30915 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399910 30911 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400044 30915 master.cpp:5943] Authenticating slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400099 30910 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400151 30910 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400316 30910 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400329 30910 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400367 30910 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400398 30910 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400431 30910 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400516 30917 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400530 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400537 30917 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400544 30917 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400550 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400554 30917 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400558 30917 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400566 30917 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400609 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400640 30912 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400682 30917 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400738 30911 slave.cpp:1108] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400790 30911 slave.cpp:1511] Will retry registration in 13.364855ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400848 30913 master.cpp:4653] Registering agent at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with id 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400950 30914 registrar.cpp:464] Applied 1 operations in 16921ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401154 30915 log.cpp:577] Attempting to append 395 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401213 30914 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401515 30914 replica.cpp:537] Replica received write request for position 3 from (18725)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402851 30914 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 1.317458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402866 30914 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.403101 30917 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404217 30917 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.100393ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404233 30917 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404239 30917 replica.cpp:697] Replica learned APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404495 30915 registrar.cpp:509] Successfully updated the 'registry' in 3.521792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404561 30913 log.cpp:596] Attempting to truncate the log to 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404621 30915 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404690 30910 master.cpp:4721] Registered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404726 30915 slave.cpp:3747] Received ping from slave-observer(429)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404747 30916 hierarchical.cpp:473] Added agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404825 30915 slave.cpp:1152] Registered with master master@172.30.2.105:40724; given agent ID 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404840 30915 fetcher.cpp:86] Clearing fetcher cache
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404880 30910 replica.cpp:537] Replica received write request for position 4 from (18726)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404911 30916 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404932 30913 status_update_manager.cpp:181] Resuming sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404942 30916 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 168147ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405025 30911 master.cpp:5772] Sending 1 offers to framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405082 30915 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/slave.info'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405177 30911 sched.cpp:897] Scheduler::resourceOffers took 55063ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405239 30915 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405299 30911 master.cpp:5066] Received update of agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405318 30896 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405387 30911 hierarchical.cpp:531] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405421 30911 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405431 30911 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405447 30911 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 40224ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405643 30914 master.cpp:3457] Processing ACCEPT call for offers: [ 27c796db-6f98-4d61-96c0-f583f22787ff-O0 ] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405668 30914 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406030 30912 master.hpp:177] Adding task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406056 30912 master.cpp:3946] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406158 30916 slave.cpp:1551] Got assigned task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406193 30912 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406214 30912 hierarchical.cpp:928] Framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 filtered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for 5secs
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406250 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406347 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.44747ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406359 30910 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406381 30916 slave.cpp:1670] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406420 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406555 30914 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406793 30916 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' to user 'root'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408360 30914 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.635458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408453 30914 leveldb.cpp:399] Deleting ~2 keys from leveldb took 53370ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408469 30914 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408480 30914 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411355 30916 slave.cpp:5734] Launching executor d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411485 30916 slave.cpp:1896] Queuing task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411516 30915 containerizer.cpp:773] Starting container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411521 30916 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411733 30914 metadata_manager.cpp:167] Looking for image 'alpine'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.412009 30911 registry_puller.cpp:235] Pulling image 'library/alpine' from 'docker-manifest://registry-1.docker.io:443library/alpine?latest#https' to '/tmp/KhgYrQ/store/staging/0cVlJm'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870712 30915 registry_puller.cpp:258] The manifest for image 'library/alpine' is '{
[22:41:54]W:	 [Step 10/10]    ""schemaVersion"": 1,
[22:41:54]W:	 [Step 10/10]    ""name"": ""library/alpine"",
[22:41:54]W:	 [Step 10/10]    ""tag"": ""latest"",
[22:41:54]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[22:41:54]W:	 [Step 10/10]    ""fsLayers"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""blobSum"": ""sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""history"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""container\"":\""571cde9b03ce6f46b78b8e9c5089d03034863a4ab9f05d3e4997d0e5e80a2a6e\"",\""container_config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:701fd33a2f463fd4bd459779276897ef01dcf998dd47f6c8eae34fa5e0886046 in /\""],\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""created\"":\""2016-06-02T21:43:31.291506236Z\"",\""docker_version\"":\""1.9.1\"",\""id\"":\""e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b\"",\""os\"":\""linux\""}""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""signatures"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""header"": {
[22:41:54]W:	 [Step 10/10]             ""jwk"": {
[22:41:54]W:	 [Step 10/10]                ""crv"": ""P-256"",
[22:41:54]W:	 [Step 10/10]                ""kid"": ""IZ4C:AKG6:LLBK:4Y62:6YWU:OI2G:K2EN:ZOJH:GHRY:5PKA:PFEE:WZWD"",
[22:41:54]W:	 [Step 10/10]                ""kty"": ""EC"",
[22:41:54]W:	 [Step 10/10]                ""x"": ""hU3h5pMhA0tgT3mF41BH5EbsLy9Tv3O-bla53S8-25g"",
[22:41:54]W:	 [Step 10/10]                ""y"": ""Y9sM4tXh_3KKKeEhikWEGgTUlQLYJxPWCXcs_bVP4Pc""
[22:41:54]W:	 [Step 10/10]             },
[22:41:54]W:	 [Step 10/10]             ""alg"": ""ES256""
[22:41:54]W:	 [Step 10/10]          },
[22:41:54]W:	 [Step 10/10]          ""signature"": ""8SZVGFKd_Ovz9FtfNMoLRWkwayOY9zaTq4bgPnKPuFPK-48nhDTMlkMz52Nqm2SHCk2xtYYkhzLtE6wUctrjqA"",
[22:41:54]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNTgsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAxNi0wNi0xOVQyMjo0MTo1NFoifQ""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ]
[22:41:54]W:	 [Step 10/10] }'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870767 30915 registry_puller.cpp:368] Fetching blob 'sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957' for layer 'e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b' of image 'library/alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357898 30910 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357965 30910 hierarchical.cpp:1488] No allocations performed
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357980 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.358002 30910 hierarchical.cpp:1139] Performed allocation for 1 agents in 238814ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.474309 30911 registry_puller.cpp:305] Extracting layer tar ball '/tmp/KhgYrQ/store/staging/0cVlJm/sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 to rootfs '/tmp/KhgYrQ/store/staging/0cVlJm/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.575764 30910 metadata_manager.cpp:155] Successfully cached image 'alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576198 30911 provisioner.cpp:294] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576556 30910 copy.cpp:128] Copying layer path '/tmp/KhgYrQ/store/layers/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs' to rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676825 30916 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--sandbox_directory=\/mnt\/mesos\/sandbox"",""--user=root"",""--rootfs=\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f""],""shell"":false,""user"":""root"",""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[{""shell"":true,""value"":""#!\/bin\/sh\nset -x -e\n\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer mount --help=\""false\"" --operation=\""make-rslave\"" --path=\""\/\""\nmount -n --rbind '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/slaves\/27c796db-6f98-4d61-96c0-f583f22787ff-S0\/frameworks\/27c796db-6f98-4d61-96c0-f583f22787ff-0000\/executors\/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2\/runs\/548370b5-05f2-4e33-8f6f-015aa3fd1af4' '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f\/mnt\/mesos\/sandbox'\n""}]}"" --help=""false"" --pipe_read=""17"" --pipe_write=""20"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4"" --user=""root""'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676923 30916 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681491 30913 cni.cpp:683] Bind mounted '/proc/13484/ns/net' to '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681712 30913 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776078 30916 cni.cpp:1066] Got assigned IPv4 address '172.17.0.1/16' from CNI network '__MESOS_TEST__' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776463 30913 cni.cpp:808] DNS nameservers for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4 are:
[22:41:55]W:	 [Step 10/10] nameserver 172.30.0.2
[22:41:55]W:	 [Step 10/10] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[22:41:55]W:	 [Step 10/10] + mount -n --rbind /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f/mnt/mesos/sandbox
[22:41:55]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.944355 13484 process.cpp:1060] libprocess is initialized on 172.17.0.1:60396 with 8 worker threads
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.946605 13484 logging.cpp:199] Logging to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947335 13484 exec.cpp:161] Version: 1.0.0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947404 13541 exec.cpp:211] Executor started at: executor(1)@172.17.0.1:60396 with pid 13484
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947883 30917 slave.cpp:2884] Got registration for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948427 13543 exec.cpp:236] Executor registered on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948524 30914 slave.cpp:2061] Sending queued task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' to executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949061 13543 exec.cpp:248] Executor::registered took 75489ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949213 13543 exec.cpp:323] Executor asked to run task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949246 13543 exec.cpp:332] Executor::launchTask took 21245ns
[22:41:55] :	 [Step 10/10] Received SUBSCRIBED event
[22:41:55] :	 [Step 10/10] Subscribed executor on ip-172-30-2-105.mesosphere.io
[22:41:55] :	 [Step 10/10] Received LAUNCH event
[22:41:55] :	 [Step 10/10] Starting task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:55] :	 [Step 10/10] Forked command at 13550
[22:41:55] :	 [Step 10/10] sh -c 'ifconfig'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953589 13547 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] Failed to exec: No such file or directory
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953891 30917 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954368 30910 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954385 30910 status_update_manager.cpp:497] Creating StatusUpdate stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954545 30910 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954637 30911 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954711 30911 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954732 30911 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954761 30914 master.cpp:5211] Status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954788 30914 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954843 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954934 13548 exec.cpp:369] Executor received status update acknowledgement 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954967 30910 sched.cpp:1005] Scheduler::statusUpdate took 57021ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955070 30914 master.cpp:4365] Processing ACKNOWLEDGE call 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955150 30911 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955219 30911 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] Command terminated with signal Aborted (pid: 13550)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054153 13541 exec.cpp:546] Executor sending status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054498 30913 slave.cpp:3267] Handling status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054955 30917 slave.cpp:6074] Terminating task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055366 30912 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055409 30912 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055485 30916 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055558 30916 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:216: Failure
[22:41:56] :	 [Step 10/10] Value of: statusFinished->state()
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055572 30916 slave.cpp:3575] Sending acknowledgement for status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:56] :	 [Step 10/10]   Actual: TASK_FAILED
[22:41:56] :	 [Step 10/10] Expected: TASK_FINISHED
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055613 30914 master.cpp:5211] Status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055640 30914 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055696 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055773 30912 sched.cpp:1005] Scheduler::statusUpdate took 29145ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055780 13546 exec.cpp:369] Executor received status update acknowledgement 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055816 30916 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055887 30911 master.cpp:4365] Processing ACKNOWLEDGE call 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055907 30911 master.cpp:6937] Removing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055971 30896 sched.cpp:1964] Asked to stop the driver
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056030 30913 sched.cpp:1167] Stopping framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056040 30916 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056073 30916 status_update_manager.cpp:528] Cleaning up status update stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056151 30915 master.cpp:6342] Processing TEARDOWN call for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056172 30915 master.cpp:6354] Removing framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056197 30916 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056216 30916 slave.cpp:6115] Completing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056218 30913 hierarchical.cpp:375] Deactivated framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056248 30916 slave.cpp:2274] Asked to shut down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 by master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056265 30916 slave.cpp:2299] Shutting down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056277 30916 slave.cpp:4470] Shutting down executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056468 30914 hierarchical.cpp:326] Removed framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056634 30911 containerizer.cpp:1576] Destroying container '548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057258 13543 exec.cpp:410] Executor asked to shutdown
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057303 13543 exec.cpp:425] Executor::shutdown took 6363ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057324 13547 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.058279 30910 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.059762 30912 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.460736ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.061364 30910 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.062861 30915 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.478912ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.064016 30910 slave.cpp:3793] executor(1)@172.17.0.1:60396 exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.078352 30915 containerizer.cpp:1812] Executor for container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' has exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179833 30916 cni.cpp:1217] Unmounted the network namespace handle '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179924 30916 cni.cpp:1228] Removed the container directory '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.180981 30913 provisioner.cpp:434] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280364 30912 slave.cpp:4152] Executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 terminated with signal Killed
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280406 30912 slave.cpp:4256] Cleaning up executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280545 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' for gc 6.99999675365926days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280575 30912 slave.cpp:4344] Cleaning up framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280647 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for gc 6.99999675293037days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280654 30914 status_update_manager.cpp:282] Closing status update streams for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280710 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000' for gc 6.99999675200296days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280745 30915 slave.cpp:839] Agent terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280810 30912 master.cpp:1367] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) disconnected
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280827 30912 master.cpp:2899] Disconnecting agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280844 30912 master.cpp:2918] Deactivating agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280912 30912 hierarchical.cpp:560] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 deactivated
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283011 30896 master.cpp:1214] Master terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283140 30916 hierarchical.cpp:505] Removed agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask (1945 ms)
{noformat}",1,1,MESOS-5667,2.0
Executors should not inherit environment variables from the agent.,"Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:

1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.

2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.

Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format.",1,1,MESOS-5657,3.0
Put initial scaffolding in place for implementing SUBSCRIBE call on v1 Master API.,"As discussed on MESOS-5498, this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the {{api/v1}} Operator API endpoint. Other events/support for snapshots would be done as part of MESOS-5498.",1,2,MESOS-5609,5.0
Modules using replicated log state API require zookeeper headers,The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. ,1,1,MESOS-5577,1.0
"Need to remove references to ""messages/messages.hpp"" from `State` API","In order to expose the `State` API for using replicated log in Mesos modules it is necessary that the `State` API does not reference headers that are not exposed as part of the Mesos installation. 

Currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `State` API unusable in a module. 

We need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. This will help us remove references to messages.hpp from the `State` API.",1,1,MESOS-5561,2.0
"Fix method of populating device entries for `/dev/nvidia-uvm`, etc.","Currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidia-uvm` are hard-coded. This causes problems for `/dev/nvidia-uvm` because its major number is part of the ""Experimental"" device range on Linux.

Because this range is experimental, there is no guarantee which device
number will be assigned to it on a given machine.  We should use `os:stat::rdev()` to extract the major/minor numbers programatically.",1,2,MESOS-5556,2.0
Change major/minor device types for Nvidia GPUs to `unsigned int`,"Currently, the GPU struct specifies the type of its `major` and `minor` fields as `dev_t`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. These macros return an `unsigned int` when handed a `dev_t`, so it makes sense for these fields to be of that type instead.",1,2,MESOS-5554,1.0
http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds,"I'm writing a controller in Go to monitor heartbeats. I'd like to use the interval as communicated by the master, which should be specified in the SUBSCRIBED event. But it's not.

{code}
2016/06/03 18:34:04 {Type:SUBSCRIBED Subscribed:&Event_Subscribed{FrameworkID:&mesos.FrameworkID{Value:ffdb6d6e-0167-4fa2-98f9-2c3f8157fc25-0004,},HeartbeatIntervalSeconds:nil,} Offers:nil Rescind:nil Update:nil Message:nil Failure:nil Error:nil}
{code}

{code}
$ dpkg -l |grep -e mesos
ii  mesos                               0.28.0-2.0.16.ubuntu1404         amd64        Cluster resource manager with efficient resource isolation
{code}

I *am* seeing HEARTBEAT events. Just not seeing the interval specified in the SUBSCRIBED event.",1,2,MESOS-5537,1.0
Re-enable style-check for stout.,"After the 3rdparty reorg, the mesos-style checker stopped checking stout.",1,2,MESOS-5531,1.0
Confirm errors in authorized persistent volume tests,The tests {{PersistentVolumeTest.BadACLDropCreateAndDestroy}} and {{PersistentVolumeTest.BadACLNoPrincipal}} check for a failed Destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation. We should also explicitly check that the operation did not succeed due to failed authorization.,0,0,MESOS-5470,1.0
CNI should not store subnet of address in NetworkInfo,"When the CNI isolator executes the CNI plugin, that CNI plugin will return an IP Address and Subnet (192.168.0.1/32). Mesos should strip the subnet before storing the address in the Task.NetworkInfo.IPAddress.

Reason being - most current mesos components are not expecting a subnet in the Task's NetworkInfo.IPAddress, and instead expect just the IP address. This can cause errors in those components, such as Mesos-DNS failing to return a NetworkInfo address (and instead defaulting to the next configured IPSource), and Marathon generating invalid links to tasks (as it includes /32 in the link)",1,4,MESOS-5453,2.0
Make the SASL dependency optional.,"Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.

In the future, it would be nice to have a pluggable authentication layer.",1,6,MESOS-5450,2.0
Allow libprocess/stout to build without first doing `make` in 3rdparty.,"After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",1,4,MESOS-5445,2.0
GPU resource broke framework data table in webUI,"In agent_framework.html and master/static/agent.html, we add {{GPUs (Used / Allocated)}} in table header. But we didn't add the corresponding column to the table body as well.

On the other hand, we didn't provide statistics for gpus on monitor endpoints.
To provide those data in webui, it requires we implement gpus statistics in monitor endpoints firstly. ",1,3,MESOS-5436,1.0
`network/cni` isolator should skip the bind mounting of the CNI network information root directory if possible,"Currently in the create() method `network/cni` isolator, for the CNI network information root directory (i.e., {{/var/run/mesos/isolators/network/cni}}), we do a self bind mount and make sure it is a shared mount of its own peer group. However, we should not do a self bind mount if the mount containing the CNI network information root directory is already a shared mount in its own share peer group, just like what we did for `filesystem/linux` isolator in [MESOS-5239 | https://issues.apache.org/jira/browse/MESOS-5239].",1,2,MESOS-5413,3.0
Delete the /observe HTTP endpoint,"The ""/observe"" endpoint was introduced a long time ago for supporting functionality that was never implemented. We should just kill this endpoint and associated code to avoid tech debt.",1,7,MESOS-5408,2.0
Make fields in authorization::Request protobuf optional.,"Currently {{authorization::Request}} protobuf declares {{subject}} and {{object}} as required fields. However, in the codebase we not always set them, which renders the message in the uninitialized state, for example:
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#L603
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#L2057

I believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. However, they are still invalid protobuf messages. Moreover, some external authorizers may serialize these messages.

We can either ensure all required fields are set or make both {{subject}} and {{object}} fields optional. This will also require updating local authorizer, which should properly handle the situation when these fields are absent. We may also want to notify authors of external authorizers to update their code accordingly.

It looks like no deprecation is necessary, mainly because we already—erroneously!—treat these fields as optional.",1,20,MESOS-5405,3.0
Introduce ObjectApprover Interface to Authorizer.,As outlined here (https://docs.google.com/document/d/1FuS79P8uj5PIBycrBlkJSBKOtmeO8ezAuiNXxwIA3qA) we plan to add the option of retrieving a FilterObject from the Authorizer with the goal of allowing for efficient authorization of a large number of (potentially large) objects. ,1,2,MESOS-5403,5.0
Slave/Agent Rename Phase 1: Update terms in the website,"The following files need to be updated

site/source/index.html.md
",1,4,MESOS-5397,1.0
v1 Executor Protos not included in maven jar,"According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.

Script to verify
{code}
wget https://repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos-0.28.1.jar && unzip -lf mesos-0.28.1.jar | grep ""v1\/executor"" | wc -l
{code}",1,2,MESOS-5390,1.0
docker containerizer should prefix relative volume.container_path values with the path to the sandbox,"docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.

ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.

/cc [~jieyu]",1,7,MESOS-5389,3.0
MesosContainerizerLaunch flags execute arbitrary commands via shell.,"For example, the docker volume isolator's containerPath is appended (without sanitation) to a command that's executed in this manner. As such, it's possible to inject arbitrary shell commands to be executed by mesos.

https://github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#L206

Perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?",1,9,MESOS-5388,5.0
Add `HANDLE` overloads for functions that take a file descriptor,,1,2,MESOS-5386,3.0
Implement os::setHostname,,1,2,MESOS-5383,1.0
Implement os::fsync,,1,2,MESOS-5382,1.0
The scheduler library should have a delay before initiating a connection with master.,"Currently, the scheduler library {{src/scheduler/scheduler.cpp}} does have an artificially induced delay when trying to initially establish a connection with the master. In the event of a master failover or ZK disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with TCP SYN requests. 

On a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. This compounds the issue further on the master.",1,10,MESOS-5359,3.0
GET /master/maintenance/schedule/ produces 404.,"Attempts to make a GET request to /master/maintenance/schedule/ result in a 404. However, if I make a GET request to /master/maintenance/schedule (without the trailing /), it works. My current (untested) theory is that this might be related to the fact that there is also a /master/maintenance/schedule/status endpoint (an endpoint built on top of a functioning endpoint), as requests to /help and /help/ (with and without the trailing slash) produce the same functioning result.",1,2,MESOS-5333,3.0
Make `os::close` always catch structured exceptions on Windows,,1,2,MESOS-5318,2.0
Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,"This is in the context of Mesos containerizer (a.k.a., unified containerizer).

I did a simple test:
{noformat}
sudo sbin/mesos-master --work_dir=/tmp/mesos/master
sudo GLOG_v=1 sbin/mesos-slave --master=10.0.2.15:5050 --isolation=docker/runtime,filesystem/linux --work_dir=/tmp/mesos/slave/ --image_providers=docker --executor_environment_variables=""{}""
sudo bin/mesos-execute --master=10.0.2.15:5050 --name=test --docker_image=alpine --command=""env"" 

MESOS_EXECUTOR_ID=test
SHLVL=1
MESOS_CHECKPOINT=0
MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD=5secs
LIBPROCESS_PORT=0
MESOS_AGENT_ENDPOINT=10.0.2.15:5051
MESOS_SANDBOX=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
MESOS_NATIVE_JAVA_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_FRAMEWORK_ID=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000
MESOS_SLAVE_ID=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0
MESOS_NATIVE_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_DIRECTORY=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
PWD=/mnt/mesos/sandbox
MESOS_SLAVE_PID=slave(1)@10.0.2.15:5051
{noformat}

`MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",1,2,MESOS-5312,2.0
Add capabilities support for mesos execute cli.,Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.,1,3,MESOS-5303,3.0
Need to add REMOVE semantics to the copy backend,"Some Dockerfiles run the `rm` command to remove files from the base image using the ""RUN"" directive in the Dockerfile. An example can be found here:
https://github.com/ngineered/nginx-php-fpm.git

In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.


Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  ",1,2,MESOS-5277,5.0
add test cases for docker volume driver,,1,5,MESOS-5266,5.0
Update mesos-execute to support docker volume isolator.,The mesos-execute needs to be updated to support docker volume isolator.,1,4,MESOS-5265,3.0
Isolator cleanup should not be invoked if they are not prepared yet.,"If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet. 

In this case, there no need to clean up any isolator, call provisioner destroy directly.",1,2,MESOS-5253,2.0
Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.,"We recently added persistent volume support in DockerContainerizer (MESOS-3413). To understand the problem, we first need to understand how persistent volumes are supported in DockerContainerizer.

To support persistent volumes in DockerContainerizer, we bind mount persistent volumes under a container's sandbox ('container_path' has to be relative for persistent volumes). When the Docker container is launched, since we always add a volume (-v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since Docker does a 'rbind').

The assumption that the above works is that the Docker daemon should see those persistent volume mounts that Mesos mounts on the host mount table. It's not a problem if Docker daemon itself is using the host mount namespace. However, on systemd enabled systems, Docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this [patch|https://github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a].

So what that means is that: in order for it to work, the parent mount of agent's work_dir should be a shared mount when docker daemon starts. This is typically true on CentOS7, CoreOS as all mounts are shared mounts by default.

However, this causes an issue with the 'filesystem/linux' isolator. To understand why, first I need to show you a typical problem when dealing with shared mounts. Let me explain that using the following commands on a CentOS7 machine:
{noformat}
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
[root@core-dev run]# mkdir /run/netns
[root@core-dev run]# mount --bind /run/netns /run/netns
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
[root@core-dev run]# ip netns add test
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
162 121 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw
163 24 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw
{noformat}

As you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). This will confuse some systems sometimes. The reason is because when we create a self bind mount (/run/netns -> /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). Then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.

The reason we need to do a self bind mount in Mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. However, on some systems, mounts are private by default (e.g., Ubuntu 14.04). In those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. For instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.

To avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a make-slave + make-shared so that the mount is its own shared mount peer group. In that way, any mounts underneath it will not be propagated back.

However, that operation will break the assumption that the persistent volume DockerContainerizer support makes. As a result, we're seeing problem with persistent volumes in DockerContainerizer when filesystem/linux isolator is turned on.",1,2,MESOS-5239,3.0
CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest,"Observed on the Mesosphere internal CI:

{noformat}
[22:56:28]W:     [Step 10/10] F0420 22:56:28.056788   629 containerizer.cpp:1634] Check failed: containers_.contains(containerId)
{noformat}

Complete test log will be attached as a file.",1,6,MESOS-5238,2.0
Implement HTTP Docker Executor that uses the Executor Library,Similar to what we did with the HTTP command executor in MESOS-3558 we should have a HTTP docker executor that can speak the v1 Executor API.,1,12,MESOS-5227,5.0
Document docker volume driver isolator.,"Should include the followings:

1. What features (driver options) are supported in docker volume driver isolator.
2. How to use docker volume driver isolator.
    *related agent flags introduction and usage.
    *isolator dependency clarification (e.g., filesystem/linux).
    *related driver daemon preprocess.
    *volumes pre-specified by users and volume cleanup.",1,2,MESOS-5216,5.0
The filesystem/linux isolator does not set the permissions of the host_path.,"The {{filesystem/linux}} isolator is not a drop in replacement for the {{filesystem/shared}} isolator. This should be considered before the latter is deprecated.

We are currently using the {{filesystem/shared}} isolator together with the following slave option. This provides us with a private {{/tmp}} and {{/var/tmp}} folder for each task.

{code}
    --default_container_info='{
            ""type"": ""MESOS"",
            ""volumes"": [
                {""host_path"": ""system/tmp"",     ""container_path"": ""/tmp"",        ""mode"": ""RW""},
                {""host_path"": ""system/vartmp"",  ""container_path"": ""/var/tmp"",    ""mode"": ""RW""}
            ]
        }'
{code}

When browsing the Mesos sandbox, one can see the following permissions:
{code}
mode	nlink	uid	gid	size	mtime		
drwxrwxrwx	3	root	root	4 KB	Apr 11 18:16	 tmp	
drwxrwxrwx	2	root	root	4 KB	Apr 11 18:15	 vartmp	
{code}

However, when running with the new {{filesystem/linux}} isolator, the permissions are different:
{code}
mode	nlink	uid	gid	size	mtime		
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 tmp	
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 vartmp
{code}

This prevents user code (running as a non-root user) from writing to those folders, i.e. every write attempt fails with permission denied. 

*Context*:
* We are using Apache Aurora. Aurora is running its custom executor as root but then switches to a non-privileged user before running the actual user code. 
* The follow code seems to have enabled our usecase in the existing {{filesystem/shared}} isolator: https://github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp#L175-L198 ",1,5,MESOS-5187,3.0
Registry puller cannot fetch blobs correctly from http Redirect 3xx urls.,"When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",1,4,MESOS-5172,3.0
Run mesos builds on PowerPC platform in ASF CI,"This is the last step to declare official support for PowerPC.

This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI.
",0,1,MESOS-5156,1.0
Sandboxes contents should be protected from unauthorized users,"MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",1,7,MESOS-5153,8.0
MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,"Observed on the ASF CI:

{code}
[ RUN      ] MasterAllocatorTest/1.RebalancedForUpdatedWeights
I0407 22:34:10.330394 29278 cluster.cpp:149] Creating default 'local' authorizer
I0407 22:34:10.466182 29278 leveldb.cpp:174] Opened db in 135.608207ms
I0407 22:34:10.516398 29278 leveldb.cpp:181] Compacted db in 50.159558ms
I0407 22:34:10.516464 29278 leveldb.cpp:196] Created db iterator in 34959ns
I0407 22:34:10.516484 29278 leveldb.cpp:202] Seeked to beginning of db in 10195ns
I0407 22:34:10.516496 29278 leveldb.cpp:271] Iterated through 0 keys in the db in 7324ns
I0407 22:34:10.516547 29278 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0407 22:34:10.517277 29298 recover.cpp:447] Starting replica recovery
I0407 22:34:10.517693 29300 recover.cpp:473] Replica is in EMPTY status
I0407 22:34:10.520251 29310 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4775)@172.17.0.3:35855
I0407 22:34:10.520611 29311 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0407 22:34:10.521164 29299 recover.cpp:564] Updating replica status to STARTING
I0407 22:34:10.523435 29298 master.cpp:382] Master f59f9057-a5c7-43e1-b129-96862e640a12 (129e11060069) started on 172.17.0.3:35855
I0407 22:34:10.523473 29298 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/3rZY8C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/3rZY8C/master"" --zk_session_timeout=""10secs""
I0407 22:34:10.523885 29298 master.cpp:433] Master only allowing authenticated frameworks to register
I0407 22:34:10.523901 29298 master.cpp:438] Master only allowing authenticated agents to register
I0407 22:34:10.523913 29298 credentials.hpp:37] Loading credentials for authentication from '/tmp/3rZY8C/credentials'
I0407 22:34:10.524298 29298 master.cpp:480] Using default 'crammd5' authenticator
I0407 22:34:10.524441 29298 master.cpp:551] Using default 'basic' HTTP authenticator
I0407 22:34:10.524564 29298 master.cpp:589] Authorization enabled
I0407 22:34:10.525269 29305 hierarchical.cpp:145] Initialized hierarchical allocator process
I0407 22:34:10.525333 29305 whitelist_watcher.cpp:77] No whitelist given
I0407 22:34:10.527331 29298 master.cpp:1832] The newly elected leader is master@172.17.0.3:35855 with id f59f9057-a5c7-43e1-b129-96862e640a12
I0407 22:34:10.527441 29298 master.cpp:1845] Elected as the leading master!
I0407 22:34:10.527545 29298 master.cpp:1532] Recovering from registrar
I0407 22:34:10.527889 29298 registrar.cpp:331] Recovering registrar
I0407 22:34:10.549734 29299 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 28.25177ms
I0407 22:34:10.549782 29299 replica.cpp:320] Persisted replica status to STARTING
I0407 22:34:10.550010 29299 recover.cpp:473] Replica is in STARTING status
I0407 22:34:10.551352 29299 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4777)@172.17.0.3:35855
I0407 22:34:10.551676 29299 recover.cpp:193] Received a recover response from a replica in STARTING status
I0407 22:34:10.552315 29308 recover.cpp:564] Updating replica status to VOTING
I0407 22:34:10.574865 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.413614ms
I0407 22:34:10.574928 29308 replica.cpp:320] Persisted replica status to VOTING
I0407 22:34:10.575103 29308 recover.cpp:578] Successfully joined the Paxos group
I0407 22:34:10.575346 29308 recover.cpp:462] Recover process terminated
I0407 22:34:10.575913 29308 log.cpp:659] Attempting to start the writer
I0407 22:34:10.577512 29308 replica.cpp:493] Replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1
I0407 22:34:10.599984 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.453613ms
I0407 22:34:10.600026 29308 replica.cpp:342] Persisted promised to 1
I0407 22:34:10.601773 29304 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0407 22:34:10.603757 29307 replica.cpp:388] Replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2
I0407 22:34:10.634392 29307 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.269987ms
I0407 22:34:10.634829 29307 replica.cpp:712] Persisted action at 0
I0407 22:34:10.637017 29297 replica.cpp:537] Replica received write request for position 0 from (4780)@172.17.0.3:35855
I0407 22:34:10.637099 29297 leveldb.cpp:436] Reading position from leveldb took 52948ns
I0407 22:34:10.676170 29297 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 38.917487ms
I0407 22:34:10.676352 29297 replica.cpp:712] Persisted action at 0
I0407 22:34:10.677564 29306 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0407 22:34:10.717959 29306 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 40.306229ms
I0407 22:34:10.718202 29306 replica.cpp:712] Persisted action at 0
I0407 22:34:10.718399 29306 replica.cpp:697] Replica learned NOP action at position 0
I0407 22:34:10.719883 29306 log.cpp:675] Writer started with ending position 0
I0407 22:34:10.721688 29305 leveldb.cpp:436] Reading position from leveldb took 75934ns
I0407 22:34:10.723640 29306 registrar.cpp:364] Successfully fetched the registry (0B) in 195648us
I0407 22:34:10.723999 29306 registrar.cpp:463] Applied 1 operations in 108099ns; attempting to update the 'registry'
I0407 22:34:10.725077 29311 log.cpp:683] Attempting to append 170 bytes to the log
I0407 22:34:10.725328 29308 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0407 22:34:10.726552 29299 replica.cpp:537] Replica received write request for position 1 from (4781)@172.17.0.3:35855
I0407 22:34:10.759747 29299 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.089719ms
I0407 22:34:10.759976 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.761739 29299 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0407 22:34:10.801522 29299 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 39.694064ms
I0407 22:34:10.801602 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.801638 29299 replica.cpp:697] Replica learned APPEND action at position 1
I0407 22:34:10.803371 29311 registrar.cpp:508] Successfully updated the 'registry' in 79.163904ms
I0407 22:34:10.803829 29311 registrar.cpp:394] Successfully recovered registrar
I0407 22:34:10.804585 29311 master.cpp:1640] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0407 22:34:10.805269 29308 log.cpp:702] Attempting to truncate the log to 1
I0407 22:34:10.805721 29310 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0407 22:34:10.805276 29296 hierarchical.cpp:172] Skipping recovery of hierarchical allocator: nothing to recover
I0407 22:34:10.806529 29307 replica.cpp:537] Replica received write request for position 2 from (4782)@172.17.0.3:35855
I0407 22:34:10.843320 29307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 36.77593ms
I0407 22:34:10.843531 29307 replica.cpp:712] Persisted action at 2
I0407 22:34:10.845369 29311 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0407 22:34:10.885098 29311 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 39.641102ms
I0407 22:34:10.885401 29311 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88701ns
I0407 22:34:10.885745 29311 replica.cpp:712] Persisted action at 2
I0407 22:34:10.885862 29311 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0407 22:34:10.900660 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:10.901793 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:10.905488 29302 slave.cpp:201] Agent started on 111)@172.17.0.3:35855
I0407 22:34:10.905553 29302 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa""
I0407 22:34:10.906365 29302 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential'
I0407 22:34:10.906787 29302 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:10.907202 29302 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials'
I0407 22:34:10.907713 29302 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:10.908499 29302 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:10.910189 29302 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:10.910362 29302 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:10.910465 29302 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:10.913280 29303 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta'
I0407 22:34:10.914621 29303 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:10.915226 29303 containerizer.cpp:416] Recovering containerizer
I0407 22:34:10.917246 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:10.917733 29301 slave.cpp:4784] Finished recovery
I0407 22:34:10.918226 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:10.918529 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:10.918908 29304 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:10.918988 29304 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:10.919098 29301 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:10.919309 29304 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:10.919535 29304 slave.cpp:975] Detecting new master
I0407 22:34:10.919747 29308 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:10.920413 29308 master.cpp:5695] Authenticating slave(111)@172.17.0.3:35855
I0407 22:34:10.920650 29308 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.921020 29308 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:10.921308 29308 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:10.921424 29308 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:10.921596 29308 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:10.921752 29308 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:10.921957 29307 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:10.922178 29308 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:10.922214 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:10.922229 29308 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:10.922281 29308 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:10.922309 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:10.922322 29308 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922332 29308 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922353 29308 authenticator.cpp:317] Authentication success
I0407 22:34:10.922436 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:10.922587 29308 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(111)@172.17.0.3:35855
I0407 22:34:10.922668 29299 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.923256 29307 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:10.923429 29307 slave.cpp:1468] Will retry registration in 3.220345ms if necessary
I0407 22:34:10.923707 29302 master.cpp:4406] Registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:10.924239 29309 registrar.cpp:463] Applied 1 operations in 105794ns; attempting to update the 'registry'
I0407 22:34:10.925787 29309 log.cpp:683] Attempting to append 339 bytes to the log
I0407 22:34:10.926028 29309 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0407 22:34:10.927139 29309 replica.cpp:537] Replica received write request for position 3 from (4797)@172.17.0.3:35855
I0407 22:34:10.929083 29305 slave.cpp:1468] Will retry registration in 39.293556ms if necessary
I0407 22:34:10.929363 29305 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.968843 29309 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 41.68025ms
I0407 22:34:10.969005 29309 replica.cpp:712] Persisted action at 3
I0407 22:34:10.969741 29309 slave.cpp:1468] Will retry registration in 54.852242ms if necessary
I0407 22:34:10.970118 29309 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.970852 29306 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0407 22:34:11.010634 29306 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 39.680272ms
I0407 22:34:11.010840 29306 replica.cpp:712] Persisted action at 3
I0407 22:34:11.011014 29306 replica.cpp:697] Replica learned APPEND action at position 3
I0407 22:34:11.014020 29306 registrar.cpp:508] Successfully updated the 'registry' in 89.684224ms
I0407 22:34:11.014181 29296 log.cpp:702] Attempting to truncate the log to 3
I0407 22:34:11.014606 29296 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0407 22:34:11.015836 29298 replica.cpp:537] Replica received write request for position 4 from (4798)@172.17.0.3:35855
I0407 22:34:11.016973 29296 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.017518 29304 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.017763 29311 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:11.018362 29311 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.018870 29311 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S0/slave.info'
I0407 22:34:11.018890 29307 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.019182 29304 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.019304 29304 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 1.077349ms
I0407 22:34:11.019493 29311 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.019726 29311 slave.cpp:3675] Received ping from slave-observer(112)@172.17.0.3:35855
I0407 22:34:11.019878 29299 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.020845 29305 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.021005 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.021065 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 173907ns
I0407 22:34:11.022289 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.023422 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.026309 29309 slave.cpp:201] Agent started on 112)@172.17.0.3:35855
I0407 22:34:11.026410 29309 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O""
I0407 22:34:11.027070 29309 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential'
I0407 22:34:11.027308 29309 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.027354 29309 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials'
I0407 22:34:11.027698 29309 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.028147 29309 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.028854 29309 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.028998 29309 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.029064 29309 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.031188 29309 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta'
I0407 22:34:11.031844 29300 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.032091 29300 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.033805 29300 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.034364 29300 slave.cpp:4784] Finished recovery
I0407 22:34:11.061807 29300 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.062371 29300 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.062450 29300 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.062469 29300 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.062630 29300 slave.cpp:975] Detecting new master
I0407 22:34:11.062737 29300 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.062820 29300 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.062952 29300 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.063413 29300 master.cpp:5695] Authenticating slave(112)@172.17.0.3:35855
I0407 22:34:11.063591 29300 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.063907 29300 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.064159 29300 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.064201 29300 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.064296 29300 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.064363 29300 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.064443 29300 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.064537 29300 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.064569 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.064584 29300 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.064640 29300 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.064668 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.064680 29300 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064689 29300 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064708 29300 authenticator.cpp:317] Authentication success
I0407 22:34:11.064856 29300 authenticatee.cpp:298] Authentication success
I0407 22:34:11.064941 29300 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(112)@172.17.0.3:35855
I0407 22:34:11.065019 29300 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.065431 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.065580 29305 slave.cpp:1468] Will retry registration in 14.268351ms if necessary
I0407 22:34:11.065948 29305 master.cpp:4406] Registering agent at slave(112)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.066653 29296 registrar.cpp:463] Applied 1 operations in 190813ns; attempting to update the 'registry'
I0407 22:34:11.075197 29298 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 59.338116ms
I0407 22:34:11.075359 29298 replica.cpp:712] Persisted action at 4
I0407 22:34:11.076177 29301 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0407 22:34:11.080481 29309 slave.cpp:1468] Will retry registration in 23.018984ms if necessary
I0407 22:34:11.080770 29309 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.100519 29301 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.288152ms
I0407 22:34:11.100792 29301 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98264ns
I0407 22:34:11.100883 29301 replica.cpp:712] Persisted action at 4
I0407 22:34:11.101002 29301 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0407 22:34:11.102180 29309 log.cpp:683] Attempting to append 505 bytes to the log
I0407 22:34:11.102334 29301 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0407 22:34:11.103551 29309 replica.cpp:537] Replica received write request for position 5 from (4813)@172.17.0.3:35855
I0407 22:34:11.105705 29305 slave.cpp:1468] Will retry registration in 49.972787ms if necessary
I0407 22:34:11.106020 29305 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.126212 29309 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 22.638848ms
I0407 22:34:11.126296 29309 replica.cpp:712] Persisted action at 5
I0407 22:34:11.127374 29305 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0407 22:34:11.150754 29305 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 23.376079ms
I0407 22:34:11.150952 29305 replica.cpp:712] Persisted action at 5
I0407 22:34:11.150992 29305 replica.cpp:697] Replica learned APPEND action at position 5
I0407 22:34:11.154031 29305 registrar.cpp:508] Successfully updated the 'registry' in 87.26784ms
I0407 22:34:11.154491 29305 log.cpp:702] Attempting to truncate the log to 5
I0407 22:34:11.154824 29305 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0407 22:34:11.155413 29308 slave.cpp:3675] Received ping from slave-observer(113)@172.17.0.3:35855
I0407 22:34:11.155467 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.155580 29308 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.155606 29308 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.155856 29304 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.156281 29308 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S1/slave.info'
I0407 22:34:11.156661 29304 replica.cpp:537] Replica received write request for position 6 from (4814)@172.17.0.3:35855
I0407 22:34:11.156949 29305 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.157217 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.157346 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 304432ns
I0407 22:34:11.157224 29308 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.157788 29303 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.158424 29303 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.158633 29303 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.158699 29303 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 178482ns
I0407 22:34:11.162139 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.192978 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.197527 29307 slave.cpp:201] Agent started on 113)@172.17.0.3:35855
I0407 22:34:11.197581 29307 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru""
I0407 22:34:11.198328 29307 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential'
I0407 22:34:11.198562 29307 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.198598 29307 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials'
I0407 22:34:11.198884 29307 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.199286 29307 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.199820 29307 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.199905 29307 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.199920 29307 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.201535 29297 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta'
I0407 22:34:11.201773 29309 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.202081 29307 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.202180 29304 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.487899ms
I0407 22:34:11.202221 29304 replica.cpp:712] Persisted action at 6
I0407 22:34:11.203219 29302 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0407 22:34:11.205412 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.205984 29301 slave.cpp:4784] Finished recovery
I0407 22:34:11.206735 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.207351 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.207679 29301 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.207804 29309 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.208039 29301 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.208072 29301 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.208431 29301 slave.cpp:975] Detecting new master
I0407 22:34:11.208650 29309 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.208976 29309 master.cpp:5695] Authenticating slave(113)@172.17.0.3:35855
I0407 22:34:11.209081 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.209432 29304 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.209971 29304 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.210103 29304 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.210382 29304 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.210515 29304 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.210726 29304 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.210940 29305 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.210980 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.210997 29305 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.211060 29305 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.211100 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.211175 29305 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211244 29305 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211272 29305 authenticator.cpp:317] Authentication success
I0407 22:34:11.211462 29305 authenticatee.cpp:298] Authentication success
I0407 22:34:11.211575 29305 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(113)@172.17.0.3:35855
I0407 22:34:11.211673 29305 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.212026 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.212280 29305 slave.cpp:1468] Will retry registration in 6.415977ms if necessary
I0407 22:34:11.212704 29304 master.cpp:4406] Registering agent at slave(113)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.213373 29311 registrar.cpp:463] Applied 1 operations in 154555ns; attempting to update the 'registry'
I0407 22:34:11.223568 29303 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.224171 29300 slave.cpp:1468] Will retry registration in 22.418267ms if necessary
I0407 22:34:11.243433 29302 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.20863ms
I0407 22:34:11.243851 29302 leveldb.cpp:399] Deleting ~2 keys from leveldb took 204965ns
I0407 22:34:11.243980 29302 replica.cpp:712] Persisted action at 6
I0407 22:34:11.244148 29302 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0407 22:34:11.245827 29302 log.cpp:683] Attempting to append 671 bytes to the log
I0407 22:34:11.246206 29310 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0407 22:34:11.247114 29296 replica.cpp:537] Replica received write request for position 7 from (4829)@172.17.0.3:35855
I0407 22:34:11.248457 29304 slave.cpp:1468] Will retry registration in 14.981599ms if necessary
I0407 22:34:11.248837 29302 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.265728 29301 slave.cpp:1468] Will retry registration in 117.285894ms if necessary
I0407 22:34:11.266026 29301 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.278012 29296 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 30.789344ms
I0407 22:34:11.278064 29296 replica.cpp:712] Persisted action at 7
I0407 22:34:11.278990 29303 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0407 22:34:11.337220 29303 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 58.231676ms
I0407 22:34:11.337312 29303 replica.cpp:712] Persisted action at 7
I0407 22:34:11.337347 29303 replica.cpp:697] Replica learned APPEND action at position 7
I0407 22:34:11.340283 29305 registrar.cpp:508] Successfully updated the 'registry' in 126.71616ms
I0407 22:34:11.340703 29309 log.cpp:702] Attempting to truncate the log to 7
I0407 22:34:11.341044 29309 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0407 22:34:11.341847 29309 slave.cpp:3675] Received ping from slave-observer(114)@172.17.0.3:35855
I0407 22:34:11.342489 29309 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.342532 29309 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.341804 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.342871 29297 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.342267 29300 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.342963 29299 replica.cpp:537] Replica received write request for position 8 from (4830)@172.17.0.3:35855
I0407 22:34:11.343101 29300 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.343178 29300 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 242921ns
I0407 22:34:11.342921 29309 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S2/slave.info'
I0407 22:34:11.343636 29309 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.343863 29309 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.344173 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.344425 29309 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.344568 29309 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.344621 29309 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 155620ns
I0407 22:34:11.345155 29303 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.345387 29303 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.345479 29303 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.346035 29303 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.346884 29303 master.cpp:5695] Authenticating scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.347530 29303 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.349140 29303 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.349580 29303 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.349707 29303 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.349957 29309 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.350040 29309 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.350168 29309 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.350275 29309 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.350309 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.350323 29309 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.350375 29309 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.350407 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.350420 29309 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350430 29309 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350450 29309 authenticator.cpp:317] Authentication success
I0407 22:34:11.350550 29303 authenticatee.cpp:298] Authentication success
I0407 22:34:11.350647 29309 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.350803 29303 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.350986 29309 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.351011 29309 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.351109 29309 sched.cpp:812] Will retry registration in 82.651114ms if necessary
I0407 22:34:11.351313 29296 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.351343 29296 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0407 22:34:11.351662 29310 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.352442 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353435 29309 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353519 29309 sched.cpp:720] Scheduler::registered took 66350ns
I0407 22:34:11.355201 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.355293 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 2.836617ms
I0407 22:34:11.356238 29301 master.cpp:5524] Sending 3 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.357260 29311 sched.cpp:876] Scheduler::resourceOffers took 327028ns
I0407 22:34:11.357628 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358330 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358959 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.360607 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.361264 29307 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.361342 29307 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.361366 29307 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.361670 29307 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.361959 29307 master.cpp:5695] Authenticating scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.362195 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.362535 29311 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.362890 29307 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.362926 29307 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.363021 29307 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.363082 29307 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.363199 29311 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.363313 29311 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.363406 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.363512 29311 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.363605 29311 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.363651 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.363673 29311 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363685 29311 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363706 29311 authenticator.cpp:317] Authentication success
I0407 22:34:11.363785 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:11.363858 29297 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.363903 29311 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.365274 29297 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.365301 29297 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.365396 29297 sched.cpp:812] Will retry registration in 1.739883809secs if necessary
I0407 22:34:11.365500 29311 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.365528 29311 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role2'
I0407 22:34:11.365952 29297 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.366518 29297 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366564 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366590 29297 sched.cpp:720] Scheduler::registered took 57363ns
I0407 22:34:11.366768 29311 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.366837 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.366914 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 340908ns
I0407 22:34:11.369886 29309 process.cpp:3165] Handling HTTP event for process 'master' with path: '/master/weights'
I0407 22:34:11.370643 29309 http.cpp:313] HTTP PUT for /master/weights from 172.17.0.3:59397
I0407 22:34:11.370762 29309 weights_handler.cpp:58] Updating weights from request: '[{""role"":""role2"",""weight"":2.0}]'
I0407 22:34:11.370908 29309 weights_handler.cpp:198] Authorizing principal 'test-principal' to update weights for roles '[ role2 ]'
I0407 22:34:11.372067 29306 registrar.cpp:463] Applied 1 operations in 136060ns; attempting to update the 'registry'
I0407 22:34:11.388222 29299 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.245469ms
I0407 22:34:11.388381 29299 replica.cpp:712] Persisted action at 8
I0407 22:34:11.389389 29305 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0407 22:34:11.435415 29305 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 45.918275ms
I0407 22:34:11.435688 29305 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98518ns
I0407 22:34:11.435835 29305 replica.cpp:712] Persisted action at 8
I0407 22:34:11.435956 29305 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0407 22:34:11.437063 29310 log.cpp:683] Attempting to append 691 bytes to the log
I0407 22:34:11.437297 29300 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 9
I0407 22:34:11.437979 29300 replica.cpp:537] Replica received write request for position 9 from (4834)@172.17.0.3:35855
I0407 22:34:11.479363 29300 leveldb.cpp:341] Persisting action (710 bytes) to leveldb took 41.36295ms
I0407 22:34:11.479432 29300 replica.cpp:712] Persisted action at 9
I0407 22:34:11.480434 29296 replica.cpp:691] Replica received learned notice for position 9 from @0.0.0.0:0
I0407 22:34:11.521299 29296 leveldb.cpp:341] Persisting action (712 bytes) to leveldb took 40.855981ms
I0407 22:34:11.521378 29296 replica.cpp:712] Persisted action at 9
I0407 22:34:11.521412 29296 replica.cpp:697] Replica learned APPEND action at position 9
I0407 22:34:11.524554 29304 registrar.cpp:508] Successfully updated the 'registry' in 152.402176ms
I0407 22:34:11.524790 29298 log.cpp:702] Attempting to truncate the log to 9
I0407 22:34:11.524960 29304 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 10
I0407 22:34:11.525243 29298 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.525387 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.525538 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 540681ns
I0407 22:34:11.525856 29296 replica.cpp:537] Replica received write request for position 10 from (4835)@172.17.0.3:35855
I0407 22:34:11.526267 29308 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O1
I0407 22:34:11.526398 29308 sched.cpp:913] Scheduler::offerRescinded took 54437ns
I0407 22:34:11.526425 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.527235 29299 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O2
I0407 22:34:11.527299 29299 sched.cpp:913] Scheduler::offerRescinded took 29764ns
I0407 22:34:11.527825 29300 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O0
I0407 22:34:11.527920 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.527990 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 1.481251ms
I0407 22:34:11.528009 29300 sched.cpp:913] Scheduler::offerRescinded took 333035ns
I0407 22:34:11.528591 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.529536 29311 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.529846 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.530747 29304 sched.cpp:876] Scheduler::resourceOffers took 128400ns
I0407 22:34:11.560456 29296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.585376ms
I0407 22:34:11.560539 29296 replica.cpp:712] Persisted action at 10
I0407 22:34:11.564628 29303 replica.cpp:691] Replica received learned notice for position 10 from @0.0.0.0:0
I0407 22:34:11.601330 29303 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 36.57815ms
I0407 22:34:11.601774 29303 leveldb.cpp:399] Deleting ~2 keys from leveldb took 221499ns
I0407 22:34:11.601899 29303 replica.cpp:712] Persisted action at 10
I0407 22:34:11.602052 29303 replica.cpp:697] Replica learned TRUNCATE action at position 10
I0407 22:34:12.531602 29308 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:12.532578 29308 hierarchical.cpp:1142] Performed allocation for 3 agents in 3.892929ms
I0407 22:34:12.532403 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
../../src/tests/master_allocator_tests.cpp:1587: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fffe87e3370, @0x2adef432e6f0 { 144-byte object <E0-9C 76-EA DE-2A 00-00 00-00 00-00 00-00 00-00 1F-00 00-00 00-00 00-00 90-4B 00-2C DF-2A 00-00 30-6A 00-2C DF-2A 00-00 80-6A 00-2C DF-2A 00-00 20-62 00-2C DF-2A 00-00 60-38 00-2C DF-2A 00-00 ... 04-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0407 22:34:12.533665 29301 sched.cpp:876] Scheduler::resourceOffers took 250853ns
I0407 22:34:12.533915 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.534454 29306 sched.cpp:876] Scheduler::resourceOffers took 157733ns
../../src/tests/master_allocator_tests.cpp:1629: Failure
Value of: framework2offers.get().size()
  Actual: 1
Expected: 2u
Which is: 2
I0407 22:34:12.534997 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:12.537264 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 disconnected
I0407 22:34:12.537297 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.537330 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.537849 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.538306 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.538394 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 0ns to failover
I0407 22:34:12.539371 29302 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540053 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540732 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540974 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 disconnected
I0407 22:34:12.541178 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541292 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541553 29300 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.542654 29300 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
W0407 22:34:12.543051 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.543525 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 0ns to failover
I0407 22:34:12.543861 29301 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.543959 29301 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.544445 29301 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545446 29301 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.544556 29300 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545661 29300 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545774 29300 slave.cpp:811] Agent terminating
I0407 22:34:12.544791 29305 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545241 29296 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.544518 29302 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
I0407 22:34:12.546140 29296 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
W0407 22:34:12.546159 29302 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.546496 29296 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.546527 29296 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546581 29296 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546752 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546782 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.546844 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546869 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547111 29296 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547302 29296 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 deactivated
I0407 22:34:12.553478 29278 slave.cpp:811] Agent terminating
I0407 22:34:12.553766 29306 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.555483 29306 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.555858 29306 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.556190 29307 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 deactivated
I0407 22:34:12.559095 29299 slave.cpp:811] Agent terminating
I0407 22:34:12.559301 29300 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.559327 29300 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559370 29300 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559516 29309 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 deactivated
I0407 22:34:12.561872 29278 master.cpp:1089] Master terminating
I0407 22:34:12.562566 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:12.562890 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:12.565459 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S0
[  FAILED  ] MasterAllocatorTest/1.RebalancedForUpdatedWeights, where TypeParam = mesos::internal::tests::Module<mesos::master::allocator::Allocator, (mesos::internal::tests::ModuleID)6> (2240 ms)
{code}",1,4,MESOS-5146,1.0
Add agent flags for HTTP authorization.,"Flags should be added to the agent to:
1. Enable authorization ({{--authorizers}})
2. Provide ACLs ({{--acls}})",1,3,MESOS-5142,2.0
Some ProvisionerDockerLocalStoreTest.* are flaky due to tar issue.,"These tests are still occasionally fail as of Mesos 1.5.0-wip:
{code}
ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
ProvisionerDockerLocalStoreTest.MetadataManagerInitialization
ProvisionerDockerLocalStoreTest.MissingLayer
{code}

Found this on ASF CI while testing 0.28.1-rc2

{code}
[ RUN      ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
E0406 18:29:30.870481   520 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:
sh: 1: hadoop: not found
E0406 18:29:30.870576   520 fetcher.cpp:59] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
I0406 18:29:30.871052   520 local_puller.cpp:90] Creating local puller with docker registry '/tmp/3l8ZBv/images'
I0406 18:29:30.873325   539 metadata_manager.cpp:159] Looking for image 'abc'
I0406 18:29:30.874438   539 local_puller.cpp:142] Untarring image 'abc' from '/tmp/3l8ZBv/images/abc.tar' to '/tmp/3l8ZBv/store/staging/5tw8bD'
I0406 18:29:30.901916   547 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
I0406 18:29:30.902304   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/123/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/123/rootfs'
I0406 18:29:30.909144   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs'
../../src/tests/containerizer/provisioner_docker_tests.cpp:183: Failure
(imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar, -C, /tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs' failed: tar: This does not look like a tar archive
tar: Exiting with failure status due to previous errors

[  FAILED  ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (243 ms)
{code}",0,6,MESOS-5139,2.0
PersistentVolumeTest.AccessPersistentVolume is flaky,"Observed on ASF CI:

{code}
[ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0
I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer
I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms
I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms
I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns
I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns
I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery
I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status
I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972
I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING
I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972
I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""
I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register
I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register
I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'
I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator
I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator
I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled
I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given
I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process
I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f
I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!
I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar
I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar
I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms
I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING
I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status
I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972
I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status
I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING
I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms
I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING
I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group
I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated
I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer
I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1
I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms
I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1
I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2
I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms
I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0
I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972
I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns
I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms
I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0
I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms
I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0
I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0
I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0
I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns
I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns
I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'
I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log
I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972
I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms
I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms
I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1
I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar
I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1
I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972
I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms
I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms
I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns
I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048
Trying semicolon-delimited string format instead
I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972
I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""
I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'
I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal
I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'
I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator
I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0
I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]
I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90
I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'
I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972
I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success
I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success
I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972
I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary
I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns
I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns
I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager
I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer
I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete
I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery
I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources
I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972
I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates
I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee
I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master
I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator
I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972
I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success
I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success
I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972
I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary
I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'
I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log
I0405 17:29:19.961879 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0405 17:29:19.963135 31857 replica.cpp:537] Replica received write request for position 3 from (14381)@172.17.0.4:43972
I0405 17:29:19.999408 31857 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 36.200109ms
I0405 17:29:19.999512 31857 replica.cpp:712] Persisted action at 3
I0405 17:29:20.001049 31869 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0405 17:29:20.038849 31869 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 37.709507ms
I0405 17:29:20.038930 31869 replica.cpp:712] Persisted action at 3
I0405 17:29:20.038965 31869 replica.cpp:697] Replica learned APPEND action at position 3
I0405 17:29:20.041484 31869 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:20.041785 31869 log.cpp:702] Attempting to truncate the log to 3
I0405 17:29:20.042364 31859 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0405 17:29:20.043767 31859 replica.cpp:537] Replica received write request for position 4 from (14382)@172.17.0.4:43972
I0405 17:29:20.044585 31869 master.cpp:4458] Registered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:20.044910 31864 slave.cpp:1105] Registered with master master@172.17.0.4:43972; given agent ID 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.045075 31864 fetcher.cpp:81] Clearing fetcher cache
I0405 17:29:20.045140 31870 hierarchical.cpp:476] Added agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] (allocated: )
I0405 17:29:20.045581 31864 slave.cpp:1128] Checkpointing SlaveInfo to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/slave.info'
I0405 17:29:20.045974 31864 slave.cpp:1165] Forwarding total oversubscribed resources 
I0405 17:29:20.046077 31864 slave.cpp:3664] Received ping from slave-observer(399)@172.17.0.4:43972
I0405 17:29:20.046193 31864 status_update_manager.cpp:181] Resuming sending status updates
I0405 17:29:20.046289 31870 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.046370 31870 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 1.153377ms
I0405 17:29:20.046499 31864 master.cpp:4802] Received update of agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with total oversubscribed resources 
I0405 17:29:20.047142 31868 hierarchical.cpp:534] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) updated with oversubscribed resources  (total: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000], allocated: disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000])
I0405 17:29:20.047960 31868 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.048009 31868 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.048065 31868 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 866803ns
I0405 17:29:20.048591 31864 master.cpp:5508] Sending 1 offers to framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.049188 31860 sched.cpp:874] Scheduler::resourceOffers took 114867ns
I0405 17:29:20.080921 31859 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.025538ms
I0405 17:29:20.081001 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.082425 31859 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0405 17:29:20.106056 31859 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.583037ms
I0405 17:29:20.106205 31859 leveldb.cpp:399] Deleting ~2 keys from leveldb took 76995ns
I0405 17:29:20.106240 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.106278 31859 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0405 17:29:20.119488 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0405 17:29:20.121356 31859 master.cpp:3288] Processing ACCEPT call for offers: [ 9565ff6f-f1b6-4259-8430-690e635c391f-O0 ] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.121485 31859 master.cpp:3046] Authorizing principal 'test-principal' to create volumes
I0405 17:29:20.121692 31859 master.cpp:2891] Authorizing framework principal 'test-principal' to launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 as user 'mesos'
I0405 17:29:20.123877 31871 master.cpp:3617] Applying CREATE operation for volumes disk(role1)[id1:path1]:2048 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.125424 31871 master.cpp:6747] Sending checkpointed resources disk(role1)[id1:path1]:2048 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.126397 31856 hierarchical.cpp:656] Updated allocation of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000] to disk(role1):2048; cpus(*):2; mem(*):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048
I0405 17:29:20.126667 31871 master.hpp:177] Adding task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90)
I0405 17:29:20.126875 31871 master.cpp:3773] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.127390 31856 slave.cpp:2523] Updated checkpointed resources from  to disk(role1)[id1:path1]:2048
I0405 17:29:20.127615 31856 slave.cpp:1497] Got assigned task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127876 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.127841 31871 hierarchical.cpp:893] Recovered disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: disk(role1)[id1:path1]:2048; cpus(*):1; mem(*):128) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127913 31871 hierarchical.cpp:930] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filtered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for 5secs
I0405 17:29:20.128667 31856 slave.cpp:1616] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.128937 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.129776 31856 paths.cpp:528] Trying to chown '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' to user 'mesos'
I0405 17:29:20.145324 31856 slave.cpp:5575] Launching executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.146057 31858 containerizer.cpp:675] Starting container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000'
I0405 17:29:20.146078 31856 slave.cpp:1834] Queuing task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.146203 31856 slave.cpp:881] Successfully attached file '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.147619 31859 posix.cpp:206] Changing the ownership of the persistent volume at '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' with uid 1000 and gid 1000
I0405 17:29:20.162421 31859 posix.cpp:250] Adding symlink from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.172133 31861 launcher.cpp:123] Forked child with pid '7927' for container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0405 17:29:20.376197  7941 process.cpp:986] libprocess is initialized on 172.17.0.4:50952 with 16 worker threads
I0405 17:29:20.378132  7941 logging.cpp:195] Logging to STDERR
I0405 17:29:20.380861  7941 exec.cpp:150] Version: 0.29.0
I0405 17:29:20.396257  7966 exec.cpp:200] Executor started at: executor(1)@172.17.0.4:50952 with pid 7941
I0405 17:29:20.399426 31860 slave.cpp:2825] Got registration for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.402995  7966 exec.cpp:225] Executor registered on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.403014 31860 slave.cpp:1999] Sending queued task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' to executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:20.405624  7966 exec.cpp:237] Executor::registered took 393272ns
I0405 17:29:20.406108  7966 exec.cpp:312] Executor asked to run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542'
Registered executor on 4090d10eba90
I0405 17:29:20.406708  7966 exec.cpp:321] Executor::launchTask took 568039ns
Starting task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
Forked command at 7972
sh -c 'echo abc > path1/file'
I0405 17:29:20.411375  7966 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.413156 31857 slave.cpp:3184] Handling status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.415714 31857 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.415788 31857 status_update_manager.cpp:497] Creating StatusUpdate stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416345 31857 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.416720 31870 slave.cpp:3582] Forwarding the update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.416954 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416997 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.417505 31870 master.cpp:4947] Status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.417549 31870 master.cpp:4995] Forwarding status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.417724 31870 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0405 17:29:20.417943  7960 exec.cpp:358] Executor received status update acknowledgement cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.418002 31870 sched.cpp:982] Scheduler::statusUpdate took 105225ns
I0405 17:29:20.418623 31870 master.cpp:4102] Processing ACKNOWLEDGE call cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.419181 31860 status_update_manager.cpp:392] Received status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.419816 31860 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.513465  7969 exec.cpp:535] Executor sending status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
Command exited with status 0 (pid: 7972)
I0405 17:29:20.515449 31870 slave.cpp:3184] Handling status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.516875 31860 slave.cpp:5885] Terminating task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.517496 31867 posix.cpp:156] Removing symlink '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.519361 31864 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.519850 31864 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.520678 31870 slave.cpp:3582] Forwarding the update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.520901 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.520949 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.521550 31864 master.cpp:4947] Status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.521610 31864 master.cpp:4995] Forwarding status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.522099 31871 sched.cpp:982] Scheduler::statusUpdate took 102502ns
I0405 17:29:20.522367 31864 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0405 17:29:20.524288 31871 hierarchical.cpp:1676] Filtered offer with disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.524379 31871 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.524451 31871 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.524551 31871 hierarchical.cpp:1141] Performed allocation for 1 agents in 961746ns
I0405 17:29:20.525182 31858 hierarchical.cpp:893] Recovered cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: ) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.525197 31864 master.cpp:4102] Processing ACKNOWLEDGE call 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.525380 31864 master.cpp:6674] Removing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.526067 31864 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526425 31864 status_update_manager.cpp:528] Cleaning up status update stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526917 31864 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.527048 31864 slave.cpp:5926] Completing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.527732  7964 exec.cpp:358] Executor received status update acknowledgement 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:21.527920 31859 slave.cpp:3710] executor(1)@172.17.0.4:50952 exited
../../src/tests/persistent_volume_tests.cpp:825: Failure
Failed to wait 15secs for offers
I0405 17:29:35.542609 31856 master.cpp:1269] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 disconnected
I0405 17:29:35.542811 31856 master.cpp:2642] Disconnecting framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.542994 31856 master.cpp:2666] Deactivating framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543349 31860 hierarchical.cpp:378] Deactivated framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.543501 31856 master.cpp:1293] Giving framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 0ns to failover
I0405 17:29:35.543903 31868 master.cpp:5360] Framework failover timeout, removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543936 31868 master.cpp:6093] Removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.544337 31861 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by master@172.17.0.4:43972
I0405 17:29:35.544381 31861 slave.cpp:2240] Shutting down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.544456 31861 slave.cpp:4398] Shutting down executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:35.544960 31872 poll_socket.cpp:110] Socket error while connecting
I0405 17:29:35.545013 31872 process.cpp:1650] Failed to send 'mesos.internal.ShutdownExecutorMessage' to '172.17.0.4:50952', connect: Socket error while connecting
E0405 17:29:35.545106 31872 process.cpp:1958] Failed to shutdown socket with fd 27: Transport endpoint is not connected
I0405 17:29:35.545474 31864 hierarchical.cpp:329] Removed framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
../../src/tests/persistent_volume_tests.cpp:819: Failure
Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...
         Expected: to be called at least once
           Actual: never called - unsatisfied and active
I0405 17:29:35.558538 31858 containerizer.cpp:1432] Destroying container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
../../src/tests/cluster.cpp:453: Failure
Failed to wait 15secs for wait
I0405 17:29:50.565403 31870 slave.cpp:800] Agent terminating
I0405 17:29:50.565512 31870 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by @0.0.0.0:0
W0405 17:29:50.565544 31870 slave.cpp:2236] Ignoring shutdown framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 because it is terminating
I0405 17:29:50.574620 31866 master.cpp:1230] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) disconnected
I0405 17:29:50.574766 31866 master.cpp:2701] Disconnecting agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575003 31866 master.cpp:2720] Deactivating agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575294 31865 hierarchical.cpp:563] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 deactivated
I0405 17:29:50.605787 31837 master.cpp:1083] Master terminating
I0405 17:29:50.606533 31866 hierarchical.cpp:508] Removed agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
[  FAILED  ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0, where GetParam() = 0 (31491 ms)
{code}",1,3,MESOS-5128,3.0
Reset `LIBPROCESS_IP` in `network\cni` isolator.,"Currently the `LIBPROCESS_IP` environment variable was being set to
    the Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in
    its network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided
    by the CNI network.",1,3,MESOS-5127,1.0
pivot_root is not available on PowerPC,"When compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error ""pivot_root is not available""

The current code logic in src/linux/fs.cpp is:

{code}
#ifdef __NR_pivot_root
  int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());
#elif __x86_64__
  // A workaround for systems that have an old glib but have a new
  // kernel. The magic number '155' is the syscall number for
  // 'pivot_root' on the x86_64 architecture, see
  // arch/x86/syscalls/syscall_64.tbl
  int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());
#else
#error ""pivot_root is not available""
#endif
{code}

There is no old glib version and the new kernel version, it will never run code in *#ifdef __NR_pivot_root* condition, and when I build on Ubuntu 16.04(It has the latest linux kernel and glibc), it still can't step into the *#ifdef __NR_pivot_root* condition.

For powerpc case, I added another condition:

{code}
#elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__
  // A workaround for powerpc. The magic number '203' is the syscall
  // number for 'pivot_root' on the powerpc architecture, see
  // https://w3challs.com/syscalls/?arch=powerpc_64
  int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());
{code}",1,3,MESOS-5121,1.0
Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.
    
We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",1,2,MESOS-5115,2.0
`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,"If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:
0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff23280d8 in __GI_abort () at abort.c:89
#2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",
    file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,
    function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92
#3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,
    function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111
Python Exception <class 'IndexError'> list index out of range:
#5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331
#6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239
#7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071
#8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471
#9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130
#10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161
#11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82
#12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570
#13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218
#14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,
    __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295
#15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353
#16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731
#17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720
#18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115
#19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312
#21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) frame 4
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",1,2,MESOS-5113,1.0
Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,"There appears to be a discrepancy between clang and gcc, which allows
clang to accept `using` declarations of the form `using ns_name::name;`
that contain nested classes, structs, and enums after the `name` field
in the declaration (e.g. `using ns_name::name::enum;`).

The language for describing this functionality is ambiguous in the
C++11 specification as referenced here:
http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",1,2,MESOS-5082,1.0
Expose per-role dominant share,"A client's dominant share is crucial measure for how likely it is to receive offers in the future. We should expose it in a dedicated allocator metric.

As currently the {{HierarchicalAllocatorProcess}} does work with generic {{Sorters}} which have no notion of DRF share we need to decide whether and where we would need to limit generality in order to expose the innards of the currently used {{DRFSorter}}.
",1,1,MESOS-5058,2.0
Authorization Action enum does not support upgrades.,"We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",1,5,MESOS-5031,2.0
Copy provisioner cannot replace directory with symlink,"I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.

Error log with Glog_v=1:

{quote}
I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6'
E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt’ with non-directory
{quote}

Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).

I believe what happened is that we executed a script at build time, which contains equivalent of:
{quote}
rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt
{quote}
",1,13,MESOS-5028,3.0
MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.,"Observed on the Apache Jenkins.

{noformat}
[ RUN      ] MesosContainerizerProvisionerTest.ProvisionFailed
I0324 13:38:56.284261  2948 containerizer.cpp:666] Starting container 'test_container' for executor 'executor' of framework ''
I0324 13:38:56.285825  2939 containerizer.cpp:1421] Destroying container 'test_container'
I0324 13:38:56.285854  2939 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'test_container'
[       OK ] MesosContainerizerProvisionerTest.ProvisionFailed (7 ms)
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0324 13:38:56.291187  2944 containerizer.cpp:666] Starting container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' for executor 'executor' of framework ''
I0324 13:38:56.292157  2944 containerizer.cpp:1421] Destroying container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
I0324 13:38:56.292179  2944 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
F0324 13:38:56.292899  2944 containerizer.cpp:752] Check failed: containers_.contains(containerId)
*** Check failure stack trace: ***
    @     0x2ac9973d0ae4  google::LogMessage::Fail()
    @     0x2ac9973d0a30  google::LogMessage::SendToLog()
    @     0x2ac9973d0432  google::LogMessage::Flush()
    @     0x2ac9973d3346  google::LogMessageFatal::~LogMessageFatal()
    @     0x2ac996af897c  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x2ac996b1f18a  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x2ac996b479d9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2ac997334fef  std::function<>::operator()()
    @     0x2ac99731b1c7  process::ProcessBase::visit()
    @     0x2ac997321154  process::DispatchEvent::visit()
    @           0x9a699c  process::ProcessBase::serve()
    @     0x2ac9973173c0  process::ProcessManager::resume()
    @     0x2ac99731445a  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2ac997320916  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2ac9973208c6  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2ac997320858  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2ac9973207af  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2ac997320748  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2ac9989aea60  (unknown)
    @     0x2ac999125182  start_thread
    @     0x2ac99943547d  (unknown)
make[4]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[4]: *** [check-local] Aborted
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/mesos/mesos-0.29.0/_build'
make: *** [distcheck] Error 1
Build step 'Execute shell' marked build as failure
{noformat}",1,5,MESOS-5023,2.0
Add docker volume driver isolator for Mesos containerizer.,"The isolator will interact with Docker Volume Driver Plugins to mount and unmount external volumes to container.
",1,13,MESOS-5013,8.0
MasterTest.MasterLost is flaky,"The test {{MasterTest.MasterLost}} and {{ExceptionTest.DisallowSchedulerActionsOnAbort}} fail at least half the time under OS X (clang, not optimized, {{30efac7}}), e.g.,
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MasterTest
[ RUN      ] MasterTest.MasterLost
*** Aborted at 1458650698 (unix time) try ""date -d @1458650698"" if you are using GNU date ***
PC: @        0x109685fcc mesos::internal::state::State::store()
*** SIGSEGV (@0x0) received by PID 18620 (TID 0x111259000) stack trace: ***
    @     0x7fff850e1f1a _sigtramp
    @        0x108c74eaf boost::uuids::detail::sha1::process_byte_impl()
    @        0x1095fd723 mesos::internal::state::protobuf::State::store<>()
    @        0x1095fbd3e mesos::internal::master::RegistrarProcess::update()
    @        0x1095fcf6c mesos::internal::master::RegistrarProcess::_apply()
    @        0x1096797a0 _ZZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS_5OwnedINS3_9OperationEEES7_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSC_FSA_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESL_
    @        0x1096795f0 _ZNSt3__128__invoke_void_return_wrapperIvE6__callIJRZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS3_5OwnedINS7_9OperationEEESB_EENS3_6FutureIT_EERKNS3_3PIDIT0_EEMSG_FSE_T1_ET2_EUlPNS3_11ProcessBaseEE_SP_EEEvDpOT_
    @        0x1096792d9 _ZNSt3__110__function6__funcIZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS2_5OwnedINS6_9OperationEEESA_EENS2_6FutureIT_EERKNS2_3PIDIT0_EEMSF_FSD_T1_ET2_EUlPNS2_11ProcessBaseEE_NS_9allocatorISP_EEFvSO_EEclEOSO_
    @        0x10b2e9e4c std::__1::function<>::operator()()
    @        0x10b2e9d9c process::ProcessBase::visit()
    @        0x10b31d26e process::DispatchEvent::visit()
    @        0x108ad7d81 process::ProcessBase::serve()
    @        0x10b2e3cb4 process::ProcessManager::resume()
    @        0x10b36c479 process::ProcessManager::init_threads()::$_1::operator()()
    @        0x10b36c0a2 _ZNSt3__114__thread_proxyINS_5tupleIJNS_6__bindIZN7process14ProcessManager12init_threadsEvE3$_1JNS_17reference_wrapperIKNS_6atomicIbEEEEEEEEEEEEPvSD_
    @     0x7fff90eca05a _pthread_body
    @     0x7fff90ec9fd7 _pthread_start
    @     0x7fff90ec73ed thread_start
{code}

Sometimes also {{FaultToleranceTest.SchedulerFailover}} fails with the same stack trace.

I could trace this to the recent refactoring of the test helpers (MESOS-4633, MESOS-4634),
{code}
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
75ca1e6c9fde655c41fdf835aa20c47570d21f10
56e9406763e8514a7557ab3862d2f352a61425d5
b377557c2bfc35c894e87becb47122955540f133
7bf6e4f70131175edd4d6d77ea0dc7692b3e72ae
c7df1d7bcb1604c95800871cc0473c946e5b5d16
951539317525f3afe9490ed098617e5d4563a80a
We cannot bisect more!
{code}

It appears the lifetimes of some objects are still not ordered correctly.
",1,4,MESOS-5000,3.0
Destroy a container while it's provisioning can lead to leaked provisioned directories.,"Here is the possible sequence of events:
1) containerizer->launch
2) provisioner->provision is called. it is fetching the image
3) executor registration timed out
4) containerizer->destroy is called
5) container->state is still in PREPARING
6) provisioner->destroy is called

So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",1,2,MESOS-4985,3.0
MasterTest.SlavesEndpointTwoSlaves is flaky,"Observed on Arch Linux with GCC 6, running in a virtualbox VM:

[ RUN      ] MasterTest.SlavesEndpointTwoSlaves
/mesos-2/src/tests/master_tests.cpp:1710: Failure
Value of: array.get().values.size()
  Actual: 1
Expected: 2u
Which is: 2
[  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)

Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",1,3,MESOS-4984,2.0
Update mesos-execute with Appc changes.,mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,1,3,MESOS-4978,3.0
ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.

Verbose logs:
{code}
[ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox
I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer
I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms
I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms
I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns
I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns
I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery
I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status
I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919
I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING
I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns
I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING
I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919
I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status
I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""
I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register
I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register
I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'
I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator
I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919
I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator
I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled
I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status
I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given
I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process
I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING
I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns
I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING
I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group
I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated
I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83
I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!
I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar
I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar
I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer
I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1
I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns
I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1
I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2
I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns
I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919
I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns
I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns
I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns
I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0
I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0
I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns
I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms
I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'
I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log
I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919
I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns
I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns
I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1
I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms
I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar
I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1
I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919
I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns
I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2
I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns
I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us
I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2
I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919
I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""
I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'
I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal
I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]
I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5
I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'
I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager
I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer
I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete
I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery
I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919
I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates
I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master
I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919
I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success
I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success
I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919
I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary
I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'
I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log
I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary
I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919
I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns
I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary
I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns
I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3
I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms
I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3
I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919
I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919
I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache
I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns
I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates
I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns
I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'
I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4
I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources 
I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources 
I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0
I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns
I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns
I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919
I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4
I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns
I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0316 14:28:51.431711  1273 sched.cpp:382] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.431737  1273 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0316 14:28:51.431982  1266 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.432369  1261 master.cpp:5659] Authenticating scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.432509  1263 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.432868  1267 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.433135  1276 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.433233  1276 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.433423  1276 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.433502  1276 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.433606  1274 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.433744  1273 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.433785  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.433801  1273 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.433861  1273 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.433897  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.433912  1273 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433924  1273 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433944  1273 authenticator.cpp:317] Authentication success
I0316 14:28:51.434037  1274 authenticatee.cpp:298] Authentication success
I0316 14:28:51.434108  1268 master.cpp:5689] Successfully authenticated principal 'test-principal' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434211  1272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.434512  1274 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.434535  1274 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.3:45919
I0316 14:28:51.434648  1274 sched.cpp:809] Will retry registration in 356.547014ms if necessary
I0316 14:28:51.434819  1266 master.cpp:2326] Received SUBSCRIBE call for framework 'default' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434905  1266 master.cpp:1845] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0316 14:28:51.435464  1265 master.cpp:2397] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0316 14:28:51.435979  1269 hierarchical.cpp:265] Added framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436213  1272 sched.cpp:703] Framework registered with c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436316  1272 sched.cpp:717] Scheduler::registered took 73782ns
I0316 14:28:51.436928  1269 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:51.436978  1269 hierarchical.cpp:1130] Performed allocation for 1 slaves in 970638ns
I0316 14:28:51.437278  1272 master.cpp:5488] Sending 1 offers to framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.437782  1262 sched.cpp:873] Scheduler::resourceOffers took 129952ns
I0316 14:28:51.440006  1274 master.cpp:3268] Processing ACCEPT call for offers: [ c7653f60-33e9-4406-9f62-dc74c906bf83-O0 ] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.440094  1274 master.cpp:2871] Authorizing framework principal 'test-principal' to launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 as user 'mesos'
I0316 14:28:51.442152  1274 master.hpp:177] Adding task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5)
I0316 14:28:51.442348  1274 master.cpp:3753] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.442749  1265 slave.cpp:1361] Got assigned task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443006  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.443624  1265 slave.cpp:1480] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443730  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.444629  1265 paths.cpp:528] Trying to chown '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' to user 'mesos'
I0316 14:28:51.449493  1265 slave.cpp:5367] Launching executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.450256  1261 containerizer.cpp:666] Starting container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:51.450299  1265 slave.cpp:1698] Queuing task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.450428  1265 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.459421  1268 launcher.cpp:147] Forked child with pid '1453' for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.613296  1274 slave.cpp:2643] Got registration for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.615416  1271 slave.cpp:1863] Sending queued task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' to executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:51.622187  1272 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.623610  1275 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.623646  1275 status_update_manager.cpp:497] Creating StatusUpdate stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624053  1275 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:51.624423  1274 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:51.624621  1274 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624677  1274 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:51.624836  1270 master.cpp:4927] Status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.624881  1270 master.cpp:4975] Forwarding status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.625077  1270 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0316 14:28:51.625355  1269 sched.cpp:981] Scheduler::statusUpdate took 141149ns
I0316 14:28:51.625671  1266 master.cpp:4082] Processing ACKNOWLEDGE call aee0de1c-8acd-46eb-8723-d26cd203228f for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.625977  1267 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.626369  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:52.340801  1266 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:52.340884  1266 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:52.340922  1266 hierarchical.cpp:1130] Performed allocation for 1 slaves in 350313ns
I0316 14:28:53.342003  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:53.342077  1263 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:53.342110  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 332715ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.619144  1451 process.cpp:986] libprocess is initialized on 172.17.0.3:40885 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.790701  1452 process.cpp:986] libprocess is initialized on 172.17.0.3:50144 for 16 cpus
I0316 14:28:53.939643  1268 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:53.940950  1267 slave.cpp:5677] Terminating task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:53.942181  1275 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942358  1275 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:53.942715  1265 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:53.942919  1265 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942961  1265 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:53.943159  1273 master.cpp:4927] Status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.943218  1273 master.cpp:4975] Forwarding status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.943392  1273 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0316 14:28:53.944248  1275 sched.cpp:981] Scheduler::statusUpdate took 172957ns
I0316 14:28:53.944351  1262 hierarchical.cpp:890] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 from framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.944548  1242 sched.cpp:1903] Asked to stop the driver
I0316 14:28:53.944672  1275 sched.cpp:1143] Stopping framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:53.944736  1263 master.cpp:4082] Processing ACKNOWLEDGE call a873c6e2-442e-439e-a13f-54bb19df1881 for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:53.944795  1263 master.cpp:6654] Removing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.945226  1263 master.cpp:6061] Processing TEARDOWN call for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945253  1263 master.cpp:6073] Removing framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945324  1275 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945412  1274 hierarchical.cpp:375] Deactivated framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945462  1276 slave.cpp:2079] Asked to shut down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 by master@172.17.0.3:45919
I0316 14:28:53.945579  1276 slave.cpp:2104] Shutting down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945669  1276 slave.cpp:4198] Shutting down executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:53.945714  1275 status_update_manager.cpp:528] Cleaning up status update stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945818  1274 hierarchical.cpp:326] Removed framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946151  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946213  1265 slave.cpp:5718] Completing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:54.343000  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:54.343056  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 213036ns
I0316 14:28:54.943627  1261 slave.cpp:3528] executor(1)@172.17.0.3:56062 exited
I0316 14:28:54.944002  1274 containerizer.cpp:1608] Executor for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' has exited
I0316 14:28:54.944205  1274 containerizer.cpp:1392] Destroying container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:54.949076  1276 provisioner.cpp:306] Ignoring destroy request for unknown container 6e2770ca-32d3-47ad-b4fe-7d9f26489621
I0316 14:28:54.949502  1276 slave.cpp:3886] Executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 exited with status 0
I0316 14:28:54.949556  1276 slave.cpp:3990] Cleaning up executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:54.949807  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' for gc 6.99998900785778days in the future
I0316 14:28:54.949931  1276 slave.cpp:4078] Cleaning up framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950188  1276 status_update_manager.cpp:282] Closing status update streams for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950196  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917' for gc 6.99998900606519days in the future
I0316 14:28:54.950458  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000' for gc 6.99998900418963days in the future
../../src/tests/container_logger_tests.cpp:461: Failure
Value of: waitpid(pstree.process.pid, __null, 0)
  Actual: -1
Expected: pstree.process.pid
Which is: 1453
I0316 14:28:54.952739  1264 slave.cpp:668] Slave terminating
I0316 14:28:54.952980  1275 master.cpp:1212] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) disconnected
I0316 14:28:54.953069  1275 master.cpp:2681] Disconnecting slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953172  1275 master.cpp:2700] Deactivating slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953404  1269 hierarchical.cpp:560] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 deactivated
I0316 14:28:54.957495  1274 master.cpp:1065] Master terminating
I0316 14:28:54.958026  1276 hierarchical.cpp:505] Removed slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
[  FAILED  ] ContainerLoggerTest.LOGROTATE_RotateInSandbox (3635 ms)
{code}",1,2,MESOS-4961,1.0
Implement reconnect funtionality in the scheduler library.,"Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",1,2,MESOS-4950,3.0
Docker runtime isolator tests may cause disk issue.,"Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",1,2,MESOS-4942,2.0
"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.","The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin.
We should consider the following cases:
1) container is using host filesystem
2) container is using a different filesystem
3) custom executor and command executor",1,5,MESOS-4922,5.0
LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.,"Observed on our CI:
{noformat}
[09:34:15] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework ''
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework ''
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:45] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure
[09:34:45] :	 [Step 11/11] Failed to wait 15secs for wait1
[09:34:48] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms)
{noformat}",1,2,MESOS-4912,3.0
Executor driver does not respect executor shutdown grace period.,"Executor shutdown grace period, configured on the agent, is
propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`
environment variable. The executor driver must use this timeout to delay
the hard shutdown of the related executor.",1,2,MESOS-4911,1.0
Allow multiple loads of module manifests,"The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",1,3,MESOS-4903,3.0
Default cmd is executed as an incorrect command.,"When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:

If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.

This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",1,2,MESOS-4888,2.0
"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")","This can be demonstrated with the {{mesos-execute}} command:

# Docker containerizer with image {{alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{alpine}}: failure
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{library/alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}

In the slave logs:

{code}
ea-4460-83
9c-838da86af34c-0007'
I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest'
I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test
/store/docker/staging/ka7MlQ'
E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0
007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized
{code}

curl command executed:

{code}
$ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest
16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull
16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest
{code}

Also got the same result with {{ubuntu}} docker image.",1,11,MESOS-4877,3.0
Fix rmdir for windows,This is due to a bug in MESOS-4415 that landed for 0.27.0.,1,2,MESOS-4836,1.0
CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky,"Verbose logs: 
{code}
[ RUN      ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess
I0302 00:43:14.127846 11755 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.267411 11758 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test after 139.46496ms
I0302 00:43:14.409395 11751 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.551304 11751 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos_test after 141.811968ms
../../src/tests/containerizer/cgroups_tests.cpp:949: Failure
Value of: ::waitpid(pid, &status, 0)
  Actual: 23809
Expected: -1
../../src/tests/containerizer/cgroups_tests.cpp:950: Failure
Value of: (*__errno_location ())
  Actual: 0
Expected: 10
[  FAILED  ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess (1055 ms)
{code}",0,1,MESOS-4835,2.0
Bind docker runtime isolator with docker image provider.,"If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.",1,2,MESOS-4830,1.0
"""filesystem/linux"" isolator does not unmount orphaned persistent volumes","A persistent volume can be orphaned when:
# A framework registers with checkpointing enabled.
# The framework starts a task + a persistent volume.
# The agent exits.  The task continues to run.
# Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.
# The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.

The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}}) 
{code}
I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97
{code}

Test implemented here: https://reviews.apache.org/r/44122/",1,2,MESOS-4824,2.0
Mesos fails to escape command health checks,"As described in https://github.com/mesosphere/marathon/issues/3333
I would like to run a command health check
{noformat}
/bin/bash -c ""</dev/tcp/$HOST/$PORT0""
{noformat}

The health check fails because Mesos, while running the command inside double quotes of a sh -c """" doesn't escape the double quotes in the command.

If I escape the double quotes myself the command health check succeeds. But this would mean that the user needs intimate knowledge of how Mesos executes his commands which can't be right.

I was told this is not a Marathon but a Mesos issue so am opening this JIRA. I don't know if this only affects the command health check.
",1,18,MESOS-4812,5.0
IOTest.BufferedRead writes to the current directory,"libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing the same test in parallel would race on the existence of the created file, and show bogus behavior.

The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",1,2,MESOS-4807,1.0
LevelDBStateTests write to the current directory,"All {{LevelDBStateTest}} tests write to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing tests from the same suite in parallel (e.g., with {{gtest-parallel}} would race on the existence of the created files, and show bogus behavior.

The tests should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",1,2,MESOS-4806,2.0
Executor env variables should not be leaked to the command task.,"Currently, command task inherits the env variables of the command executor. This is less ideal because the command executor environment variables include some Mesos internal env variables like MESOS_XXX and LIBPROCESS_XXX. Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",0,7,MESOS-4781,3.0
MasterMaintenanceTest.InverseOffers is flaky,"[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5
I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms
I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5
I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns
I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns
I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678
I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns
I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6
I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns
I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns
I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6
I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status'
I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096
I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0
I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678
I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678
I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097
I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default'
I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms
I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678
I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678
I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event
I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678
I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098
I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos'
W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host)
I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos'
I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0
I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948
I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns
I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6'
I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns
I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678
I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678
I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678
I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099
I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678
I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns
I0224 22:35:53.878082  1981 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.878675  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45100
I0224 22:35:53.878931  1978 master.cpp:3675] Processing DECLINE call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O1 ] for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
../../src/tests/master_maintenance_tests.cpp:1222: Failure
Failed to wait 15secs for event
I0224 22:36:08.881649  1948 master.cpp:1027] Master terminating
W0224 22:36:08.881925  1948 master.cpp:6502] Removing task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) in non-terminal state TASK_RUNNING
I0224 22:36:08.882961  1948 master.cpp:6545] Removing executor 'default' with resources  of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:36:08.884789  1969 hierarchical.cpp:505] Removed slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:36:08.887261  1969 hierarchical.cpp:326] Removed framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.916983  1976 slave.cpp:3528] master@172.17.0.1:36678 exited
W0224 22:36:08.917191  1976 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0224 22:36:08.934546  1975 slave.cpp:3528] executor(47)@172.17.0.1:36678 exited
I0224 22:36:08.934806  1974 slave.cpp:3886] Executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exited with status 0
I0224 22:36:08.935024  1974 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from @0.0.0.0:0
I0224 22:36:08.935505  1974 slave.cpp:5677] Terminating task 90bcae0c-9d40-40b7-9537-dae7e83479f6
I0224 22:36:08.936190  1967 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.936368  1967 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:36:08.936606  1974 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:36:08.936779  1974 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955370  1967 slave.cpp:668] Slave terminating
I0224 22:36:08.955499  1967 slave.cpp:2079] Asked to shut down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 by @0.0.0.0:0
I0224 22:36:08.955538  1967 slave.cpp:2104] Shutting down framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955606  1967 slave.cpp:3990] Cleaning up executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:36:08.956053  1967 slave.cpp:4078] Cleaning up framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956327  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956495  1973 status_update_manager.cpp:282] Closing status update streams for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956524  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956549  1973 status_update_manager.cpp:528] Cleaning up status update stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956619  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000' for gc 1.00002336880296weeks in the future
[  FAILED  ] MasterMaintenanceTest.InverseOffers (15258 ms)
{code}",1,2,MESOS-4768,1.0
Mesos containerizer should get uid/gids before pivot_root.,"Currently, we call os::su(user) after pivot_root. This is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. We should instead, get the uid/gids before pivot_root, and call setuid/setgroups after pivot_root.",1,15,MESOS-4757,3.0
"The ""executors"" field is exposed under a backwards incompatible schema.","In 0.26.0, the master's {{/state}} endpoint generated the following:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""argv"": [],
            ""uris"": [],
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": ""default"",
          ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",
          ""name"": ""Long Lived Executor (C++)"",
          ""resources"": {
            ""cpus"": 0,
            ""disk"": 0,
            ""mem"": 0
          },
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""shell"": true,
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": {
            ""value"": ""default""
          },
          ""framework_id"": {
            ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""
          },
          ""name"": ""Long Lived Executor (C++)"",
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",
          ""source"": ""cpp_long_lived_framework""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

This is a backwards incompatible API change.",1,4,MESOS-4754,2.0
ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation,"Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from ContainerLoggerTest
[ RUN      ] ContainerLoggerTest.MesosContainerizerRecover
[       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms)
[----------] 1 test from ContainerLoggerTest (13 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:728: Failure
Failed
Tests completed with child processes remaining:
-+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover
 \--- 7130 (sh)
[==========] 1 test from 1 test case ran. (23 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}

Observered on OS X with clang-trunk and an unoptimized build.
",1,3,MESOS-4747,1.0
DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6,"This test passes consistently on other OS's, but fails consistently on CentOS 6.

Verbose logs from test failure:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes
I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms
I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms
I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns
I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns
I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns
I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery
I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status
I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274
I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING
I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274
I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""
I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register
I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'
I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator
I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled
I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms
I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given
I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING
I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process
I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status
I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274
I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING
I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70
I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!
I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar
I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar
I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms
I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING
I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group
I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated
I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer
I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1
I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms
I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1
I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2
I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms
I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0
I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274
I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns
I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms
I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0
I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms
I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0
I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0
I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0
I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns
I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms
I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'
I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log
I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274
I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms
I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1
I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms
I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1
I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1
I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms
I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar
I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1
I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274
I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms
I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2
I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms
I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns
I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2
I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274
I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""
I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'
I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal
I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]
I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io
I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'
I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager
I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0
I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers
I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery
I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274
I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates
I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274
I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master
I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274
I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success
I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success
I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274
I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary
I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274
I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary
I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'
I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns
I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns
I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log
I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274
I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms
I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3
I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms
I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3
I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3
I0222 18:16:12.390281 26698 registrar.cpp:484] Successfully updated the 'registry' in 7.619072ms
I0222 18:16:12.390444 26702 log.cpp:702] Attempting to truncate the log to 3
I0222 18:16:12.390569 26701 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 18:16:12.390904 26701 slave.cpp:3482] Received ping from slave-observer(364)@172.30.2.148:35274
I0222 18:16:12.391054 26700 master.cpp:4308] Registered slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.391144 26703 slave.cpp:971] Registered with master master@172.30.2.148:35274; given slave ID 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.391168 26703 fetcher.cpp:81] Clearing fetcher cache
I0222 18:16:12.391238 26700 replica.cpp:537] Replica received write request for position 4 from (13776)@172.30.2.148:35274
I0222 18:16:12.391263 26701 status_update_manager.cpp:181] Resuming sending status updates
I0222 18:16:12.391304 26697 hierarchical.cpp:473] Added slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0222 18:16:12.391388 26703 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/slave.info'
I0222 18:16:12.391636 26703 slave.cpp:1030] Forwarding total oversubscribed resources 
I0222 18:16:12.391772 26699 master.cpp:4649] Received update of slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with total oversubscribed resources 
I0222 18:16:12.392011 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392053 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 708377ns
I0222 18:16:12.392307 26703 master.cpp:5355] Sending 1 offers to framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.392374 26697 hierarchical.cpp:531] Slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0222 18:16:12.392500 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.392531 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392556 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 136779ns
I0222 18:16:12.392704 26701 sched.cpp:873] Scheduler::resourceOffers took 94330ns
I0222 18:16:12.393086 26681 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0222 18:16:12.393600 26700 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.326382ms
I0222 18:16:12.393625 26700 replica.cpp:712] Persisted action at 4
I0222 18:16:12.394162 26696 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 18:16:12.394533 26701 master.cpp:3138] Processing ACCEPT call for offers: [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-O0 ] on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.394567 26701 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0222 18:16:12.394628 26701 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0222 18:16:12.395519 26701 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.395808 26701 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396316 26696 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.130659ms
I0222 18:16:12.396317 26703 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0222 18:16:12.396368 26696 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30004ns
I0222 18:16:12.396381 26696 replica.cpp:712] Persisted action at 4
I0222 18:16:12.396397 26696 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 18:16:12.396533 26701 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396680 26701 master.cpp:3623] Launching task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.397009 26696 slave.cpp:1361] Got assigned task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397143 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.397306 26699 hierarchical.cpp:653] Updated allocation of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0222 18:16:12.397625 26699 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397857 26696 slave.cpp:1480] Launching task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397943 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.398560 26696 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' to user 'root'
I0222 18:16:12.403491 26696 slave.cpp:5367] Launching executor 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.404115 26696 slave.cpp:1698] Queuing task '1' for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.405709 26696 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.408308 26697 docker.cpp:1019] Starting container '207172a3-0ebd-4faa-946b-75a829fc75fc' for task '1' (and executor '1') of framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:12.408592 26697 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0222 18:16:12.520663 26702 docker.cpp:390] Docker pull alpine completed
I0222 18:16:12.520853 26702 docker.cpp:479] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' with uid 0 and gid 0
I0222 18:16:12.524782 26702 docker.cpp:500] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1' for persistent volume disk(role1)[id1:path1]:64 of container 207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:12.580834 26700 slave.cpp:2643] Got registration for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:12.581961 26699 docker.cpp:1299] Ignoring updating container '207172a3-0ebd-4faa-946b-75a829fc75fc' with resources passed to update is identical to existing resources
I0222 18:16:12.582307 26698 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.295573 26703 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.295940 26703 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.296381 26701 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26701 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26703 slave.cpp:5677] Terminating task 1
I0222 18:16:13.296839 26701 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.296902 26702 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:13.299427 26699 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.299921 26699 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.299969 26699 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.300130 26696 master.cpp:4794] Status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.300176 26696 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.300375 26696 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_RUNNING)
I0222 18:16:13.300765 26703 sched.cpp:981] Scheduler::statusUpdate took 164263ns
I0222 18:16:13.300962 26700 hierarchical.cpp:892] Recovered cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: ) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301178 26699 master.cpp:3952] Processing ACKNOWLEDGE call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.301450 26699 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301697 26701 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327133 26697 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327280 26697 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.327481 26696 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.327621 26696 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327679 26696 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.327800 26698 master.cpp:4794] Status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.327850 26698 master.cpp:4842] Forwarding status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327977 26698 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0222 18:16:13.328248 26699 sched.cpp:981] Scheduler::statusUpdate took 100279ns
I0222 18:16:13.328588 26700 master.cpp:3952] Processing ACKNOWLEDGE call ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.328662 26681 sched.cpp:1903] Asked to stop the driver
I0222 18:16:13.328630 26700 master.cpp:6516] Removing task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.328747 26697 sched.cpp:1143] Stopping framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:13.329064 26696 status_update_manager.cpp:392] Received status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329069 26700 master.cpp:5926] Processing TEARDOWN call for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329100 26700 master.cpp:5938] Removing framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329200 26696 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329218 26703 hierarchical.cpp:375] Deactivated framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329309 26697 slave.cpp:2079] Asked to shut down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 by master@172.30.2.148:35274
I0222 18:16:13.329346 26697 slave.cpp:2104] Shutting down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329418 26697 slave.cpp:4198] Shutting down executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.329578 26699 hierarchical.cpp:326] Removed framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329684 26697 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329733 26697 slave.cpp:5718] Completing task 1
I0222 18:16:13.337236 26703 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:13.337266 26703 hierarchical.cpp:1127] Performed allocation for 1 slaves in 153077ns
I0222 18:16:14.297827 26702 slave.cpp:3528] executor(1)@172.30.2.148:56026 exited
I0222 18:16:14.332489 26697 docker.cpp:1915] Executor for container '207172a3-0ebd-4faa-946b-75a829fc75fc' has exited
I0222 18:16:14.332512 26697 docker.cpp:1679] Destroying container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.332600 26697 docker.cpp:1807] Running docker stop on container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333111 26697 docker.cpp:908] Unmounting volume for container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333288 26700 slave.cpp:3886] Executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 exited with status 0
I0222 18:16:14.333340 26700 slave.cpp:3990] Cleaning up executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:14.333603 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' for gc 6.99999614056593days in the future
I0222 18:16:14.333669 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.333704 26700 slave.cpp:4078] Cleaning up framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.333726 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1' for gc 6.99999613825185days in the future
I0222 18:16:14.336545 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' for gc 6.9999961115763days in the future
I0222 18:16:14.336699 26701 status_update_manager.cpp:282] Closing status update streams for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.338240 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:14.338270 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 191822ns
I0222 18:16:14.635416 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.940042 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.245256 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.339015 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:15.339053 26697 hierarchical.cpp:1127] Performed allocation for 1 slaves in 265093ns
I0222 18:16:15.549804 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.854646 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.159210 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.339910 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:16.339951 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 255857ns
I0222 18:16:16.463809 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.768708 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.073479 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.340798 26696 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:17.340864 26696 hierarchical.cpp:1127] Performed allocation for 1 slaves in 260467ns
I0222 18:16:17.377902 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.683398 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.988231 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.292505 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.330112 26700 slave.cpp:4231] Framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 seems to have exited. Ignoring shutdown timeout for executor '1'
I0222 18:16:18.341600 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:18.341634 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 252012ns
I0222 18:16:18.596279 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.901157 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.204834 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.342326 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:19.342358 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 186829ns
I0222 18:16:19.508533 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.812255 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.116345 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.343556 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:20.343588 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 194704ns
I0222 18:16:20.420814 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.724819 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.029549 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.334319 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.344859 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:21.344892 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 241099ns
I0222 18:16:21.638164 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
../../src/tests/containerizer/docker_containerizer_tests.cpp:1434: Failure
os::read(path::join(volumePath, ""file"")): Failed to open file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1/file': No such file or directory
I0222 18:16:21.943008 26703 master.cpp:1027] Master terminating
I0222 18:16:21.943635 26696 hierarchical.cpp:505] Removed slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:21.943989 26702 slave.cpp:3528] master@172.30.2.148:35274 exited
W0222 18:16:21.944016 26702 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0222 18:16:21.948807 26699 slave.cpp:668] Slave terminating
I0222 18:16:21.951902 26681 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
I0222 18:16:22.044273 26698 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:22.148877 26681 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v 422bfef31d51d2d3d2aafcf49b3e502654354bd98a98b076f4089b9a8e274d05
[  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes (10535 ms)
{code}",1,3,MESOS-4736,2.0
"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.

We should make a StoutConfigure.cmake that can be included by any package downstream.",1,2,MESOS-4703,1.0
SlaveTest.StateEndpoint is flaky,"{code}
[ RUN      ] SlaveTest.StateEndpoint
../../src/tests/slave_tests.cpp:1220: Failure
Value of: state.values[""start_time""].as<JSON::Number>().as<int>()
  Actual: 1458159086
Expected: static_cast<int>(Clock::now().secs())
  Which is: 1458159085
[  FAILED  ] SlaveTest.StateEndpoint (193 ms)
{code}

Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int.",1,2,MESOS-4695,1.0
Create base docker image for test suite.,"This should be widely used for unified containerizer testing. Should basically include:

*at least one layer.
*repositories.

For each layer:
*root file system as a layer tar ball.
*docker image json (manifest).
*docker version.",1,2,MESOS-4684,3.0
Document docker runtime isolator.,"Should include the following information:

*What features are currently supported in docker runtime isolator.
*How to use the docker runtime isolator (user manual).
*Compare the different semantics v.s. docker containerizer, and explain why.",1,1,MESOS-4683,2.0
ROOT_DOCKER_Logs is flaky.,"{noformat}
[18:06:25][Step 8/8] [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs
[18:06:25][Step 8/8] I0215 17:06:25.256103  1740 leveldb.cpp:174] Opened db in 6.548327ms
[18:06:25][Step 8/8] I0215 17:06:25.258002  1740 leveldb.cpp:181] Compacted db in 1.837816ms
[18:06:25][Step 8/8] I0215 17:06:25.258059  1740 leveldb.cpp:196] Created db iterator in 22044ns
[18:06:25][Step 8/8] I0215 17:06:25.258076  1740 leveldb.cpp:202] Seeked to beginning of db in 2347ns
[18:06:25][Step 8/8] I0215 17:06:25.258091  1740 leveldb.cpp:271] Iterated through 0 keys in the db in 571ns
[18:06:25][Step 8/8] I0215 17:06:25.258152  1740 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[18:06:25][Step 8/8] I0215 17:06:25.258936  1758 recover.cpp:447] Starting replica recovery
[18:06:25][Step 8/8] I0215 17:06:25.259177  1758 recover.cpp:473] Replica is in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.260327  1757 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13608)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.260545  1758 recover.cpp:193] Received a recover response from a replica in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.261065  1757 master.cpp:376] Master 112363e2-c680-4946-8fee-d0626ed8b21e (ip-172-30-2-239.mesosphere.io) started on 172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.261209  1761 recover.cpp:564] Updating replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.261086  1757 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/HncLLj/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/HncLLj/master"" --zk_session_timeout=""10secs""
[18:06:25][Step 8/8] I0215 17:06:25.261446  1757 master.cpp:423] Master only allowing authenticated frameworks to register
[18:06:25][Step 8/8] I0215 17:06:25.261456  1757 master.cpp:428] Master only allowing authenticated slaves to register
[18:06:25][Step 8/8] I0215 17:06:25.261462  1757 credentials.hpp:35] Loading credentials for authentication from '/tmp/HncLLj/credentials'
[18:06:25][Step 8/8] I0215 17:06:25.261723  1757 master.cpp:468] Using default 'crammd5' authenticator
[18:06:25][Step 8/8] I0215 17:06:25.261855  1757 master.cpp:537] Using default 'basic' HTTP authenticator
[18:06:25][Step 8/8] I0215 17:06:25.262022  1757 master.cpp:571] Authorization enabled
[18:06:25][Step 8/8] I0215 17:06:25.262177  1755 hierarchical.cpp:144] Initialized hierarchical allocator process
[18:06:25][Step 8/8] I0215 17:06:25.262177  1758 whitelist_watcher.cpp:77] No whitelist given
[18:06:25][Step 8/8] I0215 17:06:25.262899  1760 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.517992ms
[18:06:25][Step 8/8] I0215 17:06:25.262924  1760 replica.cpp:320] Persisted replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.263144  1754 recover.cpp:473] Replica is in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.264010  1757 master.cpp:1712] The newly elected leader is master@172.30.2.239:39785 with id 112363e2-c680-4946-8fee-d0626ed8b21e
[18:06:25][Step 8/8] I0215 17:06:25.264044  1757 master.cpp:1725] Elected as the leading master!
[18:06:25][Step 8/8] I0215 17:06:25.264061  1757 master.cpp:1470] Recovering from registrar
[18:06:25][Step 8/8] I0215 17:06:25.264117  1760 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13610)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.264197  1758 registrar.cpp:307] Recovering registrar
[18:06:25][Step 8/8] I0215 17:06:25.264827  1756 recover.cpp:193] Received a recover response from a replica in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.265219  1757 recover.cpp:564] Updating replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267302  1754 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.887739ms
[18:06:25][Step 8/8] I0215 17:06:25.267326  1754 replica.cpp:320] Persisted replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267453  1759 recover.cpp:578] Successfully joined the Paxos group
[18:06:25][Step 8/8] I0215 17:06:25.267632  1759 recover.cpp:462] Recover process terminated
[18:06:25][Step 8/8] I0215 17:06:25.268007  1757 log.cpp:659] Attempting to start the writer
[18:06:25][Step 8/8] I0215 17:06:25.269055  1759 replica.cpp:493] Replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1
[18:06:25][Step 8/8] I0215 17:06:25.270488  1759 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.406068ms
[18:06:25][Step 8/8] I0215 17:06:25.270511  1759 replica.cpp:342] Persisted promised to 1
[18:06:25][Step 8/8] I0215 17:06:25.271078  1761 coordinator.cpp:238] Coordinator attempting to fill missing positions
[18:06:25][Step 8/8] I0215 17:06:25.272146  1756 replica.cpp:388] Replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2
[18:06:25][Step 8/8] I0215 17:06:25.273478  1756 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.297217ms
[18:06:25][Step 8/8] I0215 17:06:25.273500  1756 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.274355  1757 replica.cpp:537] Replica received write request for position 0 from (13613)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.274405  1757 leveldb.cpp:436] Reading position from leveldb took 25294ns
[18:06:25][Step 8/8] I0215 17:06:25.275800  1757 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.362978ms
[18:06:25][Step 8/8] I0215 17:06:25.275823  1757 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.276348  1755 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.277765  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.391531ms
[18:06:25][Step 8/8] I0215 17:06:25.277788  1755 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.277802  1755 replica.cpp:697] Replica learned NOP action at position 0
[18:06:25][Step 8/8] I0215 17:06:25.278336  1754 log.cpp:675] Writer started with ending position 0
[18:06:25][Step 8/8] I0215 17:06:25.279371  1755 leveldb.cpp:436] Reading position from leveldb took 29214ns
[18:06:25][Step 8/8] I0215 17:06:25.280272  1758 registrar.cpp:340] Successfully fetched the registry (0B) in 16.02688ms
[18:06:25][Step 8/8] I0215 17:06:25.280385  1758 registrar.cpp:439] Applied 1 operations in 31040ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.281054  1755 log.cpp:683] Attempting to append 210 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.281165  1757 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.281780  1757 replica.cpp:537] Replica received write request for position 1 from (13614)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.283159  1757 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.348041ms
[18:06:25][Step 8/8] I0215 17:06:25.283184  1757 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.283695  1759 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.285059  1759 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.334577ms
[18:06:25][Step 8/8] I0215 17:06:25.285084  1759 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.285099  1759 replica.cpp:697] Replica learned APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.285910  1758 registrar.cpp:484] Successfully updated the 'registry' in 5.46816ms
[18:06:25][Step 8/8] I0215 17:06:25.286043  1758 registrar.cpp:370] Successfully recovered registrar
[18:06:25][Step 8/8] I0215 17:06:25.286121  1755 log.cpp:702] Attempting to truncate the log to 1
[18:06:25][Step 8/8] I0215 17:06:25.286301  1756 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.286478  1759 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
[18:06:25][Step 8/8] I0215 17:06:25.286476  1754 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
[18:06:25][Step 8/8] I0215 17:06:25.287137  1755 replica.cpp:537] Replica received write request for position 2 from (13615)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.289104  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.938609ms
[18:06:25][Step 8/8] I0215 17:06:25.289127  1755 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.289667  1759 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.290956  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.256421ms
[18:06:25][Step 8/8] I0215 17:06:25.291007  1759 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28064ns
[18:06:25][Step 8/8] I0215 17:06:25.291021  1759 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.291038  1759 replica.cpp:697] Replica learned TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.300550  1760 slave.cpp:193] Slave started on 393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.300573  1760 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N""
[18:06:25][Step 8/8] I0215 17:06:25.300868  1760 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential'
[18:06:25][Step 8/8] I0215 17:06:25.301030  1760 slave.cpp:324] Slave using credential for: test-principal
[18:06:25][Step 8/8] I0215 17:06:25.301180  1760 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.301553  1760 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.301609  1760 slave.cpp:472] Slave attributes: [  ]
[18:06:25][Step 8/8] I0215 17:06:25.301620  1760 slave.cpp:477] Slave hostname: ip-172-30-2-239.mesosphere.io
[18:06:25][Step 8/8] I0215 17:06:25.302417  1757 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta'
[18:06:25][Step 8/8] I0215 17:06:25.302515  1740 sched.cpp:222] Version: 0.28.0
[18:06:25][Step 8/8] I0215 17:06:25.302772  1755 status_update_manager.cpp:200] Recovering status update manager
[18:06:25][Step 8/8] I0215 17:06:25.302956  1758 docker.cpp:559] Recovering Docker containers
[18:06:25][Step 8/8] I0215 17:06:25.303050  1761 sched.cpp:326] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303133  1754 slave.cpp:4565] Finished recovery
[18:06:25][Step 8/8] I0215 17:06:25.303154  1761 sched.cpp:382] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303169  1761 sched.cpp:389] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303364  1759 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303467  1754 slave.cpp:4737] Querying resource estimator for oversubscribable resources
[18:06:25][Step 8/8] I0215 17:06:25.303668  1756 master.cpp:5523] Authenticating scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303707  1760 status_update_manager.cpp:174] Pausing sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.303707  1754 slave.cpp:796] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303767  1755 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303791  1754 slave.cpp:859] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303805  1754 slave.cpp:864] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303956  1754 slave.cpp:832] Detecting new master
[18:06:25][Step 8/8] I0215 17:06:25.303971  1761 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303984  1760 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304131  1754 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
[18:06:25][Step 8/8] I0215 17:06:25.304275  1757 master.cpp:5523] Authenticating slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304344  1754 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304369  1754 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304373  1761 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304440  1757 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.304491  1757 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.304548  1754 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304582  1761 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304688  1761 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304714  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.304723  1761 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.304767  1761 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304805  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.304817  1761 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304824  1761 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304836  1761 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304841  1758 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304870  1758 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304909  1757 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304983  1756 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.305033  1756 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.305042  1759 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305071  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305124  1756 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305222  1758 sched.cpp:471] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305246  1758 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305286  1760 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305310  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.305318  1760 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.305344  1760 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.305363  1758 sched.cpp:809] Will retry registration in 1.888777185secs if necessary
[18:06:25][Step 8/8] I0215 17:06:25.305379  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.305397  1760 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305408  1760 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305426  1760 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305466  1761 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305506  1756 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305534  1761 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
[18:06:25][Step 8/8] I0215 17:06:25.305625  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305701  1761 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305831  1758 slave.cpp:927] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305902  1757 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
[18:06:25][Step 8/8] I0215 17:06:25.305953  1758 slave.cpp:1321] Will retry registration in 1.941456ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.306280  1761 hierarchical.cpp:265] Added framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306352  1759 sched.cpp:703] Framework registered with 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306363  1761 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.306401  1761 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.306432  1761 hierarchical.cpp:1096] Performed allocation for 0 slaves in 126082ns
[18:06:25][Step 8/8] I0215 17:06:25.306447  1759 sched.cpp:717] Scheduler::registered took 67960ns
[18:06:25][Step 8/8] I0215 17:06:25.306437  1757 master.cpp:4237] Registering slave at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with id 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.306884  1759 registrar.cpp:439] Applied 1 operations in 63175ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.307592  1756 log.cpp:683] Attempting to append 396 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.307724  1760 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.308398  1760 replica.cpp:537] Replica received write request for position 3 from (13622)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.308473  1755 slave.cpp:1321] Will retry registration in 37.671741ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.308627  1758 master.cpp:4225] Ignoring register slave message from slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) as admission is already in progress
[18:06:25][Step 8/8] I0215 17:06:25.310000  1760 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 1.556814ms
[18:06:25][Step 8/8] I0215 17:06:25.310025  1760 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.310541  1755 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.311928  1755 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 1.357404ms
[18:06:25][Step 8/8] I0215 17:06:25.311950  1755 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.311966  1755 replica.cpp:697] Replica learned APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.313117  1755 registrar.cpp:484] Successfully updated the 'registry' in 6.16704ms
[18:06:25][Step 8/8] I0215 17:06:25.313297  1758 log.cpp:702] Attempting to truncate the log to 3
[18:06:25][Step 8/8] I0215 17:06:25.313391  1755 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.313807  1761 slave.cpp:3482] Received ping from slave-observer(360)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.313946  1754 master.cpp:4305] Registered slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.314067  1756 slave.cpp:971] Registered with master master@172.30.2.239:39785; given slave ID 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.314095  1756 fetcher.cpp:81] Clearing fetcher cache
[18:06:25][Step 8/8] I0215 17:06:25.314102  1760 replica.cpp:537] Replica received write request for position 4 from (13623)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.314164  1758 hierarchical.cpp:473] Added slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[18:06:25][Step 8/8] I0215 17:06:25.314219  1761 status_update_manager.cpp:181] Resuming sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.314370  1756 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/slave.info'
[18:06:25][Step 8/8] I0215 17:06:25.314579  1756 slave.cpp:1030] Forwarding total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314707  1756 master.cpp:4646] Received update of slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314818  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.314848  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 654176ns
[18:06:25][Step 8/8] I0215 17:06:25.315137  1758 hierarchical.cpp:531] Slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[18:06:25][Step 8/8] I0215 17:06:25.315217  1756 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.315238  1758 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.315268  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.315285  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 118646ns
[18:06:25][Step 8/8] I0215 17:06:25.315635  1755 sched.cpp:873] Scheduler::resourceOffers took 99802ns
[18:06:25][Step 8/8] I0215 17:06:25.317126  1755 master.cpp:3138] Processing ACCEPT call for offers: [ 112363e2-c680-4946-8fee-d0626ed8b21e-O0 ] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.317163  1755 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.317229  1760 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.089068ms
[18:06:25][Step 8/8] I0215 17:06:25.317261  1760 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.317845  1759 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.318722  1755 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.318886  1755 master.cpp:3623] Launching task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.319195  1757 slave.cpp:1361] Got assigned task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.319305  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.430044ms
[18:06:25][Step 8/8] I0215 17:06:25.319349  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.319363  1759 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34738ns
[18:06:25][Step 8/8] I0215 17:06:25.319380  1759 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.319396  1759 replica.cpp:697] Replica learned TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.320034  1757 slave.cpp:1480] Launching task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.320127  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.320725  1757 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' to user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.325739  1757 slave.cpp:5351] Launching executor 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.326493  1757 slave.cpp:1698] Queuing task '1' for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.326633  1757 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.331328  1761 docker.cpp:803] Starting container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for task '1' (and executor '1') of framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:25][Step 8/8] I0215 17:06:25.331699  1761 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
[18:06:25][Step 8/8] I0215 17:06:25.449668  1758 docker.cpp:384] Docker pull alpine completed
[18:06:25][Step 8/8] I0215 17:06:25.511905  1760 slave.cpp:2643] Got registration for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:25][Step 8/8] I0215 17:06:25.513098  1759 docker.cpp:1077] Ignoring updating container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' with resources passed to update is identical to existing resources
[18:06:25][Step 8/8] I0215 17:06:25.513494  1756 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] 2016-02-15 17:06:25,981:1740(0x7f870b7fe700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36716] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[18:06:26][Step 8/8] I0215 17:06:26.227973  1757 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228302  1757 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228734  1754 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228790  1754 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228837  1757 slave.cpp:5661] Terminating task 1
[18:06:26][Step 8/8] I0215 17:06:26.229243  1754 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.229346  1755 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.232147  1758 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.232383  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232419  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.232631  1759 master.cpp:4791] Status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.232681  1759 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232911  1759 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_RUNNING)
[18:06:26][Step 8/8] I0215 17:06:26.233170  1756 sched.cpp:981] Scheduler::statusUpdate took 100304ns
[18:06:26][Step 8/8] I0215 17:06:26.233613  1754 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.233642  1759 master.cpp:3949] Processing ACKNOWLEDGE call b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.233944  1759 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.234294  1761 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.264482  1759 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:26][Step 8/8] I0215 17:06:26.264554  1759 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.210209ms
[18:06:26][Step 8/8] I0215 17:06:26.264837  1757 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.265275  1760 sched.cpp:873] Scheduler::resourceOffers took 26245ns
[18:06:26][Step 8/8] I0215 17:06:26.357859  1756 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358085  1756 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.358330  1758 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.358554  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358594  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.358687  1761 master.cpp:4791] Status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.358724  1761 master.cpp:4839] Forwarding status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358921  1761 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1269: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359128  1754 sched.cpp:981] Scheduler::statusUpdate took 108209ns
[18:06:26][Step 8/8] Value of: statusFinished.get().state()
[18:06:26][Step 8/8] I0215 17:06:26.359400  1759 master.cpp:3949] Processing ACKNOWLEDGE call 05810f46-10e7-4d50-a83d-d05bf79dd8e2 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8]   Actual: TASK_FAILED
[18:06:26][Step 8/8] Expected: TASK_FINISHED
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1278: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359470  1759 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] Value of: containsLine(lines, ""err"" + uuid)
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] I0215 17:06:26.359928  1761 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.359931  1740 sched.cpp:1903] Asked to stop the driver
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1286: Failure
[18:06:26][Step 8/8] I0215 17:06:26.360031  1759 sched.cpp:1143] Stopping framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:26][Step 8/8] Value of: containsLine(lines, ""out"" + uuid)
[18:06:26][Step 8/8] I0215 17:06:26.360080  1761 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.360213  1760 master.cpp:5923] Processing TEARDOWN call for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360239  1760 master.cpp:5935] Removing framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360481  1755 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360522  1755 slave.cpp:5702] Completing task 1
[18:06:26][Step 8/8] I0215 17:06:26.360539  1756 hierarchical.cpp:375] Deactivated framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360586  1755 slave.cpp:2079] Asked to shut down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 by master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360610  1755 slave.cpp:2104] Shutting down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360666  1755 slave.cpp:4198] Shutting down executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.361110  1761 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361395  1756 hierarchical.cpp:326] Removed framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361397  1760 master.cpp:1027] Master terminating
[18:06:26][Step 8/8] I0215 17:06:26.361800  1755 hierarchical.cpp:505] Removed slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.362174  1756 slave.cpp:3528] master@172.30.2.239:39785 exited
[18:06:26][Step 8/8] W0215 17:06:26.362200  1756 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
[18:06:26][Step 8/8] I0215 17:06:26.367146  1758 docker.cpp:1455] Destroying container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.367168  1758 docker.cpp:1515] Sending SIGTERM to executor with pid: 10194
[18:06:26][Step 8/8] I0215 17:06:26.383013  1758 docker.cpp:1557] Running docker stop on container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.384843  1757 slave.cpp:3528] executor(1)@172.30.2.239:38602 exited
[18:06:26][Step 8/8] I0215 17:06:26.458639  1761 docker.cpp:1654] Executor for container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' has exited
[18:06:26][Step 8/8] I0215 17:06:26.458806  1757 slave.cpp:3886] Executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 terminated with signal Terminated
[18:06:26][Step 8/8] I0215 17:06:26.458852  1757 slave.cpp:3990] Cleaning up executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.459166  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for gc 6.99999468745481days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459262  1757 slave.cpp:4078] Cleaning up framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459336  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1' for gc 6.99999468505778days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459372  1761 status_update_manager.cpp:282] Closing status update streams for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459481  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000' for gc 6.99999468310222days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459915  1759 slave.cpp:668] Slave terminating
[18:06:26][Step 8/8] I0215 17:06:26.463074  1740 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
[18:06:26][Step 8/8] I0215 17:06:26.560154  1760 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.666368  1740 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v c7f89c245f256c03222551178f6a0c26413bb91dfb3c843bff837b365d7c7432
[18:06:26][Step 8/8] [  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_Logs (1519 ms)
{noformat}",1,6,MESOS-4676,2.0
Cannot disable systemd support,"On certain platforms the systemd init system is available, but not used.
Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",1,4,MESOS-4675,1.0
Status updates from executor can be forwarded out of order by the Agent.,"Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received. 

However, that seems to be no longer valid due to a recently introduced change in the agent:

{code}
// Before sending update, we need to retrieve the container status.
  containerizer->status(executor->containerId)
    .onAny(defer(self(),
                 &Slave::_statusUpdate,
                 update,
                 pid,
                 executor->id,
                 lambda::_1));
{code}

This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",1,6,MESOS-4671,1.0
`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.,"The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",1,2,MESOS-4670,1.0
Add common compression utility,We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,1,2,MESOS-4669,2.0
Logrotate container logger can die with agent unit on systemd.,,1,2,MESOS-4640,1.0
Posix process executor can die with agent unit on systemd.,,1,2,MESOS-4639,1.0
Docker process executor can die with agent unit on systemd.,,1,2,MESOS-4637,1.0
Tests will dereference stack allocated master objects upon assertion/expectation failure.,"Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartMaster}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",1,1,MESOS-4634,5.0
Tests will dereference stack allocated agent objects upon assertion/expectation failure.,"Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartSlave}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",1,4,MESOS-4633,5.0
ContainerLoggerTest.DefaultToSandbox is flaky,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] ContainerLoggerTest.DefaultToSandbox
I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms
I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms
I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns
I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns
I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns
I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery
I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status
I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843
I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING
I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843
I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""
I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register
I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'
I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator
I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled
I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given
I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de
I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!
I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar
I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar
I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms
I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING
I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status
I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843
I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING
I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms
I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING
I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group
I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated
I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer
I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1
I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms
I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1
I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2
I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms
I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0
I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843
I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns
I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms
I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms
I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0
I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0
I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns
I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms
I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'
I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log
I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843
I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms
I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms
I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1
I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms
I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1
I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar
I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843
I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms
I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms
I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns
I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843
I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""
I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'
I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal
I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]
I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063
I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'
I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager
I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer
I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete
I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery
I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843
I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates
I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master
I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843
I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success
I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843
I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary
I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'
I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log
I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843
I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary
I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary
I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms
I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3
I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary
I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary
I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms
I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3
I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3
I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms
I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3
I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843
I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns
I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates
I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'
I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843
I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources 
I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0
I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns
I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843
I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.250114  2854 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.250453  2854 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.250525  2854 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.250814  2853 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.250881  2853 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.250982  2853 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.251092  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.251128  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.251144  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.251200  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.251242  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.251260  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251269  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251288  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.251471  2853 authenticatee.cpp:298] Authentication success
I0206 01:25:04.251574  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.251669  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.252162  2854 sched.cpp:471] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.252188  2854 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.8:37843
I0206 01:25:04.252286  2854 sched.cpp:809] Will retry registration in 1.575999657secs if necessary
I0206 01:25:04.252583  2853 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.252694  2853 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 01:25:04.253110  2853 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0206 01:25:04.253703  2843 hierarchical.cpp:265] Added framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.255300  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.255367  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.621522ms
I0206 01:25:04.255820  2844 sched.cpp:703] Framework registered with 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.256006  2844 sched.cpp:717] Scheduler::registered took 105156ns
I0206 01:25:04.256572  2853 master.cpp:5352] Sending 1 offers to framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.257524  2853 sched.cpp:873] Scheduler::resourceOffers took 173470ns
I0206 01:25:04.260818  2855 master.cpp:3138] Processing ACCEPT call for offers: [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-O0 ] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.260968  2855 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c as user 'mesos'
I0206 01:25:04.264458  2844 master.hpp:176] Adding task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063)
I0206 01:25:04.264796  2844 master.cpp:3623] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:04.265341  2855 slave.cpp:1361] Got assigned task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.265941  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.267323  2855 slave.cpp:1480] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.267627  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.268705  2855 paths.cpp:474] Trying to chown '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' to user 'mesos'
I0206 01:25:04.274116  2855 slave.cpp:5282] Launching executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.275185  2844 containerizer.cpp:656] Starting container '5c952202-44cf-427a-8452-0f501140a4b7' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:04.275311  2846 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.403837ms
I0206 01:25:04.275390  2846 replica.cpp:712] Persisted action at 4
I0206 01:25:04.275511  2855 slave.cpp:1698] Queuing task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.275832  2855 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.276707  2855 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 01:25:04.284708  2844 launcher.cpp:132] Forked child with pid '2872' for container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.301365  2855 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.497489ms
I0206 01:25:04.301528  2855 leveldb.cpp:399] Deleting ~2 keys from leveldb took 92156ns
I0206 01:25:04.301563  2855 replica.cpp:712] Persisted action at 4
I0206 01:25:04.301640  2855 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 01:25:04.823314  2854 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.823387  2854 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.823420  2854 hierarchical.cpp:1096] Performed allocation for 1 slaves in 327509ns
I0206 01:25:05.825943  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:05.826027  2850 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:05.826066  2850 hierarchical.cpp:1096] Performed allocation for 1 slaves in 362856ns
I0206 01:25:06.827154  2857 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:06.827235  2857 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:06.827275  2857 hierarchical.cpp:1096] Performed allocation for 1 slaves in 328221ns
I0206 01:25:07.828547  2843 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:07.828753  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:07.828907  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 624979ns
I0206 01:25:08.829737  2855 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:08.829918  2855 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:08.830070  2855 hierarchical.cpp:1096] Performed allocation for 1 slaves in 596793ns
I0206 01:25:09.831233  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:09.831316  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:09.831352  2856 hierarchical.cpp:1096] Performed allocation for 1 slaves in 353864ns
I0206 01:25:10.832953  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:10.833307  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:10.833411  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 731864ns
I0206 01:25:11.834967  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:11.835149  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:11.835294  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 586988ns
I0206 01:25:12.174247  2853 slave.cpp:2643] Got registration for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.179061  2844 slave.cpp:1863] Sending queued task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' to executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.194753  2858 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.195852  2858 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.196094  2858 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.197000  2858 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to the slave
I0206 01:25:12.197739  2855 slave.cpp:3354] Forwarding the update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to master@172.17.0.8:37843
I0206 01:25:12.198442  2855 master.cpp:4791] Status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.198673  2855 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.199038  2855 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0206 01:25:12.199581  2854 sched.cpp:981] Scheduler::statusUpdate took 159022ns
I0206 01:25:12.200568  2854 master.cpp:3949] Processing ACKNOWLEDGE call 9d924a5b-76ab-4886-8091-7af3428ff179 for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.201513  2858 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
../../src/tests/container_logger_tests.cpp:350: Failure
Value of: strings::contains(stdout.get(), ""Hello World!"")
  Actual: false
Expected: true
I0206 01:25:12.201702  2824 sched.cpp:1903] Asked to stop the driver
I0206 01:25:12.202831  2848 sched.cpp:1143] Stopping framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:12.203284  2848 master.cpp:5923] Processing TEARDOWN call for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.203321  2848 master.cpp:5935] Removing framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.201762  2854 slave.cpp:3248] Status update manager successfully handled status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.203384  2854 slave.cpp:3264] Sending acknowledgement for status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to executor(1)@172.17.0.8:43659
I0206 01:25:12.204712  2843 hierarchical.cpp:375] Deactivated framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.204953  2848 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 01:25:12.205885  2854 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206082  2854 slave.cpp:2079] Asked to shut down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 by master@172.17.0.8:37843
I0206 01:25:12.206125  2854 slave.cpp:2104] Shutting down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206331  2854 slave.cpp:4129] Shutting down executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.206408  2843 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 from framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.207352  2848 master.cpp:6513] Removing task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.208258  2848 master.cpp:1027] Master terminating
I0206 01:25:12.208703  2857 hierarchical.cpp:326] Removed framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.209658  2857 hierarchical.cpp:505] Removed slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.212208  2848 slave.cpp:3482] master@172.17.0.8:37843 exited
W0206 01:25:12.212261  2848 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 01:25:12.224596  2854 containerizer.cpp:1318] Destroying container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:12.241466  2852 slave.cpp:3482] executor(1)@172.17.0.8:43659 exited
I0206 01:25:12.250931  2856 containerizer.cpp:1534] Executor for container '5c952202-44cf-427a-8452-0f501140a4b7' has exited
I0206 01:25:12.253350  2850 provisioner.cpp:306] Ignoring destroy request for unknown container 5c952202-44cf-427a-8452-0f501140a4b7
I0206 01:25:12.253885  2850 slave.cpp:3817] Executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 terminated with signal Killed
I0206 01:25:12.254125  2850 slave.cpp:3921] Cleaning up executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.254545  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' for gc 6.99999705530074days in the future
I0206 01:25:12.254803  2850 slave.cpp:4009] Cleaning up framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.254822  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c' for gc 6.99999705202667days in the future
I0206 01:25:12.255084  2857 status_update_manager.cpp:282] Closing status update streams for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255143  2856 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' for gc 6.99999704808days in the future
I0206 01:25:12.255190  2857 status_update_manager.cpp:528] Cleaning up status update stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255192  2850 slave.cpp:668] Slave terminating
[  FAILED  ] ContainerLoggerTest.DefaultToSandbox (8566 ms)
{code}",1,2,MESOS-4615,1.0
SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor
I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms
I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns
I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns
I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns
I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns
I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery
I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status
I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484
I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING
I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns
I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING
I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status
I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484
I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING
I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484
I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""
I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register
I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'
I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns
I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING
I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator
I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled
I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group
I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated
I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given
I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca
I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!
I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar
I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar
I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer
I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1
I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns
I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1
I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2
I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns
I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0
I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484
I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns
I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns
I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns
I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0
I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0
I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns
I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms
I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'
I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log
I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484
I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns
I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns
I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1
I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms
I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar
I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1
I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484
I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns
I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2
I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns
I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns
I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2
I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484
I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""
I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'
I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal
I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]
I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade
I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0
I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484
I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'
I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager
I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer
I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete
I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success
I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success
I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery
I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484
I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary
I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns
I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484
I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates
I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns
I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master
I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484
I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success
I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success
I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484
I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary
I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'
I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log
I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484
I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns
I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3
I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns
I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3
I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3
I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.020864ms
I0206 00:22:44.863106  2850 log.cpp:702] Attempting to truncate the log to 3
I0206 00:22:44.863358  2850 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 00:22:44.864321  2850 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
I0206 00:22:44.864706  2849 hierarchical.cpp:473] Added slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 00:22:44.864716  2843 replica.cpp:537] Replica received write request for position 4 from (9494)@172.17.0.2:43484
I0206 00:22:44.865309  2843 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 410199ns
I0206 00:22:44.865337  2843 replica.cpp:712] Persisted action at 4
I0206 00:22:44.866092  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.866132  2848 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 00:22:44.866137  2849 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 1.30657ms
I0206 00:22:44.866497  2856 master.cpp:4305] Registered slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.866564  2843 slave.cpp:1321] Will retry registration in 32.803438ms if necessary
I0206 00:22:44.866690  2843 slave.cpp:971] Registered with master master@172.17.0.2:43484; given slave ID 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.866716  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 00:22:44.867066  2856 master.cpp:5352] Sending 1 offers to framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.867105  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/slave.info'
I0206 00:22:44.867347  2856 master.cpp:4207] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) already registered, resending acknowledgement
I0206 00:22:44.867441  2856 status_update_manager.cpp:181] Resuming sending status updates
I0206 00:22:44.867465  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
W0206 00:22:44.867547  2843 slave.cpp:1016] Already registered with master master@172.17.0.2:43484
I0206 00:22:44.867574  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 00:22:44.867710  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.867951  2856 sched.cpp:873] Scheduler::resourceOffers took 133371ns
I0206 00:22:44.867961  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.868484  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.868599  2848 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.418545ms
I0206 00:22:44.868700  2848 leveldb.cpp:399] Deleting ~2 keys from leveldb took 54053ns
I0206 00:22:44.868751  2848 replica.cpp:712] Persisted action at 4
I0206 00:22:44.868811  2848 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 00:22:44.869241  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.869287  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.869321  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 782848ns
I0206 00:22:44.869840  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.869985  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.870028  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.870053  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 160104ns
I0206 00:22:44.871824  2853 master.cpp:3138] Processing ACCEPT call for offers: [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-O0 ] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.871868  2853 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
W0206 00:22:44.873613  2843 validation.cpp:404] Executor http for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0206 00:22:44.873667  2843 validation.cpp:416] Executor http for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0206 00:22:44.874035  2843 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade)
I0206 00:22:44.874223  2843 master.cpp:3623] Launching task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:44.874802  2843 slave.cpp:1361] Got assigned task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.874966  2843 slave.cpp:5202] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info'
I0206 00:22:44.875440  2843 slave.cpp:5213] Checkpointing framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484' to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid'
I0206 00:22:44.876106  2843 slave.cpp:1480] Launching task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.876644  2843 paths.cpp:474] Trying to chown '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' to user 'mesos'
I0206 00:22:44.884089  2843 slave.cpp:5654] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info'
I0206 00:22:44.900928  2843 slave.cpp:5282] Launching executor http of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 with resources  in work directory '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.901449  2853 containerizer.cpp:656] Starting container 'fd4649a4-1c82-4eda-b663-b568b6110d17' for executor 'http' of framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000'
I0206 00:22:44.901561  2843 slave.cpp:5677] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info'
I0206 00:22:44.902060  2843 slave.cpp:1698] Queuing task '1' for executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.902207  2843 slave.cpp:749] Successfully attached file '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907027  2850 launcher.cpp:132] Forked child with pid '8875' for container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907229  2850 containerizer.cpp:1094] Checkpointing executor's forked pid 8875 to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0206 00:22:45.080060  8875 process.cpp:991] libprocess is initialized on 172.17.0.2:49724 for 16 cpus
I0206 00:22:45.082499  8875 logging.cpp:193] Logging to STDERR
I0206 00:22:45.082862  8875 executor.cpp:172] Version: 0.28.0
I0206 00:22:45.087201  8903 executor.cpp:316] Connected with the agent
I0206 00:22:45.802878  2858 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:45.802969  2858 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:45.803014  2858 hierarchical.cpp:1096] Performed allocation for 1 slaves in 424120ns
2016-02-06 00:22:45,982:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0206 00:22:46.588022  2854 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:46.588969  2854 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:46,589:2824(0x7fd9fefd1700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:46,590:2824(0x7fda03fdb700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9d401bc10 flags=0
2016-02-06 00:22:46,590:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:46.804400  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:46.804481  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:46.804514  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 347954ns
I0206 00:22:47.805842  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:47.805934  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:47.805980  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 415449ns
I0206 00:22:48.807723  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:48.807814  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:48.807857  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 442104ns
I0206 00:22:49.808733  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:49.808816  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:49.808856  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 384959ns
2016-02-06 00:22:49,926:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:50.810307  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:50.810400  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:50.810443  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 389572ns
I0206 00:22:51.811586  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:51.811681  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:51.811722  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 404450ns
I0206 00:22:52.812860  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:52.812944  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:52.812981  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 359671ns
2016-02-06 00:22:53,263:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:53.814512  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:53.814599  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:53.814651  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 386669ns
I0206 00:22:54.815238  2852 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:54.815321  2852 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:54.815356  2852 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376235ns
I0206 00:22:55.816453  2846 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:55.816550  2846 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:55.816596  2846 hierarchical.cpp:1096] Performed allocation for 1 slaves in 416350ns
W0206 00:22:56.592408  2849 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:56.593480  2849 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:56,593:2824(0x7fda017d6700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9e401f350 flags=0
2016-02-06 00:22:56,595:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:56.817683  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:56.817766  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:56.817803  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 374115ns
I0206 00:22:57.818447  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:57.818526  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:57.818562  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344545ns
I0206 00:22:58.819828  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:58.819914  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:58.819957  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376948ns
I0206 00:22:59.820874  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:59.820957  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:59.820991  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344192ns
I0206 00:22:59.854698  2845 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:59.854991  2845 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:59.864612  2857 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
../../src/tests/slave_recovery_tests.cpp:1105: Failure
Failed to wait 15secs for updateCall1
I0206 00:22:59.876358  2852 master.cpp:1213] Framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 disconnected
I0206 00:22:59.876410  2852 master.cpp:2576] Disconnecting framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876456  2852 master.cpp:2600] Deactivating framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876569  2852 master.cpp:1237] Giving framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 0ns to failover
I0206 00:22:59.876981  2844 hierarchical.cpp:375] Deactivated framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.877049  2844 master.cpp:5204] Framework failover timeout, removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877075  2844 master.cpp:5935] Removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877276  2844 master.cpp:6447] Updating the state of task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 00:22:59.878051  2844 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878433  2844 master.cpp:6542] Removing executor 'http' with resources  of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878667  2852 slave.cpp:2079] Asked to shut down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 by master@172.17.0.2:43484
I0206 00:22:59.878733  2852 slave.cpp:2104] Shutting down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.878806  2852 slave.cpp:4129] Shutting down executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
W0206 00:22:59.878834  2852 slave.hpp:655] Unable to send event to executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000: unknown connection type
I0206 00:22:59.879550  2844 master.cpp:1027] Master terminating
I0206 00:22:59.879703  2854 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 from framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.879947  2854 hierarchical.cpp:326] Removed framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.880306  2854 hierarchical.cpp:505] Removed slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:59.880666  2852 slave.cpp:3482] master@172.17.0.2:43484 exited
W0206 00:22:59.880695  2852 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 00:22:59.885498  2857 containerizer.cpp:1318] Destroying container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:59.904532  2858 containerizer.cpp:1534] Executor for container 'fd4649a4-1c82-4eda-b663-b568b6110d17' has exited
I0206 00:22:59.907024  2858 provisioner.cpp:306] Ignoring destroy request for unknown container fd4649a4-1c82-4eda-b663-b568b6110d17
I0206 00:22:59.907428  2858 slave.cpp:3817] Executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 terminated with signal Killed
I0206 00:22:59.907538  2858 slave.cpp:3921] Cleaning up executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908213  2858 slave.cpp:4009] Cleaning up framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908555  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998949252444days in the future
I0206 00:22:59.908720  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998949082074days in the future
I0206 00:22:59.908807  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998948980444days in the future
I0206 00:22:59.908927  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998948890074days in the future
I0206 00:22:59.909009  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948710518days in the future
I0206 00:22:59.909121  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948630815days in the future
I0206 00:22:59.909211  2858 status_update_manager.cpp:282] Closing status update streams for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.910423  2853 slave.cpp:668] Slave terminating
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveRecoveryTest/0.CleanupHTTPExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (15126 ms)
{code}",1,3,MESOS-4614,3.0
Subprocess should be more intelligent about setting/inheriting libprocess environment variables ,"Mostly copied from [this comment|https://issues.apache.org/jira/browse/MESOS-4598?focusedCommentId=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15133497]

A subprocess inheriting the environment variables {{LIBPROCESS_*}} may run into some accidental fatalities:

| || Subprocess uses libprocess || Subprocess is something else ||
|| Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit | Nothing happens (?) |
|| Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |

(?) = means this is usually the case, but not 100%.

A complete fix would look something like:
* If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.  
* The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module.
* If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(""LIBPROCESS_PORT"")}}, we can LOG(WARN) and unset the env var locally.",0,3,MESOS-4609,2.0
ROOT_DOCKER_DockerHealthyTask is flaky.,"Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:
{noformat}
[18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest
[18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask
[18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)
[18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask
[18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure
[18:27:36][Step 8/8] Failed to wait 15secs for termination
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
[18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()
[18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()
[18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()
[18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()
[18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()
[18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()
[18:27:36][Step 8/8]     @           0xe54867  main
[18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)
[18:27:36][Step 8/8]     @           0x9b52d9  _start
[18:27:36][Step 8/8] Aborted (core dumped)
[18:27:36][Step 8/8] Process exited with code 134
{noformat}
Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ",1,4,MESOS-4604,2.0
Logrotate ContainerLogger should not remove IP from environment.,"The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.

Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",1,5,MESOS-4598,1.0
Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`,We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.,1,2,MESOS-4583,1.0
Design doc for scheduler HTTP Stream IDs,"This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers.",1,2,MESOS-4573,5.0
NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6,"This test fails in my CentOS 6 VM due to a cgroups issue:

{code}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0
I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0
Registered executor on localhost
Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf
sh -c 'sleep 1000'
Forked command at 25385
../../src/tests/containerizer/isolator_tests.cpp:926: Failure
pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy
I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 25385
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)
{code}",1,3,MESOS-4540,1.0
Logrotate ContainerLogger may not handle FD ownership correctly,"One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.

The way the logrotate module uses this is slightly incorrect:
# The module starts a subprocess with an output {{Subprocess::PIPE()}}.
# That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.
# When the second subprocess starts, the pipe's FD is closed in the parent.
# When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",1,2,MESOS-4535,1.0
NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky,"While running the command
{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""-CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen:CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS"" --gtest_repeat=10 --gtest_break_on_failure
{noformat}
One eventually gets the following output:
{noformat}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
../../src/tests/containerizer/isolator_tests.cpp:870: Failure
containerizer: Could not create isolator 'cgroups/net_cls': Unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/net_cls,net_prio
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (75 ms)
{noformat}",1,4,MESOS-4530,1.0
Introduce docker runtime isolator.,"Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",1,2,MESOS-4517,3.0
ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.,"{noformat}
[17:24:58][Step 7/7] logrotate: bad argument --version: unknown error
[17:24:58][Step 7/7] F0126 17:24:57.913729  4503 container_logger_tests.cpp:380] CHECK_SOME(containerizer): Failed to create container logger: Failed to create container logger module 'org_apache_mesos_LogrotateContainerLogger': Error creating Module instance for 'org_apache_mesos_LogrotateContainerLogger' 
[17:24:58][Step 7/7] *** Check failure stack trace: ***
[17:24:58][Step 7/7]     @     0x7f11ae0d2d40  google::LogMessage::Fail()
[17:24:58][Step 7/7]     @     0x7f11ae0d2c9c  google::LogMessage::SendToLog()
[17:24:58][Step 7/7]     @     0x7f11ae0d2692  google::LogMessage::Flush()
[17:24:58][Step 7/7]     @     0x7f11ae0d544c  google::LogMessageFatal::~LogMessageFatal()
[17:24:58][Step 7/7]     @           0x983927  _CheckFatal::~_CheckFatal()
[17:24:58][Step 7/7]     @           0xa9a18b  mesos::internal::tests::ContainerLoggerTest_LOGROTATE_RotateInSandbox_Test::TestBody()
[17:24:58][Step 7/7]     @          0x1623a4e  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161eab2  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x15ffdfd  testing::Test::Run()
[17:24:58][Step 7/7]     @          0x160058b  testing::TestInfo::Run()
[17:24:58][Step 7/7]     @          0x1600bc6  testing::TestCase::Run()
[17:24:58][Step 7/7]     @          0x1607515  testing::internal::UnitTestImpl::RunAllTests()
[17:24:58][Step 7/7]     @          0x16246dd  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161f608  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x1606245  testing::UnitTest::Run()
[17:24:58][Step 7/7]     @           0xde36b6  RUN_ALL_TESTS()
[17:24:58][Step 7/7]     @           0xde32cc  main
[17:24:58][Step 7/7]     @     0x7f11a8896d5d  __libc_start_main
[17:24:58][Step 7/7]     @           0x981fc9  (unknown)
{noformat}",1,2,MESOS-4515,1.0
Render quota status consistently with other endpoints.,"Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:
{code:xml}
{
  ""infos"": [
    {
      ""role"": ""role1"",
      ""guarantee"": [
        {
          ""name"": ""cpus"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 12 }
        },
        {
          ""name"": ""mem"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 6144 }
        }
      ]
    }
  ]
}
{code}

Presence of some fields, e.g. ""role"", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",1,3,MESOS-4512,3.0
Expose ExecutorInfo and TaskInfo for isolators.,"Currently we do not have these info for isolator. Image once we have docker runtime isolator, CommandInfo is necessary to support either custom executor or command executor. ",1,1,MESOS-4500,2.0
Docker provisioner store should reuse existing layers in the cache.,"Currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.

This is problematic because anytime a user deploys a new image, Mesos will fetch all layers of that new image, even though most of the layers are already cached locally.

",1,3,MESOS-4499,5.0
Delete `os::chown` on Windows,,1,2,MESOS-4495,1.0
Implement process querying/counting in Windows,,1,3,MESOS-4471,2.0
Implement `waitpid` in Windows,,1,2,MESOS-4466,5.0
Create common sha512 compute utility function.,Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ,1,2,MESOS-4454,2.0
"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos",Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.,1,3,MESOS-4434,3.0
Introduce filtering test abstractions for HTTP events to libprocess,"We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.

The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",1,2,MESOS-4425,3.0
Prevent allocator from crashing on successful recovery.,"There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/:
{noformat}
It looks like if we trip the resume call in addSlave, this delayed resume will crash the master 
due to the CHECK(paused) that currently resides in resume.
{noformat}",1,2,MESOS-4417,3.0
Traverse all roles for quota allocation.,There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.,1,4,MESOS-4411,3.0
Synchronously handle AuthZ errors for the Scheduler endpoint.,"Currently, any AuthZ errors for the {{/scheduler}} endpoint are handled asynchronously as {{FrameworkErrorMessage}}. Here is an example:

{code}
  if (authorizationError.isSome()) {
    LOG(INFO) << ""Refusing subscription of framework""
              << "" '"" << frameworkInfo.name() << ""'""
              << "": "" << authorizationError.get().message;

    FrameworkErrorMessage message;
    message.set_message(authorizationError.get().message);
    http.send(message);
    http.close();
    return;
  }
{code}

We would like to handle such errors synchronously when the request is received similar to what other endpoints like {{/reserve}}/{{/quota}} do. We already have the relevant functions {{authorizeXXX}} etc in {{master.cpp}}. We should just make the requests pass through once the relevant {{Future}} from the {{authorizeXXX}} function is fulfilled.",0,0,MESOS-4398,5.0
Add persistent volume endpoint tests with no principal,There are currently no persistent volume endpoint tests that do not use a principal; they should be added.,1,2,MESOS-4395,1.0
Offers and InverseOffers cannot be accepted in the same ACCEPT call,"*Problem*
* In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.
* If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Here's a regression test:
https://reviews.apache.org/r/42092/

*Proprosal*
The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.

Arguments for mixing:
* The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.
* Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.

Arguments against mixing:
* Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?
* What happens if we presumably add a third type of offer?
* Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",1,1,MESOS-4385,2.0
Support docker runtime configuration env var from image.,We need to support env var configuration returned from docker image in mesos containerizer.,1,2,MESOS-4383,2.0
Create common tar/untar utility function.,"As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",1,4,MESOS-4360,3.0
GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard,"The following GMock warning was seen on CentOS 7.1:

{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: executorLost(0x7ffdd74f73e0, @0x7f3e3c00fa20 e1, @0x7f3e3c00f4b0 cf212bb4-c8c5-4a43-b71f-c17b27458627-S0, -1)
Stack trace:
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard (405 ms)
{code}",0,0,MESOS-4359,2.0
GMock warning on `offerRescinded` in `ReservationTest` fixture,"Several tests involving checkpointing of resources in the {{ReservationTest}} fixture are throwing GMock warnings occasionally. Here is the output of {{GTEST_FILTER=""ReservationTest.*"" bin/mesos-tests.sh --gtest_repeat=10000 --gtest_break_on_failure=1 | grep -B 3 -A 6 WARNING}}:

{code}
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
[       OK ] ReservationTest.MasterFailover (89 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec320fab0 65537c10-285c-419e-b89f-191283402d85-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResources (52 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (46 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec796f220 bf4e1b52-02db-4763-8be0-3c759c80f1ba-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (63 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (42 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7ad92b0 42a9f1ff-122e-4df7-9530-a96126e36f84-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (65 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (49 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7af4310 d5e1005f-abb8-4bfd-92e0-3976ee150fbf-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (94 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (57 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7cdadc0 36e15f52-3299-46fa-850d-970097fef8e2-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec8c1b580 c8dd35ab-7363-40e0-8e20-8c7dc76a8497-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecbd9b5b0 031c2148-8a20-4532-b77f-b6200c3791c8-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (47 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecd52adb0 edc5a322-b220-4b13-a39b-99a523b172ba-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (76 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (63 ms)
--
--
[       OK ] ReservationTest.SendingCheckpointResourcesMessage (45 ms)
[ RUN      ] ReservationTest.ResourcesCheckpointing

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f015df8, @0x7feecfe16f00 09a90e67-a40f-4e42-8802-1a5644733a06-O1)
Stack trace:
[       OK ] ReservationTest.ResourcesCheckpointing (60 ms)
[ RUN      ] ReservationTest.MasterFailover
[       OK ] ReservationTest.MasterFailover (89 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecacceba0 84965984-28cd-4bc8-b25b-746583477d09-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (58 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (68 ms)
{code}",0,0,MESOS-4350,2.0
GMock warning in ReservationTest.ACLMultipleOperations,"{noformat}
[ RUN      ] ReservationTest.ACLMultipleOperations

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fa2a311b300)
Stack trace:
[       OK ] ReservationTest.ACLMultipleOperations (174 ms)
[----------] 1 test from ReservationTest (174 ms total)
{noformat}

Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",1,3,MESOS-4347,1.0
Create utilities for common shell commands used.,"We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.

",1,0,MESOS-4338,5.0
SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation,"Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,
{code}
% ./bin/mesos-tests.sh --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
Source directory: /ABC/DEF/src/mesos
Build directory: /ABC/DEF/src/mesos/build
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
/usr/bin/nc
/usr/bin/curl
Note: Google Test filter = SlaveTest.LaunchTaskInfoWithContainerInfo-HealthCheckTest.ROOT_DOCKER_DockerHealthyTask:HealthCheckTest.ROOT_DOCKER_DockerHealthStatusChange:HierarchicalAllocator_BENCHMARK_Test.DeclineOffers:HookTest.ROOT_DOCKER_VerifySlavePreLaunchDockerHook:SlaveTest.ROOT_RunTaskWithCommandInfoWithoutUser:SlaveTest.DISABLED_ROOT_RunTaskWithCommandInfoWithUser:DockerContainerizerTest.ROOT_DOCKER_Launch:DockerContainerizerTest.ROOT_DOCKER_Kill:DockerContainerizerTest.ROOT_DOCKER_Usage:DockerContainerizerTest.ROOT_DOCKER_Recover:DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker:DockerContainerizerTest.ROOT_DOCKER_Logs:DockerContainerizerTest.ROOT_DOCKER_Default_CMD:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args:DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer:DockerContainerizerTest.DISABLED_ROOT_DOCKER_SlaveRecoveryExecutorContainer:DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping:DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon:DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching:DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling:DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed:DockerContainerizerTest.ROOT_DOCKER_FetchFailure:DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure:DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard:DockerTest.ROOT_DOCKER_interface:DockerTest.ROOT_DOCKER_parsing_version:DockerTest.ROOT_DOCKER_CheckCommandWithShell:DockerTest.ROOT_DOCKER_CheckPortResource:DockerTest.ROOT_DOCKER_CancelPull:DockerTest.ROOT_DOCKER_MountRelative:DockerTest.ROOT_DOCKER_MountAbsolute:CopyBackendTest.ROOT_CopyBackend:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/0:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/1:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/2:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/3:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/4:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/5:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/6:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/7:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/8:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/9:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/10:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/11:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/12:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/13:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/14:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/15:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/16:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/17:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/18:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/19:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/20:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/21:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/22:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/23:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/24:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/25:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/26:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/27:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/28:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/29:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/30:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/31:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/32:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/33:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/34:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/35:SlaveCount/Registrar_BENCHMARK_Test.Performance/0:SlaveCount/Registrar_BENCHMARK_Test.Performance/1:SlaveCount/Registrar_BENCHMARK_Test.Performance/2:SlaveCount/Registrar_BENCHMARK_Test.Performance/3
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveTest
[ RUN      ] SlaveTest.LaunchTaskInfoWithContainerInfo
[       OK ] SlaveTest.LaunchTaskInfoWithContainerInfo (79 ms)
[----------] 1 test from SlaveTest (79 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:569: Failure
Failed
Tests completed with child processes remaining:
-+- 54487 /ABC/DEF/src/mesos/build/src/.libs/mesos-tests --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
 \--- 54503 /bin/sh /ABC/DEF/src/mesos/build/src/mesos-containerizer launch --command={""shell"":true,""value"":""\/ABC\/DEF\/src\/mesos\/build\/src\/mesos-executor""} --commands={""commands"":[]} --directory=/tmp --help=false --pipe_read=10 --pipe_write=13 --user=test
[==========] 1 test from 1 test case ran. (87 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}
",1,2,MESOS-4329,1.0
PersistentVolumeTest.BadACLNoPrincipal is flaky,"https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull

{noformat}
[ RUN      ] PersistentVolumeTest.BadACLNoPrincipal
I0108 01:13:16.117883  1325 leveldb.cpp:174] Opened db in 2.614722ms
I0108 01:13:16.118650  1325 leveldb.cpp:181] Compacted db in 706567ns
I0108 01:13:16.118702  1325 leveldb.cpp:196] Created db iterator in 24489ns
I0108 01:13:16.118723  1325 leveldb.cpp:202] Seeked to beginning of db in 2436ns
I0108 01:13:16.118738  1325 leveldb.cpp:271] Iterated through 0 keys in the db in 397ns
I0108 01:13:16.118793  1325 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0108 01:13:16.119627  1348 recover.cpp:447] Starting replica recovery
I0108 01:13:16.120352  1348 recover.cpp:473] Replica is in EMPTY status
I0108 01:13:16.121750  1357 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (7084)@172.17.0.2:32801
I0108 01:13:16.122297  1353 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0108 01:13:16.122747  1350 recover.cpp:564] Updating replica status to STARTING
I0108 01:13:16.123625  1354 master.cpp:365] Master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801
I0108 01:13:16.123946  1347 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 728242ns
I0108 01:13:16.123999  1347 replica.cpp:320] Persisted replica status to STARTING
I0108 01:13:16.123708  1354 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""test-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/f2rA75/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/f2rA75/master"" --zk_session_timeout=""10secs""
I0108 01:13:16.124219  1354 master.cpp:414] Master allowing unauthenticated frameworks to register
I0108 01:13:16.124236  1354 master.cpp:417] Master only allowing authenticated slaves to register
I0108 01:13:16.124248  1354 credentials.hpp:35] Loading credentials for authentication from '/tmp/f2rA75/credentials'
I0108 01:13:16.124294  1358 recover.cpp:473] Replica is in STARTING status
I0108 01:13:16.124644  1354 master.cpp:456] Using default 'crammd5' authenticator
I0108 01:13:16.124820  1354 master.cpp:493] Authorization enabled
W0108 01:13:16.124843  1354 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0108 01:13:16.125154  1348 hierarchical.cpp:147] Initialized hierarchical allocator process
I0108 01:13:16.125334  1345 whitelist_watcher.cpp:77] No whitelist given
I0108 01:13:16.126065  1346 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (7085)@172.17.0.2:32801
I0108 01:13:16.126806  1348 recover.cpp:193] Received a recover response from a replica in STARTING status
I0108 01:13:16.128237  1354 recover.cpp:564] Updating replica status to VOTING
I0108 01:13:16.128402  1359 master.cpp:1629] The newly elected leader is master@172.17.0.2:32801 with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2
I0108 01:13:16.128489  1359 master.cpp:1642] Elected as the leading master!
I0108 01:13:16.128523  1359 master.cpp:1387] Recovering from registrar
I0108 01:13:16.128756  1355 registrar.cpp:307] Recovering registrar
I0108 01:13:16.129259  1344 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 531437ns
I0108 01:13:16.129292  1344 replica.cpp:320] Persisted replica status to VOTING
I0108 01:13:16.129425  1358 recover.cpp:578] Successfully joined the Paxos group
I0108 01:13:16.129680  1358 recover.cpp:462] Recover process terminated
I0108 01:13:16.130187  1358 log.cpp:659] Attempting to start the writer
I0108 01:13:16.131613  1352 replica.cpp:493] Replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1
I0108 01:13:16.131983  1352 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 333646ns
I0108 01:13:16.132004  1352 replica.cpp:342] Persisted promised to 1
I0108 01:13:16.132627  1348 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0108 01:13:16.133896  1349 replica.cpp:388] Replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2
I0108 01:13:16.134289  1349 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 349652ns
I0108 01:13:16.134317  1349 replica.cpp:712] Persisted action at 0
I0108 01:13:16.135470  1351 replica.cpp:537] Replica received write request for position 0 from (7088)@172.17.0.2:32801
I0108 01:13:16.135537  1351 leveldb.cpp:436] Reading position from leveldb took 36181ns
I0108 01:13:16.135901  1351 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 308752ns
I0108 01:13:16.135924  1351 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136529  1347 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0108 01:13:16.136889  1347 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 327106ns
I0108 01:13:16.136916  1347 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136943  1347 replica.cpp:697] Replica learned NOP action at position 0
I0108 01:13:16.137707  1359 log.cpp:675] Writer started with ending position 0
I0108 01:13:16.138844  1348 leveldb.cpp:436] Reading position from leveldb took 31371ns
I0108 01:13:16.139878  1356 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I0108 01:13:16.140012  1356 registrar.cpp:439] Applied 1 operations in 42063ns; attempting to update the 'registry'
I0108 01:13:16.140797  1355 log.cpp:683] Attempting to append 170 bytes to the log
I0108 01:13:16.140974  1345 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0108 01:13:16.141744  1354 replica.cpp:537] Replica received write request for position 1 from (7089)@172.17.0.2:32801
I0108 01:13:16.142226  1354 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 441971ns
I0108 01:13:16.142251  1354 replica.cpp:712] Persisted action at 1
I0108 01:13:16.142860  1351 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0108 01:13:16.143198  1351 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 305928ns
I0108 01:13:16.143223  1351 replica.cpp:712] Persisted action at 1
I0108 01:13:16.143241  1351 replica.cpp:697] Replica learned APPEND action at position 1
I0108 01:13:16.144271  1354 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.144435  1354 registrar.cpp:370] Successfully recovered registrar
I0108 01:13:16.144567  1359 log.cpp:702] Attempting to truncate the log to 1
I0108 01:13:16.144780  1359 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0108 01:13:16.144989  1348 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I0108 01:13:16.144928  1354 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0108 01:13:16.145690  1357 replica.cpp:537] Replica received write request for position 2 from (7090)@172.17.0.2:32801
I0108 01:13:16.146072  1357 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345113ns
I0108 01:13:16.146097  1357 replica.cpp:712] Persisted action at 2
I0108 01:13:16.146667  1358 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0108 01:13:16.147060  1358 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 283648ns
I0108 01:13:16.147116  1358 leveldb.cpp:399] Deleting ~1 keys from leveldb took 32174ns
I0108 01:13:16.147135  1358 replica.cpp:712] Persisted action at 2
I0108 01:13:16.147153  1358 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0108 01:13:16.166832  1325 containerizer.cpp:139] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0108 01:13:16.167556  1325 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0108 01:13:16.170526  1349 slave.cpp:191] Slave started on 231)@172.17.0.2:32801
I0108 01:13:16.170718  1349 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY""
I0108 01:13:16.171269  1349 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential'
I0108 01:13:16.171505  1349 slave.cpp:322] Slave using credential for: test-principal
I0108 01:13:16.171747  1349 resources.cpp:481] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I0108 01:13:16.172266  1349 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.172327  1349 slave.cpp:400] Slave attributes: [  ]
I0108 01:13:16.172340  1349 slave.cpp:405] Slave hostname: d9632dd1c41e
I0108 01:13:16.172353  1349 slave.cpp:410] Slave checkpoint: true
I0108 01:13:16.173418  1353 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta'
I0108 01:13:16.173521  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.174054  1345 status_update_manager.cpp:200] Recovering status update manager
I0108 01:13:16.174289  1353 containerizer.cpp:387] Recovering containerizer
I0108 01:13:16.174295  1356 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.174387  1356 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.174409  1356 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.174515  1356 sched.cpp:755] Will retry registration in 1.699889272secs if necessary
I0108 01:13:16.174653  1349 master.cpp:2197] Received SUBSCRIBE call for framework 'no-principal' at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.174823  1349 master.cpp:1668] Authorizing framework principal '' to receive offers for role 'role1'
I0108 01:13:16.175250  1347 master.cpp:2268] Subscribing framework no-principal with checkpointing disabled and capabilities [  ]
I0108 01:13:16.175359  1353 slave.cpp:4429] Finished recovery
I0108 01:13:16.175715  1345 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175734  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175792  1345 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.175833  1345 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.175853  1353 slave.cpp:4601] Querying resource estimator for oversubscribable resources
I0108 01:13:16.175869  1345 hierarchical.cpp:1079] Performed allocation for 0 slaves in 127881ns
I0108 01:13:16.175923  1351 sched.cpp:663] Scheduler::registered took 27956ns
I0108 01:13:16.176110  1353 slave.cpp:729] New master detected at master@172.17.0.2:32801
I0108 01:13:16.176187  1353 slave.cpp:792] Authenticating with master master@172.17.0.2:32801
I0108 01:13:16.176216  1353 slave.cpp:797] Using default CRAM-MD5 authenticatee
I0108 01:13:16.176398  1357 status_update_manager.cpp:174] Pausing sending status updates
I0108 01:13:16.176404  1353 slave.cpp:765] Detecting new master
I0108 01:13:16.176463  1358 authenticatee.cpp:121] Creating new client SASL connection
I0108 01:13:16.176553  1353 slave.cpp:4615] Received oversubscribable resources  from the resource estimator
I0108 01:13:16.176709  1353 master.cpp:5445] Authenticating slave(231)@172.17.0.2:32801
I0108 01:13:16.176823  1359 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.177135  1348 authenticator.cpp:98] Creating new server SASL connection
I0108 01:13:16.177373  1356 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0108 01:13:16.177399  1356 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0108 01:13:16.177502  1344 authenticator.cpp:203] Received SASL authentication start
I0108 01:13:16.177563  1344 authenticator.cpp:325] Authentication requires more steps
I0108 01:13:16.177680  1346 authenticatee.cpp:258] Received SASL authentication step
I0108 01:13:16.177848  1354 authenticator.cpp:231] Received SASL authentication step
I0108 01:13:16.177883  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0108 01:13:16.177894  1354 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0108 01:13:16.177944  1354 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0108 01:13:16.177994  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0108 01:13:16.178014  1354 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178040  1354 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178066  1354 authenticator.cpp:317] Authentication success
I0108 01:13:16.178256  1355 authenticatee.cpp:298] Authentication success
I0108 01:13:16.178315  1354 master.cpp:5475] Successfully authenticated principal 'test-principal' at slave(231)@172.17.0.2:32801
I0108 01:13:16.178356  1355 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.178710  1354 slave.cpp:860] Successfully authenticated with master master@172.17.0.2:32801
I0108 01:13:16.178865  1354 slave.cpp:1254] Will retry registration in 13.009431ms if necessary
I0108 01:13:16.179138  1350 master.cpp:4154] Registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.179628  1345 registrar.cpp:439] Applied 1 operations in 71663ns; attempting to update the 'registry'
I0108 01:13:16.180505  1356 log.cpp:683] Attempting to append 343 bytes to the log
I0108 01:13:16.180711  1352 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0108 01:13:16.181499  1350 replica.cpp:537] Replica received write request for position 3 from (7103)@172.17.0.2:32801
I0108 01:13:16.182080  1350 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 537757ns
I0108 01:13:16.182112  1350 replica.cpp:712] Persisted action at 3
I0108 01:13:16.182749  1351 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0108 01:13:16.183120  1351 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 340999ns
I0108 01:13:16.183151  1351 replica.cpp:712] Persisted action at 3
I0108 01:13:16.183177  1351 replica.cpp:697] Replica learned APPEND action at position 3
I0108 01:13:16.184787  1348 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.185287  1348 log.cpp:702] Attempting to truncate the log to 3
I0108 01:13:16.185484  1349 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0108 01:13:16.186043  1353 slave.cpp:3371] Received ping from slave-observer(230)@172.17.0.2:32801
I0108 01:13:16.186074  1345 master.cpp:4222] Registered slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.186224  1353 slave.cpp:904] Registered with master master@172.17.0.2:32801; given slave ID 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.186441  1353 fetcher.cpp:81] Clearing fetcher cache
I0108 01:13:16.186486  1349 hierarchical.cpp:465] Added slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I0108 01:13:16.186658  1346 status_update_manager.cpp:181] Resuming sending status updates
I0108 01:13:16.186885  1353 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0/slave.info'
I0108 01:13:16.186905  1350 replica.cpp:537] Replica received write request for position 4 from (7104)@172.17.0.2:32801
I0108 01:13:16.187595  1350 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 645704ns
I0108 01:13:16.187628  1350 replica.cpp:712] Persisted action at 4
I0108 01:13:16.188347  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.188475  1349 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 1.861833ms
I0108 01:13:16.188560  1348 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0108 01:13:16.188385  1353 slave.cpp:963] Forwarding total oversubscribed resources 
I0108 01:13:16.189275  1344 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.189792  1344 master.cpp:4564] Received update of slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with total oversubscribed resources 
I0108 01:13:16.189851  1348 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.204958ms
I0108 01:13:16.190150  1348 leveldb.cpp:399] Deleting ~2 keys from leveldb took 62381ns
I0108 01:13:16.190265  1348 replica.cpp:712] Persisted action at 4
I0108 01:13:16.190402  1348 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0108 01:13:16.191192  1349 sched.cpp:819] Scheduler::resourceOffers took 126783ns
I0108 01:13:16.191253  1359 hierarchical.cpp:521] Slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I0108 01:13:16.191529  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.191591  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.191627  1359 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 310808ns
I0108 01:13:16.195103  1349 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.195171  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.195205  1349 hierarchical.cpp:1079] Performed allocation for 1 slaves in 368834ns
I0108 01:13:16.205402  1351 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O0 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.205471  1351 master.cpp:2843] Authorizing principal 'ANY' to create volumes
E0108 01:13:16.206641  1351 master.cpp:1737] Dropping CREATE offer operation from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801: Not authorized to create persistent volumes as ''
I0108 01:13:16.207283  1351 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.216485  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.216562  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 983574ns
I0108 01:13:16.216915  1345 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.217514  1345 sched.cpp:819] Scheduler::resourceOffers took 82354ns
I0108 01:13:16.227466  1348 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O1 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.227843  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.228489  1344 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.228989  1346 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.229118  1346 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.229143  1346 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.229277  1346 sched.cpp:755] Will retry registration in 1.383902465secs if necessary
I0108 01:13:16.229912  1348 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.230171  1346 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.230262  1348 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.230370  1348 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0108 01:13:16.230788  1348 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0108 01:13:16.231477  1346 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.232698  1346 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.232795  1346 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.282992ms
I0108 01:13:16.233512  1348 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.233728  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.233800  1351 sched.cpp:663] Scheduler::registered took 29498ns
I0108 01:13:16.234381  1359 sched.cpp:819] Scheduler::resourceOffers took 113212ns
I0108 01:13:16.239941  1348 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.240223  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.240275  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 633949ns
I0108 01:13:16.251688  1357 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O2 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.251785  1357 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
I0108 01:13:16.253445  1352 master.cpp:3384] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.253911  1352 master.cpp:6508] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.255210  1352 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I0108 01:13:16.257128  1356 hierarchical.cpp:642] Updated allocation of framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I0108 01:13:16.257844  1356 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.262976  1344 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.263068  1344 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.435723ms
I0108 01:13:16.263535  1353 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.264181  1356 sched.cpp:819] Scheduler::resourceOffers took 139353ns
I0108 01:13:16.271931  1355 master.cpp:3671] Processing REVIVE call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.272141  1359 hierarchical.cpp:973] Removed offer filters for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.272177  1355 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O3 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272423  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.272483  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.272514  1359 hierarchical.cpp:1079] Performed allocation for 1 slaves in 344563ns
I0108 01:13:16.272924  1355 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272989  1359 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.273309  1359 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
2016-01-08 01:13:18,959:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:22,295:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:25,631:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:28,968:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1211: Failure
Failed to wait 15secs for offers
I0108 01:13:31.277577  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 disconnected
I../../src/tests/persistent_volume_tests.cpp:1204: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
0108 01:13:31.277909  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279088  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279496  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280046  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 disconnected
I0108 01:13:31.280603  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280644  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280863  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280563  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.281056  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.281097  1354 master.cpp:930] Master terminating
I0108 01:13:31.281910  1355 hierarchical.cpp:496] Removed slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:31.282516  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.282817  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.282985  1352 slave.cpp:3417] master@172.17.0.2:32801 exited
W0108 01:13:31.283144  1352 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I0108 01:13:31.313812  1346 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLNoPrincipal (15203 ms)
{noformat}",1,1,MESOS-4318,1.0
Accepting an inverse offer prints misleading logs,"Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:
{code}
W1125 10:05:53.155109 29362 master.cpp:2897] ACCEPT call used invalid offers '[ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 ]': Offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 is no longer valid
{code}

Inverse offers should not trigger this warning.",1,5,MESOS-4301,1.0
fs::enter(rootfs) does not work if 'rootfs' is read only.,"I noticed this when I was testing the unified containerizer with the bind mount backend and no volumes.

The current implementation of fs::enter will put the old root under /tmp/._old_root_.XXXXXX in the new rootfs. It assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.

To solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivot_root.",1,1,MESOS-4291,2.0
Mesos command task doesn't support volumes with image,Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ,1,2,MESOS-4285,3.0
Update isolator prepare function to use ContainerLaunchInfo,"Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers. 

By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",1,4,MESOS-4282,2.0
Correctly handle disk quota usage when volumes are bind mounted into the container.,In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).,1,2,MESOS-4281,3.0
Report volume usage through ResourceStatistics.,POSIX disk isolator does not currently report volume usage through ResourceStatistics. {{PosixDiskIsolatorProcess::usage()}} should be amended to take into account volume usage as well. ,1,2,MESOS-4263,3.0
Add mechanism for testing recovery of HTTP based executors,"Currently, the slave process generates a process ID every time it is initialized via {{process::ID::generate}} function call. This is a problem for testing HTTP executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented. 

{code}
Agent PID before:
slave(1)@127.0.0.1:43915

Agent PID after restart:
slave(2)@127.0.0.1:43915
{code}

There are a couple of ways to fix this:
- Add a constructor to {{Slave}} exclusively for testing that passes on a fixed {{ID}} instead of relying on {{ID::generate}}.
- Currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the URL in libprocess i.e. {{127.0.0.1:43915/api/v1/executor}} would delegate to {{slave(1)@127.0.0.1:43915/api/v1/executor}}. Instead of defaulting to (1), we can default to the last known active ID.",1,3,MESOS-4255,5.0
Add `dist` target to CMake solution,,1,2,MESOS-4245,3.0
Test for Quota Status Endpoint,,1,2,MESOS-4218,3.0
PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky,"{noformat}
[ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy
I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms
I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns
I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns
I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns
I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns
I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery
I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status
I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408
I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING
I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns
I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING
I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status
I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408
I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status
I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING
I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns
I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING
I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group
I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated
I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408
I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""creator-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""
I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register
I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register
I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'
I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator
I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled
W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given
I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process
I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3
I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!
I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar
I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar
I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer
I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1
I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns
I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1
I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2
I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns
I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408
I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns
I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns
I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns
I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0
I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0
I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns
I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'
I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log
I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408
I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns
I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns
I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1
I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar
I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1
I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408
I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns
I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2
I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns
I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns
I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2
I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408
I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""
I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'
I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal
I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]
I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501
I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true
I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary
I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns
I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns
I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'
I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager
I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer
I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery
I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408
I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408
I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates
I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master
I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection
I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408
I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection
I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start
I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps
I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step
I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step
I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success
I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success
I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408
I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary
I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408
I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'
I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log
I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408
I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns
I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns
I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3
I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408
I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3
I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache
I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'
I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources 
I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates
I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources 
I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408
I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms
I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.723031 31905 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.723073 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.723095 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 368889ns
I1219 09:51:32.723191 31909 sched.cpp:811] Scheduler::resourceOffers took 113921ns
I1219 09:51:32.723410 31911 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.418243ms
I1219 09:51:32.723497 31911 replica.cpp:712] Persisted action at 4
I1219 09:51:32.724326 31907 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1219 09:51:32.724758 31907 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 329678ns
I1219 09:51:32.724917 31907 leveldb.cpp:399] Deleting ~2 keys from leveldb took 58317ns
I1219 09:51:32.725025 31907 replica.cpp:712] Persisted action at 4
I1219 09:51:32.725127 31907 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1219 09:51:32.731515 31910 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.731564 31910 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.731591 31910 hierarchical.cpp:1079] Performed allocation for 1 slaves in 239271ns
I1219 09:51:32.741710 31910 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O0 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.741770 31910 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
E1219 09:51:32.742707 31910 master.cpp:1737] Dropping CREATE offer operation from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408: Not authorized to create persistent volumes as 'test-principal'
I1219 09:51:32.743219 31910 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.752542 31908 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.752590 31908 hierarchical.cpp:1079] Performed allocation for 1 slaves in 888401ns
I1219 09:51:32.753018 31908 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.753435 31908 sched.cpp:811] Scheduler::resourceOffers took 92252ns
I1219 09:51:32.761533 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.761931 31897 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O1 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.762373 31897 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.762451 31897 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.762470 31897 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.762543 31897 sched.cpp:747] Will retry registration in 465.481193ms if necessary
I1219 09:51:32.762572 31898 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.762722 31898 master.cpp:2197] Received SUBSCRIBE call for framework 'creator-framework' at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.762785 31898 master.cpp:1668] Authorizing framework principal 'creator-principal' to receive offers for role 'role1'
I1219 09:51:32.763036 31897 master.cpp:2268] Subscribing framework creator-framework with checkpointing disabled and capabilities [  ]
I1219 09:51:32.763464 31898 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763562 31897 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763605 31897 sched.cpp:655] Scheduler::registered took 20669ns
I1219 09:51:32.763804 31908 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.764343 31898 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.764382 31898 hierarchical.cpp:1079] Performed allocation for 1 slaves in 893765ns
I1219 09:51:32.764428 31898 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.764746 31898 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.765127 31898 sched.cpp:811] Scheduler::resourceOffers took 83608ns
I1219 09:51:32.773298 31900 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.773339 31900 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.773365 31900 hierarchical.cpp:1079] Performed allocation for 1 slaves in 201759ns
I1219 09:51:32.782901 31898 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O2 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.782961 31898 master.cpp:2843] Authorizing principal 'creator-principal' to create volumes
I1219 09:51:32.784190 31904 master.cpp:3362] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.784548 31904 master.cpp:6486] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.786471 31904 hierarchical.cpp:642] Updated allocation of framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I1219 09:51:32.786929 31904 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.788035 31904 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I1219 09:51:32.795177 31902 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.795250 31902 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.357898ms
I1219 09:51:32.795897 31902 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.796540 31897 sched.cpp:811] Scheduler::resourceOffers took 138880ns
I1219 09:51:32.803026 31902 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O3 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804143 31902 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.804622 31907 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804729 31907 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.805140 31897 master.cpp:3649] Processing REVIVE call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.805250 31897 hierarchical.cpp:973] Removed offer filters for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.806507 31897 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.806562 31897 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.284779ms
I1219 09:51:32.807067 31897 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
../../src/tests/persistent_volume_tests.cpp:1336: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7ffff9edb3a0, @0x7f71079798f0 { 144-byte object <F0-1B 42-14 71-7F 00-00 00-00 00-00 00-00 00-00 D0-96 02-F0 70-7F 00-00 50-97 02-F0 70-7F 00-00 20-A1 02-F0 70-7F 00-00 50-E0 01-F0 70-7F 00-00 B0-9F 02-F0 70-7F 00-00 00-32 01-F0 70-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1219 09:51:32.807899 31897 sched.cpp:811] Scheduler::resourceOffers took 406435ns
I1219 09:51:32.820523 31909 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.820611 31909 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.820642 31909 hierarchical.cpp:1079] Performed allocation for 1 slaves in 448034ns
2015-12-19 09:51:33,146:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:36,482:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:39,818:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:43,155:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:46,490:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1411: Failure
Failed to wait 15secs for offers
I1219 09:51:47.829073 31909 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 disconnected
I1219 09:51:47.829169 31909 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829200 31909 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829366 31909 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 0ns to failover
I1219 09:51:47.829720 31909 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.831614 31907 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.831748 31907 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.833314 31897 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 by master@172.17.0.3:36408
W1219 09:51:47.833421 31897 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.834002 31897 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.843332 31908 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 disconnected
I1219 09:51:47.843521 31908 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.843663 31908 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
W1219 09:51:47.844665 31908 master.hpp:1758] Master attempted to send message to disconnected framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.845077 31908 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 0ns to failover
I1219 09:51:47.844887 31903 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.845728 31903 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
../../src/tests/persistent_volume_tests.cpp:1404: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1219 09:51:47.847968 31902 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848068 31902 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848553 31902 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 by master@172.17.0.3:36408
W1219 09:51:47.848644 31902 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.848999 31902 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.849782 31912 master.cpp:930] Master terminating
I1219 09:51:47.851934 31899 hierarchical.cpp:496] Removed slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:47.855919 31907 slave.cpp:3417] master@172.17.0.3:36408 exited
W1219 09:51:47.856021 31907 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1219 09:51:47.908278 31878 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLDropCreateAndDestroy (15298 ms)
{noformat}",1,3,MESOS-4208,1.0
Race in SSL socket shutdown ,"libprocess Socket shares the ownership of the file descriptor with libevent. In
the destructor of the libprocess libevent_ssl socket, we call ssl shutdown which
is executed asynchronously. This causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. Since we set the shutdown options as SSL_RECEIVED_SHUTDOWN, we leave the any write operations to continue with possibly closed file descriptor.

This issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by OS) that has the above issue.",1,2,MESOS-4202,5.0
Disk Resource Reservation is NOT Enforced for Persistent Volumes,"If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.

Disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",1,6,MESOS-4198,3.0
Port `process/file.hpp`,,1,2,MESOS-4193,3.0
Add documentation for API Versioning,"Currently, we don't have any documentation for:

- How Mesos implements API versioning ?
- How are protobufs versioned and how does mesos handle them internally ?
- What do contributors need to do when they make a change to a external user facing protobuf ?

The relevant design doc:
https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b
",1,2,MESOS-4192,3.0
Extend `Master` to authorize persistent volumes,"This ticket is the second in a series that adds authorization support for persistent volumes.

Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",1,1,MESOS-4179,1.0
Add persistent volume support to the Authorizer,"This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.

Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.

{code}
  message Create {
    // Subjects.
    required Entity principals = 1;

    // Objects? Perhaps the kind of volume? allowed permissions?
  }

  message Destroy {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity creator_principals = 2;
  }
{code}

ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",1,1,MESOS-4178,1.0
Create a user doc for Executor HTTP API,We need a user doc similar to the corresponding one for the Scheduler HTTP API.,1,2,MESOS-4177,3.0
Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles,"When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available. 

Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  ",1,5,MESOS-4143,2.0
Implement `WindowsError` to correspond with `ErrnoError`.,"In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.

In Stout, we report these errors with `ErrnoError`.

The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",1,2,MESOS-4110,5.0
Implement `os::mkdtemp` for Windows,"Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",1,2,MESOS-4108,5.0
`os::strerror_r` breaks the Windows build,`os::strerror_r` does not exist on Windows.,1,2,MESOS-4107,1.0
Quota doesn't allocate resources on slave joining.,"See attached patch. {{framework1}} is not allocated any resources, despite the fact that the resources on {{agent2}} can safely be allocated to it without risk of violating {{quota1}}. If I understand the intended quota behavior correctly, this doesn't seem intended.

Note that if the framework is added _after_ the slaves are added, the resources on {{agent2}} are allocated to {{framework1}}.",1,8,MESOS-4102,5.0
parallel make tests does not build all test targets,"When inside 3rdparty/libprocess:
Running {{make -j8 tests}} from a clean build does not yield the {{libprocess-tests}} binary.
Running it a subsequent time triggers more compilation and ends up yielding the {{libprocess-tests}} binary.
This suggests the {{test}} target is not being built correctly.",1,2,MESOS-4099,1.0
libevent_ssl_socket assertion fails ,"Have been seeing the following socket  receive error frequently:

{code}
F1204 11:12:47.301839 54104 libevent_ssl_socket.cpp:245] Check failed: length > 0 
*** Check failure stack trace: ***
    @     0x7f73227fe5a6  google::LogMessage::Fail()
    @     0x7f73227fe4f2  google::LogMessage::SendToLog()
    @     0x7f73227fdef4  google::LogMessage::Flush()
    @     0x7f7322800e08  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f73227b93e2  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f73227b9182  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f731cbc75cc  bufferevent_run_deferred_callbacks_locked
    @     0x7f731cbbdc5d  event_base_loop
    @     0x7f73227d9ded  process::EventLoop::run()
    @     0x7f73227a3101  _ZNSt12_Bind_simpleIFPFvvEvEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f73227a305b  std::_Bind_simple<>::operator()()
    @     0x7f73227a2ff4  std::thread::_Impl<>::_M_run()
    @     0x7f731e0d1a40  (unknown)
    @     0x7f731de0a182  start_thread
    @     0x7f731db3730d  (unknown)
    @              (nil)  (unknown)

{code}

In this case this was a HTTP get over SSL. The url being:

https://dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?Expires=1449259252&Signature=Q4CQdr1LbxsiYyVebmetrx~lqDgQfHVkGxpbMM3PoISn6r07DXIzBX6~tl1iZx9uXdfr~5awH8Kxwh-y8b0dTV3mLTZAVlneZlHbhBAX9qbYMd180-QvUvrFezwOlSmX4B3idvo-zK0CarUu3Ev1hbJz5y3olwe2ZC~RXHEwzkQ_&Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q


*Steps to reproduce:*

1. Run master
2. Run slave from your build directory as  as:

{code}
 GLOG_v=1;SSL_ENABLED=1;SSL_KEY_FILE=<path_to_key>;SSL_CERT_FILE=<path_to_cert>;sudo -E ./bin/mesos-slave.sh \
      --master=127.0.0.1:5050 \                                                  
      --executor_registration_timeout=5mins \                                    
      --containerizers=mesos  \                                                  
      --isolation=filesystem/linux \                                             
      --image_providers=DOCKER  \                                                
      --docker_puller_timeout=600 \                                              
      --launcher_dir=$MESOS_BUILD_DIR/src/.libs \                                
      --switch_user=""false"" \                                                    
      --docker_puller=""registry""          
{code} 

3. Run mesos-execute from your build directory as :

{code}                                                        
    ./src/mesos-execute \                                                        
    --master=127.0.0.1:5050 \                                                    
    --command=""uname -a""  \                                                      
    --name=test \                                                                
    --docker_image=ubuntu 
{code}",1,3,MESOS-4069,8.0
Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters,"Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.

*Flakiness in task acknowledgment*
{code}
I1203 18:25:04.609817 28732 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
W1203 18:25:04.610076 28732 status_update_manager.cpp:762] Unexpected status update acknowledgement (received 6afd012e-8e88-41b2-8239-a9b852d07ca1, expecting 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for update TASK_RUNNING (UUID: 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
E1203 18:25:04.610339 28736 slave.cpp:2339] Failed to handle status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000: Duplicate acknowledgemen
{code}

This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.

*Flakiness in first inverse offer filter*
See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment.",1,5,MESOS-4059,1.0
MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky,"{code:title=Output from passed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0
I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Registered executor on ubuntu
Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 5085
I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Re-registered executor on ubuntu
Shutting down
Sending SIGTERM to process tree at pid 5085
Killing the following process trees:
[ 
-+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done 
 \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp 
]
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)
{code}

{code:title=Output from failed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0
I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
Registered executor on ubuntu
Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6
Forked command at 5132
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 5132
../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure
(usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913
*** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date ***
{code}

Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",1,12,MESOS-4047,1.0
ContentType/SchedulerTest is flaky.,"SSL build, [Ubuntu 14.04|https://github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh], non-root test run.

{noformat}
[----------] 22 tests from ContentType/SchedulerTest
[ RUN      ] ContentType/SchedulerTest.Subscribe/0
[       OK ] ContentType/SchedulerTest.Subscribe/0 (48 ms)
*** Aborted at 1448928007 (unix time) try ""date -d @1448928007"" if you are using GNU date ***
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
PC: @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
*** SIGSEGV (@0x100000030) received by PID 21320 (TID 0x2b549e5d4700) from PID 48; stack trace: ***
    @     0x2b54c95940b7 os::Linux::chained_handler()
    @     0x2b54c9598219 JVM_handle_linux_signal
    @     0x2b5496300340 (unknown)
    @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe2ea6d _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe2b1bc testing::internal::FunctionMocker<>::Invoke()
    @          0x1118aed mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x111c453 _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x111c001 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @          0x111b90d _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x111ae09 std::_Function_handler<>::_M_invoke()
    @     0x2b5493c6da09 std::function<>::operator()()
    @     0x2b5493c688ee process::AsyncExecutorProcess::execute<>()
    @     0x2b5493c6db2a _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
    @     0x2b5493c765a4 _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b54946b1201 std::function<>::operator()()
    @     0x2b549469960f process::ProcessBase::visit()
    @     0x2b549469d480 process::DispatchEvent::visit()
    @           0x9dc0ba process::ProcessBase::serve()
    @     0x2b54946958cc process::ProcessManager::resume()
    @     0x2b5494692a9c _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2b549469ccac _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2b549469cc5c _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2b549469cbee _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2b549469cb45 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2b549469cade _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2b5495b81a40 (unknown)
    @     0x2b54962f8182 start_thread
    @     0x2b549660847d (unknown)
make[3]: *** [check-local] Segmentation fault
make[3]: Leaving directory `/home/vagrant/mesos/build/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/home/vagrant/mesos/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/vagrant/mesos/build/src'
make: *** [check-recursive] Error 1
{noformat}",1,12,MESOS-4029,3.0
Pass agent work_dir to isolator modules,"Some isolator modules can benefit from access to the agent's {{work_dir}}. For example, the DVD isolator (https://github.com/emccode/mesos-module-dvdi) is currently forced to mount external volumes in a hard-coded directory. Making the {{work_dir}} accessible to the isolator via {{Isolator::recover()}} would allow the isolator to mount volumes within the agent's {{work_dir}}. This can be accomplished by simply adding an overloaded signature for {{Isolator::recover()}} which includes the {{work_dir}} as a parameter.",1,2,MESOS-4003,1.0
C++ HTTP Scheduler Library does not work with SSL enabled,"The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).

The fix should be simple:
* The library should detect if SSL is enabled.
* If SSL is enabled, connections should be made with HTTPS instead of HTTP.",1,2,MESOS-3976,3.0
Ensure resources in `QuotaInfo` protobuf do not contain `role`,"{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",1,4,MESOS-3965,3.0
/reserve and /unreserve should be permissive under a master without authentication.,"Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.",1,11,MESOS-3940,1.0
Implement AuthN handling in Master for the Scheduler endpoint,"If authentication(AuthN) is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register.

{code}
$ cat /tmp/subscribe-943257503176798091.bin | http --print=HhBb --stream --pretty=colors --auth verification:password1 POST :5050/api/v1/scheduler Accept:application/x-protobuf Content-Type:application/x-protobuf
POST /api/v1/scheduler HTTP/1.1
Connection: keep-alive
Content-Type: application/x-protobuf
Accept-Encoding: gzip, deflate
Accept: application/x-protobuf
Content-Length: 126
User-Agent: HTTPie/0.9.0
Host: localhost:5050
Authorization: Basic dmVyaWZpY2F0aW9uOnBhc3N3b3JkMQ==



+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

HTTP/1.1 401 Unauthorized
Date: Fri, 13 Nov 2015 20:00:45 GMT
WWW-authenticate: Basic realm=""Mesos master""
Content-Length: 65

HTTP schedulers are not supported when authentication is required
{code}

Authorization(AuthZ) is already supported for HTTP based frameworks.",1,5,MESOS-3923,5.0
Identify and implement test cases for handling a race between optimistic lender and tenant offers.,An example is the when lender launches the task on an agent followed by a  borrower launching a task on the same agent before the optimistic offer is rescinded. ,0,0,MESOS-3898,13.0
Identify and implement test cases for verifying eviction logic in the agent,,0,2,MESOS-3897,13.0
Add accounting for reservation slack in the allocator.,"MESOS-XXX: Optimsistic accounter

{code}
    class HierarchicalAllocatorProcess 
    {
      struct Slave
      {
        ...
        struct Optimistic 
        {
          Resources total; // The total allocation slack resources
          Resources allocated; // The allocated allocation slack resources
        };
    
        Optimistic optimistic;
      };
    }
{code}

MESOS-4146: flatten & allocationSlack for Optimistic Offer

{code}
    class Resources
    {
        // Returns a Resources object with the same amount of each resource
        // type as these Resources, but with all Resource objects marked as
        // the specified `RevocableInfo::Type`; the other attribute is not
        // affected.
        Resources flatten(Resource::RevocableInfo::Type type);

        // Return a Resources object that:
        //   - if role is given, the resources did not include role's reserved
        //     resources.
        //   - the resources's revocable type is `ALLOCATION_SLACK`
        //   - the role of resources is set to ""*""
        Resources allocationSlack(Option<string> role = None());
    }
{code}

MESOS-XXX: Allocate the allocation_slack resources to framework

{code}
    void HierarchicalAllocatorProcess::allocate(
        const hashset<SlaveID>& slaveIds_)
    {
      foreach slave; foreach role; foreach framework
      {
        Resource optimistic;

        if (framework.revocable) {
          Resources total = slaves[slaveId].optimistic.total.allocationSlack(role);
          optimistic = total - slaves[slaveId].optimistic.allocated;
        }

        ...
        offerable[frameworkId][slaveId] += resources + optimistic;

        ...
        slaves[slaveId].optimistic.allocated += optimistic;
      }
    }
{code}
  
Here's some consideration about `ALLOCATION_SLACK`:

1. 'Old' resources (available/total) did not include ALLOCATION_SLACK
2. After `Quota`, `remainingClusterResources.contains` should not check ALLOCATION_SLACK; if there no enough resources,  master can still offer ALLOCATION_SALCK resources.
3. In sorter, it'll not include ALLOCATION_SLACK; as those resources are borrowed from other role/framework
4. If either normal resources or ALLOCATION_SLACK resources are allocable/!filtered, it can be offered to framework
5. Currently, allocator will assign all ALLOCATION_SALCK resources in slave to one framework

MESOS-XXX: Update ALLOCATION_SLACK for dynamic reservation (updateAllocation)

{code}
    void HierarchicalAllocatorProcess::updateAllocation(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const vector<Offer::Operation>& operations)
    {
        ...
        Try<Resources> updatedOptimistic =
            slaves[slaveId].optimistic.total.apply(operations);
        CHECK_SOME(updatedTotal);

        slaves[slaveId].optimistic.total =
            updatedOptimistic.get().stateless().reserved().flatten(ALLOCATION_SLACK);
        ...
    }
{code}
    
MESOS-XXX: Add ALLOCATION_SLACK when slaver register/re-register (addSlave)

{code}
    void HierarchicalAllocatorProcess::addSlave(
        const SlaveID& slaveId,
        const SlaveInfo& slaveInfo,
        const Option<Unavailability>& unavailability,
        const Resources& total,
        const hashmap<FrameworkID, Resources>& used)
    {
      ...
      slaves[slaveId].optimistic.total =
          total.stateless().reserved().flatten(ALLOCATION_SLACK);
      ...
    }
{code}
  
No need to handle `removeSlave`, it'll all related info from `slaves` including `optimistic`.

MESOS-XXX: return resources to allocator (recoverResources)

{code}
    void HierarchicalAllocatorProcess::recoverResources(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const Resources& resources,
        const Option<Filters>& filters)
    {
      if (slaves.contains(slaveId))
      {
        ...
        slaves[slaveId].optimistic.allocated -= resources.allocationSlack();
        ...
      }
    }
{code}",0,1,MESOS-3896,13.0
Update reservation slack allocator state during agent failover.,,1,0,MESOS-3895,13.0
Rebuild reservation slack allocator state during master failover.,,1,1,MESOS-3894,13.0
Implement tests for verifying allocator resource math.,Write a test to ensure that the allocator performs the reservation slack calculations correctly.,1,0,MESOS-3893,8.0
"Add a helper function to the Agent to retrieve the list of executors that are using optimistically offered, revocable resources.","In the agent, add a helper function to get the list of the exeuctor using ALLOCATION_SLACK.

It's short term solution which is different the design document, because master did not have executor for command line executor. Send evicatble executors from master to slave will addess in post-MVP after MESOS-1718.

{noformat}
class Slave {
...
  // If the executor used revocable resources, add it into `evictableExecutors`
  // list.
  void addEvictableExecutor(Executor* executor);

  // If the executor used revocable resources, remove it from
  // `evictableExecutors` list.
  void removeEvictableExecutor(Executor* executor);

  // Get evictable executor ID list by `request resources`. The return value is `Result<list<Executor*>>`:
  //  - if `isError()`, there's not enough resources to launch tasks
  //  - if `isNone()`, no evictable exectuors need to be terminated
  //  - if !`isNone()`, the list of executors that need to be evicted for resources
  Result<std::list<Executor*>> getEvictableExecutors(const Resources& request);

...

  // The map of evictable executor list. If there's not enough resources,
  // the evictable executor will be terminated by slave to release resources.
  hashmap<FrameworkID, std::set<ExecutorID>> evictableExecutors;
...
}
{noformat}
",0,6,MESOS-3892,5.0
Add a helper function to the Agent to check available resources before launching a task. ,"Launching a task using revocable resources should be funnelled through an accounting system:

* If a task is launched using revocable resources, the resources must not be in use when launching the task.  If they are in use, then the task should fail to start.
* If a task is launched using reserved resources, the resources must be made available.  This means potentially evicting tasks which are using revocable resources.

Both cases could be implemented by adding a check in Slave::runTask, like a new helper method:

{noformat}
class Slave {
  ...
  // Checks if the given resources are available (i.e. not utilized)
  // for starting a task.  If not, the task should either fail to
  // start or result in the eviction of revocable resources.
  virtual process::Future<bool> checkAvailableResources(
      const Resources& resources);
  ...
}
{noformat}",1,2,MESOS-3891,5.0
Add notion of evictable task to RunTaskMessage,"{code}
// Evict Resources to launch tasks.
  message Revocation {
    optional FrameworkID framework_id = 1;
    required string role = 2;
    repeated Resource revocable_resources = 3;
  }
  repeated Revocation revocations = 5;
{code}",1,3,MESOS-3890,2.0
Modify Oversubscription documentation to explicitly forbid the QoS Controller from killing executors running on optimistically offered resources.,"The oversubcription documentation currently assumes that oversubscribed resources ({{USAGE_SLACK}}) are the only type of revocable resources.  Optimistic offers will add a second type of revocable resource ({{ALLOCATION_SLACK}}) that should not be acted upon by oversubscription components.

For example, the [oversubscription doc|http://mesos.apache.org/documentation/latest/oversubscription/] says the following:
{quote}
NOTE: If any resource used by a task or executor is revocable, the whole container is treated as a revocable container and can therefore be killed or throttled by the QoS Controller.
{quote}
which we may amend to something like:
{quote}
NOTE: If any resource used by a task or executor is revocable usage slack, the whole container is treated as an oversubscribed container and can therefore be killed or throttled by the QoS Controller.
{quote}",0,1,MESOS-3889,2.0
Support distinguishing revocable resources in the Resource protobuf.,"Add enum type into RevocableInfo: 

* Framework need to assign RevocableInfo when launching task; if it’s not assign, use reserved resources. Framework need to identify which resources it’s using
* Oversubscription resources need to assign the type by Agent (MESOS-3930)
* Update Oversubscription document that OO has over-subscribe the Allocation Slack and recommend QoS to handle the usage slack only. (MESOS-3889)

{code}
message Resource {
  ...
  message RevocableInfo {
   enum Type {
     // Under-utilized, allocated resources.  Controlled by
     // oversubscription (QoSController & ResourceEstimator).
     USAGE_SLACK = 1;

     // Unallocated, reserved resources.
     // Controlled by optimistic offers (Allocator).
     ALLOCATION_SLACK = 2; 
   }

   optional Type type = 1;
  }
 ...
  optional RevocableInfo revocable = 9;
 }
{code}
",0,2,MESOS-3888,2.0
Add a flag to master to enable optimistic offers. ,,1,2,MESOS-3887,3.0
Implement `stout/os/pstree.hpp` on Windows,,1,2,MESOS-3881,2.0
Add mtime-related fetcher tests,,1,2,MESOS-3856,2.0
Mesos does not set Content-Type for 400 Bad Request,"While integrating with the HTTP Scheduler API I encountered the following scenario.

The message below was serialized to protobuf and sent as the POST body
{code:title=message}
call {
  type: ACKNOWLEDGE,
  acknowledge: {
    uuid: <bytes>,
    agentID: { value: ""20151012-182734-16777343-5050-8978-S2"" },
    taskID: { value: ""task-1"" }
  }
}
{code}
{code:title=Request Headers}
POST /api/v1/scheduler HTTP/1.1
Content-Type: application/x-protobuf
Accept: application/x-protobuf
Content-Length: 73
Host: localhost:5050
User-Agent: RxNetty Client
{code}

I received the following response
{code:title=Response Headers}
HTTP/1.1 400 Bad Request
Date: Wed, 14 Oct 2015 23:21:36 GMT
Content-Length: 74

Failed to validate Scheduler::Call: Expecting 'framework_id' to be present
{code}

Even though my accept header made no mention of {{text/plain}} the message body returned to me is {{text/plain}}. Additionally, there is no {{Content-Type}} header set on the response so I can't even do anything intelligently in my response handler.",1,3,MESOS-3739,2.0
Implement Quota support in allocator,"The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.

A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",1,4,MESOS-3718,5.0
Update Allocator interface to support quota,"An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",1,2,MESOS-3716,3.0
Make Scheduler Library use HTTP Pipelining Abstraction in Libprocess,"Currently, the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call. This was done because there was no HTTP Pipelining abstraction in Libprocess {{process::post}}.

However once {{MESOS-3332}} is resolved, we should be now able to use the new abstraction.",1,3,MESOS-3570,8.0
Revocable task CPU shows as zero in /state.json,"The slave's state.json reports revocable task resources as zero:

{noformat}
resources: {
cpus: 0,
disk: 3071,
mem: 1248,
ports: ""[31715-31715]""
},
{noformat}

Also, there is no indication that a task uses revocable CPU. It would be great to have this type of info.",1,9,MESOS-3563,2.0
Validate that slave's work_dir is a shared mount in its own peer group when LinuxFilesystemIsolator is used.,"To address this TODO in the code:

{noformat}
src/slave/containerizer/isolators/filesystem/linux.cpp +122


// TODO(jieyu): Currently, we don't check if the slave's work_dir
// mount is a shared mount or not. We just assume it is. We cannot
// simply mark the slave as shared again because that will create a
// new peer group for the mounts. This is a temporary workaround for
// now while we are thinking about fixes.
{noformat}",1,2,MESOS-3539,3.0
Add support for exposing Accept/Decline responses for inverse offers,"Current implementation of maintenance primitives does not support exposing Accept/Decline responses of frameworks to the cluster operators. 

This functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.",1,5,MESOS-3489,2.0
LinuxFilesystemIsolator should make the slave's work_dir a shared mount.,"So that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. If we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.

{noformat}
E0921 17:35:57.268159 47010 bind.cpp:182] Failed to remove rootfs mount point '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a/backends/bind/rootfses/30f7e5e2-55d0-4d4d-a662-f8aad0d56b33': Device or resource busy
E0921 17:35:57.268349 47010 provisioner.cpp:403] Failed to remove the provisioned container directory at '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a': Device or resource busy
{noformat}",1,6,MESOS-3483,3.0
Segfault when accepting or declining inverse offers,"Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/",1,1,MESOS-3458,1.0
Support running filesystem isolation with Command Executor in MesosContainerizer,,1,1,MESOS-3428,4.0
Docker containerizer does not symlink persistent volumes into sandbox,"For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.",1,19,MESOS-3413,5.0
Removing mount point fails with EBUSY in LinuxFilesystemIsolator.,"When running the tests as root, we found PersistentVolumeTest.AccessPersistentVolume fails consistently on some platforms.

{noformat}
[ RUN      ] PersistentVolumeTest.AccessPersistentVolume
I0901 02:17:26.435140 39432 exec.cpp:133] Version: 0.25.0
I0901 02:17:26.442129 39461 exec.cpp:207] Executor registered on slave 20150901-021726-1828659978-52102-32604-S0
Registered executor on hostname
Starting task d8ff1f00-e720-4a61-b440-e111009dfdc3
sh -c 'echo abc > path1/file'
Forked command at 39484
Command exited with status 0 (pid: 39484)
../../src/tests/persistent_volume_tests.cpp:579: Failure
Value of: os::exists(path::join(directory, ""path1""))
  Actual: true
Expected: false
[  FAILED  ] PersistentVolumeTest.AccessPersistentVolume (777 ms)
{noformat}

Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount.

FYI [~jieyu] [~mcypark]",1,28,MESOS-3349,5.0
Dynamic reservations are not counted as used resources in the master,"Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.

I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section:
{code}
  // Check that the Master counts the reservation as a used resource.
  {
    Future<process::http::Response> response =
      process::http::get(master.get(), ""state.json"");
    AWAIT_READY(response);

    Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);
    ASSERT_SOME(parse);

    Result<JSON::Number> cpus =
      parse.get().find<JSON::Number>(""slaves[0].used_resources.cpus"");

    ASSERT_SOME_EQ(JSON::Number(1), cpus);
  }
{code}
and got
{noformat}
../../../src/tests/reservation_tests.cpp:168: Failure
Value of: (cpus).get()
  Actual: 0
Expected: JSON::Number(1)
Which is: 1
{noformat}

Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit",0,23,MESOS-3338,3.0
Make use of C++11 atomics,"Now that we require C++11, we can make use of std::atomic. For example:

* libprocess/process.cpp uses a bare int + __sync_synchronize() for ""running""
* __sync_synchronize() is used in logging.hpp in libprocess and fork.hpp in stout
* sched/sched.cpp uses a volatile int for ""running"" -- this is wrong, ""volatile"" is not sufficient to ensure safe concurrent access
* ""volatile"" is used in a few other places -- most are probably dubious but I haven't looked closely",1,8,MESOS-3326,2.0
Master should drop HTTP calls when it's recovering,"Much like what we do with PID based frameworks, master should drop HTTP calls if it's not the leader and/or still recovering.",1,2,MESOS-3290,3.0
EventCall Test Framework is flaky,"Observed this on ASF CI. h/t [~haosdent@gmail.com]

Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.

{code}
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_k4vXkx'
I0813 19:55:15.643579 26085 exec.cpp:443] Ignoring exited event because the driver is aborted!
Shutting down
Sending SIGTERM to process tree at pid 26061
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26062
Shutting down
Killing the following process trees:
[ 

]
Sending SIGTERM to process tree at pid 26063
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26098
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26099
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus
I0813 19:55:17.161888 26100 logging.cpp:177] Logging to STDERR
I0813 19:55:17.163625 26100 scheduler.cpp:157] Version: 0.24.0
I0813 19:55:17.175302 26100 leveldb.cpp:176] Opened db in 3.167446ms
I0813 19:55:17.176393 26100 leveldb.cpp:183] Compacted db in 1.047996ms
I0813 19:55:17.176496 26100 leveldb.cpp:198] Created db iterator in 77155ns
I0813 19:55:17.176518 26100 leveldb.cpp:204] Seeked to beginning of db in 8429ns
I0813 19:55:17.176527 26100 leveldb.cpp:273] Iterated through 0 keys in the db in 4219ns
I0813 19:55:17.176708 26100 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0813 19:55:17.178951 26136 recover.cpp:449] Starting replica recovery
I0813 19:55:17.179934 26136 recover.cpp:475] Replica is in EMPTY status
I0813 19:55:17.181970 26126 master.cpp:378] Master 20150813-195517-167907756-60249-26100 (297daca2d01a) started on 172.17.2.10:60249
I0813 19:55:17.182317 26126 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --credentials=""/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/src/webui"" --work_dir=""/tmp/mesos-II8Gua"" --zk_session_timeout=""10secs""
I0813 19:55:17.183475 26126 master.cpp:427] Master allowing unauthenticated frameworks to register
I0813 19:55:17.183536 26126 master.cpp:432] Master allowing unauthenticated slaves to register
I0813 19:55:17.183615 26126 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials'
W0813 19:55:17.183859 26126 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0813 19:55:17.183969 26123 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0813 19:55:17.184306 26126 master.cpp:469] Using default 'crammd5' authenticator
I0813 19:55:17.184661 26126 authenticator.cpp:512] Initializing server SASL
I0813 19:55:17.185104 26138 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0813 19:55:17.185972 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.186058 26135 recover.cpp:566] Updating replica status to STARTING
I0813 19:55:17.187001 26138 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 654586ns
I0813 19:55:17.187037 26138 replica.cpp:323] Persisted replica status to STARTING
I0813 19:55:17.187499 26134 recover.cpp:475] Replica is in STARTING status
I0813 19:55:17.187605 26126 auxprop.cpp:66] Initialized in-memory auxiliary property plugin
I0813 19:55:17.187710 26126 master.cpp:506] Authorization enabled
I0813 19:55:17.188657 26138 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0813 19:55:17.188853 26131 hierarchical.hpp:346] Initialized hierarchical allocator process
I0813 19:55:17.189252 26132 whitelist_watcher.cpp:79] No whitelist given
I0813 19:55:17.189321 26134 recover.cpp:195] Received a recover response from a replica in STARTING status
I0813 19:55:17.190001 26125 recover.cpp:566] Updating replica status to VOTING
I0813 19:55:17.190696 26124 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 357331ns
I0813 19:55:17.190775 26124 replica.cpp:323] Persisted replica status to VOTING
I0813 19:55:17.190970 26133 recover.cpp:580] Successfully joined the Paxos group
I0813 19:55:17.192183 26129 recover.cpp:464] Recover process terminated
I0813 19:55:17.192699 26123 slave.cpp:190] Slave started on 1)@172.17.2.10:60249
I0813 19:55:17.192741 26123 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/0""
I0813 19:55:17.194514 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.194658 26123 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.194854 26123 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.194877 26123 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.196751 26132 master.cpp:1524] The newly elected leader is master@172.17.2.10:60249 with id 20150813-195517-167907756-60249-26100
I0813 19:55:17.196797 26132 master.cpp:1537] Elected as the leading master!
I0813 19:55:17.196815 26132 master.cpp:1307] Recovering from registrar
I0813 19:55:17.197032 26138 registrar.cpp:311] Recovering registrar
I0813 19:55:17.197845 26132 slave.cpp:190] Slave started on 2)@172.17.2.10:60249
I0813 19:55:17.198420 26125 log.cpp:661] Attempting to start the writer
I0813 19:55:17.197948 26132 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/1""
I0813 19:55:17.199121 26132 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.199235 26138 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/0/meta'
I0813 19:55:17.199322 26132 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.199345 26132 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.199676 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.200085 26135 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/1/meta'
I0813 19:55:17.200317 26132 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.200371 26129 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.202003 26129 replica.cpp:477] Replica received implicit promise request with proposal 1
I0813 19:55:17.202585 26131 slave.cpp:190] Slave started on 3)@172.17.2.10:60249
I0813 19:55:17.202596 26129 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 523191ns
I0813 19:55:17.202756 26129 replica.cpp:345] Persisted promised to 1
I0813 19:55:17.202770 26132 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.203061 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.202663 26131 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/2""
I0813 19:55:17.203819 26131 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.203930 26131 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.203948 26131 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.204674 26137 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/2/meta'
I0813 19:55:17.205178 26135 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.205323 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.205521 26136 slave.cpp:4069] Finished recovery
I0813 19:55:17.206074 26136 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.206424 26128 slave.cpp:4069] Finished recovery
I0813 19:55:17.206722 26137 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.206858 26136 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.206902 26138 slave.cpp:4069] Finished recovery
I0813 19:55:17.206962 26128 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249
I0813 19:55:17.208364 26136 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.208608 26136 slave.cpp:720] Detecting new master
I0813 19:55:17.208839 26138 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.209216 26123 coordinator.cpp:231] Coordinator attemping to fill missing position
I0813 19:55:17.209247 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209259 26128 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209322 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209364 26128 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209344 26138 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209455 26128 slave.cpp:720] Detecting new master
I0813 19:55:17.209492 26138 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209573 26128 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209601 26138 slave.cpp:720] Detecting new master
I0813 19:55:17.209730 26138 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209883 26136 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.211266 26136 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0813 19:55:17.211771 26136 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 462128ns
I0813 19:55:17.211797 26136 replica.cpp:679] Persisted action at 0
I0813 19:55:17.212980 26130 replica.cpp:511] Replica received write request for position 0
I0813 19:55:17.213124 26130 leveldb.cpp:438] Reading position from leveldb took 67075ns
I0813 19:55:17.213580 26130 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 301649ns
I0813 19:55:17.213603 26130 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214284 26123 replica.cpp:658] Replica received learned notice for position 0
I0813 19:55:17.214622 26123 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284547ns
I0813 19:55:17.214648 26123 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214675 26123 replica.cpp:664] Replica learned NOP action at position 0
I0813 19:55:17.215420 26136 log.cpp:677] Writer started with ending position 0
I0813 19:55:17.217463 26133 leveldb.cpp:438] Reading position from leveldb took 47943ns
I0813 19:55:17.220762 26125 registrar.cpp:344] Successfully fetched the registry (0B) in 23.649024ms
I0813 19:55:17.221081 26125 registrar.cpp:443] Applied 1 operations in 136902ns; attempting to update the 'registry'
I0813 19:55:17.223667 26133 log.cpp:685] Attempting to append 174 bytes to the log
I0813 19:55:17.223778 26125 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0813 19:55:17.224516 26127 replica.cpp:511] Replica received write request for position 1
I0813 19:55:17.225009 26127 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 466230ns
I0813 19:55:17.225042 26127 replica.cpp:679] Persisted action at 1
I0813 19:55:17.225653 26126 replica.cpp:658] Replica received learned notice for position 1
I0813 19:55:17.225953 26126 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 286966ns
I0813 19:55:17.225975 26126 replica.cpp:679] Persisted action at 1
I0813 19:55:17.226013 26126 replica.cpp:664] Replica learned APPEND action at position 1
I0813 19:55:17.227545 26137 registrar.cpp:488] Successfully updated the 'registry' in 6.328064ms
I0813 19:55:17.227722 26137 registrar.cpp:374] Successfully recovered registrar
I0813 19:55:17.227918 26124 log.cpp:704] Attempting to truncate the log to 1
I0813 19:55:17.228024 26133 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0813 19:55:17.228193 26131 master.cpp:1334] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0813 19:55:17.228659 26127 replica.cpp:511] Replica received write request for position 2
I0813 19:55:17.228972 26127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 297903ns
I0813 19:55:17.229004 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229565 26127 replica.cpp:658] Replica received learned notice for position 2
I0813 19:55:17.229837 26127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260326ns
I0813 19:55:17.229899 26127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48697ns
I0813 19:55:17.229923 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229956 26127 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0813 19:55:17.325634 26138 slave.cpp:1209] Will retry registration in 445.955946ms if necessary
I0813 19:55:17.326088 26124 master.cpp:3635] Registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.327446 26124 registrar.cpp:443] Applied 1 operations in 231072ns; attempting to update the 'registry'
I0813 19:55:17.330252 26136 log.cpp:685] Attempting to append 344 bytes to the log
I0813 19:55:17.330407 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0813 19:55:17.331418 26128 replica.cpp:511] Replica received write request for position 3
I0813 19:55:17.331753 26128 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 264140ns
I0813 19:55:17.331778 26128 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332324 26133 replica.cpp:658] Replica received learned notice for position 3
I0813 19:55:17.332809 26133 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 313064ns
I0813 19:55:17.332834 26133 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332865 26133 replica.cpp:664] Replica learned APPEND action at position 3
I0813 19:55:17.334211 26132 registrar.cpp:488] Successfully updated the 'registry' in 6.668032ms
I0813 19:55:17.334430 26127 log.cpp:704] Attempting to truncate the log to 3
I0813 19:55:17.334566 26132 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0813 19:55:17.335283 26129 replica.cpp:511] Replica received write request for position 4
I0813 19:55:17.335615 26127 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:17.335816 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 458268ns
I0813 19:55:17.335908 26137 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.335983 26129 replica.cpp:679] Persisted action at 4
I0813 19:55:17.336019 26136 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.336073 26136 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.336220 26127 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.336328 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.336599 26138 replica.cpp:658] Replica received learned notice for position 4
I0813 19:55:17.336910 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.336957 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 580663ns
I0813 19:55:17.337016 26136 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/1/meta/slaves/20150813-195517-167907756-60249-26100-S0/slave.info'
I0813 19:55:17.337035 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 403607ns
I0813 19:55:17.337138 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 77040ns
I0813 19:55:17.337167 26138 replica.cpp:679] Persisted action at 4
I0813 19:55:17.337208 26138 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0813 19:55:17.337514 26136 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.337745 26131 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.338240 26131 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.338479 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.338505 26131 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 216259ns
I0813 19:55:17.504086 26124 slave.cpp:1209] Will retry registration in 1.92618421secs if necessary
I0813 19:55:17.504408 26124 master.cpp:3635] Registering slave at slave(3)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.505203 26124 registrar.cpp:443] Applied 1 operations in 144314ns; attempting to update the 'registry'
I0813 19:55:17.507616 26124 log.cpp:685] Attempting to append 511 bytes to the log
I0813 19:55:17.507796 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5
I0813 19:55:17.508735 26128 replica.cpp:511] Replica received write request for position 5
I0813 19:55:17.509291 26128 leveldb.cpp:343] Persisting action (530 bytes) to leveldb took 527776ns
I0813 19:55:17.509328 26128 replica.cpp:679] Persisted action at 5
I0813 19:55:17.509945 26124 replica.cpp:658] Replica received learned notice for position 5
I0813 19:55:17.510393 26124 leveldb.cpp:343] Persisting action (532 bytes) to leveldb took 438543ns
I0813 19:55:17.510416 26124 replica.cpp:679] Persisted action at 5
I0813 19:55:17.510437 26124 replica.cpp:664] Replica learned APPEND action at position 5
I0813 19:55:17.511907 26125 registrar.cpp:488] Successfully updated the 'registry' in 6624us
I0813 19:55:17.512225 26138 log.cpp:704] Attempting to truncate the log to 5
I0813 19:55:17.512305 26136 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6
I0813 19:55:17.513066 26133 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:17.513242 26133 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.513221 26126 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.513089 26129 replica.cpp:511] Replica received write request for position 6
I0813 19:55:17.513393 26133 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.513380 26138 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.513805 26132 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.513949 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 340511ns
I0813 19:55:17.514046 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.514050 26129 replica.cpp:679] Persisted action at 6
I0813 19:55:17.514195 26133 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/2/meta/slaves/20150813-195517-167907756-60249-26100-S1/slave.info'
I0813 19:55:17.514140 26138 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 417609ns
I0813 19:55:17.514704 26133 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.514708 26138 replica.cpp:658] Replica received learned notice for position 6
I0813 19:55:17.514880 26133 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.515244 26127 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.515454 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 640882ns
I0813 19:55:17.515522 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 56550ns
I0813 19:55:17.515547 26138 replica.cpp:679] Persisted action at 6
I0813 19:55:17.515581 26138 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0813 19:55:17.515802 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.515866 26127 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 591007ns
I0813 19:55:17.984196 26135 slave.cpp:1209] Will retry registration in 1.542495291secs if necessary
I0813 19:55:17.984391 26138 master.cpp:3635] Registering slave at slave(1)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.985170 26133 registrar.cpp:443] Applied 1 operations in 202126ns; attempting to update the 'registry'
I0813 19:55:17.987498 26133 log.cpp:685] Attempting to append 678 bytes to the log
I0813 19:55:17.987656 26123 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7
I0813 19:55:17.988704 26138 replica.cpp:511] Replica received write request for position 7
I0813 19:55:17.989223 26138 leveldb.cpp:343] Persisting action (697 bytes) to leveldb took 490422ns
I0813 19:55:17.989248 26138 replica.cpp:679] Persisted action at 7
I0813 19:55:17.989972 26126 replica.cpp:658] Replica received learned notice for position 7
I0813 19:55:17.990401 26126 leveldb.cpp:343] Persisting action (699 bytes) to leveldb took 404333ns
I0813 19:55:17.990420 26126 replica.cpp:679] Persisted action at 7
I0813 19:55:17.990440 26126 replica.cpp:664] Replica learned APPEND action at position 7
I0813 19:55:17.994066 26123 registrar.cpp:488] Successfully updated the 'registry' in 8.788224ms
I0813 19:55:17.994436 26134 log.cpp:704] Attempting to truncate the log to 7
I0813 19:55:17.994575 26123 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8
I0813 19:55:17.995070 26134 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:17.995291 26134 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.995319 26134 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.995246 26129 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.995565 26123 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.995579 26129 replica.cpp:511] Replica received write request for position 8
I0813 19:55:17.996016 26134 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/0/meta/slaves/20150813-195517-167907756-60249-26100-S2/slave.info'
I0813 19:55:17.996039 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 440511ns
I0813 19:55:17.996067 26129 replica.cpp:679] Persisted action at 8
I0813 19:55:17.996294 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.996556 26134 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.996623 26133 replica.cpp:658] Replica received learned notice for position 8
I0813 19:55:17.997095 26134 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.997263 26133 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 442619ns
I0813 19:55:17.997385 26133 leveldb.cpp:401] Deleting ~2 keys from leveldb took 95741ns
I0813 19:55:17.997413 26133 replica.cpp:679] Persisted action at 8
I0813 19:55:17.997465 26133 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0813 19:55:17.997756 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.997925 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 1.14489ms
I0813 19:55:17.998159 26128 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.998445 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.998471 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 218856ns
I0813 19:55:18.190146 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:18.190217 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 637042ns
I0813 19:55:19.191346 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:19.191915 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.215355ms
I0813 19:55:20.193631 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:20.193709 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 834491ns
I0813 19:55:21.194805 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:21.194870 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 536547ns
I0813 19:55:22.196143 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:22.196216 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 755140ns
I0813 19:55:23.197412 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:23.197979 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.223984ms
I0813 19:55:24.199429 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:24.199735 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 904654ns
I0813 19:55:25.200978 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:25.201206 26127 hierarchical.hpp:908] Performed allocation for 3 slaves in 939979ns
I0813 19:55:26.203023 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:26.203101 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 721178ns
I0813 19:55:27.204815 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:27.204888 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 767983ns
I0813 19:55:28.206374 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:28.206444 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 745214ns
I0813 19:55:29.207515 26124 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:29.207579 26124 hierarchical.hpp:908] Performed allocation for 3 slaves in 551217ns
I0813 19:55:30.208966 26136 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:30.209053 26136 hierarchical.hpp:908] Performed allocation for 3 slaves in 649887ns
I0813 19:55:31.210078 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:31.210144 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 558919ns
I0813 19:55:32.211027 26130 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211045 26129 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211084 26132 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211386 26129 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211688 26132 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211853 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:32.212035 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 898985ns
I0813 19:55:32.212169 26133 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.336745 26135 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:32.514333 26129 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:32.996134 26128 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:33.213248 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:33.213326 26128 hierarchical.hpp:908] Performed allocation for 3 slaves in 827511ns
I0813 19:55:34.214326 26125 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:34.214391 26125 hierarchical.hpp:908] Performed allocation for 3 slaves in 546422ns
I0813 19:55:35.215909 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:35.215973 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 627190ns
I0813 19:55:36.217156 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:36.217339 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 906249ns
I0813 19:55:37.218739 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:37.219169 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.102465ms
I0813 19:55:38.220641 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:38.220711 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 643146ns
I0813 19:55:39.221976 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:39.222118 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 845334ns
I0813 19:55:40.223338 26129 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:40.223546 26129 hierarchical.hpp:908] Performed allocation for 3 slaves in 849995ns
I0813 19:55:41.225558 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:41.225752 26138 hierarchical.hpp:908] Performed allocation for 3 slaves in 958480ns
I0813 19:55:42.227176 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:42.227378 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 927048ns
I0813 19:55:43.228813 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:43.229441 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.310118ms
I0813 19:55:44.230828 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:44.231142 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 896369ns
I0813 19:55:45.232656 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:45.232903 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.357693ms
I0813 19:55:46.234973 26137 hierarchical.hpp:1008
{code}",1,24,MESOS-3273,5.0
HTTP requests with nested path are not properly handled by libprocess,"For example, if master adds a route ""/api/v1/scheduler"",  a handler named ""api/v1/scheduler"" is added to 'master' libprocess.

But when a request is posted to the above path, process::visit() looks for a http handler named ""api"" instead of ""api/v1/scheduler"".

Ideally libprocess should look for handlers in the following preference order:

""api/v1/scheduler""  --> ""api/v1"" --> ""api""

",1,2,MESOS-3237,2.0
Fix master metrics for scheduler calls,"Currently the master increments metrics for old style messages from the driver but not when it receives Calls. Since the driver is now sending Calls, master should update metrics correctly.",1,2,MESOS-3195,3.0
ContainerInfo::Image::AppC::id should be optional,"As I commented here: https://reviews.apache.org/r/34136/

Currently ContainerInfo::Image::Appc is defined as the following

{noformat:title=}
    message AppC {
      required string name = 1;
      required string id = 2;
      optional Labels labels = 3;
    }
{noformat}

In which the {{id}} is a required field. When users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use {{ubuntu}} or {{ubuntu:latest}} and seldom a SHA512 ID) and we should change it to be optional.

The motivating scenario is that: if the frameworks in the Mesos use something like {{image=ubuntu:14.04""}} to run a task and {{image=ubuntu}} defaults to {{image=ubuntu:latest}}, the operator can swap the latest version for all new tasks requesting {{image=ubuntu}}. If they allow users to specify {{image=ubuntu:live}}, they can swap the live version under the covers as well. This allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",1,2,MESOS-3192,1.0
Perform a self bind mount of rootfs itself in fs::chroot::enter.,"Syscall 'pivot_root' requires that the old and the new root are not in the same filesystem. Otherwise, the user will receive a ""Device or resource busy"" error.

Currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot_root can succeed. The drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics.

For instance, in the test, we create a test rootfs by copying the host files. We need to do a self bind mount so that we can pivot_root on it. That pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount:
https://github.com/apache/mesos/blob/master/src/tests/containerizer/launch_tests.cpp#L96-L102

What I propose is that we always perform a recursive self bind mount of rootfs itself in fs::chroot::enter (after enter the new mount namespace). Seems that this is also done in libcontainer:
https://github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go#L402",1,2,MESOS-3178,2.0
Design doc for docker image registry client,Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. ,1,1,MESOS-3166,3.0
Port bootstrap to CMake,"Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",1,2,MESOS-3134,5.0
Updating persistent volumes after slave restart is problematic.,"Just realize that while reviewing https://reviews.apache.org/r/34135

Since we don't checkpoint 'resources' in Mesos containerizer, when slave restarts and recovers, the 'resources' in Container struct will be empty, but there are symlinks exists in the sandbox.

We'll end up with trying to create already exist symlinks (and fail). I think we should ignore the creation if it already exists.",1,2,MESOS-3124,3.0
Always disable SSLV2,"The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",1,2,MESOS-3121,2.0
Master doesn't properly handle SUBSCRIBE call,"Master::subscribe() incorrectly handles re-registration. It handles it as a registration request (not ""re-registration"") because of a bug in the if loop (should have been !frameworkInfo.has_id()).

{code}
void Master::subscribe(
    const UPID& from,
    const scheduler::Call::Subscribe& subscribe)
{
  const FrameworkInfo& frameworkInfo = subscribe.framework_info();

  // TODO(vinod): Instead of calling '(re-)registerFramework()' from
  // here refactor those methods to call 'subscribe()'.
  if (frameworkInfo.has_id() || frameworkInfo.id() == """") {
    registerFramework(from, frameworkInfo);
  } else {
    reregisterFramework(from, frameworkInfo, subscribe.force());
  }
}

{code}",1,2,MESOS-3055,2.0
SSL tests can fail depending on hostname configuration,"Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate.
We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",1,3,MESOS-3005,3.0
"Create a ""demo"" HTTP API client","We want to create a simple ""demo"" HTTP API Client (in Java, Python or Go) that can serve as an ""example framework"" for people who will want to use the new API for their Frameworks.

The scope should be fairly limited (eg, launching a simple Container task?) but sufficient to exercise most of the new API endpoint messages/capabilities.

Scope: TBD

Non-Goals: 

- create a ""best-of-breed"" Framework to deliver any specific functionality;
- create an Integration Test for the HTTP API.",1,1,MESOS-3001,8.0
SSL connection failure causes failed CHECK.,"{code}
[ RUN      ] SSLTest.BasicSameProcess
F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self->bev' Must be non NULL
{code}",1,2,MESOS-2997,3.0
SSL tests don't work with --gtest_shuffle,,1,1,MESOS-2975,3.0
SSL tests don't work with --gtest_repeat,"commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Wed Jul 1 16:16:52 2015 -0700

    MESOS-2973: Allow SSL tests to run using gtest_repeat.
    
    The SSL ctx object carried some settings between reinitialize()
    calls. Re-construct the object to avoid this state transition.
    
    Review: https://reviews.apache.org/r/36074",1,2,MESOS-2973,3.0
mesos fails to compile under mac when libssl and libevent are enabled,"../configure --enable-debug --enable-libevent --enable-ssl && make

produces the following error:

poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp
libtool: compile:  g++ -DPACKAGE_NAME=\""libprocess\"" -DPACKAGE_TARNAME=\""libprocess\"" -DPACKAGE_VERSION=\""0.0.1\"" ""-DPACKAGE_STRING=\""libprocess 0.0.1\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libprocess\"" -DVERSION=\""0.0.1\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o
mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo
mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo
mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo
mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo
In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11:
In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9:
../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'
 set(u);
     ^
../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here
 return accept_queue.get()
        ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'process::network::Socket &&' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'const process::network::Socket &' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here
 bool set(const T& _t);
                   ^
1 error generated.
make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1
make[4]: *** Waiting for unfinished jobs....
mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo
mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo
mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo
make[3]: *** [all-recursive] Error 1
make[2]: *** [all-recursive] Error 1
make[1]: *** [all] Error 2
make: *** [all-recursive] Error 1",1,8,MESOS-2943,2.0
Framework can overcommit oversubscribable resources during master failover.,"This is due to a bug in the hierarchical allocator. Here is the sequence of events:

1) slave uses a fixed resource estimator which advertise 4 revocable cpus
2) a framework A launches a task that uses all the 4 revocable cpus
3) master fails over
4) slave re-registers with the new master, and sends UpdateSlaveMessage with 4 revocable cpus as oversubscribed resources
5) framework A hasn't registered yet, therefore, the slave's available resources will be 4 revocable cpus
6) framework A registered and will receive an additional 4 revocable cpus. So it can launch another task with 4 revocable cpus (that means 8 total!)

The problem is due to the way we calculate 'allocated' resource in allocator when 'updateSlave'. If the framework is not registered, the 'allocation' below is not accurate (check that if block in 'addSlave').

{code}
template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::updateSlave(
    const SlaveID& slaveId,
    const Resources& oversubscribed)
{
  CHECK(initialized);
  CHECK(slaves.contains(slaveId));

  // Check that all the oversubscribed resources are revocable.
  CHECK_EQ(oversubscribed, oversubscribed.revocable());

  // Update the total resources.

  // First remove the old oversubscribed resources from the total.
  slaves[slaveId].total -= slaves[slaveId].total.revocable();

  // Now add the new estimate of oversubscribed resources.
  slaves[slaveId].total += oversubscribed;

  // Now, update the total resources in the role sorter.
  roleSorter->update(
      slaveId,
      slaves[slaveId].total.unreserved());

  // Calculate the current allocation of oversubscribed resources.
  Resources allocation;
  foreachkey (const std::string& role, roles) {
    allocation += roleSorter->allocation(role, slaveId).revocable();
  }

  // Update the available resources.

  // First remove the old oversubscribed resources from available.
  slaves[slaveId].available -= slaves[slaveId].available.revocable();

  // Now add the new estimate of available oversubscribed resources.
  slaves[slaveId].available += oversubscribed - allocation;

  LOG(INFO) << ""Slave "" << slaveId << "" ("" << slaves[slaveId].hostname
            << "") updated with oversubscribed resources "" << oversubscribed
            << "" (total: "" << slaves[slaveId].total
            << "", available: "" << slaves[slaveId].available << "")"";

  allocate(slaveId);
}

template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::addSlave(
    const SlaveID& slaveId,
    const SlaveInfo& slaveInfo,
    const Resources& total,
    const hashmap<FrameworkID, Resources>& used)
{
  CHECK(initialized);
  CHECK(!slaves.contains(slaveId));

  roleSorter->add(slaveId, total.unreserved());

  foreachpair (const FrameworkID& frameworkId,
               const Resources& allocated,
               used) {
    if (frameworks.contains(frameworkId)) {
      const std::string& role = frameworks[frameworkId].role;

      // TODO(bmahler): Validate that the reserved resources have the
      // framework's role.

      roleSorter->allocated(role, slaveId, allocated.unreserved());
      frameworkSorters[role]->add(slaveId, allocated);
      frameworkSorters[role]->allocated(
          frameworkId.value(), slaveId, allocated);
    }
  }
  ...
}
{code}",1,2,MESOS-2919,3.0
Sandbox URL doesn't work in web-ui when using SSL,"The links to the sandbox in the web ui don't work when ssl is enabled. 
This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files.
The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.",1,2,MESOS-2890,3.0
Add SSL switch to python configuration,The python egg requires explicit dependencies for SSL. Add these to the python configuration if ssl is enabled.,1,3,MESOS-2889,3.0
Convert PortMappingStatistics to use automatic JSON encoding/decoding,"Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.

This change will simplify the implementation of MESOS-2332.",1,2,MESOS-2874,2.0
OversubscriptionTest.FixedResourceEstimator is flaky,"Came up in https://reviews.apache.org/r/35395/

{code}
[ RUN      ] OversubscriptionTest.FixedResourceEstimator
I0613 13:41:02.604904 19367 exec.cpp:132] Version: 0.23.0
I0613 13:41:02.610995 19398 exec.cpp:206] Executor registered on slave 20150613-134102-3142697795-48295-13678-S0
Registered executor on pomona.apache.org
Starting task 7d78a3ef-2de9-46c9-811c-b2c0e2d50578
Forked command at 19410
sh -c 'sleep 1000'
../../src/tests/oversubscription_tests.cpp:579: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffffbc0c4e0, @0x2ade2bffa910 96-byte object <50-3E D7-22 DE-2A 00-00 00-00 00-00 00-00 00-00 D0-C4 00-48 DE-2A 00-00 50-71 AC-01 00-00 00-00 01-00 00-00 02-00 00-00 50-71 AC-01 00-00 00-00 B0-66 00-48 DE-2A 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-2A 00-00 E7-17 A8-BB 0C-5F D5-41 10-31 01-48 DE-2A 00-00 00-00 00-00 4B-03 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
[  FAILED  ] OversubscriptionTest.FixedResourceEstimator (714 ms)
{code}",1,4,MESOS-2869,1.0
Slave should send oversubscribed resource information after master failover.,"After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",1,2,MESOS-2866,3.0
Master crashes when framework changes principal on re-registration,The master should be updated to avoid crashing when a framework re-registers with a different principal.,1,9,MESOS-2842,5.0
Flaky test: FetcherCacheHttpTest.HttpCachedSerialized,"FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:

[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b10488ff6c0  google::LogMessage::Fail()
    @     0x2b10488ff60c  google::LogMessage::SendToLog()
    @     0x2b10488ff00e  google::LogMessage::Flush()
    @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()
    @           0x9721e4  _CheckFatal::~_CheckFatal()
    @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()
    @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x114e1df  testing::Test::Run()
    @          0x114e902  testing::TestInfo::Run()
    @          0x114ee8a  testing::TestCase::Run()
    @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()
    @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1152a60  testing::UnitTest::Run()
    @           0xcbc50f  main
    @     0x2b104af78ec5  (unknown)
    @           0x867559  (unknown)
make[4]: *** [check-local] Aborted
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build'
make: *** [distcheck] Error 1
",1,3,MESOS-2815,2.0
Slave should call into resource estimator whenever it wants to forward oversubscribed resources,"Currently, the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources.

Now that the slave only sends updates when there is a change from the previous estimate, it can just poll the resource estimator whenever it wants to send an estimate. One advantage with this is that if the estimator is slow to respond, the slave doesn't keep forwarding estimates with the stale 'oversubscribable' value causing more revocable tasks to be unintentionally launched.",1,2,MESOS-2808,3.0
Allow Resource Estimator to get Resource Usage information.,"This includes two things:
1) We need to expose ResourceMonitor::Usage so that module writers can access it. We could define a protobuf message for that.
2) We need to allow ResourceEstimator to call 'ResourceMonitor::usages()'. We could either expose the ResourceMonitor, or pass in a lambda to the resources estimator.",1,0,MESOS-2764,5.0
Extend queueing discipline wrappers to expose network isolator statistics,Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics.,1,1,MESOS-2750,3.0
Exposing Resources along with ResourceStatistics from resource monitor,"Right now, the resource monitor returns a Usage which contains ContainerId, ExecutorInfo and ResourceStatistics. In order for resource estimator/qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the Resources that are currently assigned to the container.

This requires us the change the containerizer interface to get the Resources as well while calling 'usage()'.",1,5,MESOS-2741,5.0
Change the interaction between the slave and the resource estimator from polling to pushing ,"This will make the semantics more clear. The resource estimator can control the speed of sending resources estimation to the slave.

To avoid cyclic dependency, slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when there's a new estimation ready. The callback will be a defer to the slave's main event queue.",1,12,MESOS-2735,3.0
Design doc for the Executor HTTP API,"This tracks the design of the Executor HTTP API.
",1,8,MESOS-2708,2.0
C++ Scheduler library should send HTTP Calls to master,"Once the scheduler library sends Call messages, we should update it to send Calls as HTTP requests to ""/call"" endpoint on master.",1,5,MESOS-2552,3.0
Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1,4,MESOS-2514,1.0
Create synchronous validations for Calls,/call endpoint will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a 4xx code. We have to create a mechanism that will validate the 'request' and send back the appropriate code.,1,6,MESOS-2497,8.0
Refactor validators in Master.,"There are several motivation for this. We are in the process of adding dynamic reservations and persistent volumes support in master. To do that, master needs to validate relevant operations from the framework (See Offer::Operation in mesos.proto). The existing validator style in master is hard to extend, compose and re-use.

Another motivation for this is for unit testing (MESOS-1064). Right now, we write integration tests for those validators which is unfortunate.",1,1,MESOS-2305,3.0
HookTest.VerifySlaveLaunchExecutorHook is flaky,"Observed this on internal CI

{code}
[ RUN      ] HookTest.VerifySlaveLaunchExecutorHook
Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME'
I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms
I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns
I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns
I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns
I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns
I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery
I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status
I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING
I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns
I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING
I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status
I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING
I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns
I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING
I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group
I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated
I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials'
I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled
I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given
I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720
I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master!
I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar
I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar
I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer
I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns
I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1
I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns
I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0
I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0
I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns
I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns
I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0
I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns
I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0
I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns
I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms
I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry'
I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1
I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns
I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1
I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1
I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns
I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1
I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1
I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2
I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns
I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2
I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2
I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms
I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar
I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns
I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns
I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2
I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018
I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential'
I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal
I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19
I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false
W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta'
I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager
I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery
I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master
I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018
I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success
I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success
I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018
I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0
I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary
I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry'
I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success
I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success
I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary
I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms
I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns
I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns
I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3
I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary
I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress
I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3
I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns
I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3
I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms
I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3
I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns
I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4
I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018
I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms
I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4
I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4
I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms
I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms
I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns
I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4
I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19)
I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.940467  4736 test_hook_module.cpp:52] Executing 'masterLaunchTaskLabelDecorator' hook
I0114 18:51:34.941490  4740 slave.cpp:1130] Got assigned task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.942873  4740 slave.cpp:1245] Launching task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.943469  4740 test_hook_module.cpp:71] Executing 'slaveLaunchExecutorEnvironmentDecorator' hook
I0114 18:51:34.946705  4740 slave.cpp:3921] Launching executor default of framework 20150114-185134-2272962752-57018-4720-0000 in work directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.956496  4740 exec.cpp:147] Version: 0.22.0
I0114 18:51:34.960752  4737 exec.cpp:197] Executor started at: executor(56)@192.168.122.135:57018 with pid 4720
I0114 18:51:34.964501  4740 slave.cpp:1368] Queuing task '1' for executor default of framework '20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.965133  4740 slave.cpp:566] Successfully attached file '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.965605  4740 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 from executor(56)@192.168.122.135:57018
I0114 18:51:34.966933  4734 exec.cpp:221] Executor registered on slave 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.968889  4740 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.969743  4740 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185134-2272962752-57018-4720-0000' in container 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.973484  4734 exec.cpp:233] Executor::registered took 4.814445ms
I0114 18:51:34.974081  4734 exec.cpp:308] Executor asked to run task '1'
I0114 18:51:34.974431  4734 exec.cpp:317] Executor::launchTask took 184910ns
I0114 18:51:34.975292  4720 sched.cpp:1471] Asked to stop the driver
I0114 18:51:34.975817  4738 sched.cpp:808] Stopping framework '20150114-185134-2272962752-57018-4720-0000'
I0114 18:51:34.975697  4720 master.cpp:654] Master terminating
W0114 18:51:34.976610  4720 master.cpp:4980] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_STAGING
I0114 18:51:34.977880  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.978196  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185134-2272962752-57018-4720-S0 from framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.982658  4735 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:51:34.983065  4735 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:51:35.029485  4720 slave.cpp:495] Slave terminating
I0114 18:51:35.034024  4720 slave.cpp:1585] Asked to shut down framework 20150114-185134-2272962752-57018-4720-0000 by @0.0.0.0:0
I0114 18:51:35.034335  4720 slave.cpp:1610] Shutting down framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:35.034857  4720 slave.cpp:3198] Shutting down executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
tests/hook_tests.cpp:271: Failure
Value of: os::isfile(path.get())
  Actual: true
Expected: false
[  FAILED  ] HookTest.VerifySlaveLaunchExecutorHook (412 ms)

{code}",1,15,MESOS-2226,3.0
Large number of connections slows statistics.json responses.,"We observed that in our production environment with network monitoring being turned on.

If there are many connections (> 10^4) in a container, getting socket information is expensive. It might take 1min to process all the socket information.

One of the reason is that the library we are using (libnl) is not so optimized. Cong Wang has already submitted a patch:
http://lists.infradead.org/pipermail/libnl/2014-November/001715.html",1,2,MESOS-2147,2.0
Container network stats reported by the port mapping isolator is the reverse of the actual network stats.,"Looks like the TX/RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX/RX data from veth on the host.

Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.

{noformat}
[jyu@... ~]$ sudo ip netns exec 24926 /sbin/ip -s link show dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    46030857691178 12561038581 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    29792886058561 15036798198 0       0       0       0      
[jyu@... ~]$ ip -s link show dev mesos24926
7412: mesos24926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    29793066979551 15036894749 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    46031126366116 12561113732 0       0       0       0
{noformat}",1,1,MESOS-1989,1.0
Completed tasks remains in TASK_RUNNING when framework is disconnected,"We have run into a problem that cause tasks which completes, when a framework is disconnected and has a fail-over time, to remain in a running state even though the tasks actually finishes. This hogs the cluster and gives users a inconsistent view of the cluster state. Going to the slave, the task is finished. Going to the master, the task is still in a non-terminal state. When the scheduler reattaches or the failover timeout expires, the tasks finishes correctly. The current workflow of this scheduler has a long fail-over timeout, but may on the other hand never reattach.

Here is a test framework we have been able to reproduce the issue with: https://gist.github.com/nqn/9b9b1de9123a6e836f54
It launches many short-lived tasks (1 second sleep) and when killing the framework instance, the master reports the tasks as running even after several minutes: http://cl.ly/image/2R3719461e0t/Screen%20Shot%202014-09-10%20at%203.19.39%20PM.png

When clicking on one of the slaves where, for example, task 49 runs; the slave knows that it completed: http://cl.ly/image/2P410L3m1O1N/Screen%20Shot%202014-09-10%20at%203.21.29%20PM.png

Here is the log of a mesos-local instance where I reproduced it: https://gist.github.com/nqn/f7ee20601199d70787c0 (Here task 10 to 19 are stuck in running state).
There is a lot of output, so here is a filtered log for task 10: https://gist.github.com/nqn/a53e5ea05c5e41cd5a7d

The problem turn out to be an issue with the ack-cycle of status updates:
If the framework disconnects (with a failover timeout set), the status update manage on the slaves will keep trying to send the front of status update stream to the master (which in turn forwards it to the framework). If the first status update after the disconnect is terminal, things work out fine; the master pick the terminal state up, removes the task and release the resources.
If, on the other hand, one non-terminal status is in the stream. The master will never know that the task finished (or failed) before the framework reconnects.

During a discussion on the dev mailing list (http://mail-archives.apache.org/mod_mbox/mesos-dev/201409.mbox/%3cCADKthhAVR5mrq1s9HXw1BB_XFALXWWxjutp7MV4y3wP-Bh=aWg@mail.gmail.com%3e) we enumerated a couple of options to solve this problem.

First off, having two ack-cycles: one between masters and slaves and one between masters and frameworks, would be ideal. We would be able to replay the statuses in order while keeping the master state current. However, this requires us to persist the master state in a replicated storage.

As a first pass, we can make sure that the tasks caught in a running state doesn't hog the cluster when completed and the framework being disconnected.

Here is a proof-of-concept to work out of: https://github.com/nqn/mesos/tree/niklas/status-update-disconnect/

A new (optional) field have been added to the internal status update message:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/messages/messages.proto#L68

Which makes it possible for the status update manager to set the field, if the latest status was terminal: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/slave/status_update_manager.cpp#L501

I added a test which should high-light the issue as well:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/tests/fault_tolerance_tests.cpp#L2478

I would love some input on the approach before moving on.
There are rough edges in the PoC which (of course) should be addressed before bringing it for up review.",1,4,MESOS-1817,2.0
HealthCheckTest.HealthStatusChange is flaky on jenkins.,"https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2374/consoleFull

{noformat}
[ RUN      ] HealthCheckTest.HealthStatusChange
Using temporary directory '/tmp/HealthCheckTest_HealthStatusChange_IYnlu2'
I0916 22:56:14.034612 21026 leveldb.cpp:176] Opened db in 2.155713ms
I0916 22:56:14.034965 21026 leveldb.cpp:183] Compacted db in 332489ns
I0916 22:56:14.034984 21026 leveldb.cpp:198] Created db iterator in 3710ns
I0916 22:56:14.034996 21026 leveldb.cpp:204] Seeked to beginning of db in 642ns
I0916 22:56:14.035006 21026 leveldb.cpp:273] Iterated through 0 keys in the db in 343ns
I0916 22:56:14.035023 21026 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0916 22:56:14.035200 21054 recover.cpp:425] Starting replica recovery
I0916 22:56:14.035403 21041 recover.cpp:451] Replica is in EMPTY status
I0916 22:56:14.035888 21045 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0916 22:56:14.035969 21052 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0916 22:56:14.036118 21042 recover.cpp:542] Updating replica status to STARTING
I0916 22:56:14.036603 21046 master.cpp:286] Master 20140916-225614-3125920579-47865-21026 (penates.apache.org) started on 67.195.81.186:47865
I0916 22:56:14.036634 21046 master.cpp:332] Master only allowing authenticated frameworks to register
I0916 22:56:14.036648 21046 master.cpp:337] Master only allowing authenticated slaves to register
I0916 22:56:14.036659 21046 credentials.hpp:36] Loading credentials for authentication from '/tmp/HealthCheckTest_HealthStatusChange_IYnlu2/credentials'
I0916 22:56:14.036686 21045 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 480322ns
I0916 22:56:14.036700 21045 replica.cpp:320] Persisted replica status to STARTING
I0916 22:56:14.036769 21046 master.cpp:366] Authorization enabled
I0916 22:56:14.036826 21045 recover.cpp:451] Replica is in STARTING status
I0916 22:56:14.036944 21052 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0916 22:56:14.036968 21049 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.186:47865
I0916 22:56:14.037284 21054 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0916 22:56:14.037312 21046 master.cpp:1212] The newly elected leader is master@67.195.81.186:47865 with id 20140916-225614-3125920579-47865-21026
I0916 22:56:14.037333 21046 master.cpp:1225] Elected as the leading master!
I0916 22:56:14.037345 21046 master.cpp:1043] Recovering from registrar
I0916 22:56:14.037504 21040 registrar.cpp:313] Recovering registrar
I0916 22:56:14.037505 21053 recover.cpp:188] Received a recover response from a replica in STARTING status
I0916 22:56:14.037681 21047 recover.cpp:542] Updating replica status to VOTING
I0916 22:56:14.038072 21052 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 330251ns
I0916 22:56:14.038087 21052 replica.cpp:320] Persisted replica status to VOTING
I0916 22:56:14.038127 21053 recover.cpp:556] Successfully joined the Paxos group
I0916 22:56:14.038202 21053 recover.cpp:440] Recover process terminated
I0916 22:56:14.038364 21048 log.cpp:656] Attempting to start the writer
I0916 22:56:14.038812 21053 replica.cpp:474] Replica received implicit promise request with proposal 1
I0916 22:56:14.038925 21053 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 92623ns
I0916 22:56:14.038944 21053 replica.cpp:342] Persisted promised to 1
I0916 22:56:14.039201 21052 coordinator.cpp:230] Coordinator attemping to fill missing position
I0916 22:56:14.039676 21047 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0916 22:56:14.039836 21047 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 144215ns
I0916 22:56:14.039850 21047 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040243 21047 replica.cpp:508] Replica received write request for position 0
I0916 22:56:14.040267 21047 leveldb.cpp:438] Reading position from leveldb took 10323ns
I0916 22:56:14.040362 21047 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 79471ns
I0916 22:56:14.040375 21047 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040556 21054 replica.cpp:655] Replica received learned notice for position 0
I0916 22:56:14.040658 21054 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 83975ns
I0916 22:56:14.040676 21054 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040689 21054 replica.cpp:661] Replica learned NOP action at position 0
I0916 22:56:14.041023 21043 log.cpp:672] Writer started with ending position 0
I0916 22:56:14.041342 21052 leveldb.cpp:438] Reading position from leveldb took 10642ns
I0916 22:56:14.042325 21050 registrar.cpp:346] Successfully fetched the registry (0B)
I0916 22:56:14.042346 21050 registrar.cpp:422] Attempting to update the 'registry'
I0916 22:56:14.043306 21054 log.cpp:680] Attempting to append 140 bytes to the log
I0916 22:56:14.043354 21050 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0916 22:56:14.043637 21047 replica.cpp:508] Replica received write request for position 1
I0916 22:56:14.044042 21047 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 386690ns
I0916 22:56:14.044057 21047 replica.cpp:676] Persisted action at 1
I0916 22:56:14.044271 21040 replica.cpp:655] Replica received learned notice for position 1
I0916 22:56:14.044435 21040 leveldb.cpp:343] Persisting action (161 bytes) to leveldb took 145186ns
I0916 22:56:14.044448 21040 replica.cpp:676] Persisted action at 1
I0916 22:56:14.044456 21040 replica.cpp:661] Replica learned APPEND action at position 1
I0916 22:56:14.044729 21055 registrar.cpp:479] Successfully updated 'registry'
I0916 22:56:14.044776 21047 log.cpp:699] Attempting to truncate the log to 1
I0916 22:56:14.044795 21055 registrar.cpp:372] Successfully recovered registrar
I0916 22:56:14.044831 21051 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0916 22:56:14.044899 21053 master.cpp:1070] Recovered 0 slaves from the Registry (102B) ; allowing 10mins for slaves to re-register
I0916 22:56:14.045133 21055 replica.cpp:508] Replica received write request for position 2
I0916 22:56:14.045450 21055 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 300867ns
I0916 22:56:14.045465 21055 replica.cpp:676] Persisted action at 2
I0916 22:56:14.045725 21052 replica.cpp:655] Replica received learned notice for position 2
I0916 22:56:14.045925 21052 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 182657ns
I0916 22:56:14.045948 21052 leveldb.cpp:401] Deleting ~1 keys from leveldb took 10733ns
I0916 22:56:14.045958 21052 replica.cpp:676] Persisted action at 2
I0916 22:56:14.045964 21052 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0916 22:56:14.055306 21026 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0916 22:56:14.057139 21048 slave.cpp:169] Slave started on 102)@67.195.81.186:47865
I0916 22:56:14.057178 21048 credentials.hpp:84] Loading credential for authentication from '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/credential'
I0916 22:56:14.057283 21048 slave.cpp:276] Slave using credential for: test-principal
I0916 22:56:14.057354 21048 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0916 22:56:14.057457 21048 slave.cpp:317] Slave hostname: penates.apache.org
I0916 22:56:14.057468 21048 slave.cpp:318] Slave checkpoint: false
I0916 22:56:14.057754 21043 state.cpp:33] Recovering state from '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/meta'
I0916 22:56:14.057864 21042 status_update_manager.cpp:193] Recovering status update manager
I0916 22:56:14.057958 21042 containerizer.cpp:252] Recovering containerizer
I0916 22:56:14.058226 21042 slave.cpp:3219] Finished recovery
I0916 22:56:14.058452 21047 slave.cpp:600] New master detected at master@67.195.81.186:47865
I0916 22:56:14.058485 21047 slave.cpp:674] Authenticating with master master@67.195.81.186:47865
I0916 22:56:14.058506 21042 status_update_manager.cpp:167] New master detected at master@67.195.81.186:47865
I0916 22:56:14.058539 21047 slave.cpp:647] Detecting new master
I0916 22:56:14.058555 21042 authenticatee.hpp:128] Creating new client SASL connection
I0916 22:56:14.058656 21043 master.cpp:3653] Authenticating slave(102)@67.195.81.186:47865
I0916 22:56:14.058737 21040 authenticator.hpp:156] Creating new server SASL connection
I0916 22:56:14.058830 21047 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0916 22:56:14.058852 21047 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0916 22:56:14.058884 21047 authenticator.hpp:262] Received SASL authentication start
I0916 22:56:14.058936 21047 authenticator.hpp:384] Authentication requires more steps
I0916 22:56:14.058981 21047 authenticatee.hpp:265] Received SASL authentication step
I0916 22:56:14.059052 21040 authenticator.hpp:290] Received SASL authentication step
I0916 22:56:14.059074 21040 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0916 22:56:14.059087 21040 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0916 22:56:14.059101 21040 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0916 22:56:14.059111 21040 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0916 22:56:14.059118 21040 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.059123 21040 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.059135 21040 authenticator.hpp:376] Authentication success
I0916 22:56:14.059182 21047 authenticatee.hpp:305] Authentication success
I0916 22:56:14.059192 21040 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(102)@67.195.81.186:47865
I0916 22:56:14.059309 21047 slave.cpp:731] Successfully authenticated with master master@67.195.81.186:47865
I0916 22:56:14.059348 21047 slave.cpp:994] Will retry registration in 12.6149ms if necessary
I0916 22:56:14.059396 21040 master.cpp:2843] Registering slave at slave(102)@67.195.81.186:47865 (penates.apache.org) with id 20140916-225614-3125920579-47865-21026-0
I0916 22:56:14.059495 21054 registrar.cpp:422] Attempting to update the 'registry'
I0916 22:56:14.059558 21026 sched.cpp:137] Version: 0.21.0
I0916 22:56:14.059710 21041 sched.cpp:233] New master detected at master@67.195.81.186:47865
I0916 22:56:14.059730 21041 sched.cpp:283] Authenticating with master master@67.195.81.186:47865
I0916 22:56:14.059788 21052 authenticatee.hpp:128] Creating new client SASL connection
I0916 22:56:14.059890 21043 master.cpp:3653] Authenticating scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.059960 21055 authenticator.hpp:156] Creating new server SASL connection
I0916 22:56:14.060039 21040 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0916 22:56:14.060061 21040 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0916 22:56:14.060107 21055 authenticator.hpp:262] Received SASL authentication start
I0916 22:56:14.060158 21055 authenticator.hpp:384] Authentication requires more steps
I0916 22:56:14.060189 21055 authenticatee.hpp:265] Received SASL authentication step
I0916 22:56:14.060220 21055 authenticator.hpp:290] Received SASL authentication step
I0916 22:56:14.060236 21055 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0916 22:56:14.060250 21055 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0916 22:56:14.060277 21055 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0916 22:56:14.060288 21055 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0916 22:56:14.060295 21055 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.060300 21055 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.060312 21055 authenticator.hpp:376] Authentication success
I0916 22:56:14.060349 21040 authenticatee.hpp:305] Authentication success
I0916 22:56:14.060364 21055 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060480 21046 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:47865
I0916 22:56:14.060499 21046 sched.cpp:476] Sending registration request to master@67.195.81.186:47865
I0916 22:56:14.060564 21050 master.cpp:1331] Received registration request from scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060593 21050 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0916 22:56:14.060767 21053 master.cpp:1390] Registering framework 20140916-225614-3125920579-47865-21026-0000 at scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060797 21049 log.cpp:680] Attempting to append 337 bytes to the log
I0916 22:56:14.060873 21042 hierarchical_allocator_process.hpp:329] Added framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.060873 21040 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0916 22:56:14.060899 21042 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0916 22:56:14.060909 21042 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11862ns
I0916 22:56:14.061061 21044 sched.cpp:407] Framework registered with 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.061115 21044 sched.cpp:421] Scheduler::registered took 34395ns
I0916 22:56:14.061173 21047 replica.cpp:508] Replica received write request for position 3
I0916 22:56:14.061298 21047 leveldb.cpp:343] Persisting action (356 bytes) to leveldb took 108843ns
I0916 22:56:14.061311 21047 replica.cpp:676] Persisted action at 3
I0916 22:56:14.061553 21049 replica.cpp:655] Replica received learned notice for position 3
I0916 22:56:14.061965 21049 leveldb.cpp:343] Persisting action (358 bytes) to leveldb took 392670ns
I0916 22:56:14.061985 21049 replica.cpp:676] Persisted action at 3
I0916 22:56:14.061996 21049 replica.cpp:661] Replica learned APPEND action at position 3
I0916 22:56:14.062268 21050 registrar.cpp:479] Successfully updated 'registry'
I0916 22:56:14.062331 21051 log.cpp:699] Attempting to truncate the log to 3
I0916 22:56:14.062355 21040 master.cpp:2883] Registered slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:14.062386 21043 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0916 22:56:14.062376 21040 master.cpp:4126] Adding slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0916 22:56:14.062510 21045 slave.cpp:765] Registered with master master@67.195.81.186:47865; given slave ID 20140916-225614-3125920579-47865-21026-0
I0916 22:56:14.062573 21045 slave.cpp:2346] Received ping from slave-observer(98)@67.195.81.186:47865
I0916 22:56:14.062599 21049 hierarchical_allocator_process.hpp:442] Added slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0916 22:56:14.062669 21049 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 to framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.062764 21041 replica.cpp:508] Replica received write request for position 4
I0916 22:56:14.062788 21049 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140916-225614-3125920579-47865-21026-0 in 145691ns
I0916 22:56:14.062839 21050 master.hpp:861] Adding offer 20140916-225614-3125920579-47865-21026-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.062891 21041 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 110169ns
I0916 22:56:14.062907 21041 replica.cpp:676] Persisted action at 4
I0916 22:56:14.062911 21050 master.cpp:3600] Sending 1 offers to framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.063065 21043 sched.cpp:544] Scheduler::resourceOffers took 39808ns
I0916 22:56:14.063163 21046 replica.cpp:655] Replica received learned notice for position 4
I0916 22:56:14.063272 21046 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 89981ns
I0916 22:56:14.063308 21046 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18542ns
I0916 22:56:14.063323 21046 replica.cpp:676] Persisted action at 4
I0916 22:56:14.063333 21046 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0916 22:56:14.063482 21044 master.hpp:871] Removing offer 20140916-225614-3125920579-47865-21026-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.063535 21044 master.cpp:2201] Processing reply for offers: [ 20140916-225614-3125920579-47865-21026-0 ] on slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org) for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.063561 21044 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
I0916 22:56:14.063824 21040 master.hpp:833] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.063860 21040 master.cpp:2350] Launching task 1 of framework 20140916-225614-3125920579-47865-21026-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:14.063943 21050 slave.cpp:1025] Got assigned task 1 for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.064158 21050 slave.cpp:1135] Launching task 1 for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.065439 21050 slave.cpp:1248] Queuing task '1' for executor 1 of framework '20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.065460 21041 containerizer.cpp:394] Starting container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' for executor '1' of framework '20140916-225614-3125920579-47865-21026-0000'
I0916 22:56:14.065477 21050 slave.cpp:554] Successfully attached file '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:14.066735 21055 launcher.cpp:137] Forked child with pid '21858' for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:14.067486 21044 containerizer.cpp:510] Fetching URIs for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I0916 22:56:15.037449 21050 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 43708ns
I0916 22:56:15.038743 21054 slave.cpp:2559] Monitoring executor '1' of framework '20140916-225614-3125920579-47865-21026-0000' in container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:15.078441 21053 slave.cpp:1758] Got registration for executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.078866 21053 slave.cpp:1876] Flushing queued task 1 for executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.084800 21043 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:15.084969 21041 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.084995 21041 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085160 21041 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to master@67.195.81.186:47865
I0916 22:56:15.085314 21043 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085332 21041 master.cpp:3212] Forwarding status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085335 21043 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:15.085435 21041 master.cpp:3178] Status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 from slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:15.085675 21044 sched.cpp:635] Scheduler::statusUpdate took 113998ns
I0916 22:56:15.085888 21052 master.cpp:2693] Forwarding status update acknowledgement a16d2819-e9f4-4119-bde6-f00ad33033e5 for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:15.086109 21051 status_update_manager.cpp:398] Received status update acknowledgement (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.086205 21051 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I../../src/tests/health_check_tests.cpp:330: Failure
Failed to wait 10secs for statusHealth1
0916 22:56:16.038705 21049 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 40061ns
I0916 22:56:16.126260 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190274 21045 master.cpp:741] Framework 20140916-225614-3125920579-47865-21026-0000 disconnected
I0916 22:56:28.190304 21045 master.cpp:1687] Deactivating framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:19.037235 21050 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0916 22:56:28.190394 21045 master.cpp:763] Giving framework 20140916-225614-3125920579-47865-21026-0000 0ns to failover
../../src/tests/health_check_tests.cpp:319: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called 4 times
           Actual: called once - unsatisfied and active
I0916 22:56:28.190624 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190757 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190773 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190831 21040 hierarchical_allocator_process.hpp:405] Deactivated framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190856 21054 master.cpp:3471] Framework failover timeout, removing framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190846 21046 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to master@67.195.81.186:47865
I0916 22:56:28.190887 21054 master.cpp:3976] Removing framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190887 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190994 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190996 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190999 21054 master.hpp:851] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:28.191090 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
W0916 22:56:28.191141 21054 master.cpp:4419] Removing task 1 of framework 20140916-225614-3125920579-47865-21026-0000 and slave 20140916-225614-3125920579-47865-21026-0 in non-terminal state TASK_RUNNING
I0916 22:56:28.191093 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.191181 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.191256 21054 master.cpp:650] Master terminating
I0916 22:56:28.191258 21043 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140916-225614-3125920579-47865-21026-0 from framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369088 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.191319 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369132 21043 hierarchical_allocator_process.hpp:360] Removed framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369225 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369283 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369323 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369415 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369420 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369536 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369642 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369685 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369753 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369802 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369884 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369889 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369943 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369978 21052 slave.cpp:1431] Asked to shut down framework 20140916-225614-3125920579-47865-21026-0000 by master@67.195.81.186:47865
I0916 22:56:28.369998 21052 slave.cpp:1456] Shutting down framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370009 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370018 21052 slave.cpp:2899] Shutting down executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370183 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370206 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370426 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370447 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370635 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370657 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370815 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370837 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370972 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.371000 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.371155 21052 slave.cpp:2378] master@67.195.81.186:47865 exited
W0916 22:56:28.371177 21052 slave.cpp:2381] Master disconnected! Waiting for a new master to be elected
I0916 22:56:28.371202 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540035 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.371701 21053 containerizer.cpp:882] Destroying container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:28.540177 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540196 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540324 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540350 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540403 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540421 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540530 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540556 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540664 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540681 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540889 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540918 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.541082 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.541111 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:29.047708 21053 containerizer.cpp:997] Executor for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' has exited
I0916 22:56:29.048037 21050 slave.cpp:2617] Executor '1' of framework 20140916-225614-3125920579-47865-21026-0000 terminated with signal Killed
I0916 22:56:29.048197 21050 slave.cpp:2753] Cleaning up executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048373 21050 slave.cpp:2828] Cleaning up framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048444 21043 status_update_manager.cpp:282] Closing status update streams for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048457 21050 slave.cpp:477] Slave terminating
I0916 22:56:29.048476 21043 status_update_manager.cpp:530] Cleaning up status update stream for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048462 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd' for gc 6.99999944121481days in the future
I0916 22:56:29.048568 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1' for gc 6.99999944031111days in the future
I0916 22:56:29.048607 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000' for gc 6.99999943939852days in the future
[  FAILED  ] HealthCheckTest.HealthStatusChange (15019 ms)
{noformat}",1,15,MESOS-1802,5.0
Reconciliation can send out-of-order updates.,"When a slave re-registers with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.

However, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.

As a result, out-of-order updates are possible, e.g.

(1) Slave has task T in TASK_FINISHED, with unacknowledged updates: [TASK_RUNNING, TASK_FINISHED].
(2) Master fails over.
(3) New master re-registers the slave with T in TASK_FINISHED.
(4) Reconciliation request arrives, master sends TASK_FINISHED.
(5) Slave sends TASK_RUNNING to master, master sends TASK_RUNNING.

I think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. This means when the slave re-registers, it should instead send the latest acknowledged state of each task.",1,3,MESOS-1799,3.0
Slave should send exited executor message when the executor is never launched.,"When the slave sends TASK_LOST before launching an executor for a task, the slave does not send an exited executor message to the master.

Since the master receives no exited executor message, it still thinks the executor's resources are consumed on the slave.

One possible fix for this would be to send the exited executor message to the master in these cases.",1,3,MESOS-1720,8.0
The slave does not send pending tasks during re-registration.,"In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.

For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",1,3,MESOS-1715,3.0
Improve reconciliation between master and slave.,"As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:

{code}
Master           Slave
 {}               {}
{Tn}              {}  // Master receives Task T, non-terminal. Forwards to slave.
{Tn}             {Tn} // Slave receives Task T, non-terminal.
{Tn}             {Tt} // Task becomes terminal on slave. Update forwarded.
{Tt}             {Tt} // Master receives update, forwards to framework.
 {}              {Tt} // Master receives ack, forwards to slave.
 {}               {}  // Slave receives ack.
{code}

In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.

Note the following properties:

*(1)* The master may have a non-terminal task, not present in the slave's re-registration message.
*(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state.
*(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.

In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!

After chatting with [~vinodkone], we're considering updating the reconciliation to occur as follows:


→ Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.

→ If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.

→ The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration.",1,2,MESOS-1696,3.0
Handle a temporary one-way master --> slave socket closure.,"In MESOS-1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:

→ Master and Slave connected operating normally.
→ Temporary one-way network failure, master→slave link breaks.
→ Master marks slave as disconnected.
→ Network restored and health checking continues normally, slave is not removed as a result. Slave does not attempt to re-register since it is receiving pings once again.
→ Slave remains disconnected according to the master, and the slave does not try to re-register. Bad!

We were originally thinking of using a failover timeout in the master to remove these slaves that don't re-register. However, it can be dangerous when ZooKeeper issues are preventing the slave from re-registering with the master; we do not want to remove a ton of slaves in this situation.

Rather, when the slave is health checking correctly but does not re-register within a timeout, we could send a registration request from the master to the slave, telling the slave that it must re-register. This message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master.",1,4,MESOS-1668,2.0
Network isolator should tolerate slave crashes while doing isolate/cleanup.,"A slave may crash while we are installing/removing filters. The slave recovery for the network isolator should tolerate those partially installed filters. Also, we want to avoid leaking a filter on host eth0 and host lo.

The current code cannot tolerate that, thus may cause the following error:

{noformat}
Failed to perform recovery: Collect failed: Failed to recover container d409a100-2afb-497c-864f-fe3002cf65d9 with pid 50405: No ephemeral ports found
To remedy this do as follows:
Step 1: rm -f /var/lib/mesos/meta/slaves/latest
       This ensures slave doesn't recover old live executors.
Step 2: Restart the slave.
{noformat}",1,1,MESOS-1649,3.0
SlaveRecoveryTest/0.ReconcileKillTask is flaky,"Observed this on Jenkins.

{code}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG'
I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms
I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms
I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns
I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns
I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns
I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery
I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status
I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING
I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850
I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register
I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register
I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials'
I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled
I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850
I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216
I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master!
I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar
I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar
I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms
I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING
I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status
I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status
I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING
I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms
I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING
I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group
I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated
I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer
I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms
I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1
I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position
I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms
I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0
I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns
I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms
I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0
I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms
I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0
I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0
I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0
I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns
I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B)
I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log
I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1
I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms
I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1
I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms
I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1
I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1
I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2
I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar
I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms
I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2
I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2
I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms
I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns
I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2
I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850
I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery
I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master
I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850
I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success
I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850
I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success
I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary
I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0
I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850
I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log
I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3
I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success
I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success
I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850
I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress
I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary
I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns
I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns
I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary
I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms
I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3
I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms
I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3
I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3
I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info'
I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns
I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850
I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns
I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4
I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins'
I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info'
I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:44.209710 27237 slave.cpp:1111] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.210978 27237 slave.cpp:3720] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info'
I0714 15:08:44.211520 27237 slave.cpp:3835] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info'
I0714 15:08:44.211714 27237 slave.cpp:1221] Queuing task '4a6783aa-8d07-46e3-8399-2a5d047f0021' for executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework '20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.211937 27236 containerizer.cpp:427] Starting container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:44.212242 27236 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.216187 27236 launcher.cpp:137] Forked child with pid '28451' for container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.217281 27236 containerizer.cpp:705] Checkpointing executor's forked pid 28451 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid'
I0714 15:08:44.219408 27236 containerizer.cpp:537] Fetching URIs for container '19c466f8-bb5a-4842-a152-f585ff88762a' using command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher'
I0714 15:08:44.223963 27241 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.554461ms
I0714 15:08:44.224501 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.225051 27241 replica.cpp:655] Replica received learned notice for position 4
I0714 15:08:44.242923 27241 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 17.806547ms
I0714 15:08:44.243057 27241 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57154ns
I0714 15:08:44.243078 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.243096 27241 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0714 15:08:44.401140 27241 slave.cpp:2468] Monitoring executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' in container '19c466f8-bb5a-4842-a152-f585ff88762a'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0714 15:08:44.434221 28486 process.cpp:1671] libprocess is initialized on 127.0.1.1:34669 for 8 cpus
I0714 15:08:44.436146 28486 exec.cpp:131] Version: 0.20.0
I0714 15:08:44.438555 28500 exec.cpp:181] Executor started at: executor(1)@127.0.1.1:34669 with pid 28486
I0714 15:08:44.440846 27241 slave.cpp:1732] Got registration for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.440917 27241 slave.cpp:1817] Checkpointing executor pid 'executor(1)@127.0.1.1:34669' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid'
I0714 15:08:44.442373 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.442790 27241 slave.cpp:1851] Flushing queued task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.443192 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.443994 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444144 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444741 28500 exec.cpp:205] Executor registered on slave 20140714-150843-16842879-55850-27216-0
Registered executor on quantal
I0714 15:08:44.446338 28500 exec.cpp:217] Executor::registered took 534236ns
I0714 15:08:44.446715 28500 exec.cpp:292] Executor asked to run task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
Starting task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.447548 28500 exec.cpp:301] Executor::launchTask took 584306ns
sh -c 'sleep 1000'
Forked command at 28509
I0714 15:08:44.451202 28506 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452327 27239 slave.cpp:2086] Handling status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:44.452503 27239 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452520 27239 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452775 27239 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.472384 27239 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:44.472764 27237 master.cpp:3115] Status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.472854 27237 sched.cpp:637] Scheduler::statusUpdate took 17656ns
I0714 15:08:44.472920 27237 master.cpp:2639] Forwarding status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.473122 27239 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473146 27239 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473244 27237 slave.cpp:2244] Status update manager successfully handled status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473258 27237 slave.cpp:2250] Sending acknowledgement for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:44.473567 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.474095 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.474676 28502 exec.cpp:338] Executor received status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491111 27239 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491761 27216 slave.cpp:484] Slave terminating
I0714 15:08:44.492559 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.494635 27237 master.cpp:766] Slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) disconnected
I0714 15:08:44.494663 27237 master.cpp:1608] Disconnecting slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.495120 27237 slave.cpp:168] Slave started on 44)@127.0.1.1:55850
I0714 15:08:44.495133 27237 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.495226 27237 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.495322 27237 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.495407 27237 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.495419 27237 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.495939 27242 master.cpp:2469] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.496207 27238 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.498291 27240 slave.cpp:3194] Recovering framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498325 27240 slave.cpp:3570] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498940 27240 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.498956 27240 status_update_manager.cpp:201] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498975 27240 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.499092 27240 status_update_manager.hpp:306] Replaying status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.499241 27240 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.499433 27240 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.499457 27240 containerizer.cpp:329] Recovering container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483] Slave 20140714-150843-16842879-55850-27216-0 disconnected
I0714 15:08:44.501255 27240 slave.cpp:3067] Sending reconnect request to executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 at executor(1)@127.0.1.1:34669
I0714 15:08:44.502030 28501 exec.cpp:251] Received reconnect request from slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.502627 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.502681 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.503211 27240 slave.cpp:1911] Re-registering executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.504238 28501 exec.cpp:228] Executor re-registered on slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.505033 28501 exec.cpp:240] Executor::reregistered took 45053ns
Re-registered executor on quantal
I0714 15:08:44.505507 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.505558 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 124255ns
I0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 61521ns
I0714 15:08:46.503978 27238 slave.cpp:2035] Cleaning up un-reregistered executors
I0714 15:08:46.504050 27238 slave.cpp:3126] Finished recovery
I0714 15:08:46.504590 27238 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504639 27238 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:46.504729 27238 slave.cpp:648] Detecting new master
I0714 15:08:46.504772 27238 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504863 27238 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:46.505091 27238 master.cpp:3507] Authenticating slave(44)@127.0.1.1:55850
I0714 15:08:46.505239 27238 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:46.505363 27238 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:46.505393 27238 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:46.505420 27238 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:46.505476 27238 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:46.505506 27238 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:46.505542 27238 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:46.505558 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:46.505566 27238 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:46.505584 27238 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:46.505595 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:46.505601 27238 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505606 27238 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505616 27238 authenticator.hpp:376] Authentication success
I0714 15:08:46.505646 27238 authenticatee.hpp:305] Authentication success
I0714 15:08:46.505671 27238 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(44)@127.0.1.1:55850
I0714 15:08:46.505769 27238 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:46.505873 27238 slave.cpp:970] Will retry registration in 17.903094ms if necessary
W0714 15:08:46.505991 27238 master.cpp:2904] Slave at slave(44)@127.0.1.1:55850 (quantal) is being allowed to re-register with an already in use id (20140714-150843-16842879-55850-27216-0)
W0714 15:08:46.506063 27238 master.cpp:3679]  Slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) has non-terminal task 4a6783aa-8d07-46e3-8399-2a5d047f0021 that is supposed to be killed. Killing it now!
I0714 15:08:46.506150 27238 slave.cpp:816] Re-registered with master master@127.0.1.1:55850
I0714 15:08:46.506186 27238 slave.cpp:1277] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.507275 27241 hierarchical_allocator_process.hpp:497] Slave 20140714-150843-16842879-55850-27216-0 reconnected
I0714 15:08:46.508061 28504 exec.cpp:312] Executor asked to kill task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
I0714 15:08:46.508117 28504 exec.cpp:321] Executor::killTask took 24954ns
Shutting down
Sending SIGTERM to process tree at pid 28509
I0714 15:08:46.512238 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.512508 27238 slave.cpp:1582] Updating framework 20140714-150843-16842879-55850-27216-0000 pid to scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:46.512846 27238 slave.cpp:1590] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:46.513419 28508 process.cpp:1037] Socket closed while receiving
Killing the following process trees:
[ 
-+- 28509 sh -c sleep 1000 
 \--- 28510 sleep 1000 
]
Command terminated with signal Terminated (pid: 28509)
I0714 15:08:46.940232 28506 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.940918 27240 slave.cpp:2086] Handling status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:46.940979 27240 slave.cpp:3768] Terminating task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:46.941603 27240 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.941644 27240 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.949417 27236 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 63926ns
I0714 15:08:46.965200 27240 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:46.965625 27239 master.cpp:3115] Status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.965724 27239 master.hpp:791] Removing task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:46.965903 27239 sched.cpp:637] Scheduler::statusUpdate took 39326ns
I0714 15:08:46.966022 27239 hierarchical_allocator_process.hpp:635] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140714-150843-16842879-55850-27216-0 from framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966120 27239 master.cpp:2639] Forwarding status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.966501 27241 slave.cpp:2244] Status update manager successfully handled status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966519 27241 slave.cpp:2250] Sending acknowledgement for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:46.966754 27240 status_update_manager.cpp:398] Received status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966785 27240 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967386 28500 exec.cpp:338] Executor received status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967562 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.968147 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:46.984608 27240 status_update_manager.cpp:530] Cleaning up status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985239 27236 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985280 27236 slave.cpp:3810] Completing task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:47.940703 27243 process.cpp:1037] Socket closed while receiving
I0714 15:08:47.940984 27238 containerizer.cpp:1019] Executor for container '19c466f8-bb5a-4842-a152-f585ff88762a' has exited
I0714 15:08:47.941007 27238 containerizer.cpp:903] Destroying container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:47.950192 27241 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950405 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 320604ns
I0714 15:08:47.950518 27241 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.950572 27241 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950774 27241 sched.cpp:546] Scheduler::resourceOffers took 37944ns
I0714 15:08:47.951179 27216 master.cpp:625] Master terminating
I0714 15:08:47.951263 27216 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.953447 27242 sched.cpp:747] Stopping framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:47.953547 27242 slave.cpp:2330] master@127.0.1.1:55850 exited
W0714 15:08:47.953567 27242 slave.cpp:2333] Master disconnected! Waiting for a new master to be elected
I0714 15:08:47.964512 27238 slave.cpp:2526] Executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 exited with status 0
I0714 15:08:47.968690 27238 slave.cpp:2660] Cleaning up executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.969348 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998878298667days in the future
I0714 15:08:47.969751 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998877682963days in the future
I0714 15:08:47.970082 27239 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998877336889days in the future
I0714 15:08:47.970379 27242 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998876968889days in the future
I0714 15:08:47.970587 27238 slave.cpp:2735] Cleaning up framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.970960 27237 status_update_manager.cpp:282] Closing status update streams for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.971225 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875966519days in the future
I0714 15:08:47.971549 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875612148days in the future
W0714 15:08:47.975971 27235 containerizer.cpp:893] Ignoring destroy of unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
./tests/cluster.hpp:530: Failure
(wait).failure(): Unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000000005a0299, pid=27216, tid=47907931709760
#
# JRE version: OpenJDK Runtime Environment (7.0_55-b14) (build 1.7.0_55-b14)
# Java VM: OpenJDK 64-Bit Server VM (24.51-b03 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [lt-mesos-tests+0x1a0299]  mlock@@GLIBC_2.2.5+0x1a0299
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/hs_err_pid27216.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
make[3]: *** [check-local] Aborted
make[3]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make: *** [check-recursive] Error 1
Build step 'Execute shell' marked build as failure
erreicht: 1854109
Sending e-mails to: kernel-test@twitter.com apache-mesos@twitter.com
Finished: FAILURE
 Help us localize this page Page generated: Jul 14, 2014 5:57:17 PMREST 
{code}",1,2,MESOS-1594,1.0
Improve framework rate limiting by imposing the max number of outstanding messages per framework principal,"* Rate limits config takes a configurable *capacity* for each principal.
* To ensure that Master maintain the message order of a framework it's important that Master sends an FrameworkErrorMessage back to the scheduler to ask it to abort.",1,4,MESOS-1578,5.0
Race between executor exited event and launch task can cause overcommit of resources,"The following sequence of events can cause an overcommit

--> Launch task is called for a task whose executor is already running

--> Executor's resources are not accounted for on the master

--> Executor exits and the event is enqueued behind launch tasks on the master

--> Master sends the task to the slave which needs to commit for resources for task and the (new) executor.

--> Master processes the executor exited event and re-offers the executor's resources causing an overcommit of resources.",1,4,MESOS-1466,8.0
SlaveRecoveryTest/0.MultipleFrameworks is flaky,"--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure

{noformat}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0
I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task 51991f97-f5fd-4905-ad0f-02668083af7c
Forked command at 4367
sh -c 'sleep 1000'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0
I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83
sh -c 'sleep 1000'
Forked command at 4439
I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0
Re-registered executor on artoo
I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 4367
Killing the following process trees:
[ 
-+- 4367 sh -c sleep 1000 
 \--- 4368 sleep 1000 
]
../../src/tests/slave_recovery_tests.cpp:2807: Failure
Value of: status1.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED

Program received signal SIGSEGV, Segmentation fault.
testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
3795          *static_cast<volatile int*>(NULL) = 1;
(gdb) bt
#0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
#1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
#3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161
#6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338
#7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445
#8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237
#9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872
#12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107
(gdb) frame 2
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
(gdb) p status1
$1 = {data = {<std::__shared_ptr<process::Future<mesos::TaskStatus>::Data, 2>> = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <No data fields>}}
(gdb) p status1.get()
$2 = (const mesos::TaskStatus &) @0x7fffdc5bf5f0: {<google::protobuf::Message> = {<google::protobuf::MessageLite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for mesos::TaskStatus+16>}, <No data fields>}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4, 
  static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kEmptyString>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252, 
  state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0}
(gdb) p status1.get().state()
$3 = mesos::TASK_FAILED
(gdb) list
2802      // Kill task 1.
2803      driver1.killTask(task1.task_id());
2804
2805      // Wait for TASK_KILLED update.
2806      AWAIT_READY(status1);
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
2808
2809      // Kill task 2.
2810      driver2.killTask(task2.task_id());
2811
{noformat}",1,6,MESOS-1365,1.0
systemd.slice + cgroup enablement fails in multiple ways. ,"When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator: 

I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem
Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems

------ details ------
/sys/fs/cgroup
total 0
drwxr-xr-x. 12 root root 280 Mar 18 08:47 .
drwxr-xr-x.  6 root root   0 Mar 18 08:47 ..
drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset
drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices
drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer
drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb
drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory
drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls
drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event
drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd
",1,19,MESOS-1195,3.0
Allocator should make an allocation decision per slave instead of per framework/role.,"Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.

This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",1,4,MESOS-1119,2.0
Update semantics of when framework registered()/reregistered() get called,"Current semantics:

1) Framework connects w/ master very first time --> registered()
2) Framework reconnects w/ same master after a zk blip --> reregistered()
3) Framework reconnects w/ failed over master --> registered()
4) Failed over framework connects w/ same master --> registered()
5) Failed over framework connects w/ failed over master --> registered() 

Updated semantics:

Everything same except 
3) Framework reconnects w/ failed over master --> reregistered()",1,6,MESOS-786,3.0
[Front-end]: Minor issues connected with localization,"*1.*

*Preconditions:*
 1. Billing is available on DataLab Web UI

*Steps to reproduce:*
 1. Go to 'List of Resources' page

*Actual result:*
 1. Billing on the 'List of Resources' page is not localized

*Expected result:*
 1. Billing on the 'List of Resources' page is localized
----
*2.* Convert date period according to selected language for 'Billing report' page
----
*3. (-)* Convey language key to back-end for billing export (will do it in branch with back-end fix) - it will be done in task 2089.",1,2,DATALAB-2087,4.0
[Environment management page]: Drop down list values is broken,"*Preconditions:*

1. User is located on 'Environment management' page

*Steps to reproduce:*
 # Expand the filter header
 # Click 'Project'/'Endpoint' drop down list

*Actual result:*
 # Values of dropdown list is broken
 # A part of dropdown list name is visible in the right side

*Expected:*
 # Values of dropdown list is not broken
 # A part of dropdown list name is not visible in the right side
 ",1,1,DATALAB-2073,8.0
Libraries are not installed due to permission absence to the directory,"*Preconditions:*
 # DLab is deployed on K8S
 # Notebook is created

*Steps to reproduce:*
 # Go to 'resources_list' page
 # Click action menu for notebook
 # Choose 'manage libraries'
 # Choose available resource in 'Select resource' drop down list
 # Select apt/Yum/Python/Others groups
 # Choose available libraries
 # Click installed

*Actual result:*
 # Docker runs with '0'
 # Libraries installation failed

*Expected result:*
 # Docker runs with '0'
 # Libraries installation is successful

For java and r package groups I could not check because there were issues not from our side.",1,0,DATALAB-1250,4.0
Provisioning service have no access to response files,"*Preconditions:*
1. Notebook is created 

*Steps to reproduce:*
1. Go to 'resources_list' page
2. Click action menu for notebook
3. Choose 'Manage libraries'
4. Choose available resource in 'Select resource' drop down list

*Actual result:*
1. Available lib list getting is stuck on UI
2. User is not able to install library

*Expected result:*
1. User is able to install library


Could not find file because it finds from dlab-user, but this json is available for root.
",1,0,DATALAB-1240,8.0
[Scheduler]: 'Default timezone_offset' is rewritten if switching scheduler types for notebook,"*Preconditions:*

1. Notebook is created (in 'running' or 'stopped' statuses)

*Steps to reproduce:*
 # Turn on scheduler by inactivity for notebook
 # Click 'Save' button
 # Turn on scheduler by time

*Actual result:*

1. Value 'Select offset' is absent ('Z' in F12)

*Expected result:*

1. Value 'Select offset' is default value of your browser",1,3,DATALAB-1101,8.0
[Terraform]: It is impossible to do any git actions via ungit,"*Preconditions:*
 # User is located on ungit page
 # Git account is created in DLab

*Steps to reproduce:*
 # Put in </home/dlab-user> in ungit search
 # Paste https path to git repository in ungit
 # Click 'clone' button

*Actual result:*
 # For every git actions username and password are required in ungit
 # required credentials are so frequent, that it is impossible to do git actions 

*Expected result:*
 # Git action is done without credential requiring in ungit

Previously git credentials were stored in notebook in <.netrc>, but now this file is absent now",1,0,DATALAB-1041,8.0
PreemptorService failure does not trigger shutdown,"While observing AURORA-1510 in production I noticed the bug caused the {{PreemptorService}} to transition to the FAILED state. The {{/services}} endpoint had:
{noformat}
{
name: ""PreemptorService"",
state: ""FAILED"",
failureCause: ""java.util.ConcurrentModificationException""
},
{noformat}

However the scheduler continued to run. I believe this is a bug.",1,4,AURORA-1511,2.0
In-progress instances on Update page continue to pulse after update is aborted,"When an update is aborted, any instances that were in progress still have the css class of {{instance-updating}} and as such, continue to pulse.",1,1,AURORA-1445,3.0
Remove duplicate thermos_observer,"There are currently two thermos_observer python_binaries in the codebase, one in src/main/python/apache/aurora/tools/BUILD and the other in ./src/main/python/apache/thermos/observer/bin/BUILD. This is confusing.

Let's get rid of one of them, I think the latter is the one that's not meant to be used.",0,8,AURORA-1381,2.0
Referential integrity violation when replaying storage,"At startup in the Vagrant environment I observed:

{noformat}
E0630 20:45:54.427 THREAD1 org.apache.aurora.scheduler.SchedulerLifecycle$9.execute: Caught unchecked exception: org.apache.aurora.scheduler.storage.Storage$StorageExce
ption: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) R
EFERENCES PUBLIC.HOST_ATTRIBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
### The error may involve defaultParameterMap
### The error occurred while setting parameters
### SQL: DELETE FROM host_attributes     WHERE host = ?
### Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) REFERENCES PUBLIC.HOST_ATTR
IBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
org.apache.aurora.scheduler.storage.Storage$StorageException: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) R
EFERENCES PUBLIC.HOST_ATTRIBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
### The error may involve defaultParameterMap
### The error occurred while setting parameters
### SQL: DELETE FROM host_attributes     WHERE host = ?
### Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) REFERENCES PUBLIC.HOST_ATTR
IBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
        at org.apache.aurora.scheduler.storage.db.DbStorage.write(DbStorage.java:144)
        at org.mybatis.guice.transactional.TransactionalMethodInterceptor.invoke(TransactionalMethodInterceptor.java:101)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.write(LogStorage.java:632)
        at org.apache.aurora.scheduler.storage.log.LogStorage$3.execute(LogStorage.java:316)
        at org.apache.aurora.scheduler.storage.log.LogStorage$3.execute(LogStorage.java:313)
        at org.apache.aurora.scheduler.storage.log.LogStorage.replay(LogStorage.java:525)
        at org.apache.aurora.scheduler.storage.log.LogStorage.access$1200(LogStorage.java:116)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21$1.execute(LogStorage.java:503)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21$1.execute(LogStorage.java:500)
        at org.apache.aurora.scheduler.storage.log.StreamManagerImpl.readFromBeginning(StreamManagerImpl.java:127)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21.execute(LogStorage.java:500)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult.apply(Storage.java:131)
        at org.apache.aurora.scheduler.storage.db.DbStorage.bulkLoad(DbStorage.java:165)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.recover(LogStorage.java:496)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage$20.execute(LogStorage.java:477)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult.apply(Storage.java:131)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult$Quiet.apply(Storage.java:148)
        at org.apache.aurora.scheduler.storage.db.DbStorage.write(DbStorage.java:142)
        at org.mybatis.guice.transactional.TransactionalMethodInterceptor.invoke(TransactionalMethodInterceptor.java:101)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.write(LogStorage.java:632)
        at org.apache.aurora.scheduler.storage.log.LogStorage.start(LogStorage.java:471)
        at org.apache.aurora.scheduler.storage.CallOrderEnforcingStorage.start(CallOrderEnforcingStorage.java:92)
        at org.apache.aurora.scheduler.SchedulerLifecycle$6.execute(SchedulerLifecycle.java:252)
{noformat}",1,1,AURORA-1379,2.0
Update status page for large jobs is killed by Chrome,"When I say ""killed by Chrome"" I mean if left open long enough the page turns into the Chrome ""Aw Snap"" page.

This is presumably due to the continued addition of instance events to the page causing it to OOM and Chrome to kill it. One suggestion made to me by those more familiar with Angular was to add {{track by}} to the {{ng-repeat}} used to build the DOM for the instance events.",1,1,AURORA-1345,5.0
auth_module is not installed in child injector,"The recent patch to enable HTTP Basic Authentication moved construction of ThriftAuthModule to the Jetty child injector, but the user-supplied module was still installed in the parent injector. This leads to a stack trace for anyone using their own {{auth_module}}:

{noformat}
INFO: Connecting to master using authentication (principal: TwitterScheduler).
Exception in thread ""main"" com.google.inject.CreationException: Guice creation errors:

1) No implementation for java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String> was bound.
  while locating java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String>
    for parameter 1 at com.twitter.aurora.internal.auth.TwitterCapabilityValidator.<init>(TwitterCapabilityValidator.java:28)
  at com.twitter.aurora.internal.auth.TwitterAuthModule.configure(TwitterAuthModule.java:97)

2) No implementation for java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String> was bound.
  at com.twitter.aurora.internal.auth.TwitterAuthModule.configure(TwitterAuthModule.java:94)

2 errors
        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:435)
        at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:154)
        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106)
        at com.google.inject.Guice.createInjector(Guice.java:95)
        at com.google.inject.Guice.createInjector(Guice.java:83)
        at com.twitter.common.application.AppLauncher.configureInjection(AppLauncher.java:120)
        at com.twitter.common.application.AppLauncher.run(AppLauncher.java:87)
        at com.twitter.common.application.AppLauncher.launch(AppLauncher.java:181)
        at com.twitter.common.application.AppLauncher.launch(AppLauncher.java:142)
        at org.apache.aurora.scheduler.app.SchedulerMain.main(SchedulerMain.java:279)
{noformat}",1,1,AURORA-1201,2.0
The scheduler synchronously writes a backup while writing a snapshot to the replicated log,"In the course of writing a snapshot to the replicated log, the scheduler may block while writing a snapshot to the disk.  There is no need for this activity to be done synchronously, and doing so causes the write lock to be unnecessarily held for an additional period of time.

From StorageBackup.java:
{code}
    @Override
    public Snapshot createSnapshot() {
      Snapshot snapshot = delegate.createSnapshot();
      if (clock.nowMillis() >= (lastBackupMs + backupIntervalMs)) {
        save(snapshot);
      }
      return snapshot;
    }
{code}

{{StorageBackup}} happens to be the unqualified binding to {{SnapshotStore<Snapshot>}} that is used in {{LogStorage}}.",1,2,AURORA-1108,3.0
"Remove the ""enable_legacy_constraints"" flag.","As a part of AURORA-184 we added a flag called ""enable_legacy_constraints"" which enables behaviour that injects a host and rack limit into every job.

We should deprecate and remove this flag in some future release.",1,2,AURORA-1074,2.0
